creating models for obj12...
done

loading annotations into memory...
Done (t=5.73s)
creating index...
index created!
loading annotations into memory...
Done (t=16.50s)
creating index...
index created!
len(val_dataset)):  40137
len(train_dataset)):  82081
FORWARD MAX-ING
tensor([0.3431, 0.6094, 0.3249, 0.7614, 0.5848, 0.5332, 0.7668, 0.7281, 0.0290,
        0.7713, 0.3124, 0.9457, 0.8132, 0.7442, 0.1783, 0.0607, 0.9596, 0.8791,
        0.9785, 0.0833, 0.7050, 0.3128, 0.2590, 0.7470, 0.3550, 0.7317, 0.3077,
        0.8358, 0.4874, 0.3471, 0.4226, 0.4663, 0.1316, 0.7303, 0.1155, 0.4864,
        0.2390, 0.5420, 0.5508, 0.5085, 0.9609, 0.6458, 0.5176, 0.2935, 0.7735,
        0.7454, 0.5686, 0.7332, 0.5257, 0.4073, 0.7465, 0.9724, 0.8096, 0.5624,
        0.4209, 0.5420, 0.6318, 0.6418, 0.7027, 0.3896, 0.5737, 0.4306, 0.6403,
        0.4401, 0.9854, 0.2919, 0.6103, 0.3267, 0.3626, 0.8391, 0.4455, 0.1889,
        0.8032, 0.7343, 0.4788, 0.5523, 0.9184, 0.3305, 0.8959, 0.6047],
       device='cuda:0')
Max Train Loss:  tensor([31.4933, 51.5768, 29.4213, 69.8334, 52.6468, 50.2885, 65.6151, 67.6431,
         3.6750, 67.1160, 35.4377, 68.4249, 67.9129, 81.9214, 14.4024,  6.1056,
        94.4950, 73.0508, 90.0828,  7.5059, 68.0204, 29.4385, 18.7222, 60.4027,
        37.6431, 55.7877, 20.2535, 78.3501, 49.4616, 29.1268, 34.4213, 46.4597,
        11.0244, 65.7285,  9.6114, 60.7010, 22.7410, 53.1233, 50.0069, 57.8143,
        99.6998, 53.7649, 62.3252, 29.5560, 70.7653, 54.6277, 54.2090, 48.1753,
        44.2937, 37.7779, 82.3788, 89.7056, 66.7746, 51.4850, 30.5072, 65.2439,
        56.0632, 62.2262, 57.7456, 40.5269, 54.4751, 44.2697, 49.3180, 42.5507,
        82.3238, 33.3590, 45.2720, 33.2981, 38.2488, 85.4471, 43.0193, 15.4579,
        66.7610, 62.3982, 47.4952, 43.7070, 97.0832, 30.7636, 77.9449, 50.3186],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [0/80], Step [000/642], LR 1.0e-04, Loss: 99.7
Max Train Loss:  tensor([31.1976, 17.0411, 30.3509, 17.4835, 16.7083, 20.7820, 18.0371, 17.9192,
         2.8547, 20.5940, 30.3928, 15.1851, 17.2510, 23.8629, 15.4727,  5.5357,
        17.0820, 20.4104, 22.4117,  7.9844, 33.1163, 28.2010, 20.3193, 22.7031,
        25.3494, 24.8178, 21.9882, 28.8141, 21.0801, 29.2982, 16.2499, 19.1613,
        11.1251, 19.3515, 10.0432, 27.1637, 22.2987, 22.1119, 19.5176, 29.8866,
        22.5888, 27.5728, 21.3198, 28.5213, 27.5733, 35.7254, 19.0136, 17.6333,
        18.8262, 17.2045, 28.1271, 24.9710, 17.4736, 20.5826, 28.9526, 27.4676,
        22.3284, 28.7068, 28.6444, 18.8698, 26.2058, 17.1567, 21.9153, 15.1180,
        27.3322, 26.4881, 15.5342, 29.3695, 18.1691, 22.3713, 15.1449, 16.2967,
        18.4369, 27.0940, 17.8943, 21.0425, 28.3666, 22.9398, 24.7173, 22.1004],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [0/80], Step [100/642], LR 1.0e-04, Loss: 35.7
Max Train Loss:  tensor([28.2069, 17.5647, 14.3313, 18.2840, 12.9986, 20.3461, 18.9234, 23.0673,
         2.6266, 26.3301,  8.5172,  9.9994, 14.6717, 22.8895, 16.2735,  5.4153,
        21.2961, 14.1529, 16.7043,  7.7705, 14.9836, 26.0137, 22.2420, 10.4887,
        12.3034, 15.2886, 24.4555, 15.7161, 20.6182,  6.6292, 11.9550, 15.6743,
        11.5704, 20.1871, 10.3655, 24.5544, 21.3251, 20.2694, 20.9282, 17.8876,
        16.1470, 21.6093, 19.5712, 27.8041, 29.1846, 29.1806, 16.9454, 14.7800,
        16.3853,  9.4818, 10.3832, 14.6593, 15.3794, 15.8849,  5.9677, 25.8128,
        28.7404, 15.4615, 11.9180, 11.8465, 26.3503, 20.1123, 17.3005, 12.8008,
        12.6674, 25.8046, 11.9062, 10.0206, 10.8890, 16.3161, 12.5125, 16.4835,
        18.1262, 18.8029, 15.3917, 14.7240, 21.4235, 10.5006, 21.0869, 19.9746],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [0/80], Step [200/642], LR 1.0e-04, Loss: 29.2
Max Train Loss:  tensor([29.6061, 16.1443, 13.0647, 12.8887, 16.2714, 17.6283, 20.3996, 16.7268,
         2.5674, 21.0177,  7.6383, 12.8993, 16.5888, 16.9095, 16.3685,  5.4232,
        25.8552, 16.2785,  5.1734,  7.6229, 11.4743, 26.2530, 22.7145, 14.7232,
        16.5200,  9.5908, 25.8539, 26.3760, 20.5408,  7.3943, 11.2141, 17.1050,
        11.7851, 14.3776, 10.4177, 22.9882, 21.2368, 21.7285, 19.4220, 18.2150,
        13.1785, 25.6673, 20.7498, 27.2679, 16.7080, 19.9198, 18.7556, 16.0470,
        17.9712, 11.8112, 11.2255, 16.6405, 13.2136, 18.3451,  6.6074,  7.7058,
        23.4885, 12.7461, 15.6511, 10.2831, 25.4183, 16.8911, 21.6548, 11.3556,
        10.1027, 25.8131, 11.5025, 11.0619,  9.4065, 13.8779, 12.4794, 16.6315,
        20.4291, 16.9984, 18.6037, 14.4719,  7.2429, 10.0462, 21.2389, 21.5030],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [0/80], Step [300/642], LR 1.0e-04, Loss: 29.6
Max Train Loss:  tensor([32.2457, 17.0311, 13.5525, 16.2032, 17.2235, 15.6634, 17.5361, 13.5518,
         2.5506, 16.6744,  8.6048, 15.3670, 14.4620, 20.8243, 16.2802,  5.4226,
        17.0307, 13.0307, 13.8638,  7.5102, 12.6919, 26.8603, 22.8093, 10.8628,
         9.1699, 12.6073, 26.6451, 27.1909, 16.4453,  7.3521, 12.3242, 16.2847,
        11.7895, 13.7517, 10.3878, 23.8611, 21.2793, 19.4993, 16.3230, 19.6176,
        17.4647, 29.7894, 17.5861,  7.2621, 12.8672, 25.8191, 18.1374, 15.8156,
        15.8261,  9.5910, 10.7864, 20.6831, 12.6493, 17.5374,  7.2101,  5.9904,
        28.8269, 15.6573, 13.0173, 10.5396, 21.6972, 17.2401, 20.9588, 11.2947,
        12.3473, 25.9599, 12.2416, 10.7046,  8.5610, 13.7400, 12.4156, 16.6863,
        13.8347, 15.9227, 15.8347, 15.7322, 11.4156, 10.7631, 20.9858, 17.8899],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [0/80], Step [400/642], LR 1.0e-04, Loss: 32.2
Max Train Loss:  tensor([28.1721, 18.9655, 11.1115, 10.3357, 11.3261, 15.3618, 21.8899, 17.7412,
         2.5469, 14.9665,  7.5754, 15.6884, 13.2985, 16.6822, 16.2136,  5.4042,
        13.4898, 10.7223,  7.6674,  7.4457,  9.3354,  3.9462, 22.8201, 15.8944,
        11.2153, 15.8710, 12.0972, 19.7453, 16.3259,  7.5045, 10.9767, 16.7662,
        11.7910, 16.4995, 10.3335, 23.8874, 21.3317, 19.9816, 18.4338, 18.8891,
        13.5742, 18.8205, 16.5457,  6.3363, 21.5349, 23.7377, 17.2970, 11.9245,
        14.1692,  8.0068,  9.2015, 16.8039, 13.9176, 15.9629,  5.7617, 10.7771,
        27.6153, 14.0717, 17.1997,  9.1385, 18.2648, 15.6584, 22.7079, 12.4778,
        14.5776, 20.5702, 15.1558,  9.2901,  8.4626, 17.4084, 11.6783, 16.7303,
        13.9108, 19.5315, 16.5502, 14.8643,  9.1367,  8.1658, 20.5920, 17.4381],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [0/80], Step [500/642], LR 1.0e-04, Loss: 28.2
Max Train Loss:  tensor([28.9110, 21.5427, 11.2079, 16.0789, 18.3614, 20.7764, 15.8604, 21.5310,
         2.5509, 18.0053,  7.3307,  9.7545, 16.2594, 20.0645, 16.1004,  5.3918,
        21.9326, 15.0887, 10.3064,  7.4199, 12.7399,  6.2130, 22.8318, 12.4092,
        13.9103, 20.5338,  9.4315, 12.7800, 16.6325,  8.7885, 13.5084, 17.2939,
        11.8028, 16.3434, 10.3111,  2.6127, 21.3298, 20.0656, 15.8659, 17.0511,
        12.5222, 26.8081, 16.6836,  2.9044, 11.2538, 15.1136, 16.1301, 15.6180,
        14.2121, 10.5436, 11.9616, 13.5808, 12.5953, 18.7656,  7.0524, 12.0372,
        19.8567, 14.2756, 11.1999,  8.3900, 21.0644, 16.1941, 15.7188, 13.0700,
         9.7324,  2.9943, 12.1512,  9.4170,  9.9627, 18.9317, 12.5525, 16.7621,
        20.8032, 18.7492, 14.0749, 16.7477,  9.1843,  8.3295, 22.1810, 18.2901],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [0/80], Step [600/642], LR 1.0e-04, Loss: 28.9
Max_Val Meta Model:  tensor([ 27.3171,  37.4788,  28.2237,  35.3846,  10.7900,  17.6873,  14.7413,
         20.5244,   2.5504,  14.8328,   6.3107,  17.6903,  16.8784,  15.5404,
         16.1178,   5.3869, 344.4543,  12.8211,  14.3838,   7.4137,   8.5742,
          2.0361,  22.8254,   8.9753,  15.0342,  14.9207,  10.4892,   7.9556,
         17.2327,   7.1104,   9.9628,  16.1053,  11.8028,  12.4769,  10.3051,
          2.8579,  21.3402,  20.1106,  14.9728,  26.0651,  15.6045,  21.8737,
         17.3605,   6.5232,  14.8267,  21.9333,  16.0788,  14.8994,  15.1901,
          9.9753,   8.0744,  10.4749,  15.1261,  15.0645,   6.0602,   6.6111,
         20.3562,  11.7985,  16.7272,   8.2334,  15.2950,  20.0037,  16.9968,
         11.6865,   8.5612,   1.5953,  10.5829,   8.8869,  10.4399,  24.6078,
         12.1309,  16.7739,  21.0062,  12.2328,  15.2352,  15.5618,   7.8559,
          9.2154,  22.4199,  19.5537], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 25.5702,  38.1200,  26.9773,  37.2566,   9.7969,  18.8028,  14.9220,
         22.7209,   2.5489,  14.9209,   6.0877,  12.8538,  17.2133,  17.0442,
         16.1450,   5.3900, 320.5110,  13.7954,  18.8925,   7.4119,   9.1341,
          1.9354,  22.8350,  10.3089,  16.1404,  15.5406,  12.0365,   8.1568,
         18.6178,   7.1893,  10.1868,  15.3479,  11.7986,  13.1914,  10.3026,
          2.8504,  21.3282,  20.7731,  14.7091,  26.2026,  16.5846,  22.5458,
         17.3631,   6.5599,  14.8293,  21.7347,  14.6268,  14.5210,  15.4225,
          9.2113,   9.0344,  13.0118,  14.7028,  15.8700,   5.9843,   6.2185,
         23.8371,  11.6171,  17.9312,   7.9474,  15.9119,  19.7097,  18.4663,
         12.5035,   9.7704,   1.5119,  10.9864,   8.8752,  10.2869,  25.4902,
         11.4461,  16.7745,  22.6314,  12.7944,  14.2187,  16.2591,   9.5562,
          8.8980,  30.0994,  20.0915], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 74.5237,  62.5495,  83.0439,  48.9338,  16.7526,  35.2673,  19.4605,
         31.2054,  87.9083,  19.3455,  19.4879,  13.5919,  21.1675,  22.9013,
         90.5473,  88.7854, 334.0100,  15.6929,  19.3080,  88.9456,  12.9562,
          6.1880,  88.1552,  13.8005,  45.4721,  21.2395,  39.1187,   9.7593,
         38.2001,  20.7142,  24.1038,  32.9131,  89.6883,  18.0637,  89.2337,
          5.8605,  89.2268,  38.3250,  26.7065,  51.5336,  17.2591,  34.9139,
         33.5423,  22.3508,  19.1707,  29.1572,  25.7225,  19.8040,  29.3344,
         22.6174,  12.1027,  13.3817,  18.1616,  28.2162,  14.2165,  11.4726,
         37.7304,  18.1006,  25.5171,  20.4008,  27.7367,  45.7713,  28.8381,
         28.4100,   9.9148,   5.1788,  18.0025,  27.1693,  28.3662,  30.3769,
         25.6925,  88.8032,  28.1783,  17.4247,  29.6944,  29.4373,  10.4051,
         26.9209,  33.5950,  33.2271], device='cuda:0')
Outer loop valEpocw Maximum [0/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 344.5
model_train val_loss valEpocw [0/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 320.5
model_train val_loss  valEpocw [0/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 334.0
Max_Val Meta Model:  tensor([42.6181, 37.8778, 50.6731, 46.2099, 38.9941, 41.2923, 62.2412, 37.2138,
        44.2868, 39.5165, 37.0337, 54.7341, 41.2962, 40.4675, 42.5491, 38.3904,
        43.4616, 38.3178, 47.0258, 44.9336, 35.0775, 34.5318, 37.1641, 42.9891,
        50.9652, 36.4128, 53.8414, 38.0327, 51.1461, 46.8721, 44.5877, 46.8391,
        47.4331, 35.9961, 46.6073, 40.0595, 33.4306, 44.9342, 36.5883, 44.9618,
        37.5476, 34.5147, 37.2978, 36.0311, 46.7568, 35.0305, 38.2512, 31.8514,
        37.0183, 30.8599, 40.2626, 39.1573, 38.8266, 37.4619, 32.9052, 38.6177,
        42.0860, 44.3804, 37.5576, 32.1693, 46.2848, 43.3209, 37.7146, 45.3362,
        38.5809, 43.2351, 40.6178, 39.1779, 32.5051, 42.5760, 47.5565, 44.0984,
        38.5380, 39.3489, 34.6003, 33.7809, 40.7707, 36.4687, 34.4657, 36.6030],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 38.4244,  13.7089,  45.7996,   9.2405,   7.0577,  22.1352,  54.3708,
         23.4754,  44.1521,  17.6020,  11.4923, 100.1995,  10.7157,   7.9970,
         42.8970,  38.4766,   7.3720,  12.1584,   9.9860,  45.0238,   4.3616,
          2.3155,  37.1841,   6.9677,  16.9291,  11.7704,  23.0363,   8.9083,
         15.8966,   9.0500,  11.9130,  16.8831,  47.4218,   8.3673,  46.6565,
          2.5852,  39.7628,  18.6260,  10.7400,  10.4112,   3.8562,   8.4073,
         12.8240,   3.0690,   4.7768,   6.7488,  10.9403,   7.8773,  13.0759,
          8.5029,   4.7212,   5.7634,   6.5094,  12.4655,   5.4151,   5.3326,
         14.1722,   7.5159,   6.5921,   6.8294,  10.9896,  18.9277,  13.4705,
         12.6573,   3.5998,   2.3499,   8.2548,  12.7111,   8.7687,   7.1049,
         14.2809,  46.0773,   7.7408,   4.8943,  10.5774,   9.7913,   4.4254,
         10.6549,  13.9093,  14.9206], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 88.8312,  27.9450,  77.4110,  16.4904,  14.5621,  41.5779,  84.7549,
         53.0308,  88.0658,  34.0232,  25.4116, 197.0364,  20.6600,  15.4602,
         90.4480,  88.7917,  13.8846,  23.8948,  17.4348,  88.9520,  10.0505,
          5.0055,  88.1277,  13.4350,  35.7328,  25.8008,  42.8917,  17.3429,
         35.3704,  15.4447,  21.1390,  31.1500,  89.7303,  17.7549,  89.2469,
          4.8817,  89.2229,  34.8910,  23.9190,  20.2032,   9.0614,  19.0088,
         29.2730,   6.6526,   8.2407,  13.4690,  23.9822,  16.4705,  26.6677,
         20.7512,   9.6499,  10.8468,  12.7585,  25.9799,  12.3582,  12.7028,
         24.0106,  13.4167,  13.9154,  17.6742,  18.8010,  33.5518,  25.1950,
         24.0844,   7.7324,   4.1082,  16.0590,  27.6136,  21.8662,  15.4667,
         24.3029,  88.7764,  16.3024,  10.1178,  25.4158,  22.4966,   8.4638,
         23.4705,  29.1598,  32.2811], device='cuda:0')
Outer loop valEpocw Maximum [0/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 62.2
model_train val_loss valEpocw [0/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 100.2
model_train val_loss  valEpocw [0/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 197.0
Max_Val Meta Model:  tensor([36.4989, 35.4577, 40.5366, 42.7897, 36.8487, 40.2095, 43.1805, 32.7108,
        43.8204, 33.8962, 34.4130, 32.7074, 38.7030, 36.9627, 43.5519, 38.8577,
        35.0964, 37.1178, 36.3431, 44.3481, 32.3545, 35.5592, 37.2460, 26.2348,
        38.5954, 33.2965, 36.8524, 36.5182, 40.7926, 34.5367, 36.5261, 29.2241,
        47.0587, 33.7416, 46.6057, 34.6920, 32.6500, 44.6965, 35.7093, 45.3705,
        35.7590, 44.2011, 43.6754, 38.5510, 32.4080, 51.2185, 42.6187, 31.6289,
        32.6027, 28.9545, 28.3243, 40.6879, 28.9615, 37.4700, 34.1508, 42.1782,
        55.0756, 43.4473, 37.7460, 29.7511, 67.5360, 42.7125, 35.0771, 44.8474,
        39.0968, 40.0509, 39.0382, 38.0909, 31.7204, 40.1441, 45.8414, 42.7863,
        37.0844, 37.3599, 34.2290, 33.5350, 39.4223, 35.1122, 34.8767, 35.6738],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([27.0927, 12.1643, 15.0976,  8.0078,  9.1404, 18.9204,  9.0666,  7.4179,
        43.6171,  5.2549,  8.8417,  4.8479,  9.4493, 11.2938, 44.0254, 38.9468,
         4.2236,  7.6014,  8.3891, 44.4287,  6.5641,  3.7895, 37.2842,  6.2708,
        14.4795,  7.1298, 14.5408,  8.3213, 18.4996,  9.2295, 14.2232, 13.4096,
        47.0471,  8.9095, 46.6600,  3.4704, 40.0832, 23.5469, 13.6879, 20.5244,
        13.5820, 33.4542, 32.7528, 43.1009, 28.8209, 49.1545, 16.1581, 11.2475,
        20.7264, 10.7039, 11.6673, 24.9722, 21.1617, 17.5992, 26.9968, 45.6073,
        62.1165, 12.7821,  8.1742,  8.5121, 77.4430, 23.1980, 16.8701, 18.2194,
         8.3215,  3.5401, 12.1167, 15.1167, 11.4359,  7.1794, 16.0154, 45.8021,
        10.6313, 16.6070, 12.8689, 13.5865,  8.1462, 12.0034, 15.1401, 15.7950],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 67.4053,  25.6650,  26.5309,  14.6047,  18.9416,  34.1715,  15.6680,
         17.0404,  87.8566,  10.5817,  20.0404,   9.5437,  18.5118,  22.4718,
         90.7405,  88.7882,   8.6400,  14.3852,  15.9884,  88.9361,  15.7697,
          7.3647,  88.1706,  15.0181,  30.9265,  15.6107,  27.8972,  15.8753,
         41.4012,  19.7367,  26.9586,  36.2492,  89.6958,  19.3862,  89.2566,
          7.1204,  89.1869,  44.2783,  30.4975,  39.2869,  32.1872,  75.5911,
         75.0198,  94.2986,  66.9835,  77.8340,  29.6228,  23.1431,  47.2679,
         26.9835,  34.2046,  46.4204,  55.5931,  36.2395,  60.7570, 112.5837,
        111.0837,  22.4355,  17.2786,  22.6076, 127.8203,  40.2144,  32.7589,
         33.9479,  17.5465,   6.2520,  23.3082,  32.6954,  28.1358,  15.8248,
         27.5002,  88.7101,  21.9266,  34.9317,  30.4956,  30.6269,  15.6858,
         26.2123,  29.8748,  33.9487], device='cuda:0')
Outer loop valEpocw Maximum [0/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 67.5
model_train val_loss valEpocw [0/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 77.4
model_train val_loss  valEpocw [0/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 127.8
Max_Val Meta Model:  tensor([32.6042, 37.2465, 43.3605, 42.3867, 37.1513, 38.2822, 43.5415, 34.7815,
        44.8853, 34.4438, 33.5992, 31.6062, 36.5969, 26.5406, 44.4165, 38.4862,
        36.3322, 36.2601, 35.8205, 43.8238, 31.5587, 34.3285, 37.4008, 24.3955,
        38.3734, 33.3153, 36.8744, 37.2391, 19.8216, 32.4041, 26.9535, 27.6185,
        21.1968, 29.5472, 45.3839, 34.1981, 31.7300, 34.4361, 21.3271, 51.9855,
        36.0910, 31.5621, 37.6851, 33.7992, 24.6671, 23.1326, 31.2130, 31.4255,
        30.6546, 27.1192, 24.9544, 37.7332, 21.8834, 31.8756, 37.5719, 36.0666,
        34.0670, 32.4087, 50.0904, 29.2702, 36.2938, 41.9468, 47.5732, 27.9829,
        35.1394, 37.2930, 36.2005, 68.8139, 32.1887, 39.7066, 45.5347, 43.3904,
        43.0227, 36.5149, 51.1421, 32.9708, 36.6996, 34.3049, 37.8319, 30.5240],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([33.2575, 10.9179, 23.4864,  6.7880,  7.6014, 18.9288, 10.4755, 11.1955,
        44.7563, 12.0967,  8.3453,  5.7548,  9.6798,  8.3286, 44.8044, 38.5666,
         4.5085,  8.3364,  8.5417, 43.9219,  4.6443,  2.9079, 37.4112,  4.9127,
        16.6701,  6.8383, 20.8140,  4.6277,  9.2755,  7.5489, 10.0023, 11.8167,
        28.9426,  8.6209, 45.4274,  2.6299, 41.9931, 18.7842,  9.7980, 12.3204,
         4.2895,  8.6207, 12.8110,  3.5124,  3.1476,  4.6254, 12.6984,  8.5420,
        12.3521,  7.6224,  3.5403,  6.1031,  4.1263, 12.3836,  6.9541,  4.6832,
        11.1520,  7.9876,  9.6199,  7.1010, 11.4813, 19.0953, 16.3286, 10.5261,
         4.2611,  2.6938,  9.3037, 17.2882,  9.7086,  6.6504, 14.9272, 46.7057,
         8.8983,  6.4192, 96.4650, 11.4741,  4.2387, 12.2278, 16.0318, 14.4813],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 85.2834,  22.6078,  40.0134,  12.4867,  15.5356,  35.0022,  17.9213,
         25.5050,  88.0264,  24.3432,  19.0872,  11.4005,  19.1127,  21.2072,
         90.4922,  88.7620,   9.2808,  16.1943,  16.5830,  88.9731,  11.4387,
          5.6327,  88.1037,  12.1070,  36.3168,  15.0777,  39.6232,   8.6313,
         38.4500,  17.0580,  23.0639,  33.6840,  89.7514,  17.8245,  89.2365,
          5.4076,  89.2393,  36.2639,  25.7960,  22.8755,   9.8942,  19.8987,
         29.3259,   7.4884,   8.5453,  12.0873,  27.5724,  17.6070,  28.4965,
         20.4532,  10.9862,  11.6882,  13.0973,  26.8071,  13.1492,  11.3397,
         20.2692,  13.6194,  18.2174,  18.8285,  21.8325,  33.1966,  26.6386,
         24.6835,   8.9688,   4.6537,  17.6194,  25.4367,  23.6682,  14.6430,
         25.9945,  88.7759,  16.9465,  13.5506, 225.7107,  25.9778,   8.3393,
         27.1040,  31.4044,  32.8396], device='cuda:0')
Outer loop valEpocw Maximum [0/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 68.8
model_train val_loss valEpocw [0/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 96.5
model_train val_loss  valEpocw [0/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 225.7
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [67.65268454 97.2137279  89.51523495 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44276995 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119  2.73022989 68.89414115 96.29512311 97.48053752
 98.60016325  2.11863891 98.15060733 99.18616976 98.38574091 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877  3.63786991 98.02024829  2.19782897 97.70470633
  3.07744789 97.14550261 97.11504489 92.72913342 97.84237521 92.05906361
 96.90915072 96.22689782 96.9627563  93.87190702 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 89.12172123 96.13796128 96.24273583 96.9067141
 89.79300934 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 39.60356233
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [57.18741243 97.2137279  89.51523495 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.05906361
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 89.09857336 96.13796128 96.24273583 96.9067141
 89.79300934 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [76.73789088  2.70400164  9.64792086  4.04690658  2.06024325  3.31959592
  2.78043078  9.8435549   2.76304952  7.07806817  1.17432998  1.24746357
  0.68856483  7.0200361   2.70131385  3.0935759   5.10568627  2.88077006
  6.69589659  2.71976321  1.79068808  0.66917451  1.84551439  1.70726045
  4.27884575  4.18538174  6.21368424  5.14030409  2.54308939  1.36334672
  1.90666472  0.98606573  3.5683299   1.24551602  2.31015254  1.81716827
  2.64244335  1.74661413  2.34402124 12.86559422  3.05519658 19.66176603
  4.6303456   6.44031012  4.52911829 15.00808401  2.12909213  1.55951938
  2.81148245  1.73297405  1.65273892  1.53587236  1.68721049  2.60830875
  2.3793749   3.4578406  26.92625199  7.20359961  5.40338843  5.04271008
 27.47100038  3.2060434   6.14647008  5.3354327   3.29330037  5.0643804
  3.04275932  5.61975225  2.6537649   4.78300926  0.31300436  6.68659236
  4.45395075 10.10570232  4.29094622  4.71917047  1.24688375  2.24920524
  0.17770507  1.16846998]
Accuracy th:0.5 is [45.24676844 97.2137279  73.53346085 97.02489005 97.26733349 78.67106882
 78.97930093 77.65987256 79.84186353 96.42548215 80.25487019 98.52097319
 99.41399349 80.65934869 79.70906787 96.56680596 96.29512311 79.52266663
 98.65376884 98.30776915 80.88960904 80.78970773 98.38695922 79.71028618
 80.66544024 96.65086926 94.0778012  79.15839232 98.01293844 80.11110976
 97.30875598 98.57457877 96.36213009 98.02024829 85.94437202 79.72246927
 79.44103995 90.10489638 97.11504489 76.64014815 79.98440565 92.05906361
 78.96346292 78.56873089 96.9627563  93.87434364 98.02877645 98.57336046
 84.52382403 87.67437044 87.00795556 98.55508583 98.99976852 79.17423033
 98.70615611 79.4763709  73.86240421 93.36630889 96.24273583 96.9067141
 89.79300934 97.17717864 90.46673408 79.50682862 98.42838172 79.95638455
 98.20786784 78.55532949 80.65934869 97.55850928 81.22951718 95.99054592
 80.26461666 95.45083515 78.80264617 82.57453004 85.57156955 80.12938439
 81.28921431 99.14718388]
Accuracy th:0.7 is [45.37712747 97.2137279  73.53346085 97.02489005 97.26733349 78.67106882
 78.97930093 77.74271756 79.84186353 96.47299619 80.25487019 98.52097319
 99.41399349 81.10159477 79.70906787 96.56680596 96.29512311 79.52266663
 98.65376884 98.30776915 81.287996   80.78970773 98.38695922 79.71150449
 81.16494682 96.65086926 94.0778012  79.15839232 98.01293844 80.11110976
 97.30875598 98.57457877 96.36213009 98.02024829 86.19899855 79.72246927
 79.44103995 90.34733982 97.11504489 76.64014815 80.47051084 92.05906361
 78.96346292 78.56873089 96.9627563  93.87434364 98.02877645 98.57336046
 86.0662029  88.4638345  87.20166665 98.55508583 98.99976852 79.17423033
 98.70615611 79.4763709  73.86240421 93.73180151 96.24273583 96.9067141
 89.79300934 97.17717864 90.66166348 79.50682862 98.42838172 79.95638455
 98.20786784 78.55532949 80.65934869 97.55972759 81.22951718 95.99054592
 80.26461666 95.45083515 78.80264617 82.66224827 85.66537932 80.12938439
 81.28921431 99.14718388]
Avg Prec: is [55.8747252   3.05252988 11.21666481  3.43483503  2.20054897  3.82657227
  3.22848061  5.50205025  2.45580585  3.81859303  1.65275355  1.624
  0.61977233  5.09041607  2.74439115  3.22671659  3.7037393   2.75514867
  1.37427138  1.75650271  1.92811481  0.86340519  1.74308906  2.46990449
  5.05328998  3.52462352  6.49203744  3.41686251  2.04016586  1.86979663
  2.62460383  1.32142182  3.65006671  1.67898962  2.36548042  2.41599692
  3.01640337  2.58626321  2.83432931  7.34216316  2.2441505   8.18410069
  3.31744341  3.93400981  3.18713325  6.43265115  2.11726869  1.487354
  2.16021274  1.59758731  1.88355231  1.59104079  1.11191077  3.00541125
  1.21992034  2.71316693 11.1517561   3.72513196  3.88326566  2.8544554
 10.66361344  2.19660219  3.82243292  2.94898614  1.52619969  2.53566592
  1.70687375  4.15568211  1.25743221  2.44955191  0.21005193  3.45978414
  1.88561246  4.55751557  4.02250307  3.11350047  0.76717056  1.85881981
  0.17547288  0.72299265]
mAP score regular 5.46, mAP score EMA 3.75
starting validation
Accuracy th:0.5 is [70.85980517 97.22450607 89.58566908 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375  2.7929342  55.40025413 96.21047911 97.50604181
 98.02426689  2.02805392 98.22109276 99.15040985 98.30580263 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823  3.60016942 98.18870369  1.99068191 97.89471062
  2.8004086  96.78102499 97.0276802  92.74484889 97.82744101 92.37362035
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 88.9104816  96.39235618 96.16314124 96.78102499
 90.2060443  97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 45.05070135
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [64.72830555 97.22450607 89.58566908 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.76921544 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.37362035
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 89.04003787 96.39235618 96.16314124 96.78102499
 90.13379176 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [80.27785413  2.78320684  8.94729085  5.33366408  1.44236503  3.25269079
  3.13410903 11.48965956  3.32505453  8.01755809  1.08642778  1.15707204
  0.85580427  8.8389274   3.03504947  3.51082066  5.68596592  3.11870273
  8.22203825  3.35670563  1.87655029  0.73874972  2.06528182  1.83638451
  3.82155248  5.27346367  5.78138757  5.7724547   2.83209763  1.26284941
  1.55791474  0.80847389  3.47754116  1.11458079  1.68918863  1.47781892
  1.91215177  1.79289349  2.08920962 13.81340624  3.60495843 22.24925756
  4.66994202  6.80522888  4.76084911 16.09910755  2.00162185  1.59751155
  3.56248942  1.72005788  1.42587393  1.42823056  1.88407083  2.3808477
  3.13872682  3.57297846 31.29335226  7.41673031  5.52566002  5.72973942
 31.85349202  3.53928293  6.55614641  7.05064386  5.52362629  5.91717723
  4.22825849  6.39571692  3.64786121  6.13489404  0.45077761  9.14894841
  5.73762682 13.12678441  4.52312489  4.94234697  1.2851467   2.26125334
  0.27987807  1.32543101]
Accuracy th:0.5 is [45.33722002 97.22450607 72.49171587 96.96290206 97.90716795 78.17724294
 78.31925655 77.01621945 79.67710591 96.41976231 79.94618432 98.5325261
 99.34972718 79.17133817 79.75932431 96.31262924 96.21047911 79.26103097
 98.78167277 98.34068316 80.20280539 80.58649127 98.31327703 79.35072377
 79.23860777 96.52938685 94.3393876  79.20871017 97.81747515 79.89386352
 97.52597354 98.67204823 96.39983058 98.18870369 86.33928794 79.49024591
 79.39806164 91.78314274 97.0276802  76.33355757 79.39058724 92.37362035
 78.60328375 78.23205521 97.03764606 94.02795426 98.18621222 98.77668984
 84.7547151  87.77686424 85.67904926 98.55993223 98.87385704 78.66307895
 98.6969629  79.25604804 72.80065775 94.3244388  96.16314124 96.78102499
 90.13379176 97.04761193 90.2434163  79.15888083 98.32075143 80.00348805
 98.13139996 78.33918828 80.5541022  97.53593941 81.08229315 96.07843137
 80.23519446 95.44559882 78.24700401 83.70829908 87.26611356 79.92126965
 81.15703715 99.15040985]
Accuracy th:0.7 is [45.52906296 97.22450607 72.49171587 96.96290206 97.90716795 78.17724294
 78.31925655 77.03365972 79.67710591 96.41976231 79.94618432 98.5325261
 99.34972718 79.54007524 79.75932431 96.31262924 96.21047911 79.26103097
 98.78167277 98.34068316 80.5017814  80.58649127 98.31327703 79.35321524
 79.60734484 96.52938685 94.3393876  79.20871017 97.81747515 79.89386352
 97.52597354 98.67204823 96.39983058 98.18870369 86.59341754 79.49024591
 79.39806164 91.97498567 97.0276802  76.33355757 79.57993871 92.37362035
 78.60328375 78.23205521 97.03764606 94.02795426 98.18621222 98.77668984
 86.30191594 88.10075491 85.84099459 98.55993223 98.87385704 78.66307895
 98.6969629  79.25604804 72.80065775 94.57607694 96.16314124 96.78102499
 90.13379176 97.04761193 90.38293844 79.15888083 98.32075143 80.00348805
 98.13139996 78.33918828 80.5541022  97.53593941 81.08229315 96.07843137
 80.23519446 95.44559882 78.24700401 83.78553454 87.3757381  79.92126965
 81.15703715 99.15040985]
Avg Prec: is [53.22904316  3.64956084 14.93835553  4.54034779  1.46460405  4.66260248
 14.70529286  8.76572846  8.70867367  5.69539178  3.43352456  4.81539777
  2.59617878  5.72752965  3.00659521  3.81021251 13.85048692  6.56145824
  1.58584572  2.89170805  3.57457667  1.57982339  1.21966411  5.19780956
  5.51150373  7.45162223  7.73902561  4.75372836  3.90297936  4.06749313
  2.0352803   0.82896094  2.89118199  1.14017302  1.68469229  2.08480001
  1.90196068  2.10633599  2.18866371  6.22092585  1.7009981   6.01894743
  2.14547994  2.67875273  2.3408238   4.91521357  1.633484    1.01649586
  1.39809652  1.14982568  1.19550508  0.98889681  0.73341701  2.24716311
  0.84355236  1.81628713  9.91025824  2.9932627   3.84002812  2.69200346
  7.77488395  2.00900668  3.24342043  2.5298481   1.35655304  1.94104128
  1.53032466  3.53006397  1.10019524  2.26185028  0.20053431  3.35072868
  1.6634912   3.96107858  3.19688995  2.3015345   0.55923535  1.43191839
  0.1204341   0.61538102]
mAP score regular 6.03, mAP score EMA 4.17
Train_data_mAP: current_mAP = 5.46, highest_mAP = 5.46
Val_data_mAP: current_mAP = 6.03, highest_mAP = 6.03
tensor([0.3883, 0.4838, 0.5910, 0.5482, 0.4849, 0.5380, 0.5900, 0.4378, 0.5089,
        0.4931, 0.4289, 0.5081, 0.5073, 0.3908, 0.4932, 0.4405, 0.4814, 0.5129,
        0.5205, 0.4880, 0.4014, 0.5199, 0.4186, 0.4041, 0.4592, 0.4601, 0.5269,
        0.5325, 0.2306, 0.4448, 0.4316, 0.3434, 0.3112, 0.4474, 0.5142, 0.4874,
        0.4608, 0.5119, 0.3586, 0.5301, 0.4306, 0.4360, 0.4372, 0.4695, 0.3557,
        0.3802, 0.4619, 0.4794, 0.4324, 0.3716, 0.3150, 0.5273, 0.3089, 0.4521,
        0.5343, 0.4028, 0.5589, 0.5913, 0.5146, 0.3718, 0.5347, 0.5883, 0.6127,
        0.4162, 0.4794, 0.5815, 0.4498, 0.5000, 0.4078, 0.4500, 0.5756, 0.5255,
        0.5599, 0.4766, 0.4260, 0.5299, 0.5089, 0.4982, 0.5093, 0.4386],
       device='cuda:0')
Max Train Loss:  tensor([28.8031, 11.9860, 27.5881, 15.6376,  9.8978, 17.2244,  9.4104, 13.1240,
        44.7645,  9.2004,  9.0210,  4.7460,  9.9402,  9.7841, 44.5444, 39.0928,
        13.1244, 11.5535,  8.2965, 43.4070,  5.4486,  4.5424, 36.8905,  7.5892,
        16.3049, 11.0054, 21.2860,  9.7944,  8.2200, 11.3150, 11.6344, 12.6644,
        27.9067,  9.0354, 45.8746,  7.6851, 41.1203, 18.5732, 12.4056, 17.2279,
        10.5588, 17.9654, 12.9409,  9.2049,  4.1085, 11.8106, 11.9758, 10.2431,
        13.3494,  9.6955,  7.7414,  7.0207,  4.1094, 14.0862,  8.7068,  5.7849,
        24.6217, 12.2923,  9.9783,  9.3713, 22.3222, 21.2042, 18.6756, 14.0372,
         8.3530, 10.8389, 11.4157, 16.0560,  9.7751,  9.0482, 15.2756, 46.6572,
        11.2036, 12.8220, 14.2461, 15.7101,  3.7303, 13.4700, 12.0753, 14.8240],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [1/80], Step [000/642], LR 1.0e-04, Loss: 46.7
Max Train Loss:  tensor([28.5367,  7.0182, 26.6741, 25.7662, 11.0105, 12.6961,  7.0578, 26.0747,
        20.9951, 22.4323, 10.6683, 20.6237, 24.4803, 15.0171, 12.0869, 16.4902,
        22.0089, 23.3812, 19.0922, 18.4057, 19.2358,  7.7503, 16.7715, 22.5788,
        16.2931, 26.2119, 21.3104, 23.7613, 18.2121, 12.0287, 11.1209, 27.2489,
        27.8981, 26.8939, 17.9498, 22.4815, 14.5386,  9.9923, 23.0233, 13.2285,
        16.2014, 16.9476, 13.3965, 14.5989, 13.8186, 19.7831,  7.2762, 27.5261,
         8.8936, 26.1862, 14.5768, 18.4224, 15.5188, 10.3300, 11.9074, 21.6530,
        22.9204,  9.7881, 26.3175, 26.3050, 19.0574, 12.8452, 13.6544,  8.9103,
        17.1042, 13.0944,  8.6439, 14.1990,  7.0077, 17.8455,  9.8162, 18.2520,
        10.6756, 20.6580, 14.1727,  7.7212, 15.6417, 12.0996, 28.2165,  6.0021],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [1/80], Step [100/642], LR 1.0e-04, Loss: 28.5
Max Train Loss:  tensor([25.3253,  6.8585, 18.5171,  7.1056,  6.1549, 10.7547, 11.4441,  8.7595,
        21.1295,  5.7047, 10.7313, 11.0497,  3.9330, 24.0813, 17.0412, 18.0908,
         6.4975,  4.4311,  8.0712, 19.4365,  6.7254,  7.6300, 16.9902,  5.5344,
        14.8844,  7.0128, 21.3263,  8.3944, 20.3361, 11.6861, 13.3110,  4.4836,
         5.2266,  7.8679, 17.9748,  4.8332, 13.4257, 11.5275,  9.2889, 16.6976,
         5.9516, 13.0263,  9.3822, 14.2330, 21.5997, 12.7833,  6.2375,  5.9308,
         7.3940,  4.0074, 22.1739,  4.5117, 22.6627,  9.0085,  9.4513,  5.5538,
        23.0482, 11.5143, 12.1536,  7.4453, 21.9510, 12.7691, 15.2894,  8.9778,
         8.0411, 13.7956,  7.4940, 15.2365,  7.4447,  5.3523,  9.7126, 18.6896,
         8.5208,  9.9455,  9.4516, 15.1580,  5.5704, 13.3293,  3.7434,  8.3446],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [1/80], Step [200/642], LR 1.0e-04, Loss: 25.3
Max Train Loss:  tensor([29.7536, 10.8134, 24.5119,  7.9068,  9.1737, 13.0642, 18.0066, 11.1066,
        21.5544,  8.1761, 11.7234,  7.1367,  4.8464, 12.9775, 15.1215, 18.8691,
        13.0354,  4.8885,  6.2780, 19.8354,  4.0197, 11.3411, 18.9246,  7.8899,
        14.5202,  8.8660, 23.3884, 12.3258, 20.6403, 10.4837, 10.7282,  7.5477,
         9.6716, 13.6245, 20.1357,  7.4701, 15.4535, 15.3801,  6.2180, 18.1123,
         4.7755, 13.7928,  7.1859, 14.0749,  4.7902, 11.0444,  8.6258,  4.3057,
         6.9063,  5.8656,  6.4239,  6.0214,  4.3517,  9.1980, 11.7784,  8.2135,
        19.5124, 10.8761, 10.9014,  6.1702, 16.4968, 13.8263, 11.6126,  6.6655,
         3.9696, 13.8184,  6.5123, 16.8382,  7.3891,  3.7287, 10.7354, 21.3589,
         5.0172,  8.0364,  7.6695, 15.6378,  2.8296, 13.2877,  4.6462,  9.8076],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [1/80], Step [300/642], LR 1.0e-04, Loss: 29.8
Max Train Loss:  tensor([23.4956,  9.3503, 20.4532, 11.5812, 11.6743, 14.2472,  8.5102,  6.0146,
        20.5525,  8.0263, 12.6178,  5.6933,  4.1051,  5.5959, 14.2993, 19.7382,
         7.7965,  6.5098,  3.8557, 19.5544,  2.2503,  9.0869, 16.8790,  5.6306,
        13.4309, 13.0090, 13.6206,  9.9876, 20.6492, 13.9957,  7.0719,  5.9401,
         9.3799,  6.1972, 18.6476,  4.3927, 15.9869, 12.3250,  5.8830, 18.9604,
         7.4997, 13.1707,  6.7392, 13.4252,  6.3715,  6.3513,  5.8111,  7.6114,
         8.2740,  7.1297,  2.1364,  8.5282,  4.0691,  8.8895,  9.7948,  3.7448,
        21.1784, 10.8008,  6.4420,  9.8292, 19.2267, 14.8243, 13.6016, 10.4924,
         3.5212, 16.5796,  7.6830, 15.6311,  5.8595,  3.4367, 11.3031, 19.0847,
         8.6096, 10.6819,  7.1074,  9.6523,  3.8683, 12.5009,  5.4269,  7.9354],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [1/80], Step [400/642], LR 1.0e-04, Loss: 23.5
Max Train Loss:  tensor([25.1981,  6.1845, 26.5610,  3.4180,  5.7364, 13.5883, 12.9294,  6.1141,
        18.3911,  9.7464, 14.2715,  2.9013,  5.4706,  5.5571, 14.9423, 17.5941,
        13.4031,  7.6394,  4.2703, 18.0186,  2.2954,  8.0952, 18.3149,  6.3316,
        13.0768,  6.4173, 13.4016, 10.3238, 20.6557, 12.4055,  6.6005,  6.9834,
         8.5106,  9.1414, 20.4635,  8.7663, 12.9443, 14.0335,  8.5840, 20.3252,
         5.3788, 13.1359,  6.5345, 12.6447,  5.8091, 12.5464,  7.2908,  3.7267,
         6.2646,  3.2501,  1.3509,  2.5375,  6.1389,  6.4375, 12.2767,  3.6840,
        25.0047, 16.2128,  7.8638,  6.0763, 15.9427, 15.7857, 13.1691, 10.5708,
         6.5002, 17.3206,  8.1705, 15.8719,  9.7194,  9.4975, 10.1005, 23.5199,
         9.2912,  8.3028,  8.7122,  8.6356,  6.2664, 13.4979,  4.0290,  6.0990],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [1/80], Step [500/642], LR 1.0e-04, Loss: 26.6
Max Train Loss:  tensor([22.6667, 11.2035, 22.6022, 13.4163,  6.0933, 14.2239, 12.5849, 11.0643,
        19.2399, 14.5245, 11.8919,  4.6082,  5.5463,  6.5941, 12.6216, 17.2449,
        11.6782, 11.0964,  4.6076, 18.8333,  5.0552, 11.0185, 17.5569, 10.1946,
        17.1873,  8.8200, 13.7885, 10.0850, 20.6667, 13.4147,  6.8412,  7.8304,
         6.6611,  5.5025, 20.2579,  7.7625, 14.8460, 10.4437,  9.7884, 21.0530,
         4.5698,  9.6103,  9.7613, 14.0302,  7.3368,  9.2417,  8.4712,  3.8561,
         7.1196,  3.3857,  1.4424,  2.7327,  3.0735,  9.4604, 12.3660,  7.9882,
        15.0658, 15.8716,  4.9246,  5.3540, 12.7568, 19.2977, 11.8930, 11.2256,
         3.9261, 12.9491,  9.0427, 14.6710,  7.1168,  7.4131, 10.2560, 23.0899,
        11.0964,  7.1967,  8.9235,  7.2765,  5.9001, 11.7770,  4.2305,  6.9962],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [1/80], Step [600/642], LR 1.0e-04, Loss: 23.1
Max_Val Meta Model:  tensor([ 28.2805,  33.2003,  49.4258,  29.2583,   5.1405,  12.2712,   7.8946,
         11.4557,  18.7272,   9.4079,  11.3272,   5.9232,   5.7412,   7.5368,
         13.1471,  17.1869, 195.6483,   3.1052,   2.8679,  18.3949,   2.5266,
          8.5342,  15.7491,   2.7003,  19.1754,   9.4589,  18.6454,   3.0374,
         20.6689,  12.3354,   6.0154,   4.0791,   4.1184,   4.0430,  18.1422,
          3.4152,  11.3813,  11.3955,   2.5852,  28.6069,   5.7421,  13.8397,
          6.5830,  14.9942,   6.5889,  10.3869,   5.3899,   5.2218,   5.6174,
          4.3692,   1.5202,   2.8318,   3.0910,   6.8321,  10.3140,   2.8688,
         16.9886,  15.1352,  11.1680,   3.6267,  11.7469,  20.6128,   9.8289,
          6.8118,   2.5719,  12.3196,   5.3386,  12.4839,   8.5172,  11.9914,
         10.4155,  11.9097,  12.4479,   6.2766,   7.6127,   9.6684,   2.6069,
         12.0409,   4.3040,   6.4473], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 24.3563,  30.7398,  47.0428,  31.2856,   5.4020,  12.8022,   7.9869,
         11.3906,  18.9957,   8.5604,  11.2002,   5.9045,   5.8591,   7.1084,
         13.1985,  17.3721, 193.9211,   3.1402,   3.0503,  18.5897,   2.6234,
          8.7088,  15.9182,   2.8582,  18.9987,   9.6185,  19.7055,   3.1872,
         20.6723,  12.6190,   5.9713,   4.0896,   3.9988,   4.0510,  18.3529,
          3.4091,  11.5442,  10.9633,   2.6551,  28.8231,   5.7801,  14.5029,
          6.8543,  15.1818,   6.2524,   9.8656,   5.4688,   5.6037,   5.5153,
          4.3717,   1.5773,   2.9865,   3.0701,   6.9778,  10.6948,   3.0055,
         19.8932,  16.1727,  10.8180,   3.7682,  12.6779,  20.0824,  10.3429,
          7.0444,   2.6488,  12.5030,   5.4824,  13.0830,   8.8387,  12.8149,
         10.2786,  11.9042,  12.9171,   5.9928,   7.7179,   9.9998,   2.5760,
         11.5347,   4.7903,   6.3438], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 62.7190,  63.5339,  79.6040,  57.0653,  11.1412,  23.7937,  13.5361,
         26.0208,  37.3276,  17.3596,  26.1122,  11.6200,  11.5486,  18.1883,
         26.7599,  39.4368, 402.8475,   6.1219,   5.8601,  38.0957,   6.5357,
         16.7496,  38.0261,   7.0730,  41.3736,  20.9070,  37.3967,   5.9851,
         89.6297,  28.3720,  13.8339,  11.9104,  12.8482,   9.0548,  35.6941,
          6.9944,  25.0534,  21.4190,   7.4051,  54.3696,  13.4223,  33.2602,
         15.6764,  32.3338,  17.5786,  25.9505,  11.8391,  11.6891,  12.7561,
         11.7642,   5.0069,   5.6636,   9.9393,  15.4331,  20.0161,   7.4619,
         35.5959,  27.3512,  21.0232,  10.1353,  23.7101,  34.1347,  16.8812,
         16.9248,   5.5253,  21.5032,  12.1876,  26.1638,  21.6751,  28.4747,
         17.8563,  22.6530,  23.0683,  12.5750,  18.1174,  18.8699,   5.0614,
         23.1537,   9.4056,  14.4648], device='cuda:0')
Outer loop valEpocw Maximum [1/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 195.6
model_train val_loss valEpocw [1/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 193.9
model_train val_loss  valEpocw [1/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 402.8
Max_Val Meta Model:  tensor([40.7716, 47.8690, 58.4384, 48.0647, 38.5790, 47.1506, 57.3521, 37.1073,
        43.1631, 35.2815, 39.1075, 46.6807, 40.8524, 32.9788, 39.3952, 37.7937,
        46.8859, 41.7078, 54.4343, 42.1369, 34.5480, 40.5199, 36.5772, 33.5237,
        39.9327, 37.5476, 41.5057, 42.0145, 27.4207, 39.9681, 36.3611, 32.3596,
        30.0580, 36.7151, 40.9504, 38.0849, 39.6230, 43.1335, 32.4805, 46.2460,
        40.6735, 36.5156, 35.8487, 38.9137, 32.0905, 30.9782, 37.0460, 34.8312,
        37.7924, 33.3434, 28.0149, 41.6666, 27.0386, 36.7206, 41.0893, 36.4741,
        41.6866, 48.5129, 47.5259, 32.8151, 40.2329, 41.8986, 42.3268, 38.0715,
        36.6455, 48.1730, 36.1669, 38.5366, 33.6201, 37.3768, 47.8918, 50.4053,
        49.3004, 39.2381, 35.0565, 49.8584, 39.1736, 43.8289, 38.4861, 36.6159],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 33.3377,  11.3297,  40.1659,   7.4084,   5.6590,  20.8737,  60.9242,
         28.1514,  25.5250,  17.2763,  15.3467, 103.0154,   7.2399,   3.3692,
         14.4679,  17.9895,   6.3026,   9.5442,   4.3665,  22.5357,   2.8921,
          8.7743,  16.8338,   4.3300,  16.6609,   9.9565,  19.1257,   8.4126,
         27.4169,  11.2004,   6.2632,   4.8991,   4.1775,   4.6262,  17.5765,
          3.5681,  13.0306,  10.6437,   3.1882,  14.2616,   3.0051,   6.2602,
          5.2720,  11.7220,   1.9909,   4.0442,   5.8969,   4.4602,   6.2436,
          5.4219,   1.8891,   3.1285,   1.7946,   7.0086,  11.1538,   4.7406,
         12.2622,  14.8528,   6.7627,   4.4051,   7.5331,  10.1531,   8.0598,
          7.3836,   2.7292,  12.9203,   5.6957,  13.1236,   6.6922,   4.4414,
         11.2461,   3.7846,   6.9457,   4.7478,   6.0631,   8.6491,   2.7438,
         13.4582,   4.7537,   6.8680], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 78.5154,  21.5325,  62.7100,  12.9235,  11.6971,  37.7774,  95.2159,
         62.5235,  50.0397,  35.9683,  33.3092, 198.4856,  14.1250,   8.0156,
         29.7893,  40.1219,  11.7637,  17.9219,   6.3194,  45.3760,   6.7080,
         17.3502,  38.6382,  10.1842,  36.0979,  21.5535,  37.6570,  15.6582,
         89.6112,  23.4591,  14.0292,  12.5606,  11.2580,   9.9042,  36.3505,
          7.3846,  27.4774,  20.5503,   7.7962,  28.0911,   6.3719,  13.5462,
         11.9599,  25.3279,   4.7221,  10.0117,  12.7021,   9.0994,  13.3601,
         12.8991,   5.2613,   5.9237,   5.1151,  15.4016,  21.2666,  11.0301,
         21.8195,  25.6855,  12.3399,  10.6903,  15.3744,  19.4207,  14.3351,
         16.1681,   5.7498,  22.1310,  12.6508,  29.2998,  16.2361,   9.6178,
         19.3493,   6.3059,  11.9101,   9.6293,  14.0449,  13.5677,   5.4173,
         23.3851,   9.5033,  15.1696], device='cuda:0')
Outer loop valEpocw Maximum [1/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 58.4
model_train val_loss valEpocw [1/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 103.0
model_train val_loss  valEpocw [1/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 198.5
Max_Val Meta Model:  tensor([37.0626, 43.1334, 43.2941, 42.7484, 38.9759, 43.8504, 42.3609, 35.7442,
        36.8616, 32.2046, 38.9327, 40.3451, 40.7503, 33.1295, 34.4127, 37.0967,
        45.0737, 33.5971, 42.7555, 41.8901, 34.3581, 37.0055, 36.4146, 40.4727,
        39.9298, 42.3434, 40.5802, 37.9713, 28.3892, 40.0725, 36.5861, 32.6193,
        30.4141, 36.6549, 40.6629, 38.1833, 39.3065, 42.6654, 32.2543, 44.0835,
        40.5684, 38.5588, 37.8590, 39.5791, 33.5582, 33.5974, 40.5720, 35.2543,
        48.5412, 33.8350, 28.9397, 45.7168, 28.8881, 37.5956, 42.6428, 38.2124,
        53.8080, 41.3696, 42.6749, 32.7405, 43.3949, 41.4459, 42.1263, 37.8609,
        36.1496, 39.1247, 36.6327, 38.5286, 32.9388, 37.4623, 29.4463, 44.1674,
        44.7264, 38.9836, 34.5833, 32.2359, 39.2767, 42.9535, 38.4707, 36.1219],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([19.6410,  4.9226,  6.0285,  3.2708,  4.9126,  9.1545,  3.7112,  7.2687,
        16.0502,  2.5210, 10.7035,  2.8014,  3.3917,  9.1928, 10.4009, 16.1714,
         2.5914,  2.4061,  2.7757, 18.0951,  2.4988,  7.5604, 15.8219,  2.9843,
        10.2287,  4.2374, 13.2065,  7.0315, 28.3872, 10.0885,  6.7257,  5.0876,
         5.5202,  3.7689, 16.3727,  3.0368, 13.8872, 10.6290,  2.6080, 21.9201,
        15.4190, 44.0901, 41.2111, 35.5072, 34.1213, 45.5460,  8.0562,  6.9537,
        24.3013,  5.7947, 13.5892, 31.7865, 27.9873, 12.6090, 30.7642, 60.8642,
        69.2884, 15.6978,  7.1453,  3.7143, 73.1874,  9.5761, 11.8444,  9.4672,
         7.0990, 10.0485,  7.3214, 12.5337,  6.7742,  2.2180,  5.8247,  6.0274,
         6.6570, 21.4608,  4.6406,  8.3760,  7.5892, 10.6279,  3.8663,  5.5419],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 45.7130,   9.2960,  10.3946,   5.9185,  10.0067,  16.9703,   6.8282,
         16.2607,  34.7906,   5.4493,  23.2233,   5.3896,   6.6583,  21.8144,
         23.4661,  36.7113,   4.9029,   5.0373,   4.6199,  36.4962,   5.7914,
         15.3863,  36.4318,   6.2974,  22.2867,   7.8051,  26.3417,  12.9624,
         89.6620,  21.0971,  14.9733,  12.8473,  14.6697,   8.0537,  34.0530,
          6.2295,  29.5472,  20.8948,   6.3833,  43.8163,  32.7620,  96.5925,
         92.3159,  77.1852,  81.6905, 112.2944,  15.6566,  13.9952,  43.0230,
         13.5337,  37.0508,  53.8606,  77.6529,  27.1623,  58.8735, 138.1678,
        121.7511,  27.9485,  13.7298,   8.9769, 160.2297,  18.4167,  21.4203,
         20.8985,  15.1819,  19.9823,  16.0938,  27.9198,  16.7738,   4.7665,
         15.2327,  10.6377,  11.9642,  44.2768,  10.8548,  18.3679,  15.0033,
         18.8093,   7.7124,  12.3870], device='cuda:0')
Outer loop valEpocw Maximum [1/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 53.8
model_train val_loss valEpocw [1/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 73.2
model_train val_loss  valEpocw [1/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 160.2
Max_Val Meta Model:  tensor([39.7240, 43.6022, 43.1188, 42.4199, 35.4198, 41.3896, 39.5325, 32.9488,
        34.6250, 28.9060, 36.0384, 36.9461, 37.8514, 29.8037, 31.7323, 37.5808,
        40.8398, 27.7009, 42.1914, 40.5216, 30.4902, 33.9843, 34.2479, 39.8675,
        38.7264, 42.1220, 39.9118, 35.2830, 25.1358, 36.8496, 33.5590, 27.9006,
        26.5809, 32.2397, 40.1472, 36.5735, 43.5011, 41.7390, 28.3487, 45.6219,
        38.9822, 34.4920, 33.7716, 37.3631, 27.7536, 27.5132, 31.6407, 32.1176,
        27.6874, 29.1925, 24.2479, 32.8996, 23.4766, 32.1558, 36.4404, 33.4930,
        36.8305, 37.1453, 46.8563, 29.8875, 30.0016, 36.6290, 40.4052, 34.7196,
        42.9256, 33.4377, 32.7673, 42.3010, 31.7403, 33.3142, 22.6361, 43.1249,
        45.6558, 35.2283, 57.0137, 27.0409, 39.8610, 43.9088, 35.7902, 29.8367],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 31.7969,   7.1382,  19.9369,   4.7359,   6.2711,  14.9716,   6.8969,
          9.9254,  18.1124,  10.4386,  12.8358,   5.2944,   5.6563,   7.6032,
         14.2735,  18.8779,   4.0911,   3.4985,   4.3987,  20.3487,   3.2097,
          9.3356,  17.2737,   3.9862,  16.7038,   5.9609,  19.0202,   3.8241,
         25.1327,  12.0705,   6.9585,   5.2465,   4.5036,   5.0333,  19.0198,
          4.8389,  14.7007,  11.8296,   3.4638,  17.9752,   3.8316,   6.7466,
          5.9581,  12.8970,   2.1535,   3.0932,   6.6410,   5.0735,   6.4364,
          4.5216,   2.0726,   4.0344,   1.9788,   7.6191,  11.8806,   3.8593,
         11.8376,  15.3420,  10.0887,   4.8451,   8.0619,  11.0116,  10.0753,
          7.8458,   4.0537,  12.0025,   6.3018,  13.2938,   7.4950,   3.2279,
          6.9323,   4.3479,   6.4480,   5.7539, 153.6826,   7.6691,   3.6027,
         17.2638,   5.4521,   7.6090], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 77.1787,  13.0294,  33.7186,   8.2660,  13.1206,  27.8302,  12.5739,
         22.7632,  39.5635,  23.0217,  28.8955,  10.2666,  11.1202,  18.3875,
         32.6364,  41.3896,   7.8550,   7.8595,   6.9405,  41.2047,   7.7281,
         19.2608,  41.0808,   8.1711,  36.3070,  10.4996,  36.9619,   6.9660,
         89.6130,  26.2185,  15.8691,  14.5372,  12.7566,  11.2638,  38.8234,
          8.3924,  28.0370,  22.3829,   8.9346,  34.4283,   7.3623,  13.0201,
         13.4508,  27.7233,   5.3766,   7.8614,  14.4103,  10.3540,  15.1480,
         11.3685,   6.0665,   6.7896,   5.8808,  17.1233,  22.5323,   9.0452,
         21.6528,  27.2707,  17.9211,  12.0039,  19.1132,  21.5202,  16.8720,
         17.7919,   6.7193,  24.3337,  14.3999,  27.6178,  18.3543,   7.1640,
         21.7717,   7.3913,  10.8736,  11.8672, 299.8850,  17.8968,   6.1981,
         27.5553,  10.8845,  17.2293], device='cuda:0')
Outer loop valEpocw Maximum [1/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 57.0
model_train val_loss valEpocw [1/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 153.7
model_train val_loss  valEpocw [1/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 299.9
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [75.01979752 97.2137279  89.67361509 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058  1.98706156 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.54273218 97.84237521 92.05906361
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 89.11075645 96.13796128 96.24273583 96.9067141
 89.79300934 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [70.90922382 97.2137279  89.51523495 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.05906361
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 89.09857336 96.13796128 96.24273583 96.9067141
 89.79300934 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [86.61545108  3.85812505 34.29772522  4.18816433  2.11573406  8.34521623
  4.25093773 10.66961963  2.1475515   6.67716735  1.54205049  1.80735245
  0.9987091   5.03359315  2.1013824   3.92889585  3.43686921  2.04705577
  1.01448588  1.23850266  1.49156686  0.53962118  0.90651624  1.50810653
  6.07581944  4.05336043  9.09832409  3.42111365  2.36359438  1.20956313
  1.86191238  0.97204458  2.38867058  1.28417674  1.41842276  1.43952803
  2.48965144  1.71873452  1.90754109 16.70204543  3.24802827 16.27089398
  4.12380329  5.76515128  4.91369628 10.50177142  2.26508146  1.73436925
  2.32719069  1.83447339  1.32900941  1.28294932  1.03698244  2.73339368
  1.66860224  3.22482757 29.38296062  7.62781144  6.75410164  3.81176086
 27.82364009  3.8456576  13.61993323  6.09830191  4.64511515  3.64998068
  4.35065999  5.65069368  2.48893196  5.23986921  0.38147172  7.4270763
  4.86728319 10.29398289  5.88903945  5.07498556  1.39488744  1.88354955
  0.20242698  0.9579264 ]
Accuracy th:0.5 is [45.49164849 97.2137279  73.58950305 97.02489005 97.26733349 78.62477309
 78.93056858 77.61357683 79.77120162 96.43157369 80.22563078 98.52097319
 99.41399349 80.74584861 79.74318052 96.56680596 96.29512311 79.50073708
 98.65376884 98.30776915 80.86524287 80.7019895  98.38695922 79.53484972
 80.81651052 96.65086926 94.0778012  79.08042056 98.01293844 80.09405344
 97.30875598 98.57457877 96.36213009 98.02024829 86.01990716 79.76389177
 79.39474422 89.9952486  97.11504489 76.55242992 79.9770958  92.05906361
 78.95127983 78.47370281 96.9627563  93.87434364 98.02877645 98.57336046
 84.87469695 87.81691256 87.06887099 98.55508583 98.99976852 79.13524445
 98.70615611 79.44956811 73.94281259 93.24691463 96.24273583 96.9067141
 89.79300934 97.17717864 90.42531158 79.49464553 98.42838172 80.0051169
 98.20786784 78.57482243 80.54726429 97.55972759 81.19296792 95.99054592
 80.25243357 95.45083515 78.67350544 82.58793143 85.51796396 80.08552527
 81.23317211 99.14718388]
Accuracy th:0.7 is [45.53063437 97.2137279  73.58950305 97.02489005 97.26733349 78.62477309
 78.93056858 77.67571058 79.77120162 96.47299619 80.22563078 98.52097319
 99.41399349 81.16129189 79.74318052 96.56680596 96.29512311 79.50073708
 98.65376884 98.30776915 81.2599749  80.7019895  98.38695922 79.53484972
 81.26362983 96.65086926 94.0778012  79.08042056 98.01293844 80.09405344
 97.30875598 98.57457877 96.36213009 98.02024829 86.2477309  79.76389177
 79.39474422 90.25840328 97.11504489 76.55242992 80.45710944 92.05906361
 78.95127983 78.47370281 96.9627563  93.87434364 98.02877645 98.57336046
 86.44997015 88.60515832 87.22603282 98.55508583 98.99976852 79.13524445
 98.70615611 79.44956811 73.94281259 93.62702696 96.24273583 96.9067141
 89.79300934 97.17717864 90.60562128 79.49464553 98.42838172 80.0051169
 98.20786784 78.57482243 80.54726429 97.55972759 81.19296792 95.99054592
 80.25243357 95.45083515 78.67350544 82.66833981 85.60446388 80.08552527
 81.23317211 99.14718388]
Avg Prec: is [56.06474038  3.21034449 11.18376256  3.2842074   2.25017503  3.74833505
  3.35408784  5.56314236  2.53443646  3.71517661  1.50413256  1.6143402
  0.63673838  4.93898121  2.59803013  3.1239344   3.59591819  2.61440425
  1.34100207  1.77302372  1.97136903  0.87363646  1.8455903   2.35792152
  5.21436588  3.59585818  6.60594391  3.37204322  2.12652001  1.8911136
  2.53921542  1.32549077  3.8138387   1.67918765  2.30394355  2.38870171
  3.08181621  2.61928025  2.99496109  7.36551124  2.21538583  8.25597133
  3.35043104  3.99640424  3.21568106  6.36283656  2.0096292   1.48150401
  2.14046076  1.56019408  1.83564819  1.56696787  1.07085181  3.05874775
  1.40352262  2.68114788 11.27512258  3.81801919  3.91068626  2.84853699
 10.73982977  2.26514838  3.84360904  2.95512569  1.64505111  2.49128753
  1.81643473  4.19065762  1.26817068  2.27379425  0.1747174   3.41622424
  1.83185705  4.66131037  3.9551433   3.04553252  0.92460814  1.92060753
  0.12526167  0.7449155 ]
mAP score regular 5.96, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [78.71539976 97.22450607 90.06153923 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086  2.18252485 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.14440541 97.82744101 92.37362035
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 88.9104816  96.39235618 96.16314124 96.78102499
 90.1462491  97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [77.42481999 97.22450607 89.58566908 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.37362035
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 89.0898672  96.39235618 96.16314124 96.78102499
 90.13379176 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [89.8108374   4.84605711 40.63976366  5.19188899  1.64216353 11.12054919
  5.3935147  12.99184815  2.24184369  7.43224464  1.66281359  2.09376032
  1.24263052  5.00737591  2.14160005  4.1015375   3.40740473  2.03375236
  0.89755372  1.18456468  1.37951459  0.53055422  0.89990777  1.37100164
  5.4184589   4.45789169  8.59051049  2.76452607  9.46424158  1.17925754
  1.65847841  0.86403284  2.21073344  1.18508148  1.18938614  1.21672249
  2.29592521  1.90183972  1.86999232 17.75700419  3.35487317 16.99927401
  3.90782126  5.14611204  4.86181589 10.51191888  2.32656754  1.82692648
  2.3876718   2.113967    1.4086424   1.41801902  1.12339114  2.6237968
  1.62302154  2.86481171 32.30273668  7.2056232   6.53408806  4.04464736
 30.37402513  4.0000317  15.08539744  6.67111544  6.21494014  3.39548163
  5.12095416  5.62684054  1.99202998  4.82304836  0.47861644  6.87001765
  4.02038288 10.85102959  6.76116718  4.93794471  1.21602382  1.81525213
  0.15812627  0.85901479]
Accuracy th:0.5 is [45.32974562 97.22450607 72.3771084  96.96290206 97.90716795 78.05266961
 78.18970028 76.89662905 79.55751551 96.41976231 79.81662805 98.5325261
 99.34972718 79.11403443 79.63475098 96.31262924 96.21047911 79.13645763
 98.78167277 98.34068316 80.14550166 80.46191793 98.31327703 79.22615043
 79.17382963 96.52938685 94.3393876  79.0941027  97.81747515 79.76929018
 97.52597354 98.67204823 96.39983058 98.18870369 86.37915141 79.36567257
 79.2734883  91.7706854  97.0276802  76.2338989  79.27847124 92.37362035
 78.47372748 78.10249894 97.03764606 94.02795426 98.18621222 98.77668984
 85.06614844 87.6971373  85.63171139 98.55993223 98.87385704 78.53850562
 98.6969629  79.12649177 72.72093081 94.29204973 96.16314124 96.78102499
 90.13379176 97.04761193 90.22348457 79.03929043 98.32075143 79.88389765
 98.13139996 78.21959788 80.4345118  97.53593941 80.95273688 96.07843137
 80.12058699 95.44559882 78.12243067 83.70829908 87.3084685  79.79669632
 81.02748088 99.15040985]
Accuracy th:0.7 is [45.54152029 97.22450607 72.3771084  96.96290206 97.90716795 78.05266961
 78.18970028 76.91905225 79.55751551 96.41976231 79.81662805 98.5325261
 99.34972718 79.49522884 79.63475098 96.31262924 96.21047911 79.13645763
 98.78167277 98.34068316 80.42952886 80.46191793 98.31327703 79.2286419
 79.55004111 96.52938685 94.3393876  79.0941027  97.81747515 79.76929018
 97.52597354 98.67204823 96.39983058 98.18870369 86.63826395 79.36567257
 79.2734883  91.94010514 97.0276802  76.2338989  79.49522884 92.37362035
 78.47372748 78.10249894 97.03764606 94.02795426 98.18621222 98.77668984
 86.65321275 88.01355358 85.79614819 98.55993223 98.87385704 78.53850562
 98.6969629  79.12649177 72.72093081 94.5486708  96.16314124 96.78102499
 90.13379176 97.04761193 90.3929043  79.03929043 98.32075143 79.88389765
 98.13139996 78.21959788 80.4345118  97.53593941 80.95273688 96.07843137
 80.12058699 95.44559882 78.12243067 83.79051748 87.4280589  79.79669632
 81.02748088 99.15040985]
Avg Prec: is [53.15806372  3.68235813 14.98719877  4.53728952  1.46677915  4.61863339
 14.73794211  8.77804198  8.64426154  5.70461942  3.42316674  4.86835454
  2.55570684  5.77037136  3.04786801  3.76957455 14.22140539  6.55274276
  1.57020558  2.75767965  3.54615203  1.57123692  1.10862659  5.14456196
  5.51005445  7.51082021  7.71870763  4.67925471  3.89176625  4.12982512
  2.04187019  0.83696392  3.01131841  1.10389197  1.6115122   2.09246821
  1.92588397  2.18048823  2.24069101  6.18049099  1.71952522  5.99667442
  2.1744455   2.70419682  2.36883783  4.83556616  1.65106816  1.02259539
  1.3814166   1.15762381  1.17654891  0.96602777  0.72519856  2.28461543
  0.83610754  1.81636896  9.75676788  2.81193763  3.7288406   2.59969809
  7.71842964  2.09760155  3.04163952  2.5107171   1.34314844  1.80837193
  1.50056404  3.41630734  1.10457603  2.30610044  0.19770969  3.33047302
  1.6104389   3.82534563  3.54416515  2.30672236  0.59422214  1.48103993
  0.12877586  0.59269348]
mAP score regular 6.36, mAP score EMA 4.16
Train_data_mAP: current_mAP = 5.96, highest_mAP = 5.96
Val_data_mAP: current_mAP = 6.36, highest_mAP = 6.36
tensor([0.4098, 0.5499, 0.5939, 0.5655, 0.4774, 0.5340, 0.5518, 0.4367, 0.4660,
        0.4604, 0.4458, 0.5183, 0.5113, 0.4099, 0.4405, 0.4551, 0.5194, 0.4461,
        0.6329, 0.4937, 0.4137, 0.4901, 0.4228, 0.4875, 0.4596, 0.5673, 0.5179,
        0.5553, 0.2808, 0.4663, 0.4450, 0.3585, 0.3503, 0.4423, 0.4927, 0.5163,
        0.5296, 0.5237, 0.3892, 0.5190, 0.5213, 0.5243, 0.4422, 0.4658, 0.4016,
        0.3930, 0.4631, 0.4898, 0.4330, 0.3966, 0.3462, 0.6032, 0.3360, 0.4495,
        0.5284, 0.4290, 0.5800, 0.5598, 0.5702, 0.4022, 0.4237, 0.5075, 0.5919,
        0.4408, 0.5641, 0.4942, 0.4379, 0.5307, 0.4081, 0.4556, 0.3138, 0.5923,
        0.6321, 0.4845, 0.6036, 0.5134, 0.6522, 0.6281, 0.4973, 0.4447],
       device='cuda:0')
Max Train Loss:  tensor([22.6752, 12.8600, 21.6369, 11.3256,  8.5185, 12.1664,  8.9705, 13.2562,
        20.8776, 13.9151, 13.1578,  4.7438,  4.6517, 10.8665, 12.8211, 17.1777,
         6.0553,  6.1558,  5.5470, 19.2142,  3.5878,  8.0282, 18.0003, 11.4203,
        13.7599, 16.7864, 15.7741, 15.6883, 25.1603, 11.4472, 11.1505,  4.2479,
         8.5984, 11.1036, 20.3763, 14.6871, 17.2572, 17.4468,  4.7393, 17.7070,
         4.4213, 16.5731,  6.0023, 15.7561,  3.1688,  7.3862,  8.9845,  5.3703,
        10.6829,  4.8436,  4.2382,  3.2830,  2.8164,  9.2364, 13.7872, 14.5235,
        19.6029, 16.5959, 16.7489,  7.3206, 15.4154, 10.4594, 12.7156,  6.7338,
         3.0082, 14.2957,  5.2139, 17.5714,  6.2648,  3.8444,  5.7015,  8.6089,
         7.3395,  7.8551, 16.6532,  8.6082,  3.3784, 15.3069,  4.3018,  7.3809],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [2/80], Step [000/642], LR 1.0e-04, Loss: 25.2
Max Train Loss:  tensor([26.1755, 13.1676, 23.9596, 14.7528, 11.4614, 10.8964,  8.2004, 13.2624,
        15.0767, 10.5690,  9.9726,  8.0794,  7.0955, 14.9109, 17.1511, 17.0727,
        11.6289,  9.2729,  8.8328, 13.5474, 10.4598, 15.0963, 15.8304, 16.5488,
        12.0899, 14.8358, 15.1039, 14.2693,  8.8336, 12.1993, 11.1262,  9.0736,
        15.8099,  9.3759, 13.2214, 12.3532, 17.3321, 14.8317, 15.2670, 18.1741,
        10.9086, 17.8392, 14.8396, 20.3895, 15.4188, 15.3350,  8.5325, 10.7172,
        14.2725, 15.2749, 12.6261, 16.5797, 13.3767,  9.1780,  9.8929, 15.0011,
        25.7450,  8.3236, 11.9515, 14.3873, 21.6170, 10.4315,  9.2547, 10.1578,
        12.6079, 16.6969,  8.0117, 10.1414,  8.3986,  7.1154, 21.6028, 13.7629,
         9.1004, 10.3377, 10.2677, 10.5920, 13.8729, 13.4801,  6.7567,  7.2659],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [2/80], Step [100/642], LR 1.0e-04, Loss: 26.2
Max Train Loss:  tensor([27.2916, 10.3991, 21.9781, 12.8041,  9.0391,  8.5978, 12.8766, 13.4437,
        17.3343, 10.5662,  7.9409,  7.4635,  6.6694, 14.6024, 18.1120, 16.4498,
        14.2132,  5.7226,  9.3907, 10.1895, 11.8386, 17.8766, 16.0451, 15.0779,
        15.3511, 16.1369, 17.0519, 11.0254,  9.6068, 11.5727, 13.0158,  7.6296,
        17.1886, 10.4444, 13.7817, 15.0322, 18.6278, 10.2578, 14.8829, 15.8444,
        13.1659, 17.6122, 11.1563, 16.9219, 13.0779, 13.3530,  7.9685,  9.7687,
         6.4049, 15.4532, 12.1667,  9.1423, 12.9657, 10.7992, 10.4522, 14.0537,
        26.3016, 13.7009, 13.7601, 15.2369, 13.7408, 16.4435, 14.3215,  9.5999,
        14.1231, 18.0843,  9.0699, 11.2190,  9.2752,  7.8925,  2.4395, 14.5832,
        13.5764, 11.2218, 13.2668,  9.2893, 14.9237, 10.7827,  5.0600,  8.5595],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [2/80], Step [200/642], LR 1.0e-04, Loss: 27.3
Max Train Loss:  tensor([25.2705, 13.7487, 21.2242, 14.1952,  9.1725,  9.3692, 10.8327, 12.4081,
        16.2150,  9.5682,  9.6950,  8.4533,  7.8230, 14.1946, 19.1689, 12.8078,
        15.7199,  8.3966,  8.0576, 14.7370, 11.4187, 17.1474, 18.4433, 15.2677,
        13.1892, 14.5055, 15.5282,  7.8738, 10.2967, 10.7452, 11.7712,  9.4840,
        16.4917,  8.4746, 16.3926, 16.0054, 16.8203, 10.6424, 15.0150, 14.8529,
        14.9533, 15.1837, 12.4840, 16.3035, 13.9022, 16.8610,  8.6775,  9.3320,
         5.3675, 15.3039, 12.6582, 10.6319, 12.9985,  8.7279, 13.7591, 13.4831,
        20.3320, 13.5897, 15.2939, 16.0665, 15.5310, 10.0668, 11.2642, 12.0392,
        15.5942, 16.4600,  8.9435, 15.9739,  9.7036,  8.6624,  2.4428, 13.7382,
        11.1600, 15.9700, 12.4859, 12.5512, 12.9570, 13.1147,  5.1400,  7.0151],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [2/80], Step [300/642], LR 1.0e-04, Loss: 25.3
Max Train Loss:  tensor([23.8789, 10.8175, 23.9584,  8.4025, 15.5756, 12.4873, 11.7042, 11.1559,
        14.8578,  7.8719, 12.0823,  7.3972,  8.5938, 15.6976, 19.2281, 13.3894,
        11.6750,  8.6694, 11.2280, 12.1141, 13.6596, 16.4845, 16.6633, 14.6148,
        14.2115, 18.0019, 16.7580,  8.4618,  8.7558, 10.6902, 12.0664,  7.4808,
        17.1605,  7.6801, 13.0164, 14.3604, 14.2193,  6.8660, 14.9566, 19.6708,
        13.0375, 21.5314,  8.3934, 16.3203, 14.9259, 15.2657,  6.6319,  7.7854,
         8.4745, 14.2896, 12.1950,  7.4131, 13.8052,  7.6811, 11.0480, 13.3975,
        21.9048, 15.1982, 14.9872, 14.9881, 20.0845, 12.8483,  8.0056,  9.1440,
        13.4004, 17.8479,  7.5554,  9.3138,  8.0192,  8.8419,  2.3617, 15.5078,
         7.1095, 12.1503, 18.5786,  8.8133, 12.7724, 14.0016,  6.5138,  7.7349],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [2/80], Step [400/642], LR 1.0e-04, Loss: 24.0
Max Train Loss:  tensor([25.2507, 15.3762, 21.3196,  8.4272,  7.9281, 10.8835, 12.1595, 11.6492,
        18.8079, 14.2419,  7.7839, 10.4486,  7.3128, 15.8992, 19.1840, 18.4682,
         8.0899,  6.2994, 17.2400, 11.9804, 10.8035, 17.3845, 16.0910, 14.8321,
        10.2614, 15.4493, 17.3165, 13.3371, 10.0589,  9.3620, 13.7792,  7.4709,
        16.8059,  7.1057, 14.8657, 13.9636, 13.5746,  8.3076, 17.1292, 20.9711,
        13.9676, 21.1171, 10.8760, 20.0208, 16.7138, 15.2220,  8.1852, 10.9150,
         7.5561, 16.6870, 13.6204, 12.5934, 14.3765, 10.1085, 10.2326, 16.5894,
        24.3194, 14.5907, 12.5619, 16.8450, 17.9467, 10.7634, 14.1218, 12.4321,
        15.1990, 21.0291,  9.7423, 12.1163,  8.5865,  8.4825,  2.7172, 15.1143,
        12.7526, 13.4844, 20.4231, 11.2253, 13.8101, 13.5229,  5.6864,  8.0406],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [2/80], Step [500/642], LR 1.0e-04, Loss: 25.3
Max Train Loss:  tensor([27.3677, 11.2183, 26.2829, 13.4375, 13.5706, 10.3070, 12.4287, 10.8988,
        15.3037, 13.3740,  8.8321,  9.2078,  6.5927, 15.0556, 19.5073, 16.0396,
        17.5381,  6.6895,  9.3226, 11.7178, 11.8036, 15.8060, 14.9990, 16.6870,
        15.1049,  9.7085, 13.9329, 10.5693,  9.3709,  8.3614, 10.2211,  7.2915,
        15.9228, 11.9824, 13.3521, 16.1652, 15.0287,  7.9366, 15.1486, 21.1137,
        12.6571, 15.7997,  7.6020, 17.2988, 14.9069, 14.6900,  6.5701,  9.6232,
         8.5900, 14.6349, 13.4706,  7.4013, 13.0530,  7.7363,  9.3132, 14.1813,
        22.4813, 12.7258, 16.5073, 15.6775, 14.9167,  9.4763, 11.9091, 11.1410,
        15.2890, 17.7821, 10.8714, 15.4076,  9.1862, 12.3766,  3.2416, 17.4640,
        10.8433, 10.0252, 11.0643, 15.3312, 13.7127, 15.9002,  4.9609,  7.8845],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [2/80], Step [600/642], LR 1.0e-04, Loss: 27.4
Max_Val Meta Model:  tensor([ 25.7667,  35.8761,  51.7092,  26.6280,   7.5503,   8.7340,   9.3376,
         12.5506,  15.0388,   9.9008,   8.0923,   9.9172,   8.0485,  13.0672,
         18.1083,  13.6900, 161.8983,   5.9773,   8.4477,  10.5621,   9.8813,
         16.1204,  15.2579,  13.6661,  18.4907,  12.2021,  19.1869,   6.9599,
          9.3257,  10.6686,   7.7622,   5.4409,  15.6555,   5.8164,  11.5874,
         12.7231,   5.2883,   8.9586,  14.3455,  28.3387,  14.2405,  18.8554,
          8.2563,  17.5444,  15.4333,  15.5529,   6.1026,   8.2439,   5.6275,
         14.9381,  12.4307,   7.9081,  13.7361,   5.7483,   9.0033,  12.8791,
         17.2359,  10.6915,  14.2823,  14.3762,  11.8795,  16.6236,   8.2537,
          8.3166,  13.0941,  16.2159,   6.9096,  10.0913,   9.5351,  13.2772,
          2.5620,  15.8280,  15.2128,   9.5463,  12.1467,   9.9487,  13.3470,
         11.1694,   5.3536,   6.0843], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 23.8682,  32.5858,  51.1677,  27.5369,   8.5612,   9.1013,  10.0260,
         13.1395,  16.2623,  10.3077,   8.9617,  10.4763,   8.7944,  13.7545,
         19.3333,  14.9044, 151.9284,   6.8793,   9.9106,  11.7775,  11.1216,
         17.4232,  16.3704,  15.2361,  18.0749,  12.5315,  20.1017,   7.9590,
         10.0728,  11.4327,   8.5931,   6.2183,  16.4607,   6.6534,  12.8409,
         13.7446,   6.1390,   9.3529,  15.4175,  27.1059,  15.5608,  21.4672,
          9.3297,  18.8332,  16.1920,  16.3746,   6.9035,   9.3060,   6.4559,
         15.8006,  13.4604,   9.0951,  14.6049,   6.7741,  10.2431,  14.3766,
         21.6571,  11.9898,  14.7935,  15.5832,  13.4350,  16.4712,   9.4236,
          9.4492,  14.6830,  17.5146,   7.9109,  11.6215,  10.4984,  14.2714,
          2.9290,  16.7584,  16.6062,  10.3423,  13.1360,  11.0452,  14.3807,
         12.2048,   6.3843,   7.0006], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 58.2423,  59.2588,  86.1559,  48.6906,  17.9317,  17.0427,  18.1685,
         30.0870,  34.8965,  22.3909,  20.1017,  20.2136,  17.1991,  33.5557,
         43.8869,  32.7484, 292.5139,  15.4225,  15.6585,  23.8554,  26.8809,
         35.5526,  38.7215,  31.2530,  39.3263,  22.0912,  38.8158,  14.3341,
         35.8771,  24.5172,  19.3092,  17.3465,  46.9852,  15.0444,  26.0627,
         26.6197,  11.5908,  17.8584,  39.6171,  52.2234,  29.8507,  40.9442,
         21.1003,  40.4275,  40.3211,  41.6696,  14.9079,  19.0004,  14.9091,
         39.8378,  38.8808,  15.0793,  43.4716,  15.0714,  19.3865,  33.5109,
         37.3382,  21.4168,  25.9429,  38.7445,  31.7113,  32.4530,  15.9200,
         21.4389,  26.0282,  35.4433,  18.0646,  21.8989,  25.7234,  31.3252,
          9.3348,  28.2914,  26.2729,  21.3453,  21.7628,  21.5131,  22.0498,
         19.4314,  12.8375,  15.7418], device='cuda:0')
Outer loop valEpocw Maximum [2/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 161.9
model_train val_loss valEpocw [2/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 151.9
model_train val_loss  valEpocw [2/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 292.5
Max_Val Meta Model:  tensor([38.5824, 47.0004, 55.9013, 46.7839, 28.6041, 45.6694, 59.9567, 35.9191,
        41.3562, 37.2371, 37.0930, 49.5476, 40.2404, 35.3078, 37.7517, 32.9029,
        48.3947, 36.7904, 54.9066, 47.4893, 34.7944, 38.5582, 35.1647, 38.2249,
        36.8668, 44.0263, 41.1822, 42.0356, 27.3221, 37.6971, 35.8329, 31.1378,
        32.9073, 35.6410, 38.8819, 39.5068, 39.9333, 40.8195, 34.4031, 49.2470,
        43.9373, 40.8588, 34.9485, 38.9681, 35.8253, 33.5445, 35.5985, 35.6466,
        34.3014, 34.9404, 29.9551, 44.8787, 29.6638, 36.2173, 39.0982, 38.2565,
        42.6322, 45.3090, 44.3061, 34.4175, 37.8106, 40.1069, 39.3816, 36.4637,
        41.8467, 41.2592, 36.2632, 39.3429, 32.1800, 36.6752, 27.1626, 42.8433,
        41.7423, 38.8642, 41.4324, 48.2778, 42.0981, 36.2855, 38.2461, 35.5755],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([31.9535, 14.8383, 41.5540, 10.0799,  7.2886, 19.6460, 53.3879, 24.5211,
        21.8829, 16.5997, 13.0120, 74.3531,  9.9029, 12.7842, 20.4256, 13.6629,
        10.5258, 10.8085, 10.5818, 19.2014, 11.2737, 17.0710, 16.6432, 15.0099,
        15.6907, 12.6009, 20.5190, 10.7886, 12.2289,  8.9218,  8.4636,  6.7875,
        18.0258,  6.9285, 12.5178, 12.9181,  7.8174,  6.6834, 16.5874, 12.0557,
        13.6640, 13.5420,  7.8246, 16.2039, 15.9298, 14.3136,  6.8240,  8.3502,
         6.5473, 17.0756, 14.0208,  8.6049, 15.2539,  6.8301, 10.1634, 15.7933,
        14.1681, 11.1927, 11.0207, 16.4155, 11.1329,  7.4745,  7.5927,  9.1889,
        13.6526, 17.8610,  8.3322, 12.3635,  8.5768,  8.6256,  3.4626, 10.1906,
         8.5266,  9.2428,  9.3841, 10.1521, 13.1818, 10.3131,  6.2627,  7.0608],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 74.0992,  24.6011,  64.6781,  17.2629,  17.8901,  34.9828,  84.8371,
         54.9570,  47.0516,  35.2573,  27.9528, 144.9129,  19.3438,  29.4814,
         45.3601,  32.9161,  18.4540,  22.6238,  15.9173,  33.6740,  26.3794,
         35.6307,  38.6961,  32.0600,  33.8635,  22.8242,  39.9734,  19.9089,
         36.9632,  18.6747,  18.8313,  17.2355,  46.1011,  14.9607,  25.9620,
         26.5035,  15.1053,  12.6912,  39.8773,  21.1693,  26.2772,  26.3851,
         17.4720,  34.3672,  35.9290,  34.2119,  14.8141,  16.9004,  14.8623,
         39.9253,  38.7250,  14.9930,  42.3881,  14.6961,  19.6531,  35.0504,
         23.8188,  19.4882,  19.4835,  39.1082,  23.8559,  14.3023,  13.6403,
         20.1401,  25.8709,  35.4359,  17.9128,  25.0238,  20.9528,  18.6114,
          9.5616,  18.0007,  15.9506,  18.8361,  17.8228,  16.1880,  22.0639,
         18.8752,  12.5987,  15.5701], device='cuda:0')
Outer loop valEpocw Maximum [2/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 60.0
model_train val_loss valEpocw [2/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 74.4
model_train val_loss  valEpocw [2/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 144.9
Max_Val Meta Model:  tensor([37.5158, 42.2145, 40.5752, 42.4782, 27.5418, 37.2215, 37.9956, 34.9890,
        38.1089, 35.7316, 36.8104, 39.8832, 39.6251, 35.2939, 37.4665, 32.3707,
        41.3333, 26.1943, 41.8147, 28.1732, 34.4040, 38.0679, 35.2131, 37.6389,
        35.9936, 28.0042, 40.1551, 34.1302, 27.4605, 37.8649, 35.7920, 31.5835,
        32.8706, 34.6865, 38.4602, 38.5802, 39.8594, 39.0142, 33.8133, 43.9490,
        31.6598, 42.9565, 36.9394, 39.8113, 36.3410, 37.3310, 44.5661, 42.3236,
        42.2786, 35.3820, 30.5556, 41.6245, 31.1899, 36.5912, 45.0120, 41.8065,
        57.1134, 39.1176, 42.7366, 34.4286, 46.4657, 38.9220, 38.5365, 35.8310,
        41.8899, 41.2977, 35.6897, 38.7991, 31.6407, 36.3738, 27.2250, 42.2475,
        40.2727, 43.5146, 40.1730, 32.7442, 39.5435, 33.8938, 37.3086, 34.5995],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([19.4246,  8.4797,  5.1334,  7.2548,  6.7146,  5.0907,  6.4348,  9.7106,
        15.3019,  6.7975,  8.0583,  7.6246,  6.5524, 14.3363, 18.6329, 12.3903,
         6.6896,  5.2818,  8.0274,  8.9910, 10.8974, 16.2561, 16.1259, 13.7475,
         7.7429,  4.7725, 16.3739,  9.1260, 10.7995,  8.2636,  8.7365,  7.0127,
        18.2675,  6.3936, 11.7948, 12.2155, 10.5354,  9.0564, 15.4370, 25.0263,
        14.7166, 38.0731, 37.1475, 34.2488, 26.0497, 31.0551,  9.6031, 12.2930,
        21.2597, 17.1941, 17.8856, 26.2320, 22.2507, 12.3744, 30.4678, 38.3746,
        53.0129, 11.6560, 10.2925, 15.6574, 77.0002,  6.3006,  9.9054, 10.5073,
        14.9925, 17.2053,  9.1434, 11.4962,  8.6667,  6.9986,  2.9305, 11.2809,
         7.8769, 19.2608,  7.8418, 10.5313, 14.1426,  8.1894,  5.6132,  6.3595],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 45.1335,  14.5022,   9.0046,  12.7941,  16.4890,   9.7910,  12.4921,
         21.5844,  32.4857,  14.5507,  17.1566,  14.7355,  12.8307,  33.1781,
         41.5694,  30.0075,  12.5446,  13.5610,  13.5910,  22.3427,  25.6613,
         34.2179,  37.3439,  29.6343,  16.6888,  11.0571,  32.1869,  18.2439,
         32.2724,  16.9809,  19.3155,  17.3636,  46.7799,  13.9434,  24.5356,
         25.2638,  19.8325,  17.4995,  37.4464,  46.5865,  36.4499,  74.9638,
         82.7711,  72.8446,  59.2423,  73.6923,  17.9201,  20.5365,  42.1580,
         39.6943,  48.5937,  46.0065,  60.3725,  26.2337,  59.2313,  83.7124,
         94.2431,  21.6222,  18.5206,  37.1390, 164.0471,  12.0868,  18.0030,
         23.0389,  28.2510,  33.9618,  19.7184,  23.4106,  21.3398,  14.9927,
          7.8906,  20.1022,  15.0443,  38.7510,  15.0592,  21.1669,  24.6676,
         15.3226,  11.3492,  14.1821], device='cuda:0')
Outer loop valEpocw Maximum [2/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 57.1
model_train val_loss valEpocw [2/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 77.0
model_train val_loss  valEpocw [2/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 164.0
Max_Val Meta Model:  tensor([39.3493, 44.5412, 45.0537, 43.6607, 27.1757, 37.3023, 38.1040, 34.4063,
        38.1193, 36.4780, 36.8189, 40.0094, 39.4770, 34.8503, 37.6355, 32.3553,
        43.0385, 24.7662, 43.2772, 27.9105, 33.8774, 38.2164, 35.3628, 38.2465,
        36.9801, 27.8665, 41.0325, 34.1305, 26.9121, 37.4664, 35.2729, 30.5129,
        31.9908, 34.7309, 38.3951, 42.8475, 33.1625, 39.6707, 33.8934, 45.1773,
        27.1101, 35.8314, 34.5330, 39.1607, 35.0534, 32.8046, 36.6717, 37.1161,
        37.2711, 35.1562, 29.6330, 44.1614, 29.9579, 41.1106, 35.4254, 38.1098,
        36.2902, 35.7797, 48.7015, 33.8514, 28.7231, 39.1676, 42.2721, 35.9449,
        42.0187, 41.0853, 35.2889, 60.3607, 31.7499, 35.9948, 26.4431, 43.7069,
        37.5976, 38.6585, 56.6719, 32.4862, 40.8467, 42.1480, 37.2432, 41.8703],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 22.9301,  11.2662,  16.4721,   8.4774,   7.9957,   9.5253,   9.1335,
         11.6126,  16.9580,  12.9557,  10.0811,  10.6479,   8.8466,  14.6000,
         21.1952,  13.8114,   8.9551,   6.4860,  10.5613,  10.2077,  11.7370,
         17.9263,  17.6515,  15.3564,  13.9479,   5.6796,  19.5327,   7.7227,
         12.7132,   9.8254,   9.2509,   7.4530,  18.2738,   7.6505,  13.4125,
         14.8121,   6.5836,   7.6527,  17.2179,  14.3888,  10.0105,  14.7250,
          8.4773,  17.3922,  16.3834,  14.4655,   8.0190,  10.4284,   8.0571,
         17.6848,  14.6756,  10.1065,  16.1031,   8.6107,  10.4458,  15.9790,
         13.6709,  10.0455,  13.9319,  16.8682,  11.6886,   8.3382,   9.5389,
         10.0118,  15.2781,  18.8806,   8.9992,  13.5146,   9.2869,   8.4998,
          3.8854,  11.5984,   8.2632,  10.6991, 140.7380,   9.7112,  13.9570,
         14.2877,   6.8806,   9.0487], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 54.2233,  18.8190,  27.5046,  14.7219,  19.9471,  18.4093,  17.8100,
         26.3755,  36.1447,  27.7979,  21.5854,  20.6953,  17.4463,  34.1699,
         47.3779,  33.6172,  16.3845,  17.5470,  17.3748,  25.8027,  28.1919,
         37.7563,  40.8651,  32.5824,  29.9034,  13.3408,  38.1108,  15.3792,
         39.0801,  20.4528,  20.7863,  19.1704,  48.2148,  16.6864,  28.0746,
         28.1070,  12.8643,  14.2792,  42.0552,  25.8844,  28.0791,  29.5828,
         19.0083,  36.6937,  37.7276,  35.2964,  16.5012,  18.7618,  16.6492,
         41.2243,  40.9698,  16.4074,  44.4263,  16.3828,  20.9607,  35.5605,
         23.6161,  19.3631,  23.4756,  40.8796,  28.6338,  16.0176,  15.9752,
         22.0245,  28.0787,  37.6227,  19.6535,  22.4392,  22.9488,  18.4882,
         10.8469,  20.2052,  15.7001,  21.8224, 262.5435,  19.6807,  23.6645,
         22.9505,  14.0521,  17.4193], device='cuda:0')
Outer loop valEpocw Maximum [2/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 60.4
model_train val_loss valEpocw [2/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 140.7
model_train val_loss  valEpocw [2/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 262.5
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [77.61601345 97.2137279  89.71869251 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.05906361
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 89.3263971  96.13796128 96.24273583 96.9067141
 89.79300934 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [70.5315481  97.2137279  89.51523495 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.05906361
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 89.14365078 96.13796128 96.24273583 96.9067141
 89.79300934 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [90.20736638  4.84306802 38.50283476  4.85702858  2.25709497  9.79249836
  4.20765979 10.64719345  2.25520989  8.03659581  1.80100898  2.05629685
  1.34247052  5.44608735  2.05208008  3.65723574  3.51855422  2.16263907
  0.94669384  1.2478008   1.52894465  0.51706897  0.90210979  1.49029277
  6.24381307  4.94735176 10.3048964   3.30939133  3.39963902  1.17359523
  1.7459853   0.92485887  2.34191718  1.26572893  1.39853127  1.42121158
  2.61490183  1.61663069  1.86971879 22.75692276  3.22493282 25.02043741
  3.90132886  5.42099399  4.66035006 10.4030637   2.19096542  1.72815821
  2.13444992  1.71170002  1.18802066  1.18788228  0.92198885  2.60061866
  1.58139277  2.90680223 35.74437235  6.68099786  7.8083597   3.5326246
 23.41204442  3.44239202 12.65367032  5.49376873  4.17561335  3.71832487
  3.89085354  5.38763646  2.70717704  5.48817897  0.33332406  7.9138511
  4.80753566 10.34506325  6.41364363  5.84085101  1.27374462  1.88920123
  0.21666386  0.89996588]
Accuracy th:0.5 is [45.29550079 97.2137279  73.56391857 97.02489005 97.26733349 78.67959698
 79.02681498 77.67449227 79.87232124 96.43888354 80.2707082  98.52097319
 99.41399349 80.68249656 79.81506073 96.56680596 96.29512311 79.57261729
 98.65376884 98.30776915 80.86767949 80.81285559 98.38695922 79.67739184
 80.72635567 96.65086926 94.0778012  79.24489224 98.01293844 80.12207454
 97.30875598 98.57457877 96.36213009 98.02024829 86.37930824 79.78460301
 79.45444134 90.37414262 97.11504489 76.54146514 80.04288447 92.05906361
 79.04996284 78.63330125 96.9627563  93.87434364 98.02877645 98.57336046
 85.45461191 87.65244088 86.86785005 98.55508583 98.99976852 79.27778658
 98.70615611 79.59454685 73.96108722 93.38823845 96.24273583 96.9067141
 89.79300934 97.17717864 90.3168821  79.52023002 98.42838172 79.94541977
 98.20786784 78.62720971 80.68493318 97.55972759 81.27703123 95.99054592
 80.36086305 95.45083515 78.78437154 82.83890303 85.8858932  80.23537725
 81.32210865 99.14718388]
Accuracy th:0.7 is [45.32839512 97.2137279  73.56391857 97.02489005 97.26733349 78.67959698
 79.02681498 77.7622105  79.87232124 96.47055957 80.2707082  98.52097319
 99.41399349 81.06870043 79.81506073 96.56680596 96.29512311 79.57261729
 98.65376884 98.30776915 81.28434108 80.81285559 98.38695922 79.68226508
 81.14301726 96.65086926 94.0778012  79.24489224 98.01293844 80.12207454
 97.30875598 98.57457877 96.36213009 98.02024829 86.61200521 79.78460301
 79.45444134 90.61658606 97.11504489 76.54146514 80.51071503 92.05906361
 79.04996284 78.63330125 96.9627563  93.87434364 98.02877645 98.57336046
 87.10054702 88.33225716 87.01892033 98.55508583 98.99976852 79.27778658
 98.70615611 79.59454685 73.96108722 93.77566063 96.24273583 96.9067141
 89.79300934 97.17717864 90.50693827 79.52023002 98.42838172 79.94541977
 98.20786784 78.62720971 80.68493318 97.55972759 81.27703123 95.99054592
 80.36086305 95.45083515 78.78437154 82.93880435 85.97726636 80.23537725
 81.32210865 99.14718388]
Avg Prec: is [55.7337329   3.00797201 11.03879689  3.41448661  2.27469773  3.59284792
  3.26345743  5.55566208  2.48925301  3.68884196  1.61158375  1.66270101
  0.59483378  5.04911136  2.62750854  3.05244939  3.63788455  2.63464414
  1.34777429  1.77800748  2.03524564  0.8630134   1.78824493  2.46971323
  5.13231618  3.64565686  6.54386675  3.3480593   2.04798678  1.96201338
  2.50605216  1.27348941  3.66532415  1.69635308  2.37190913  2.41301565
  3.05008795  2.62504971  2.80691483  7.54727518  2.36513246  8.28247167
  3.32811771  4.08236563  3.26118916  6.45194101  2.0525183   1.52429752
  2.13246146  1.51936556  1.7953641   1.53053927  1.08756257  2.96077988
  1.41484957  2.72782687 11.27230368  3.75427984  3.95169545  2.81202774
 11.02758161  2.21932631  3.85515103  2.9452983   1.62878707  2.55445552
  1.81296904  4.21690137  1.24302907  2.44086436  0.20346274  3.43150784
  1.99154744  4.64747054  4.02044894  3.14021448  0.86374696  1.83750987
  0.14747584  0.72462078]
mAP score regular 6.33, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [81.70515983 97.22450607 89.87218776 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.75481476 97.82744101 92.37362035
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 89.04751227 96.39235618 96.16314124 96.78102499
 90.13379176 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [76.88168025 97.22450607 89.58566908 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.37362035
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 89.19700027 96.39235618 96.16314124 96.78102499
 90.13379176 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [92.67530258  6.25545084 43.00604615  5.91083784  1.77635981 12.89637949
  5.34485612 12.91851315  2.4370974   9.23672468  2.032604    2.11561278
  1.56884557  5.60463013  2.09896812  3.87642718  3.40199017  2.15929323
  0.84828281  1.24676908  1.39817039  0.51827764  0.91824494  1.40481908
  5.93058575  5.48023213  9.70075332  2.7112338   3.86574144  1.17551995
  1.54899972  0.84669183  2.19613718  1.12686919  1.19710383  1.2416726
  2.32904451  1.79206135  1.82868653 25.06815414  3.15486691 26.71619729
  3.49003707  4.93058     4.09419281  9.86040862  2.17874645  1.82593473
  1.9841761   1.79676341  1.17061442  1.29140597  1.07597807  2.49505014
  1.60542574  2.60143441 39.03719223  5.82355075  7.49117475  3.82347173
 23.93766987  3.48592834 13.44877997  5.49557947  4.67358709  3.43273412
  4.04536152  5.4302567   2.4205871   5.33066295  0.75095774  7.72173236
  4.03503478 10.4152902   7.12935259  5.65267756  1.05845938  1.89939148
  0.18849731  0.84281946]
Accuracy th:0.5 is [45.32227122 97.22450607 72.2799412  96.96290206 97.90716795 77.94553654
 78.08256721 76.78949598 79.44041657 96.41976231 79.70451205 98.5325261
 99.34972718 79.03929043 79.52263498 96.31262924 96.21047911 79.0193587
 98.78167277 98.34068316 80.04833445 80.34481899 98.31327703 79.1090515
 79.1240003  96.52938685 94.3393876  79.0118843  97.81747515 79.65219124
 97.52597354 98.67204823 96.39983058 98.18870369 86.34925381 79.25355657
 79.15638937 91.73580487 97.0276802  76.12676583 79.1688467  92.37362035
 78.36659441 77.99536587 97.03764606 94.02795426 98.18621222 98.77668984
 85.68652366 87.67969704 85.59184792 98.55993223 98.87385704 78.42140668
 98.6969629  79.02434163 72.64369534 94.2870668  96.16314124 96.78102499
 90.13379176 97.04761193 90.2882627  78.93714029 98.32075143 79.78174751
 98.13139996 78.10748187 80.32239579 97.53593941 80.84062087 96.07843137
 80.01345392 95.44559882 78.02526347 83.64850387 87.30348556 79.68458031
 80.91536488 99.15040985]
Accuracy th:0.7 is [45.58138376 97.22450607 72.2799412  96.96290206 97.90716795 77.94553654
 78.08256721 76.81441064 79.44041657 96.41976231 79.70451205 98.5325261
 99.34972718 79.42297631 79.52263498 96.31262924 96.21047911 79.0193587
 98.78167277 98.34068316 80.3522934  80.34481899 98.31327703 79.11154297
 79.50021177 96.52938685 94.3393876  79.0118843  97.81747515 79.65219124
 97.52597354 98.67204823 96.39983058 98.18870369 86.59590901 79.25355657
 79.15638937 91.88280141 97.0276802  76.12676583 79.39557017 92.37362035
 78.36659441 77.99536587 97.03764606 94.02795426 98.18621222 98.77668984
 87.32590876 87.99611331 85.74881032 98.55993223 98.87385704 78.42140668
 98.6969629  79.02434163 72.64369534 94.52873907 96.16314124 96.78102499
 90.13379176 97.04761193 90.44024217 78.93714029 98.32075143 79.78174751
 98.13139996 78.10748187 80.32239579 97.53593941 80.84062087 96.07843137
 80.01345392 95.44559882 78.02526347 83.73570521 87.4205845  79.68458031
 80.91536488 99.15040985]
Avg Prec: is [53.2964123   3.68640782 14.95018998  4.53954489  1.46344542  4.56411573
 14.62160871  8.76177696  8.60108302  5.63532333  3.4343166   5.01317966
  2.57837046  5.76278376  2.99584089  3.73877651 14.73361873  6.53520008
  1.57483871  2.88498545  3.54430522  1.58085324  1.22449891  5.1583015
  5.52901297  7.63061031  7.74676336  4.66183658  3.8881499   4.25671792
  2.03553692  0.83434343  2.88132141  1.14002258  1.67022003  2.11011987
  1.91192272  2.19012199  2.19015302  6.28036233  1.70370173  6.03307566
  2.15108898  2.69023743  2.33634603  4.8325899   1.63795583  1.02087197
  1.43670832  1.15352506  1.1939042   0.95425626  0.7315723   2.25686381
  0.84874401  1.835537    9.92962491  2.94766499  3.83779414  2.68154474
  7.80108491  2.11412702  3.28104049  2.52672852  1.35700549  1.93795406
  1.53140126  3.51593368  1.10401772  2.26780203  0.19698114  3.34964704
  1.65634739  3.95745923  3.17661085  2.29502227  0.5575225   1.43164836
  0.11874     0.61173404]
mAP score regular 6.59, mAP score EMA 4.19
Train_data_mAP: current_mAP = 6.33, highest_mAP = 6.33
Val_data_mAP: current_mAP = 6.59, highest_mAP = 6.59
tensor([0.4278, 0.5797, 0.5843, 0.5690, 0.4127, 0.5157, 0.5114, 0.4431, 0.4714,
        0.4678, 0.4742, 0.5239, 0.5112, 0.4347, 0.4539, 0.4117, 0.5290, 0.3867,
        0.5856, 0.4031, 0.4276, 0.4708, 0.4380, 0.4647, 0.4644, 0.4279, 0.5136,
        0.4921, 0.3438, 0.4889, 0.4453, 0.4034, 0.3948, 0.4626, 0.4759, 0.5172,
        0.4849, 0.5163, 0.4159, 0.5414, 0.3708, 0.4865, 0.4498, 0.4701, 0.4405,
        0.4215, 0.4730, 0.5448, 0.4807, 0.4404, 0.3730, 0.5770, 0.3783, 0.5232,
        0.4914, 0.4583, 0.5912, 0.4986, 0.5501, 0.4202, 0.4146, 0.5212, 0.5743,
        0.4554, 0.5163, 0.5000, 0.4602, 0.4675, 0.4066, 0.4598, 0.3805, 0.5621,
        0.5135, 0.4963, 0.5658, 0.5418, 0.5991, 0.6174, 0.4931, 0.5108],
       device='cuda:0')
Max Train Loss:  tensor([25.9002, 13.0702, 18.5648, 11.6863,  8.5239,  8.3700,  9.9079, 11.5229,
        15.8841,  8.4243,  9.2163,  8.1649,  7.3301, 18.1873, 20.8955, 15.1320,
        15.0525, 13.0700,  9.4530,  9.9991, 12.4179, 17.3854, 17.2353, 14.4941,
        17.7568, 10.1215, 14.2311, 12.5573, 12.1820, 12.7328, 12.2875,  9.7688,
        19.9552,  9.5400, 15.4497, 17.9302, 10.2217, 11.5932, 18.3499, 18.5507,
         9.3159, 15.3963,  7.1265, 16.7096, 16.7005, 17.4366,  7.7979, 10.8510,
        10.1206, 16.9330, 14.8041, 10.8396, 16.7386,  6.9499,  9.3807, 14.4368,
        24.1368, 14.4720, 13.8427, 16.9447, 13.6304, 11.3612, 11.1790, 11.9212,
        12.1838, 17.5656, 10.0381, 11.9899,  8.3189,  7.8407,  3.2821, 12.6930,
         8.2586, 13.3434, 11.7683, 12.5307, 14.0703, 11.2311,  5.5474,  8.0759],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [3/80], Step [000/642], LR 1.0e-04, Loss: 25.9
Max Train Loss:  tensor([25.8938, 18.0945, 16.8887, 14.4514, 11.8125, 11.2081, 12.9003, 14.0024,
        16.6489, 11.5479, 19.3404, 12.5689,  8.8654,  9.0892, 14.9911, 19.6239,
        16.3023,  6.8829, 10.8554, 19.4841, 10.1777, 15.2259, 14.5910, 12.0409,
        13.6991,  8.6393, 20.0401, 12.0544,  7.5329, 11.3114,  9.7357, 18.5947,
        16.5247, 15.4866, 19.0149, 14.3720, 19.6333, 12.5742, 10.9679, 22.5029,
         6.6843, 17.6257, 12.4737, 19.7196, 14.4887, 18.2300, 15.9892, 19.9152,
        20.0497, 11.7261,  8.4834, 18.3075,  7.9723, 20.0315, 11.1732, 14.1510,
        16.2904, 12.4832, 15.0255, 12.7819, 14.1479, 12.7550, 16.8134, 10.2632,
        12.0518, 11.7205, 10.8187, 10.1000,  7.1116, 17.1739, 12.6315, 20.2086,
         8.5724, 20.3294, 18.0468, 12.6734, 15.6157, 14.0776, 12.9167, 18.8668],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [3/80], Step [100/642], LR 1.0e-04, Loss: 25.9
Max Train Loss:  tensor([23.2132, 21.1635, 22.7727, 16.6720, 11.1696, 10.8570, 11.4704, 15.0358,
        15.3157, 13.6103, 20.0857, 14.2544,  9.8843, 12.4788, 15.8622, 19.1771,
        18.7044,  9.9039, 10.7163, 20.3824,  8.2502, 14.8694, 13.2835, 15.6303,
        17.1073,  8.6385, 13.6429,  9.5826,  8.1685,  9.4655, 10.3950, 18.8052,
        13.9819, 16.3832, 19.3583, 17.6471, 14.7260, 12.6477,  9.0454, 16.1985,
         6.7049, 19.5358,  9.7514, 18.0278, 11.5369, 13.4031, 17.0678, 18.8236,
        19.1929, 14.3711,  6.0240, 15.8954,  6.7960, 18.1644, 13.1077, 13.8119,
        22.0397, 14.7940, 13.9113, 11.3065, 15.5552, 12.6740, 13.7042, 12.9135,
        11.5361, 12.1309, 11.1887, 13.7955,  7.3376, 18.0797, 12.8729, 12.1106,
        10.5022, 16.5733, 13.4481, 15.3818, 12.2238, 11.0395, 13.2645, 18.5626],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [3/80], Step [200/642], LR 1.0e-04, Loss: 23.2
Max Train Loss:  tensor([25.2477, 20.6435, 20.1672, 14.0694,  9.5251, 13.2117, 15.1531, 12.2367,
        15.8397, 11.1978, 19.8075, 14.5014,  9.1604, 11.7317, 12.1532, 19.1925,
        20.4985,  8.4713, 13.6447, 18.5167,  8.8055, 14.4082, 13.4910, 14.0718,
        18.3344,  9.1248, 10.2575, 13.4570,  9.0811, 12.1667, 13.8157,  7.3178,
        16.3538, 18.0442, 19.2052, 15.4789,  8.2985, 12.9019,  9.6066, 18.8161,
         8.9274, 20.5567, 12.4335, 20.9642, 11.2309, 13.8458, 15.1790,  7.5245,
        10.1591, 12.1195,  8.7202, 18.6651,  6.9720,  9.9318, 11.7329, 11.9514,
        20.8554,  9.8202, 14.3388, 11.2268, 19.5992,  9.8204, 15.3442, 12.1724,
        11.0769, 10.0936, 11.4404,  7.7707,  9.0057, 17.5094, 12.8625, 14.2867,
        10.5054,  8.3632, 14.9911, 13.1719, 10.9206, 14.6488, 13.3224, 20.1578],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [3/80], Step [300/642], LR 1.0e-04, Loss: 25.2
Max Train Loss:  tensor([25.4350,  9.4858, 19.6176, 18.9971, 10.2997, 12.9283, 14.8093, 15.5185,
        16.4435, 13.8849, 19.5328, 12.1584,  9.0018, 12.6132, 13.9777,  5.9552,
        17.9020,  7.6277, 13.2996, 20.9277,  8.8597, 15.4242, 14.3617, 12.1216,
        13.3966,  8.7133, 17.5135, 13.7379,  6.7524, 11.5364, 14.6132,  9.1626,
        13.2343, 15.3008, 17.1050, 13.4764,  8.2449, 12.6059,  8.0893, 12.4245,
         7.2715, 14.4499, 11.1540, 17.9886, 12.6075, 14.8189, 16.6555, 12.1095,
         6.4245, 12.9973,  6.7811, 17.6481,  8.8468,  8.6732, 13.9141, 12.9938,
        26.8806, 13.4283, 15.0305, 10.9786, 16.8568,  9.3205, 15.4649,  9.0283,
        10.8526, 10.7763,  9.8170, 11.9783,  8.0256, 16.9762, 12.7991, 13.0773,
         9.0633,  6.4549, 14.9520, 14.3950, 11.8992,  9.9093, 13.1666, 18.8535],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [3/80], Step [400/642], LR 1.0e-04, Loss: 26.9
Max Train Loss:  tensor([24.5842, 10.1118, 18.7112, 17.1270,  7.7888, 12.5522, 10.2685, 13.5598,
        18.4500, 11.9339, 19.0985, 12.1423,  9.7810, 14.3720, 14.0766,  5.4572,
        17.5636,  6.4483, 12.5338, 19.1126, 11.0916, 14.4978, 12.3773, 14.4226,
        14.4355,  7.6106, 12.1906, 10.3735,  5.6548, 13.5936, 10.9543,  9.8966,
        15.0737, 17.7543, 17.1079, 13.5990, 11.7555, 16.3589,  8.5377, 18.0391,
         8.2208, 18.8681, 10.5820, 18.6536, 13.1183, 14.0501, 17.8929,  8.9086,
        10.9652, 11.3746,  7.0579, 17.0893,  7.6519,  9.3810, 12.4335, 12.1051,
        16.1045, 10.3113, 14.1726,  7.3498, 17.3907, 12.7587,  9.7606, 10.2640,
        13.1127,  9.7235, 10.7547, 14.5562,  6.5706, 17.0407, 12.8572, 12.4870,
         7.5188, 10.4160, 15.6019, 14.2018, 10.8320, 18.1385, 13.2373,  7.2978],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [3/80], Step [500/642], LR 1.0e-04, Loss: 24.6
Max Train Loss:  tensor([22.2607, 11.1110, 20.2593, 20.2546,  9.7713, 13.0763, 11.3079, 13.8979,
        16.9765, 15.5719, 20.9184, 14.1735,  9.9692, 12.7442, 16.1321,  5.9881,
        16.6371, 11.6707, 11.3531, 18.8643,  8.2433, 14.0135, 13.0738, 13.5515,
        12.2332, 11.4572, 13.4332, 13.5046,  6.8631, 10.5228, 10.1583,  7.3423,
        16.7772, 20.0601,  6.2288, 14.2086, 11.0588, 12.3072, 15.9388, 18.1378,
         6.0672, 13.5419, 10.5914,  5.8156, 12.7985, 10.1102, 15.8549, 12.5391,
         6.7352, 12.4874,  7.6863, 17.2188,  6.9018,  6.8929,  9.6035, 13.2880,
        20.0998, 13.5289, 13.5333,  9.5472, 15.6450, 13.1900, 11.1286, 10.7187,
        10.1969, 10.9935,  9.2117, 12.6647,  7.3928, 17.1861, 13.3806, 15.0810,
        11.3700, 11.6835, 12.5609, 13.4021, 12.1385, 12.4675, 13.4415,  5.3792],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [3/80], Step [600/642], LR 1.0e-04, Loss: 22.3
Max_Val Meta Model:  tensor([ 26.2623,  41.7548,  50.6262,  28.7740,   6.0292,   9.7434,  10.5098,
         13.6143,  15.6416,  12.0605,  18.9871,  12.9404,   9.8092,  10.7351,
         11.1008,   4.5242, 118.4446,   5.5726,   9.3790,  18.4145,   6.8363,
         13.7051,  12.2333,   9.2732,  19.1899,   9.7192,  17.4885,   8.4283,
          6.7146,  11.8514,   7.9876,   6.2442,  11.8754,  15.1869,   2.7474,
          9.9331,   6.9823,   9.6534,   7.1196,  30.5435,   7.3696,  15.9985,
          9.6765,  10.3980,  12.8492,  12.7473,  14.4802,   7.4668,   5.3493,
         11.2334,   5.8297,   3.1992,   7.1609,   6.5318,   9.2271,  10.4635,
         16.7427,   9.7931,  15.5379,   7.1872,  12.4364,  17.6841,   9.5789,
          9.4926,   9.8404,   7.8231,   8.8784,   7.5764,   8.6589,  19.7895,
         12.7521,  15.1636,  13.8103,   7.0051,  10.2888,  11.8785,  10.6015,
         10.7964,  13.0687,   3.6026], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 23.5919,  37.4679,  50.7330,  30.3589,   6.3700,   9.7797,  10.8472,
         14.0644,  16.2622,  11.8260,  19.5309,  13.1102,  10.0115,  10.7010,
         11.7742,   4.7870, 116.0949,   5.8597,   9.8709,  18.9274,   7.2490,
         14.3313,  12.7633,   9.8228,  18.2280,   9.6159,  17.6140,   8.8191,
          7.1116,  11.8630,   8.3700,   6.4915,  12.3239,  15.8432,   2.8866,
         10.3325,   7.2910,   9.3441,   7.5070,  27.2565,   7.7537,  17.0239,
         10.2544,  11.1355,  13.2079,  13.2709,  14.9729,   7.8370,   5.6085,
         11.7417,   6.1703,   3.4016,   7.4395,   7.0731,   9.7386,  11.1856,
         17.5185,  10.3677,  15.5067,   7.6018,  13.6693,  17.4989,  10.0092,
         10.0927,  10.3560,   8.2580,   9.3265,   8.2373,   9.1722,  20.5781,
         12.8805,  15.7112,  14.4953,   7.1157,  10.3482,  12.5033,  10.9481,
         11.0567,  13.7836,   3.8292], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 55.1409,  64.6309,  86.8205,  53.3564,  15.4342,  18.9635,  21.2121,
         31.7421,  34.4972,  25.2818,  41.1867,  25.0261,  19.5844,  24.6174,
         25.9391,  11.6269, 219.4593,  15.1546,  16.8554,  46.9506,  16.9521,
         30.4435,  29.1430,  21.1395,  39.2542,  22.4732,  34.2918,  17.9196,
         20.6858,  24.2627,  18.7944,  16.0909,  31.2161,  34.2506,   6.0659,
         19.9783,  15.0356,  18.0987,  18.0487,  50.3406,  20.9135,  34.9920,
         22.7995,  23.6874,  29.9861,  31.4835,  31.6573,  14.3843,  11.6668,
         26.6602,  16.5425,   5.8958,  19.6648,  13.5192,  19.8181,  24.4069,
         29.6318,  20.7944,  28.1877,  18.0927,  32.9690,  33.5748,  17.4273,
         22.1625,  20.0577,  16.5148,  20.2651,  17.6214,  22.5595,  44.7508,
         33.8476,  27.9486,  28.2295,  14.3373,  18.2909,  23.0758,  18.2737,
         17.9092,  27.9546,   7.4964], device='cuda:0')
Outer loop valEpocw Maximum [3/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 118.4
model_train val_loss valEpocw [3/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 116.1
model_train val_loss  valEpocw [3/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 219.5
Max_Val Meta Model:  tensor([47.9539, 49.7444, 51.5152, 47.1130, 34.8609, 46.7253, 50.6495, 39.1160,
        40.1012, 40.9351, 41.4494, 58.4136, 44.0881, 37.1197, 39.6098, 46.8969,
        46.8529, 33.8868, 51.8928, 35.7628, 38.1287, 40.0071, 37.7595, 40.6646,
        40.3381, 38.5358, 43.3321, 39.9926, 31.9227, 42.1223, 38.2031, 37.0231,
        35.4594, 40.7484, 40.0302, 43.5585, 41.8546, 44.1747, 36.3402, 48.2141,
        33.7022, 41.1437, 38.7049, 40.2904, 38.9475, 35.4713, 39.9076, 42.0124,
        41.8137, 37.9128, 32.2858, 43.9205, 33.1316, 43.6181, 40.9242, 39.8826,
        46.5055, 43.7860, 51.0638, 37.2265, 38.0619, 42.7813, 46.7235, 39.7754,
        42.0721, 41.9265, 39.1642, 35.3339, 34.9418, 41.0322, 33.9446, 47.0463,
        43.5961, 41.2412, 45.2270, 48.3313, 46.0861, 48.8824, 42.8158, 41.6691],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([31.1444, 12.9788, 38.2161,  6.8013,  6.6841, 19.3171, 45.2720, 25.1168,
        21.6877, 17.9962, 22.0550, 64.1626, 11.3696,  8.5531, 13.4090,  6.0227,
        15.8156,  9.3701, 10.4910, 21.6928,  7.7490, 14.8581, 13.3089, 11.0464,
        16.2262, 11.1849, 19.7592, 11.1808,  8.5709, 10.1635,  8.6659,  7.2411,
        12.5511, 16.7077,  3.1223, 10.5752,  8.9707,  8.3370,  8.0101,  5.6763,
         6.1034,  9.0065,  9.2058,  3.6431, 11.5733,  9.1746, 15.1264,  6.6197,
         6.0832, 12.1063,  6.5512,  3.3805,  6.9388,  7.0928, 10.1540, 12.4780,
         9.2568,  9.9694, 12.8524,  8.3253, 10.5019,  9.5445,  9.6070, 10.0640,
        10.4397,  8.5314,  9.6825,  9.3245,  7.1237, 17.9116, 14.1780,  9.3933,
         8.9619,  6.1565,  7.8232, 11.1160, 11.4201, 11.0673, 14.1344,  3.9118],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 69.7771,  20.2817,  62.7344,  12.3721,  16.0201,  36.1425,  81.5760,
         54.9242,  46.2751,  37.5305,  46.1822, 124.4099,  22.1580,  19.6112,
         29.1650,  12.3102,  29.6082,  23.0747,  17.7104,  52.3600,  17.4077,
         31.2333,  29.8960,  23.3970,  34.5684,  24.8364,  38.6006,  23.4759,
         22.9834,  20.4086,  19.4516,  16.7447,  30.5783,  34.7602,   6.5088,
         20.6788,  18.1939,  16.0567,  18.7847,   9.4637,  15.3052,  18.3833,
         20.0916,   7.6310,  24.9048,  21.5596,  32.1587,  12.1983,  12.2698,
         26.9012,  17.1813,   6.2271,  17.6330,  13.7484,  20.6103,  27.1626,
         15.0316,  19.5789,  22.8866,  18.7820,  23.8327,  18.0766,  16.3778,
         21.5944,  20.8026,  17.2160,  20.8682,  21.9571,  17.2486,  37.8585,
         35.5790,  16.9551,  17.5739,  12.6498,  14.0082,  18.6306,  18.9434,
         17.7398,  28.4823,   7.8807], device='cuda:0')
Outer loop valEpocw Maximum [3/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 58.4
model_train val_loss valEpocw [3/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 64.2
model_train val_loss  valEpocw [3/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 124.4
Max_Val Meta Model:  tensor([39.4713, 43.8578, 43.1311, 44.5177, 33.4512, 36.2387, 34.1852, 36.8384,
        39.5554, 38.8245, 40.8919, 38.0039, 42.0532, 35.4512, 38.4729, 41.3154,
        43.4403, 32.3932, 45.5379, 35.1578, 36.0633, 33.2521, 36.0359, 39.3454,
        38.1572, 36.2606, 44.0949, 31.1637, 29.7010, 26.8417, 37.1237, 35.1373,
        34.8182, 39.1601, 37.2449, 47.9099, 39.0273, 44.6879, 34.7420, 48.7589,
        32.0956, 40.6367, 47.5586, 38.9046, 38.2323, 40.8790, 44.6411, 46.8765,
        46.9498, 42.1461, 31.4310, 41.8136, 31.4959, 41.2168, 40.3206, 39.1493,
        60.0164, 41.0657, 39.8297, 35.1764, 44.0088, 40.6990, 45.4800, 37.9778,
        40.1089, 39.9388, 37.3686, 34.2030, 33.5442, 40.6302, 33.0103, 45.2009,
        41.1551, 39.4969, 42.5198, 42.9099, 43.4424, 44.3646, 40.9097, 38.7090],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([18.4242,  5.1236,  4.6553,  3.6434,  6.3295,  6.8036,  7.7820, 11.3629,
        15.6412,  9.2480, 19.2252, 10.2735,  8.4748, 11.7349, 10.8952,  4.1340,
        12.6239,  5.9621,  9.2336, 19.2456,  7.3216, 12.8766, 12.5294,  9.8749,
         9.7020,  6.2041, 16.3085,  8.9719,  5.9956,  8.0219,  8.8985,  7.2942,
        13.4844, 16.2249,  2.7114, 11.2773, 10.9549, 10.3097,  7.4774, 19.9159,
        12.5330, 37.2701, 44.2480, 46.6417, 26.1796, 32.6777, 17.6247, 10.9447,
        23.1558, 12.4798, 13.6520, 26.9300, 22.0199, 13.2529, 28.6457, 40.6581,
        81.7460, 11.0993, 11.7721,  7.7080, 66.0667,  8.4602, 12.1766, 11.4980,
        12.2672,  7.9266, 10.6939,  8.8700,  7.5055, 16.7087, 12.8814, 11.6902,
         8.8556, 18.5609,  5.7409, 13.7781, 13.0950,  9.3245, 13.5415,  3.5462],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 40.8820,   8.2227,   7.5515,   6.5939,  15.0761,  14.0456,  17.1229,
         25.2294,  32.9019,  19.4471,  40.0675,  20.7571,  16.5914,  27.1181,
         23.5431,   8.4309,  24.0919,  14.5977,  16.1155,  46.5592,  16.6399,
         30.0741,  28.6704,  20.7846,  20.9226,  13.9310,  28.3091,  21.9695,
         16.5020,  19.2026,  19.7472,  17.0240,  32.5788,  34.2392,   5.6457,
         19.6366,  22.8601,  19.0909,  17.5601,  33.1733,  31.8792,  78.1327,
         78.4848, 100.2583,  57.1119,  77.7781,  34.0114,  17.3711,  41.9166,
         27.5603,  35.6024,  49.8572,  57.9215,  26.0824,  58.7194,  90.0282,
        131.2984,  22.2461,  22.0183,  17.6454, 153.1929,  16.0075,  21.0180,
         24.8859,  24.6771,  16.0411,  23.2884,  20.5884,  18.0962,  34.8792,
         32.4857,  21.0839,  17.4217,  37.9202,  10.1867,  23.7859,  21.8424,
         15.1902,  27.6677,   7.1614], device='cuda:0')
Outer loop valEpocw Maximum [3/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 60.0
model_train val_loss valEpocw [3/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 81.7
model_train val_loss  valEpocw [3/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 153.2
Max_Val Meta Model:  tensor([27.5579, 40.5583, 42.1747, 41.9232, 32.1451, 33.9231, 31.9715, 36.0029,
        38.6815, 37.7641, 40.0387, 36.4825, 39.7843, 34.0752, 37.1213, 41.6267,
        40.6513, 30.5243, 36.7990, 35.0614, 34.0944, 31.2103, 35.0035, 36.6140,
        37.0839, 34.7823, 43.3750, 28.8709, 29.3159, 24.6249, 35.4318, 33.2565,
        34.0148, 37.6991, 25.3102, 36.2599, 36.5298, 44.3170, 32.9132, 44.0558,
        30.5800, 36.7410, 42.7017, 34.8482, 35.5522, 32.5281, 39.8460, 41.2793,
        39.3028, 36.0340, 29.8734, 38.1295, 29.6571, 40.4375, 23.9419, 36.6828,
        40.7244, 39.3132, 39.8203, 33.9199, 35.1429, 38.4893, 39.4079, 41.6887,
        37.9592, 38.1497, 36.1155, 44.9175, 31.8220, 39.0539, 32.8079, 42.5134,
        41.8579, 36.4083, 70.5611, 39.4568, 41.5751, 40.5814, 38.9757, 33.4587],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 14.8005,   6.3864,  16.3051,   4.3081,   7.0871,  10.0704,   9.2819,
         13.3171,  16.7467,  14.3566,  20.4953,  12.8624,   9.9773,  10.6766,
         13.4557,   4.7588,  13.9426,   6.6834,   9.6034,  20.3580,   7.8608,
         13.5033,  13.4598,  10.3038,  14.8384,   6.9893,  18.7454,   7.8211,
          9.0253,   8.6114,   9.0398,   7.3905,  13.1300,  16.7919,   2.3967,
         10.5834,   7.6765,   8.9050,   8.2055,   7.3685,   6.3474,   9.2985,
         11.3572,   3.7855,  11.6599,   8.9706,  16.1238,   7.7162,   6.7863,
         12.1094,   6.8916,   3.5823,   7.0491,   7.7609,   7.9992,  11.8030,
          7.4001,   9.1032,  13.7190,   8.5405,  12.3802,   9.9243,   9.6189,
         11.3363,  10.4941,   8.8224,   9.9812,   9.0843,   7.4011,  17.7516,
         14.6641,   9.8436,   8.5156,   7.4461, 161.5708,  12.3652,  11.9939,
         12.2577,  14.0952,   4.0574], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 48.1749,  10.6264,  27.5098,   7.7603,  16.6572,  20.9683,  20.3831,
         29.1448,  34.8627,  29.9182,  42.4336,  25.5227,  19.5989,  24.6447,
         29.2030,   9.8655,  27.0788,  16.4621,  18.0649,  48.2870,  17.9163,
         31.8321,  30.5743,  22.2277,  31.9741,  15.4810,  33.1041,  18.9350,
         24.0392,  20.9777,  19.9996,  17.2667,  31.2713,  35.6589,   6.6688,
         21.1230,  16.1212,  16.0612,  19.3574,  12.4611,  15.8246,  19.3186,
         20.6166,   7.9613,  25.4994,  21.0706,  32.9406,  12.8062,  12.7991,
         26.3192,  17.7447,   6.4405,  18.1749,  14.3841,  20.9787,  25.7552,
         11.7362,  18.0028,  25.9055,  19.3111,  28.1516,  18.8086,  17.4295,
         22.1419,  21.2122,  17.7523,  21.4267,  18.1098,  17.8476,  37.2422,
         36.1782,  17.8848,  16.2130,  15.2436, 308.9087,  21.2978,  19.1564,
         20.5263,  28.9157,   8.2665], device='cuda:0')
Outer loop valEpocw Maximum [3/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 70.6
model_train val_loss valEpocw [3/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 161.6
model_train val_loss  valEpocw [3/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 308.9
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [78.93422351 97.2137279  89.90265713 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.05906361
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 89.52132649 96.13796128 96.24273583 96.9067141
 89.79300934 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [72.87922905 97.2137279  89.51523495 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.05906361
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 89.21796762 96.13796128 96.24273583 96.9067141
 89.79300934 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [91.55778598  5.67441466 40.62588664  6.41407733  2.74816902 10.1549056
  4.35991216 12.28469264  3.04780888  9.16658194  2.3853405   2.2435789
  1.45766283  5.52990871  2.14434196  3.6006155   3.53346182  2.18681297
  0.94334849  1.3091136   1.39649967  0.52737297  0.90321609  1.54957986
  6.17820458  5.0575556  14.35414617  3.16037507  3.10016073  1.2646404
  1.69082128  0.95067092  2.32021673  1.41971662  1.52563047  1.44959709
  2.92943714  1.93180283  1.80492149 22.14572054  3.21082373 23.94631406
  3.88428578  5.33896859  4.02915133 11.25564854  2.15749832  1.69406624
  2.28007372  1.67256927  1.19760933  1.34661977  1.01184202  3.33582599
  1.57453671  2.99204615 39.65121222  4.86446705  6.20235392  3.34893094
 27.0476768   2.87377175  7.18047625  4.04435689  2.31351391  2.88927283
  2.42334866  4.85223569  1.595621    3.25914768  0.21461963  5.64388544
  2.8503548   7.79427305  5.55158956  4.09852246  0.95043848  1.81878875
  0.16826788  0.77563743]
Accuracy th:0.5 is [45.40636688 97.2137279  73.55173548 97.02489005 97.26733349 78.4724845
 78.84163205 77.53316846 79.70419464 96.42791876 80.12451115 98.52097319
 99.41399349 80.76655986 79.62013133 96.56680596 96.29512311 79.42398363
 98.65376884 98.30776915 80.89570059 80.61548958 98.38695922 79.49708215
 80.92981323 96.65086926 94.0778012  79.04021637 98.01293844 79.96613102
 97.30875598 98.57457877 96.36213009 98.02024829 86.15270282 79.587237
 79.32773724 89.98915705 97.11504489 76.44156382 79.95394793 92.05906361
 78.84041374 78.40425921 96.9627563  93.87434364 98.02877645 98.57336046
 85.75309755 87.7109197  87.0274485  98.55508583 98.99976852 79.01706851
 98.70615611 79.41180054 73.75641135 93.16894287 96.24273583 96.9067141
 89.79300934 97.17717864 90.57150863 79.31555415 98.42838172 79.8942508
 98.20786784 78.52243516 80.47538407 97.55850928 81.09915815 95.99054592
 80.12207454 95.45083515 78.64792096 82.55260048 85.68121733 80.00633521
 81.1198694  99.14718388]
Accuracy th:0.7 is [45.47093724 97.2137279  73.55173548 97.02489005 97.26733349 78.4724845
 78.84163205 77.62941485 79.70419464 96.47177788 80.12451115 98.52097319
 99.41399349 81.16372851 79.62013133 96.56680596 96.29512311 79.42398363
 98.65376884 98.30776915 81.3013974  80.61548958 98.38695922 79.49951877
 81.40617195 96.65086926 94.0778012  79.04021637 98.01293844 79.96613102
 97.30875598 98.57457877 96.36213009 98.02024829 86.36103361 79.587237
 79.32773724 90.24500189 97.11504489 76.44156382 80.52898966 92.05906361
 78.84041374 78.40425921 96.9627563  93.87434364 98.02877645 98.57336046
 87.49893398 88.51987671 87.20044834 98.55508583 98.99976852 79.01706851
 98.70615611 79.41180054 73.75641135 93.53809042 96.24273583 96.9067141
 89.79300934 97.17717864 90.76400141 79.31555415 98.42838172 79.8942508
 98.20786784 78.52243516 80.47538407 97.55972759 81.09915815 95.99054592
 80.12329284 95.45083515 78.64792096 82.64762856 85.75553417 80.00633521
 81.1198694  99.14718388]
Avg Prec: is [55.78537226  3.09265828 11.16333597  3.38481609  2.27433488  3.6842481
  3.21236559  5.57212283  2.49085861  3.71688221  1.51832947  1.60081628
  0.61897028  5.21719353  2.70708741  3.19059865  3.48384819  2.64947126
  1.3969541   1.74607871  1.9483249   0.89055579  1.84630132  2.39662635
  5.05469148  3.65050867  6.35880962  3.3919974   1.99395401  1.80756139
  2.63681639  1.36199975  3.67470673  1.79188971  2.3627513   2.37516954
  3.06094893  2.61922984  2.76890037  7.18681806  2.18604483  8.08159145
  3.26388596  3.89952638  3.18470206  6.30167275  2.06989059  1.4486519
  2.1382042   1.58574518  1.91847528  1.65671581  0.99598368  2.96540103
  1.28413055  2.56270848 11.27691519  3.62778464  3.81364507  2.88439039
 10.67495813  2.20210165  3.89972044  3.02300525  1.64622526  2.48681529
  1.8764522   4.20578642  1.29815585  2.31561759  0.16589055  3.39400789
  1.89744585  4.63977564  4.00537062  3.10643699  0.84095036  1.84749367
  0.14450698  0.76268177]
mAP score regular 6.25, mAP score EMA 3.74
starting validation
Accuracy th:0.5 is [83.4442036  97.22450607 90.46515684 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.37362035
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 89.68532775 96.39235618 96.16314124 96.78102499
 90.13379176 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [79.48526297 97.22450607 89.59314348 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.37362035
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 89.31908214 96.39235618 96.16314124 96.78102499
 90.13379176 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [93.87942544  7.54023785 45.73536864  9.0410084   2.45709944 13.24417917
  4.93875658 15.4112581   4.18621672 11.68353208  2.88888581  2.7310747
  1.91199753  5.7884528   2.22662381  3.83956759  3.50402183  2.27198878
  0.83137966  1.27406561  1.21162306  0.51411114  0.90753381  1.4097366
  5.99203972  5.70175742 15.04911731  2.62964267  3.57964641  1.26049744
  1.49118836  0.87167377  2.21893046  1.36142518  1.30190245  1.26841156
  2.65616664  2.10173087  1.80453993 23.94148701  2.89408408 27.01196382
  3.8192763   5.00534481  3.70134217 10.9026008   1.96052419  1.50561398
  2.29086723  1.53063102  1.25081998  1.44453021  1.19998238  3.6841736
  1.62660922  2.59058648 41.98159265  3.95444838  5.85211469  3.40988161
 30.47987767  2.81304524  6.54989347  3.72969103  2.15505033  2.43696837
  2.19776726  4.86307929  1.37539104  2.83468232  0.18410455  4.8778279
  2.38652957  7.19324811  6.33986742  3.65775392  0.83669142  1.80565815
  0.16324175  0.75220663]
Accuracy th:0.5 is [45.30981389 97.22450607 72.14789346 96.96290206 97.90716795 77.8134888
 77.9405536  76.66492264 79.3033859  96.41976231 79.56249844 98.5325261
 99.34972718 78.94959763 79.38560431 96.31262924 96.21047911 78.89229389
 98.78167277 98.34068316 79.95365872 80.20280539 98.31327703 78.97202083
 79.0492563  96.52938685 94.3393876  78.87983656 97.81747515 79.51017764
 97.52597354 98.67204823 96.39983058 98.18870369 86.45389541 79.1165259
 79.0193587  91.72334753 97.0276802  75.98973516 79.03929043 92.37362035
 78.22956374 77.86331814 97.03764606 94.02795426 98.18621222 98.77668984
 86.03283753 87.60495304 85.50962952 98.55993223 98.87385704 78.27939308
 98.6969629  78.89727683 72.51663054 94.2646436  96.16314124 96.78102499
 90.13379176 97.04761193 90.26085657 78.80509256 98.32075143 79.65468271
 98.13139996 77.96546827 80.18536512 97.53344794 80.69860727 96.07843137
 79.87144032 95.44559882 77.89321574 83.60614894 87.30597703 79.54754964
 80.77335127 99.15040985]
Accuracy th:0.7 is [45.58138376 97.22450607 72.14789346 96.96290206 97.90716795 77.8134888
 77.9405536  76.68734584 79.3033859  96.41976231 79.56249844 98.5325261
 99.34972718 79.34574084 79.38560431 96.31262924 96.21047911 78.89229389
 98.78167277 98.34068316 80.28004086 80.20280539 98.31327703 78.9745123
 79.41051897 96.52938685 94.3393876  78.87983656 97.81747515 79.51017764
 97.52597354 98.67204823 96.39983058 98.18870369 86.75287142 79.1165259
 79.0193587  91.85788674 97.0276802  75.98973516 79.29092857 92.37362035
 78.22956374 77.86331814 97.03764606 94.02795426 98.18621222 98.77668984
 87.6522909  87.90392904 85.69150659 98.55993223 98.87385704 78.27939308
 98.6969629  78.89727683 72.51663054 94.5038244  96.16314124 96.78102499
 90.13379176 97.04761193 90.44771657 78.80509256 98.32075143 79.65468271
 98.13139996 77.96546827 80.18536512 97.53593941 80.69860727 96.07843137
 79.87144032 95.44559882 77.89321574 83.69085881 87.41809303 79.54754964
 80.77335127 99.15040985]
Avg Prec: is [53.06993256  3.72826843 14.92950603  4.53296314  1.46278202  4.50783913
 14.71032741  8.74298498  8.5809072   5.48124137  3.47512704  5.17340942
  2.46248424  5.76440578  2.99245106  3.75011191 15.43769665  6.49864286
  1.57012528  2.86625739  3.51360226  1.58498318  1.22426447  5.13527777
  5.52411553  7.77358583  7.76157435  4.65686981  3.88542472  4.35862036
  2.05989838  0.83454518  2.88061476  1.13799889  1.660087    2.11792433
  1.92490876  2.18724877  2.18927469  6.27656559  1.70478226  6.03462204
  2.15460575  2.68844917  2.33734509  4.84047893  1.64859443  1.01861689
  1.41701562  1.15678951  1.19297957  0.96475087  0.73372422  2.25524622
  0.8478521   1.8412922   9.93970533  2.95542913  3.84528306  2.69570213
  7.80465961  2.10853381  3.29839261  2.52605121  1.35623859  1.93490326
  1.5270356   3.51785727  1.10275484  2.26327243  0.19685111  3.34264662
  1.65641382  3.95733222  3.17196769  2.34397951  0.55823985  1.43301157
  0.11908853  0.61219657]
mAP score regular 6.60, mAP score EMA 4.19
Train_data_mAP: current_mAP = 6.25, highest_mAP = 6.33
Val_data_mAP: current_mAP = 6.60, highest_mAP = 6.60
tensor([0.2755, 0.6340, 0.6217, 0.5686, 0.4120, 0.4759, 0.4499, 0.4505, 0.4811,
        0.4748, 0.4765, 0.4966, 0.5026, 0.4283, 0.4606, 0.5043, 0.5346, 0.3928,
        0.5540, 0.4105, 0.4276, 0.4310, 0.4275, 0.4768, 0.4730, 0.4533, 0.5969,
        0.4075, 0.3490, 0.4011, 0.4514, 0.4144, 0.3980, 0.4649, 0.3314, 0.5202,
        0.4866, 0.5791, 0.4243, 0.6357, 0.3826, 0.4837, 0.5739, 0.4832, 0.4513,
        0.4100, 0.5049, 0.6303, 0.5413, 0.4399, 0.3769, 0.5986, 0.3680, 0.5607,
        0.3630, 0.4513, 0.6743, 0.5138, 0.5716, 0.4367, 0.4212, 0.5220, 0.5757,
        0.4711, 0.5021, 0.4955, 0.4651, 0.4794, 0.4141, 0.4755, 0.3780, 0.5651,
        0.5979, 0.4846, 0.6514, 0.6793, 0.6877, 0.6450, 0.4844, 0.4927],
       device='cuda:0')
Max Train Loss:  tensor([16.4872,  8.3245, 19.8717, 11.0232, 13.3868, 10.9618,  8.8358, 14.3023,
        18.4456, 11.3882, 18.9641, 11.5170,  9.4588, 15.3718, 18.5266,  9.6287,
        19.0062,  7.9026,  9.9936, 20.1262,  8.7350, 13.0920, 12.7455, 12.1383,
        19.2596, 12.0864, 20.0402,  8.4142,  7.8411,  9.5890, 13.4076,  8.9370,
        13.6999, 15.9835,  4.2554, 13.0474, 10.8382, 16.1081, 10.2578, 22.8513,
         9.3070, 15.9720, 17.9492,  6.8573, 11.7448, 13.0406, 17.5724,  9.2959,
         7.5962, 12.1444,  8.1040,  6.8263,  5.8697, 10.4382,  6.7931, 11.0659,
        24.2483, 10.7831, 16.8042,  8.8872, 16.2891, 10.9231, 13.8656, 13.9532,
        12.0808,  8.8446, 12.1752, 10.6482,  7.9564, 18.7444, 12.6405, 13.7959,
        16.7260,  9.0350, 15.2030, 15.9318, 13.3852, 10.2093, 12.7957,  3.4407],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [4/80], Step [000/642], LR 1.0e-04, Loss: 24.2
Max Train Loss:  tensor([20.6237, 17.6936, 22.0662, 21.9995, 10.3547, 13.1924, 10.2007, 13.4737,
        13.1954, 11.4743,  8.7204, 10.9347,  9.4567, 11.7931, 21.8435, 14.5946,
        18.5008,  8.5789, 15.6554,  8.5937, 10.5112, 10.7072, 10.2263, 10.0153,
        13.2294, 13.7982, 19.3820,  8.8812,  7.3174,  6.6215, 13.7240,  8.5984,
         6.9938, 14.5391,  6.0193, 14.7736, 12.9826, 19.3356,  8.5407, 20.6909,
         8.4522, 21.1251, 20.8962, 22.8382, 12.1537, 16.4654, 16.8536, 24.6262,
        17.8184, 11.3221,  7.4992, 19.5441,  8.6197, 14.5728,  6.4617, 11.4038,
        22.2410, 14.5529, 24.0549, 10.2374, 16.9649, 11.8797, 14.7219, 12.4000,
        11.8676, 22.9875, 10.7170, 12.9516,  8.7446, 11.7912,  6.5906, 20.8160,
        22.2333, 19.6037, 14.1398, 18.6508, 16.1188, 17.7092,  9.8670, 16.5551],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [4/80], Step [100/642], LR 1.0e-04, Loss: 24.6
Max Train Loss:  tensor([23.2538, 16.3328, 20.2127, 19.6872, 12.2978, 10.5858,  9.1460, 13.2261,
        14.6638, 11.3763,  8.6968, 11.9647,  9.8035, 16.7794, 11.4436, 17.4285,
        17.5174,  9.9388,  9.6097, 10.8038,  9.8414, 11.3646,  6.7074, 11.4245,
        20.2414, 12.0315, 20.9631, 11.4985,  6.6965,  5.6216, 10.8448,  8.4054,
        10.8028, 14.0049,  4.1675, 10.0830, 11.9507, 12.7360, 11.7604, 20.4149,
        10.5053, 19.3078, 21.7742, 13.5749, 12.1874,  9.4690, 10.8604, 11.0937,
        13.2549,  7.8963,  9.8616,  9.9807,  7.0359, 14.0361,  5.7359,  8.4435,
        28.4542, 12.7713,  8.6713, 10.6207, 16.3077, 14.8274, 16.1490, 10.3342,
        12.0828, 10.3092, 11.6103, 15.5453,  8.9712,  9.9317,  5.2487, 12.5839,
        10.7061, 19.3943, 16.3659, 14.8260, 16.7043, 12.8942,  9.5330, 16.2582],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [4/80], Step [200/642], LR 1.0e-04, Loss: 28.5
Max Train Loss:  tensor([20.6894, 15.3277, 20.5201, 13.3962, 11.7693, 10.3856, 10.5516, 14.6647,
        11.7524, 12.8565,  8.6968, 10.4383, 10.5602, 10.3191, 10.7553, 17.1289,
        15.7484,  9.2572, 15.1127, 11.5226, 12.4283,  9.7961,  8.1119, 14.4797,
        16.6537, 13.0374, 18.0763,  9.8309,  6.8243,  7.6071, 13.3574,  7.7531,
        10.1959, 16.7126,  5.9213, 12.0557,  7.2895,  7.9869, 11.9195, 23.4454,
         8.2030, 12.9377, 10.0337, 14.0716, 11.6566, 14.2729, 13.6943, 14.0849,
        10.2178,  9.7082,  8.4164, 13.1358,  5.4202, 13.7833,  6.0231, 13.8393,
        27.1180, 15.0203, 19.2598, 10.6693, 16.6444, 11.1868, 17.0259, 12.4821,
        11.0470, 11.7695, 10.7010, 16.7705,  9.2419, 13.3458,  5.5598, 12.7210,
        15.0724,  7.7084, 14.7957, 13.3304, 14.1979, 14.9848,  9.9644, 16.6566],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [4/80], Step [300/642], LR 1.0e-04, Loss: 27.1
Max Train Loss:  tensor([16.8419, 12.7544, 25.0240, 15.9653,  7.9519, 12.6234, 14.2208, 14.4044,
        13.7797, 15.4124,  7.1613, 12.5657, 11.3142, 15.1507, 13.4547, 15.3007,
        16.9293,  8.2699, 10.2563, 13.5587, 10.7722,  9.9545, 12.2675,  9.4866,
        15.9763, 10.1139, 18.1940,  8.9434,  7.0822, 10.9790, 13.0822,  9.4020,
         9.3924, 14.3989,  5.4809, 12.5047, 10.5827, 16.5284, 11.9900, 15.4417,
         7.1345, 12.6396, 11.6663, 13.0687,  9.5647,  9.2942, 12.2113, 12.4042,
        10.3573,  7.5408,  8.2193, 12.2213,  6.7518, 14.6727,  7.1035, 10.8757,
        29.0046, 13.8839, 12.7418, 11.1606, 15.1003, 13.7694, 16.8727, 12.7340,
        11.1584, 11.9265, 11.4189, 17.3107,  8.0028, 12.0792,  5.6748, 14.6820,
        10.2085, 10.9161, 12.7844, 13.9458, 15.3295, 13.9881, 10.1329, 16.8607],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [4/80], Step [400/642], LR 1.0e-04, Loss: 29.0
Max Train Loss:  tensor([15.8784, 10.7439, 21.5242, 11.5065,  9.9387, 11.4771,  9.0926, 11.2516,
        16.1644, 12.1211,  9.8918, 10.8606, 10.4949, 13.7593,  9.3091, 15.7949,
         8.4722,  7.3235, 12.1105, 13.7359, 10.8479, 10.8371,  9.1820, 15.5880,
        11.6564, 14.5866, 15.5227, 11.3113,  6.4849,  8.1920, 13.1149,  9.7549,
         9.9432, 16.3430,  6.1562, 18.1712, 11.6051, 13.3219, 11.0186, 19.1672,
         7.3477, 15.9322, 12.1496, 13.0930, 12.6862, 12.1655, 14.2574, 14.0459,
        11.4483, 10.3011,  8.6550, 11.9228,  7.0461, 15.3818,  5.9922, 12.0045,
        16.6227, 12.2555, 16.9413, 11.6062, 15.1305, 10.4036, 15.3232, 14.5205,
        11.2600, 11.7511, 11.5428, 13.7503,  8.5766, 11.5306,  6.3214, 11.7308,
        14.3327, 11.0161, 12.2418, 12.4027, 15.9595,  9.8806, 10.2896, 16.3661],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [4/80], Step [500/642], LR 1.0e-04, Loss: 21.5
Max Train Loss:  tensor([16.3898, 18.2380, 25.0083, 12.2529,  9.0126, 11.3681, 10.2116, 15.5438,
        14.4207, 11.8757, 12.6441, 11.4387,  9.7093, 12.6230, 11.7644, 16.7397,
        11.7056,  7.8341, 13.4050, 12.3451, 13.5957, 11.0096, 10.1389, 13.8595,
        12.6665, 11.9647, 14.8967, 11.2535,  5.1590,  7.9434,  9.2170,  8.8580,
        13.6931, 15.6335, 10.5332, 14.8074, 10.3646,  9.4725, 11.3253, 19.1592,
         7.6784, 13.8065, 11.7330, 13.2967, 10.7812,  9.5541, 13.8932, 11.8764,
        10.3929,  8.4945,  8.4586, 10.6192,  6.8985, 17.1092,  5.8491, 11.1883,
        25.6126, 13.8748, 12.5137, 11.3974, 15.5113, 11.1292, 14.7951,  9.7188,
        11.0556, 11.6466, 10.1304, 13.3043,  8.5540,  9.7732,  5.6746, 11.7345,
        13.0932, 10.4204, 14.5461,  7.5477, 14.4156, 12.4908, 10.7119, 16.2762],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [4/80], Step [600/642], LR 1.0e-04, Loss: 25.6
Max_Val Meta Model:  tensor([ 19.6617,  41.8130,  60.6755,  26.8223,   7.2554,  11.0477,  10.1539,
         14.9049,  11.2099,  12.5715,   8.2715,  11.5425,  10.6214,  12.5123,
          8.0410,   7.4784, 168.6950,   6.6609,  10.4624,   8.9608,   8.6315,
         10.1070,   7.3705,   9.6528,  19.7396,  12.7171,  18.1754,   6.3540,
          6.5638,   8.4240,   9.3817,   8.0573,   7.8084,  12.7846,   2.3454,
         10.0387,   5.9241,   9.9081,   8.7266,  33.4323,   7.8378,  14.7981,
         10.7875,  13.6091,  12.2991,  11.9737,  11.6895,  11.8857,  10.6116,
          8.2956,   6.1849,   9.6956,   6.6786,   6.8217,   5.3264,   9.1523,
         21.8278,  11.9018,  15.0333,   8.9253,  12.9573,  18.0377,  11.6222,
         10.4093,   9.9579,   9.7048,   9.7215,  10.2999,   9.8881,  14.6149,
          5.8171,  14.9428,  15.2365,   8.5502,  11.2981,  11.4202,  14.6721,
          6.2119,  10.3145,  16.3967], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 17.7659,  37.9627,  47.9166,  27.8514,   8.6824,  12.5408,  11.5804,
         16.0068,  13.0403,  13.7715,   9.8032,  13.1632,  12.3316,  13.2837,
          9.6204,   8.7639, 152.5831,   8.0029,  12.6293,  10.5298,  10.2212,
         11.7918,   8.8642,  11.5108,  20.0787,  13.9207,  20.3286,   7.7129,
          7.6103,   9.4102,  11.0993,   9.5793,   9.1275,  14.6735,   3.0690,
         11.8400,   7.3884,  11.3867,  10.3046,  30.5354,   8.9712,  15.6432,
         12.7945,  15.3713,  13.6856,  12.7844,  13.6210,  13.8169,  12.3999,
          9.8163,   7.4857,  11.6495,   7.7364,   8.5168,   6.4960,  10.9731,
         20.7892,  13.7676,  16.3664,  10.5585,  14.0332,  18.7545,  13.6037,
         12.1734,  11.8980,  11.5393,  11.4695,  12.1616,  11.3062,  15.9692,
          7.0692,  16.3074,  16.7055,   9.7118,  12.4388,  12.8500,  17.1378,
          7.3590,  12.2641,  18.5589], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 64.4833,  59.8783,  77.0690,  48.9844,  21.0742,  26.3537,  25.7410,
         35.5314,  27.1038,  29.0075,  20.5724,  26.5076,  24.5358,  31.0186,
         20.8869,  17.3794, 285.4348,  20.3737,  22.7982,  25.6536,  23.9010,
         27.3596,  20.7348,  24.1434,  42.4509,  30.7100,  34.0584,  18.9262,
         21.8038,  23.4617,  24.5905,  23.1163,  22.9318,  31.5621,   9.2620,
         22.7596,  15.1827,  19.6641,  24.2833,  48.0335,  23.4462,  32.3413,
         22.2922,  31.8103,  30.3282,  31.1790,  26.9797,  21.9213,  22.9078,
         22.3140,  19.8604,  19.4628,  21.0238,  15.1903,  17.8949,  24.3130,
         30.8329,  26.7964,  28.6312,  24.1757,  33.3207,  35.9310,  23.6292,
         25.8399,  23.6963,  23.2893,  24.6626,  25.3670,  27.3010,  33.5831,
         18.7003,  28.8571,  27.9422,  20.0413,  19.0957,  18.9160,  24.9194,
         11.4098,  25.3185,  37.6666], device='cuda:0')
Outer loop valEpocw Maximum [4/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 168.7
model_train val_loss valEpocw [4/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 152.6
model_train val_loss  valEpocw [4/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 285.4
Max_Val Meta Model:  tensor([28.4875, 59.7814, 57.2719, 46.2291, 34.7284, 41.0185, 40.6945, 39.6596,
        40.6594, 42.0425, 41.9356, 52.5538, 43.6122, 37.7448, 40.5455, 59.7348,
        59.5514, 35.9133, 45.5876, 35.8265, 39.1538, 37.6764, 37.7510, 39.5991,
        40.4607, 38.7431, 47.0959, 35.0086, 33.0090, 35.2795, 38.3839, 38.0348,
        36.8026, 41.7985, 29.0143, 43.7606, 40.2483, 47.1273, 37.2930, 59.6573,
        35.5636, 40.3987, 46.1147, 40.7929, 38.1752, 36.4645, 41.5328, 46.9590,
        45.8772, 38.9577, 34.2051, 47.1765, 32.9998, 46.1112, 32.1455, 39.1014,
        55.7419, 44.2281, 58.8532, 37.3738, 38.0624, 45.1453, 58.8290, 39.7684,
        41.6281, 41.8698, 40.3902, 36.6367, 36.2724, 41.0636, 35.0461, 45.5763,
        50.2471, 41.7701, 50.1240, 59.9137, 56.9161, 54.3521, 42.2803, 42.2940],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([24.0927, 15.0869, 38.4973,  9.1541,  7.0085, 17.8012, 39.2605, 25.5875,
        18.6621, 18.0822, 12.2596, 68.8505, 11.2160,  9.1489,  9.8297,  8.8831,
         9.8195,  9.7182,  9.9323, 13.4325,  8.8558, 10.0362,  7.4525,  9.7780,
        15.8144, 12.3763, 19.2930,  8.6932,  7.7257,  6.2694,  8.9333,  8.2662,
         7.0962, 13.2249,  2.3156,  9.4628,  6.7659,  7.7847,  8.6581,  6.3049,
         6.3258,  7.5930,  7.7028,  9.3507,  9.5133,  7.9454, 10.9115,  9.9919,
        10.0225,  8.5393,  6.3621,  8.5720,  5.8087,  6.3355,  5.4588,  9.9150,
        11.3157, 10.5170, 11.1997,  8.7733,  9.5342,  9.6726, 10.7959,  9.5106,
         9.6029,  9.3099,  9.6286, 10.7984,  8.1109,  9.6692,  6.2276,  8.4640,
         9.8401,  6.7164,  7.7133,  7.1601, 13.8080,  4.5746, 10.1626, 15.7695],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 77.2381,  22.5152,  59.1149,  16.5719,  16.9693,  37.4402,  83.4467,
         55.5223,  39.2586,  36.9675,  25.1322, 139.4783,  22.2218,  20.7964,
         20.8201,  14.5134,  16.4992,  22.7216,  18.5589,  32.1304,  19.5202,
         22.8330,  16.6712,  21.3218,  33.7779,  27.6139,  34.6331,  20.8629,
         20.0885,  15.0437,  20.0948,  18.7633,  16.5761,  26.8243,   6.6959,
         18.4857,  14.2652,  14.0464,  19.9297,   9.2255,  14.9374,  15.6695,
         14.1149,  19.7949,  20.9216,  18.2296,  22.3730,  16.6948,  18.6838,
         18.4313,  15.8487,  15.5707,  14.8622,  11.5123,  14.1848,  21.4263,
         15.6747,  20.5885,  18.4256,  19.8533,  21.6823,  18.1926,  17.9790,
         20.5394,  19.3846,  18.9476,  20.2612,  24.8162,  19.0980,  20.3161,
         14.9708,  15.8254,  16.8689,  13.7260,  13.0773,  10.0756,  20.3887,
          7.0975,  20.7044,  32.5692], device='cuda:0')
Outer loop valEpocw Maximum [4/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 59.9
model_train val_loss valEpocw [4/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 68.9
model_train val_loss  valEpocw [4/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 139.5
Max_Val Meta Model:  tensor([27.6385, 39.7329, 43.4928, 43.4335, 32.8994, 39.2571, 38.1083, 37.3117,
        38.8854, 39.6456, 39.2374, 40.2927, 41.4091, 35.9325, 37.7346, 40.9293,
        44.0115, 40.9686, 43.5522, 34.1770, 37.2752, 36.2776, 39.7009, 37.7086,
        38.4363, 37.1923, 39.2112, 33.0271, 31.1018, 33.0969, 36.8847, 36.2633,
        34.8734, 39.5972, 26.8214, 36.9869, 37.6809, 45.4065, 35.3320, 45.4981,
        33.8108, 40.4328, 48.6139, 40.3849, 38.0380, 36.2581, 39.7372, 37.1736,
        51.4719, 37.5455, 32.7538, 43.7147, 32.6999, 43.3113, 31.3188, 39.3949,
        76.8895, 41.4829, 37.4175, 35.8373, 39.5539, 42.8810, 45.3425, 37.8814,
        39.6332, 39.2922, 38.7654, 35.3006, 34.5733, 39.6181, 33.3487, 43.6649,
        42.0839, 39.9199, 44.0565, 42.2355, 38.7610, 41.0169, 40.7390, 40.7068],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([17.5609, 10.8469,  6.2565, 10.2850,  9.9394, 12.0688, 11.7684, 14.8201,
        13.8326, 13.3055, 10.4504, 13.1087, 12.7475, 14.9960,  9.8397,  9.9537,
         9.7455, 10.9088, 13.8382, 12.0520, 12.2802, 13.5234, 11.9832, 12.5429,
        13.4786, 12.1392, 15.0352, 10.4514,  8.2281,  9.0065, 13.0914, 12.1502,
        11.6187, 17.1572,  3.9456, 13.9216, 11.9056, 12.2970, 11.8467, 28.1897,
        14.7104, 35.1390, 44.3335, 34.7857, 25.8028, 32.7528, 15.8986, 15.4807,
        27.8180, 12.5810, 16.3683, 25.3014, 22.5218, 16.4552, 23.4653, 38.2939,
        69.4252, 15.5107, 14.2868, 12.1132, 60.3858, 13.2977, 16.7493, 14.6640,
        15.2667, 12.8077, 14.3673, 13.1610, 11.7636, 12.5490,  8.9231, 14.1391,
        13.2117, 18.9479,  8.5830, 15.6272, 18.1495,  6.8070, 14.2504, 19.9308],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 56.5209,  17.2688,  10.3868,  18.7319,  24.1518,  25.1831,  24.8874,
         32.4662,  28.9576,  27.4168,  21.6377,  26.4284,  25.3884,  34.3419,
         21.1456,  17.9352,  17.4488,  23.2677,  25.6778,  28.8413,  27.1390,
         30.6318,  23.7362,  27.3904,  28.8873,  26.8610,  28.9836,  25.2162,
         21.4446,  21.8181,  29.2780,  27.5983,  27.2442,  35.1711,  11.3951,
         25.9017,  25.3686,  22.2162,  27.4786,  43.5423,  35.1358,  73.3996,
         76.3050,  73.4090,  56.2990,  75.9109,  32.8384,  28.7809,  45.6730,
         26.8540,  40.8410,  46.8799,  56.6966,  29.8262,  61.7916,  83.1924,
        101.7066,  30.9660,  26.2750,  27.2956, 138.9691,  25.0476,  28.9506,
         31.8179,  30.9860,  26.4752,  30.1708,  29.9764,  27.7390,  26.0875,
         21.3404,  26.2909,  23.9177,  38.6880,  14.6178,  23.8945,  31.0956,
         11.2277,  28.7976,  41.3807], device='cuda:0')
Outer loop valEpocw Maximum [4/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 76.9
model_train val_loss valEpocw [4/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 69.4
model_train val_loss  valEpocw [4/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 139.0
Max_Val Meta Model:  tensor([27.9286, 41.1458, 43.4177, 42.3934, 32.9401, 38.3896, 37.3683, 36.4146,
        37.9617, 39.1372, 38.5784, 39.6737, 40.7010, 35.0356, 37.1316, 40.9763,
        43.4911, 40.6463, 42.5262, 34.0112, 36.5752, 35.1463, 33.8058, 36.6812,
        38.1861, 36.7209, 38.4323, 32.9053, 30.8296, 32.7852, 36.7063, 35.6297,
        34.4748, 45.6266, 25.7947, 35.3233, 38.1250, 42.2169, 34.8143, 44.1958,
        32.3602, 36.8637, 29.7715, 38.1286, 36.2596, 33.3925, 38.7520, 34.5317,
        34.5075, 36.6162, 31.4710, 36.5750, 31.3200, 28.9649, 42.4944, 37.0853,
        40.4803, 40.7917, 37.9591, 35.2225, 35.8130, 41.9369, 39.2405, 37.3832,
        38.5359, 39.2213, 37.4950, 39.1019, 33.8282, 38.7886, 32.5991, 43.0565,
        41.8636, 38.6662, 71.4444, 39.2441, 37.5119, 33.2887, 39.9552, 37.6326],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 19.5698,   8.6974,  15.9948,   7.4087,   7.4856,  11.5352,   9.6306,
         13.3115,  10.5968,  14.7207,   8.4582,  11.0005,  10.1813,  11.7970,
          9.7085,   6.9915,   7.2750,   8.1091,  10.3226,   9.3738,   9.2344,
         10.3400,   8.0449,   9.3035,  15.2263,   9.2552,  15.0262,   6.6675,
          8.1985,   6.5855,   9.5331,   8.6872,   7.4945,  15.0006,   2.4376,
         10.2302,   6.1357,   7.9029,   9.0311,   7.5051,   6.5621,   8.5781,
          7.2121,   9.7601,  10.0249,   7.5277,  11.3001,   9.1960,   9.9601,
          8.3098,   6.6134,   8.1316,   6.2504,   5.8760,   6.9673,   9.7209,
          8.3332,  10.1174,  12.3901,   9.2081,  11.3819,  10.0950,  10.5878,
          9.9183,   9.8828,   9.7377,   9.9302,   9.9925,   8.4478,   9.5156,
          6.5545,   9.2434,   8.8394,   7.7465, 190.4003,   8.7276,  12.1560,
          5.4187,  10.5598,  16.0267], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 61.7679,  13.8323,  26.9634,  13.5183,  17.8457,  24.2425,  20.4199,
         29.4824,  22.3798,  30.4505,  17.5042,  22.2110,  20.2882,  27.2430,
         20.9042,  12.7676,  13.2566,  17.2327,  19.3613,  22.1964,  20.4552,
         23.8219,  17.5095,  20.5289,  32.6645,  20.4084,  29.1849,  15.7603,
         21.2246,  15.8065,  21.0622,  19.7186,  17.4196,  28.0477,   7.1196,
         19.3097,  12.2990,  14.4982,  20.9206,  11.6853,  15.8301,  18.0253,
         15.1780,  20.7547,  21.8544,  17.5866,  23.4631,  17.6075,  19.8785,
         17.8007,  16.6638,  16.2190,  15.6435,  12.2750,  14.9172,  20.8056,
         11.9221,  20.1444,  22.8271,  20.7755,  26.0638,  19.1183,  19.0815,
         21.4146,  20.1864,  19.8311,  21.1682,  21.5372,  20.0077,  19.8442,
         15.7269,  17.0618,  15.9095,  15.7910, 318.5765,  13.4258,  20.9607,
          9.7060,  21.4694,  33.8710], device='cuda:0')
Outer loop valEpocw Maximum [4/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 71.4
model_train val_loss valEpocw [4/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 190.4
model_train val_loss  valEpocw [4/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 318.6
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [77.92180894 97.2137279  90.48257209 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.05906361
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 89.80275581 96.13796128 96.24273583 96.9067141
 89.79300934 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [66.65123476 97.2137279  89.57493208 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.05906361
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 89.55787576 96.13796128 96.24273583 96.9067141
 89.79300934 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [89.39591194  4.86789744 46.17385818  4.34829054  1.97067468  3.98913546
  2.5096025   5.77557009  1.88105846  4.05382461  1.33967273  1.37030433
  0.8476353   4.46107312  1.87571052  4.0138481   3.60881847  1.97470632
  0.81868915  1.12161225  1.24889546  0.47360464  0.87420861  1.28961291
  6.15482678  4.12659609 18.17677516  4.38170528  3.49292884  1.26407428
  1.61665133  0.88325533  2.46080278  1.2505891   1.55928584  1.55219759
  2.60488469  1.72984947  1.86272894 24.48781486  7.05141828 24.09631747
  9.23850069  9.74237339  8.69022992 14.60833077  2.95518328  2.18807781
  3.65101925  2.19225253  1.42647771  1.63498145  1.51461794  4.22014992
  2.33889702  4.97994515 42.91404359  8.74652747  9.53667486  4.37383832
 29.60911472  3.38190351 12.32783589  7.34893571  4.68198979  5.47683163
  4.91514045  7.19757106  3.32858543  6.53487969  0.50350937  9.87959102
  5.80995395 13.26127678  6.05670023  8.05140379  1.55904361  2.49630185
  0.30697866  1.12288935]
Accuracy th:0.5 is [45.34545144 97.2137279  73.3178202  97.02489005 97.26733349 78.34821701
 78.78558984 77.40159111 79.58480038 96.41451737 80.02217322 98.52097319
 99.41399349 80.65569377 79.56652575 96.56680596 96.29512311 79.2534204
 98.65376884 98.30776915 80.80798236 80.47416576 98.38695922 79.40205407
 80.73366553 96.65086926 94.0778012  78.96955446 98.01293844 79.85161
 97.30875598 98.57457877 96.36213009 98.02024829 86.03696349 79.52144833
 79.2253993  89.89534728 97.11504489 76.29293015 79.80653257 92.05906361
 78.72589272 78.28486495 96.9627563  93.87434364 98.02877645 98.57336046
 85.66659763 87.75965205 86.98237107 98.55508583 98.99976852 78.93909675
 98.70615611 79.28753305 73.64676356 93.21158368 96.24273583 96.9067141
 89.79300934 97.17717864 90.73598031 79.23514577 98.42838172 79.66033552
 98.20786784 78.3055762  80.38522922 97.55485435 80.9675808  95.99054592
 80.06846895 95.45083515 78.57482243 82.46610056 85.59593572 79.92836345
 81.03215117 99.14718388]
Accuracy th:0.7 is [45.41854997 97.2137279  73.3178202  97.02489005 97.26733349 78.34821701
 78.78558984 77.4978375  79.58480038 96.4742145  80.02217322 98.52097319
 99.41399349 81.07113705 79.56652575 96.56680596 96.29512311 79.2534204
 98.65376884 98.30776915 81.19662285 80.47416576 98.38695922 79.4069273
 81.23926365 96.65086926 94.0778012  78.96955446 98.01293844 79.85161
 97.30875598 98.57457877 96.36213009 98.02024829 86.27209707 79.52144833
 79.2253993  90.15728366 97.11504489 76.29293015 80.38522922 92.05906361
 78.72589272 78.28486495 96.9627563  93.87434364 98.02877645 98.57336046
 87.49406074 88.52962318 87.16511738 98.55508583 98.99976852 78.93909675
 98.70615611 79.28753305 73.64676356 93.6184988  96.24273583 96.9067141
 89.79300934 97.17717864 90.92725478 79.23514577 98.42838172 79.66033552
 98.20786784 78.3055762  80.38522922 97.55972759 80.9675808  95.99054592
 80.06846895 95.45083515 78.57482243 82.56722018 85.69340042 79.92836345
 81.03215117 99.14718388]
Avg Prec: is [56.17141247  3.09321633 11.43901749  3.2410154   2.3188348   3.76326087
  3.38977312  5.65061038  2.41776331  3.82911412  1.56456794  1.56159015
  0.65942517  5.04779595  2.64198207  3.12070664  3.56370092  2.59337554
  1.31120671  1.70717535  1.86019402  0.87442955  1.80088604  2.31793097
  5.0805997   3.74054166  6.60960007  3.21681529  2.18511104  1.88873416
  2.62602631  1.28139594  3.736086    1.71197685  2.32973938  2.40200881
  3.04844687  2.69244182  2.8336262   7.42725129  2.22137911  8.07080993
  3.33272677  4.08464793  3.2475444   6.48552657  2.02311298  1.46622563
  2.07536412  1.6330256   1.79254172  1.60449277  1.02604944  2.9727837
  1.30546063  2.73782407 11.27189575  3.74760423  4.08083429  2.90798547
 10.68378166  2.21248701  3.89825472  3.14251136  1.68756563  2.58737731
  1.85064509  4.26203511  1.25739614  2.37734148  0.20788297  3.4555883
  1.92349855  4.62956824  3.87716198  3.0597628   0.84699793  1.85339282
  0.18493288  0.73897003]
mAP score regular 7.10, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [81.10969928 97.22450607 91.08553205 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74983183 97.82744101 92.37362035
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 89.68781922 96.39235618 96.16314124 96.78102499
 90.13379176 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [72.145402   97.22450607 89.88464509 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.37362035
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 89.85973042 96.39235618 96.16314124 96.78102499
 90.13379176 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [91.6561528   5.77631015 51.47702601  4.68390199  1.50700398  3.82727095
  2.66330962  5.78498924  1.97537167  4.0362201   1.36035559  1.31472257
  0.86772653  4.59308442  1.90455443  4.37865284  3.64741717  1.96200403
  0.73276179  1.09521467  1.13666699  0.47659495  0.89227186  1.21389786
  6.02449605  4.72118236 18.34509084  4.17357491  3.92266835  1.31468657
  1.45812124  0.8135371   2.42064841  1.16554087  1.36273691  1.39610736
  2.38455353  2.00005342  1.89460942 26.35920396  7.12194462 24.66751279
  9.38172907  9.43369776  8.12169432 13.80769532  2.84826882  2.03932269
  3.67093268  2.1261847   1.34023701  1.75124603  1.86969192  4.59409005
  2.43327413  4.79057191 44.81339518  7.61571651  8.83530119  4.64185952
 30.93053454  3.47732361 12.24267712  8.22969752  5.57716619  5.57847726
  5.45642776  8.25602777  3.0651997   6.61347072  0.5814389   9.78702571
  5.16528761 13.46328072  6.38679753  7.90508284  1.65511181  2.56392036
  0.2659005   1.05794597]
Accuracy th:0.5 is [45.31230535 97.22450607 72.0557092  96.96290206 97.90716795 77.67645813
 77.80352293 76.53038344 79.16635523 96.41976231 79.43045071 98.5325261
 99.34972718 78.84495603 79.25355657 96.31262924 96.21047911 78.75028029
 98.78167277 98.34068316 79.87144032 80.06079179 98.31327703 78.83499016
 78.95707203 96.52938685 94.3393876  78.76772056 97.81747515 79.36816404
 97.52597354 98.67204823 96.39983058 98.18870369 86.52614794 78.97949523
 78.88232803 91.69843287 97.0276802  75.86765329 78.91222563 92.37362035
 78.08755014 77.73625333 97.03764606 94.02795426 98.18621222 98.77668984
 86.1549194  87.5177517  85.43488552 98.55993223 98.87385704 78.14236241
 98.6969629  78.77021202 72.40451454 94.25218626 96.16314124 96.78102499
 90.13379176 97.04761193 90.3106859  78.68301069 98.32075143 79.52761791
 98.13139996 77.8284376  80.04833445 97.53095647 80.5615766  96.07843137
 79.74437551 95.44559882 77.761168   83.54884521 87.3159429  79.41051897
 80.63133767 99.15040985]
Accuracy th:0.7 is [45.59134963 97.22450607 72.0557092  96.96290206 97.90716795 77.67645813
 77.80352293 76.55280664 79.16635523 96.41976231 79.43045071 98.5325261
 99.34972718 79.2510651  79.25355657 96.31262924 96.21047911 78.75028029
 98.78167277 98.34068316 80.18785659 80.06079179 98.31327703 78.83748163
 79.3108603  96.52938685 94.3393876  78.76772056 97.81747515 79.36816404
 97.52597354 98.67204823 96.39983058 98.18870369 86.83010688 78.97949523
 78.88232803 91.83546354 97.0276802  75.86765329 79.18877843 92.37362035
 78.08755014 77.73625333 97.03764606 94.02795426 98.18621222 98.77668984
 87.80177891 87.83416797 85.59683085 98.55993223 98.87385704 78.14236241
 98.6969629  78.77021202 72.40451454 94.47890973 96.16314124 96.78102499
 90.13379176 97.04761193 90.4825971  78.68301069 98.32075143 79.52761791
 98.13139996 77.8284376  80.04833445 97.53593941 80.5615766  96.07843137
 79.74437551 95.44559882 77.761168   83.61611481 87.44051623 79.41051897
 80.63133767 99.15040985]
Avg Prec: is [53.36659253  3.6846677  14.90292452  4.53126855  1.46990283  4.4529816
 14.57239066  8.72057385  8.61561006  5.3581098   3.78153592  5.37503656
  2.35142451  5.79264295  3.02556176  3.7419938  16.41095545  6.49919916
  1.56904688  3.0890765   3.48395137  1.5835015   1.11645503  5.13251189
  5.52732517  7.87497018  7.71624208  4.58570288  3.86866121  4.44547471
  2.10699348  0.84085913  3.01514119  1.10276806  1.6246211   2.11469355
  1.94907631  2.17861375  2.24528907  6.19192605  1.71964758  5.98482961
  2.18222291  2.70720381  2.36913492  4.82863254  1.65385869  1.02412919
  1.37477812  1.15623118  1.19570112  0.96904642  0.72597711  2.2916596
  0.83710002  1.81603399  9.79406116  2.82873712  3.6874516   2.62850473
  7.71013494  2.09060147  3.02878486  2.50020346  1.34003905  1.80363709
  1.48909185  3.42236076  1.10305283  2.2863085   0.19073191  3.30897567
  1.60872375  3.84077181  3.54514689  2.30740328  0.59356842  1.48513453
  0.1273763   0.59147815]
mAP score regular 7.29, mAP score EMA 4.20
Train_data_mAP: current_mAP = 7.10, highest_mAP = 7.10
Val_data_mAP: current_mAP = 7.29, highest_mAP = 7.29
tensor([0.2992, 0.6587, 0.6117, 0.5507, 0.4137, 0.4724, 0.4697, 0.4545, 0.4788,
        0.4815, 0.4754, 0.4907, 0.5031, 0.4345, 0.4650, 0.5759, 0.5699, 0.4649,
        0.5503, 0.4185, 0.4470, 0.4407, 0.4604, 0.4571, 0.4670, 0.4558, 0.5272,
        0.4141, 0.3721, 0.4215, 0.4496, 0.4312, 0.4197, 0.5331, 0.3383, 0.5550,
        0.5080, 0.5558, 0.4266, 0.6623, 0.4027, 0.4831, 0.4862, 0.4751, 0.4512,
        0.4250, 0.4905, 0.5352, 0.5126, 0.4597, 0.3928, 0.5247, 0.3907, 0.4817,
        0.4752, 0.4571, 0.7331, 0.5048, 0.5674, 0.4366, 0.4241, 0.5316, 0.5689,
        0.4673, 0.4988, 0.4877, 0.4679, 0.4872, 0.4275, 0.4770, 0.3962, 0.5474,
        0.5984, 0.4892, 0.6265, 0.6877, 0.6037, 0.5749, 0.4840, 0.4833],
       device='cuda:0')
Max Train Loss:  tensor([19.7035, 12.3120, 16.7107, 12.5027, 10.8605, 12.1755, 13.2632, 14.0423,
        13.1235, 14.2354,  8.2696, 11.6447, 11.0514, 15.3147, 13.1921, 16.7018,
        16.8509,  8.9007, 15.4763, 11.0408,  9.2828, 13.9272, 14.6485, 13.8644,
        13.0348, 11.4961, 16.7012,  8.3384,  7.7554, 11.8293, 12.6527, 10.3338,
        10.0623, 15.9873,  4.7307, 12.4674, 14.4601, 18.7067, 11.6038, 18.6945,
         7.8281, 15.5693, 10.0756, 12.0872, 11.0219, 11.2179, 11.6473,  9.4625,
        11.0622,  8.3206,  8.9471, 10.5370,  6.7978,  6.1264,  9.5005, 10.5418,
        21.5850, 14.6712, 13.7951, 10.3975, 15.6535, 15.5178, 15.0253, 12.0565,
        10.7265, 11.2028, 11.7718, 13.4322,  9.8063, 10.8438,  6.3291, 14.4698,
        12.5818, 11.9051, 13.3389, 14.3111, 16.1273,  8.9916, 11.2542, 16.3398],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [5/80], Step [000/642], LR 1.0e-04, Loss: 21.6
Max Train Loss:  tensor([25.4199, 26.0153, 23.3224, 16.9010, 10.5291, 14.0766, 16.3530, 14.4215,
        21.4360, 14.5464, 13.3437, 12.0162, 23.4640, 13.3017, 14.9631, 18.4576,
        20.4293, 13.3851, 18.0732,  9.7624, 11.2785,  9.0901, 10.1253, 12.6192,
        15.0686, 10.4275, 21.3335, 11.3854,  7.8626,  8.9911,  9.8589,  9.0736,
        13.8519, 13.5057,  6.4661, 15.7733, 22.2437, 17.7030, 10.5643, 11.4317,
         7.2427, 17.2701, 13.2297, 11.3382,  8.2823, 12.8135, 12.5240, 11.5419,
        11.7360, 11.9414,  8.7768,  9.8864,  7.4991, 12.5147, 21.8595, 10.7461,
        17.9912, 13.7319, 14.0802,  9.3823, 15.7041, 20.6442, 15.4302, 11.0134,
        10.7175, 21.8975, 10.7539, 14.8944,  9.1894, 12.8027,  7.4205, 19.8082,
        22.1669, 19.1925, 21.3035, 19.1856, 14.6654, 21.8722,  9.9970, 13.9982],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [5/80], Step [100/642], LR 1.0e-04, Loss: 26.0
Max Train Loss:  tensor([20.0878, 16.1848, 18.2544, 12.0862,  8.6275, 12.9542, 12.3547, 10.3099,
        16.2966, 14.1963, 10.7061, 12.5749, 10.3134, 11.9506, 15.2805, 12.4433,
        13.2928, 12.2943, 20.9473,  9.0508, 11.4396,  7.3303, 13.6795, 13.7033,
        13.1557, 11.8824, 16.6171,  7.6187,  5.6962,  8.6272, 10.6414,  9.9265,
         8.8699, 11.5461,  3.6330, 11.4184, 11.7031, 14.2260,  7.5077, 19.4352,
         8.0404, 17.3639, 16.2994, 12.8607, 10.6238, 18.5363, 13.5536, 13.7698,
        12.8601, 11.6241,  9.0348, 12.5319,  6.9639, 11.2919, 10.4923,  9.2704,
        20.9222, 13.2002, 15.2140,  9.4909, 17.7998, 10.1719, 17.8906, 11.0224,
        10.2479, 11.5521, 10.2772, 10.3614,  8.0364, 12.3144,  5.5116, 14.5147,
        10.4145, 10.3347, 17.0904, 13.1254, 14.0021, 10.1292, 10.3766, 12.2605],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [5/80], Step [200/642], LR 1.0e-04, Loss: 20.9
Max Train Loss:  tensor([18.1858, 20.1832, 23.9859, 13.5423,  9.2426, 13.2093, 14.3770, 14.2237,
         8.7145, 11.5247, 10.1575, 12.6130,  7.7801, 14.4699, 13.9287, 11.7775,
        16.8650, 12.0395, 12.2073,  7.7952,  8.9404,  7.6116, 12.6327, 13.6292,
        12.7415, 14.4970, 14.6287,  9.7664,  6.0177, 10.4532, 10.2784,  9.1783,
        12.8419, 12.5399,  4.6333, 11.7836, 14.5689, 14.2879, 13.0600, 22.8536,
         9.9243, 18.5536, 17.9270, 13.5526, 10.3294, 11.5968, 10.6481, 10.7433,
        12.3725,  9.7936,  7.9167, 11.9052,  8.1957, 15.0897, 10.6189,  9.7066,
        21.8838, 13.0655, 15.5668, 11.0367, 21.1855, 10.1341, 18.7685, 10.5431,
        10.3461, 10.0497, 11.6059, 11.7109,  9.2932, 11.3980,  6.2287, 12.3049,
         7.9091, 14.2279,  9.5084, 13.8364, 11.4356, 13.1814,  9.8636, 14.5282],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [5/80], Step [300/642], LR 1.0e-04, Loss: 24.0
Max Train Loss:  tensor([17.4104, 14.7547, 15.7811, 13.1976,  7.0225, 13.4231, 10.0398, 15.9262,
         6.7127, 11.4399,  9.7500, 12.2966,  8.4947, 11.8520, 12.5900, 12.1147,
        13.5855, 11.7417, 10.3788,  8.6506, 10.3817,  9.3513,  9.0025, 13.1227,
        18.0027, 14.7320, 18.3353, 11.0782,  7.8645,  7.2457, 11.1529, 11.1433,
        11.7580, 10.8198,  7.2973, 13.7100, 14.2530, 13.1436, 10.8152, 23.3693,
         7.9237, 17.1873, 15.7932, 14.5400,  9.0042, 15.1427, 12.2195, 11.4305,
        12.8909, 11.5015,  8.9842, 12.1948,  7.3638, 14.3520,  8.5720, 11.5775,
        23.7558, 15.0418, 11.4754, 12.4020, 16.8769, 12.7177, 15.6848, 13.4078,
        11.6294, 11.2515, 11.6159, 13.4491,  8.2649, 12.8525,  6.3988, 16.8159,
        10.3478, 13.6501, 16.1721, 10.3310, 11.1251, 11.3002, 10.5912, 12.1887],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [5/80], Step [400/642], LR 1.0e-04, Loss: 23.8
Max Train Loss:  tensor([16.8182, 13.2732, 22.6164, 16.4193, 10.6238, 12.4895, 14.3171, 12.0896,
        13.7152, 11.4862, 11.7487, 10.0049,  9.0887, 17.0239, 12.9814, 13.4970,
        12.5524, 10.7644,  6.7760,  8.4332, 11.8433,  8.8259, 13.1902, 12.2087,
        17.6145, 12.7425, 19.2393,  9.5520,  6.9614,  7.8461, 15.0864, 10.3230,
        13.9244, 12.4441,  8.0849, 15.2835, 15.2197, 15.4054,  9.9729, 21.2805,
         8.4473, 12.0334, 15.3096, 12.3010, 12.9693, 14.2121, 10.6724, 10.9971,
        14.2533, 11.5737,  8.2896, 14.9177,  8.7086, 13.2470, 10.6885, 11.3777,
        24.4364, 12.0820, 11.1561, 11.3052, 16.5865, 10.5350, 15.8740, 11.5217,
        11.6103, 11.2205, 11.4946, 15.7374,  7.8065, 11.8039,  5.8610, 13.0976,
         9.9620, 11.5199, 12.9797, 11.4262, 13.7190, 12.7867, 10.0520, 12.7947],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [5/80], Step [500/642], LR 1.0e-04, Loss: 24.4
Max Train Loss:  tensor([18.7371, 10.0255, 21.7825, 16.2974,  6.0735, 11.1871, 13.6534, 15.3675,
         6.2808, 11.0032, 10.6788, 11.4639,  8.6627,  9.9922, 12.8351, 13.7696,
        12.5651, 11.0393,  7.7522,  6.4153, 10.5531,  8.8208, 10.2677, 11.4153,
        16.6062, 13.8340, 16.9142,  8.8094,  8.9940,  9.5733, 14.5907, 12.7245,
         7.3925, 13.9795,  4.9705,  6.8088, 13.3708, 13.1526,  7.5303, 26.1094,
         6.5985, 20.2617, 16.1237, 11.1560, 12.7298, 13.3338, 12.6406,  9.4913,
        13.2181, 10.2497,  5.7504,  9.6482,  7.2325, 15.0038, 10.2955, 12.2541,
        19.5060, 13.8531, 11.5928,  9.3289, 15.2020, 17.2685, 17.6377, 12.6011,
         9.6806,  8.9909, 10.4773, 12.9378,  9.9300, 13.8166,  5.6481, 17.3631,
         8.7416, 14.5535, 11.6934, 13.3619, 12.2266, 13.7972,  9.7532, 12.5963],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [5/80], Step [600/642], LR 1.0e-04, Loss: 26.1
Max_Val Meta Model:  tensor([ 25.3975,  42.7517,  63.1592,  26.4252,   6.0833,  11.6394,  11.2477,
         14.7618,   6.1055,  12.6984,  10.0630,  11.4171,   8.5672,  11.3613,
         11.5725,   9.9487, 182.2714,   9.0758,   6.5046,   6.4292,   8.8607,
          7.5146,   9.1391,   9.4678,  18.8114,  13.0186,  17.2926,   6.2171,
          6.8339,   8.8332,   8.4892,   8.0506,   8.2826,   8.1845,   2.1276,
          3.8058,  10.8239,   9.4065,   6.8260,  33.9375,   7.9507,  15.0501,
         14.0183,  13.5523,  10.5170,  13.5183,   9.7584,  10.4641,   9.9780,
          9.5611,   5.7614,   9.6931,   6.7730,  11.5283,   8.7157,   8.1285,
         21.8710,  12.2705,  13.5304,   7.1687,  12.0638,  17.4328,  13.7991,
         10.4212,   9.7079,   8.2504,   9.7639,  10.5641,   8.9205,  15.0856,
          5.6575,  13.7218,  13.7089,   9.7535,  11.6748,  11.4733,  11.3247,
         10.2368,   9.7774,  11.8025], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 22.9683,  41.1595,  60.4510,  27.6021,   6.5876,  11.8552,  11.6484,
         15.1306,   6.4831,  13.2084,  10.6854,  12.0197,   8.9766,  11.8424,
         12.4938,  10.7879, 173.3793,   9.7217,   6.9983,   6.9541,   9.4994,
          8.0997,   9.7887,  10.1642,  19.2651,  13.1486,  17.7062,   6.7264,
          7.3261,   9.0052,   9.1178,   8.6496,   8.7726,   8.8737,   2.3695,
          4.2316,  11.5171,   9.6661,   7.3743,  32.5631,   8.4853,  15.4717,
         14.7627,  14.3815,  10.9462,  14.1683,  10.4327,  11.2458,  10.6607,
         10.2212,   6.2428,  10.3713,   7.1402,  12.2765,   9.3125,   8.7753,
         22.9535,  12.9793,  13.3448,   7.7329,  13.8519,  17.7033,  14.6149,
         11.0398,  10.4516,   8.9353,  10.4451,  11.4983,   9.4516,  15.8535,
          6.1243,  14.1348,  14.2315,  10.5121,  11.8896,  12.1031,  11.8969,
         10.9137,  10.5296,  12.5637], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 76.7731,  62.4893,  98.8254,  50.1183,  15.9228,  25.0962,  24.8017,
         33.2876,  13.5390,  27.4296,  22.4761,  24.4973,  17.8439,  27.2551,
         26.8710,  18.7337, 304.2241,  20.9131,  12.7181,  16.6177,  21.2538,
         18.3807,  21.2625,  22.2372,  41.2528,  28.8501,  33.5835,  16.2451,
         19.6860,  21.3670,  20.2785,  20.0609,  20.9024,  16.6463,   7.0033,
          7.6252,  22.6731,  17.3907,  17.2859,  49.1665,  21.0701,  32.0264,
         30.3662,  30.2730,  24.2614,  33.3374,  21.2693,  21.0121,  20.7961,
         22.2341,  15.8929,  19.7666,  18.2737,  25.4854,  19.5973,  19.1985,
         31.3123,  25.7133,  23.5175,  17.7115,  32.6600,  33.3007,  25.6881,
         23.6265,  20.9542,  18.3232,  22.3217,  23.6022,  22.1097,  33.2351,
         15.4559,  25.8221,  23.7822,  21.4866,  18.9768,  17.5995,  19.7053,
         18.9849,  21.7545,  25.9946], device='cuda:0')
Outer loop valEpocw Maximum [5/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 182.3
model_train val_loss valEpocw [5/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 173.4
model_train val_loss  valEpocw [5/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 304.2
Max_Val Meta Model:  tensor([30.8331, 45.4395, 52.7123, 41.8992, 35.3722, 40.9574, 45.1800, 38.4758,
        39.2852, 41.1018, 40.6774, 44.1235, 42.4523, 37.1447, 44.4934, 48.3931,
        45.8122, 39.7544, 48.4711, 35.7068, 38.8149, 36.6161, 38.0933, 49.1806,
        39.1416, 37.8477, 41.0565, 35.7106, 33.9133, 35.5400, 37.4649, 37.8561,
        37.0483, 42.8620, 30.0981, 41.1326, 40.1076, 43.5929, 36.1109, 47.5592,
        35.8842, 39.2515, 39.0117, 39.0958, 38.3195, 36.2706, 39.6282, 39.4269,
        42.2308, 39.7109, 33.7747, 40.4028, 34.2975, 39.7600, 38.7087, 38.6344,
        44.3011, 42.3745, 44.6603, 36.6841, 39.8385, 46.6768, 47.2795, 39.7130,
        39.4565, 40.9150, 39.6153, 40.7148, 35.0433, 40.1731, 35.7649, 42.9271,
        46.4195, 40.3932, 41.9094, 42.0530, 44.5841, 43.8916, 40.7189, 39.6723],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([29.4265, 11.1241, 36.6038, 11.7235,  5.8782, 17.5759, 40.2362, 26.1264,
        17.5332, 18.4603, 12.6008, 71.8951,  8.9189,  7.9549, 13.6133,  8.9747,
         8.3962, 12.0025,  6.2759, 11.6323,  8.5345,  6.9275,  8.5491, 10.9499,
        15.4577, 12.6561, 18.2520,  8.7994,  7.6951,  6.3412,  7.8277,  7.8192,
         7.2475,  7.4737,  2.0904,  3.0681, 10.5123,  6.4652,  6.4197,  4.8717,
         5.9470,  6.8632, 11.9406,  8.9948,  7.4970,  9.6199,  8.8490,  8.4878,
         9.1629,  9.4412,  5.5174,  8.2712,  5.4848, 10.7426,  8.1067,  8.7881,
        11.4720, 10.4674,  7.1602,  6.8031,  9.4575,  7.9853, 11.5619,  9.3554,
         8.7881,  7.6890,  9.3284, 11.8911,  6.3507,  9.9187,  5.7483,  7.0144,
         7.0266,  8.0357,  7.3607,  5.8261,  9.1484,  8.0612,  9.1694, 10.7035],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 85.2569,  18.9889,  58.8142,  22.3488,  13.4581,  36.4030,  79.6370,
         57.0643,  36.7540,  37.6406,  25.8988, 142.9299,  17.4171,  17.7714,
         26.6941,  16.6762,  15.3849,  24.7188,  10.8658,  26.8901,  18.4195,
         15.7214,  18.4663,  20.7858,  33.4579,  28.1532,  36.8057,  20.0118,
         18.8090,  14.6152,  17.4670,  17.2863,  16.3127,  14.0672,   5.5362,
          5.9320,  22.0001,  12.1410,  14.6984,   8.6353,  13.4313,  14.1838,
         25.5619,  19.3076,  15.8567,  21.8251,  18.3840,  16.7658,  17.9805,
         19.4613,  13.4221,  17.1295,  13.0233,  22.5049,  17.0423,  18.5492,
         16.1822,  20.7919,  13.5176,  15.1240,  20.3090,  14.0406,  21.3920,
         19.6676,  18.1851,  15.4624,  19.4535,  23.9501,  14.9367,  20.7225,
         13.0507,  12.8978,  12.3659,  16.5116,  14.3257,   9.8873,  16.8752,
         15.1745,  18.8078,  22.7852], device='cuda:0')
Outer loop valEpocw Maximum [5/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 52.7
model_train val_loss valEpocw [5/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 71.9
model_train val_loss  valEpocw [5/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 142.9
Max_Val Meta Model:  tensor([30.1920, 43.4004, 39.8014, 39.4473, 32.9891, 38.4360, 39.2440, 35.9334,
        32.6429, 38.6342, 38.2280, 39.3914, 39.6106, 35.0393, 40.7228, 40.4135,
        43.8331, 30.4345, 41.9547, 33.5019, 38.3741, 34.6124, 36.3499, 41.4653,
        37.3671, 35.7991, 39.1406, 33.8283, 31.2133, 33.2578, 35.6059, 35.5567,
        35.0533, 40.7723, 27.1865, 33.1890, 38.4704, 41.5003, 33.8613, 44.2381,
        34.0942, 40.6750, 41.3007, 38.5994, 37.5030, 36.7862, 45.3926, 45.6651,
        46.6088, 37.6967, 32.7634, 39.3885, 36.3341, 38.2284, 39.2580, 39.1795,
        71.6202, 40.5196, 42.2346, 34.5791, 44.6217, 41.8205, 43.9730, 37.6471,
        37.8373, 38.4196, 37.8647, 38.6872, 33.2837, 38.1229, 33.4620, 41.0857,
        43.8886, 38.6414, 39.5442, 39.3220, 43.4018, 42.1325, 38.7010, 37.9139],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([21.8397,  6.9514,  3.1476, 12.7651,  7.4711, 11.1644, 10.9156, 12.0671,
         6.0150, 11.3885, 10.7852, 11.2724,  8.5943, 12.7522, 12.9630,  9.3588,
         7.5426,  9.7310,  7.4204,  7.7819, 11.4996,  8.7240, 10.8528, 11.7440,
        11.7857, 10.7331, 15.0628,  9.4426,  6.7069,  7.9726, 10.6221, 10.2799,
        10.5008,  9.8194,  2.9261,  4.4781, 14.9508, 10.8615,  8.1088, 20.5692,
        14.1863, 42.0836, 35.5183, 34.2184, 26.5614, 31.8734, 13.9990, 15.2118,
        24.8994, 12.1927, 15.3629, 22.8924, 23.3467, 16.7693, 29.6302, 41.9823,
        81.6157, 14.0778,  9.5074,  8.5718, 60.0759,  9.8308, 16.1051, 13.2419,
        13.0117,  9.8657, 12.8079, 13.3195,  8.7076, 11.0027,  7.3404, 11.2100,
         9.0456, 19.1963,  6.9559, 12.5584, 13.6598,  9.9420, 11.3792, 13.0163],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 65.2059,  11.3956,   5.1200,  24.1695,  17.3348,  23.1814,  22.7544,
         26.5714,  12.7663,  23.3690,  22.3605,  22.5082,  16.9413,  28.8542,
         26.2461,  17.9533,  13.6632,  22.4071,  13.3132,  18.0474,  22.8045,
         19.8625,  22.7815,  23.8464,  25.1964,  24.0100,  29.9264,  21.4610,
         16.7638,  18.5911,  23.7585,  23.0260,  23.8063,  18.3433,   7.9326,
          8.5529,  31.0488,  19.9758,  18.7465,  36.1231,  32.3933,  87.0352,
         75.4732,  73.6301,  56.5808,  72.9299,  26.3322,  26.4957,  46.2768,
         25.2784,  38.0625,  46.9419,  56.3635,  35.2040,  62.2633,  89.6998,
        113.3707,  27.9314,  17.5930,  19.1598, 131.1060,  17.5634,  29.4990,
         28.1166,  26.8850,  20.0358,  26.7249,  26.9333,  20.4484,  23.0939,
         16.8188,  20.5861,  15.7578,  39.6167,  13.4072,  20.9272,  24.7465,
         18.4655,  23.3922,  27.6977], device='cuda:0')
Outer loop valEpocw Maximum [5/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 71.6
model_train val_loss valEpocw [5/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 81.6
model_train val_loss  valEpocw [5/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 131.1
Max_Val Meta Model:  tensor([28.3069, 39.7513, 41.4225, 37.5244, 31.6363, 37.2091, 37.8748, 34.9940,
        30.3601, 37.4751, 36.8035, 37.9917, 37.5531, 33.4395, 39.5504, 38.6088,
        39.2840, 27.8457, 40.3218, 31.9320, 36.4021, 33.0686, 31.1628, 40.4705,
        36.2396, 34.6667, 37.9579, 32.3141, 30.1444, 31.9025, 34.1782, 34.0855,
        26.1780, 38.5572, 25.5997, 30.7647, 36.6278, 38.3066, 32.5165, 46.6572,
        31.9035, 39.2628, 36.4721, 35.6487, 34.5312, 33.5645, 38.6638, 35.3829,
        36.8733, 36.6879, 29.8130, 45.0626, 30.5484, 43.5709, 37.5976, 34.7033,
        41.6292, 38.9215, 36.0456, 42.2278, 38.0632, 40.0543, 44.6728, 36.2465,
        36.0930, 37.3858, 36.3710, 51.4689, 31.7786, 36.4633, 31.9628, 40.1188,
        44.7512, 36.5046, 53.4180, 40.0770, 28.8594, 43.2376, 37.1469, 32.3808],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 18.5497,   6.3693,  12.8287,  10.4812,   6.0897,  11.8227,  10.0367,
         12.4722,   4.7537,  14.8702,   9.8908,  10.4322,   7.4046,  10.7764,
         13.4928,   7.5389,   6.1253,   8.1733,   6.2807,   6.3286,   9.4996,
          7.1812,   8.6723,   9.8447,  15.0261,   9.1123,  16.8877,   6.3484,
          7.9183,   6.5544,   8.1570,   8.0383,   6.8552,   8.0202,   2.1531,
          3.0042,   9.8783,   7.1143,   6.6659,   6.6371,   6.1301,   7.1246,
         12.2658,   9.2977,   7.7372,   9.8810,   9.6708,   9.1888,   9.5195,
          9.2381,   5.6417,   9.6161,   5.6667,  11.6091,   8.8476,   8.0239,
          8.4095,   9.9929,   9.6620,   8.3775,  12.1553,   8.2171,  12.1248,
          9.6571,   9.0656,   8.0761,   9.6776,  11.6682,   6.6291,   9.3237,
          5.9580,   7.5898,   6.2740,   8.7163, 166.2310,   8.3214,   8.0894,
          9.7605,   9.4243,  10.6247], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 54.0555,  10.7281,  20.6765,  19.8983,  13.9997,  24.5807,  20.8650,
         27.1664,  10.1374,  30.3953,  20.3816,  20.7306,  14.5147,  24.3802,
         27.2784,  14.7145,  11.3490,  18.8059,  11.2641,  14.6421,  19.0907,
         16.3153,  19.1245,  19.9475,  32.3195,  20.1858,  33.5231,  14.2966,
         19.7200,  15.1658,  18.1379,  17.9291,  16.9240,  14.6996,   5.7548,
          5.8010,  20.5378,  12.2653,  15.2752,  11.5177,  13.9810,  13.3963,
         26.0058,  19.9983,  16.4446,  22.4561,  19.0675,  17.3899,  18.7274,
         18.7741,  13.9487,  17.5673,  13.5355,  23.2056,  17.6759,  17.0850,
         11.8022,  19.7285,  19.0153,  15.6984,  26.0676,  14.7594,  22.2533,
         20.3322,  18.7206,  16.0815,  20.1095,  21.4544,  15.5083,  19.5361,
         13.5603,  13.8828,  10.5764,  17.9572, 308.4872,  13.2023,  17.1262,
         17.3852,  19.3595,  23.5933], device='cuda:0')
Outer loop valEpocw Maximum [5/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 53.4
model_train val_loss valEpocw [5/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 166.2
model_train val_loss  valEpocw [5/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 308.5
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [70.05031615 97.2137279  90.18408645 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.05906361
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 90.13291748 96.13796128 96.24273583 96.9067141
 89.79300934 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [56.6525749  97.2137279  89.59564333 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.05906361
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 89.6468123  96.13796128 96.24273583 96.9067141
 89.79300934 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [88.74499731  5.25278933 48.64684124  4.20578908  1.90428495  4.06268912
  2.92568567  6.14108924  2.13729279  4.29852735  1.51166282  1.45379444
  0.93394576  5.12261848  1.95370317  4.04635959  3.93256988  2.12237296
  0.88110913  1.18353994  1.28550795  0.4716636   0.87141051  1.38074844
  6.24143394  4.40497032 15.29107169  3.88387326  3.54099819  1.34279249
  1.60065372  0.86730288  2.7474164   1.32546746  1.7309015   1.93507045
  2.63782433  1.78590368  2.15599028 24.15458511  5.67591073 22.71058562
  4.45830255  7.47877466  6.50212286 15.86527857  2.50768738  1.97830903
  2.54945451  1.95158109  1.22429973  1.31915957  1.14502885  3.32783961
  1.73431278  3.85483743 46.01322855 10.04840153 11.07565043  4.41038171
 34.28854345  3.02731842 14.16839091  7.49058726  5.24151329  5.77830757
  5.27563326  6.61038077  3.57446828  6.71925597  0.55901625  8.86173296
  6.22042964 15.74987103  5.86627834  9.32595627  1.35839619  2.29831817
  0.24952464  0.88605658]
Accuracy th:0.5 is [45.47337386 97.2137279  73.10339786 97.02489005 97.26733349 78.08993555
 78.49563236 77.21762649 79.37281466 96.41451737 79.7541453  98.52097319
 99.41399349 80.51680657 79.27900489 96.56680596 96.29512311 79.13889938
 98.65376884 98.30776915 80.61183465 80.23294063 98.38695922 79.19006835
 80.68127825 96.65086926 94.0778012  78.77218845 98.01293844 79.63718766
 97.30875598 98.57457877 96.36213009 98.02024829 86.13930142 79.26316687
 79.05970931 89.92824161 97.11504489 76.26856398 79.61403979 92.05906361
 78.49441405 78.11673834 96.9627563  93.87434364 98.02877645 98.57336046
 86.2477309  87.46116641 86.77525859 98.55508583 98.99976852 78.6686322
 98.70615611 79.02437836 73.59072136 93.11290067 96.24273583 96.9067141
 89.79300934 97.17717864 90.50815658 78.88427285 98.42838172 79.41423716
 98.20786784 78.11795665 80.1586238  97.55607266 80.73366553 95.99054592
 79.85648323 95.45083515 78.30679451 82.55869202 85.53867521 79.63596935
 80.81529221 99.14718388]
Accuracy th:0.7 is [45.45144431 97.2137279  73.10339786 97.02489005 97.26733349 78.08993555
 78.49563236 77.31996443 79.37281466 96.47055957 79.7541453  98.52097319
 99.41399349 80.92494    79.27900489 96.56680596 96.29512311 79.13889938
 98.65376884 98.30776915 81.0504258  80.23294063 98.38695922 79.20712467
 81.17956653 96.65086926 94.0778012  78.77218845 98.01293844 79.63718766
 97.30875598 98.57457877 96.36213009 98.02024829 86.37443501 79.26316687
 79.05970931 90.21698079 97.11504489 76.26856398 80.22684909 92.05906361
 78.49441405 78.11673834 96.9627563  93.87434364 98.02877645 98.57336046
 88.07519402 88.23601077 86.95556828 98.55508583 98.99976852 78.6686322
 98.70615611 79.02437836 73.59072136 93.5210341  96.24273583 96.9067141
 89.79300934 97.17717864 90.7067409  78.88427285 98.42838172 79.41423716
 98.20786784 78.11795665 80.1586238  97.55972759 80.73366553 95.99054592
 79.85891985 95.45083515 78.30679451 82.65006518 85.63613991 79.63596935
 80.81529221 99.14718388]
Avg Prec: is [56.05706823  2.99587393 11.27709049  3.28667896  2.34682164  3.78804448
  3.23302452  5.52773512  2.43576011  3.91073432  1.61716312  1.5859674
  0.62649862  5.08258005  2.59316396  3.08272354  3.70198193  2.59461032
  1.44127332  1.70507053  1.99216475  0.89992128  1.85108628  2.32925668
  5.06900327  3.61490081  6.47795704  3.36501209  2.01440409  2.01060864
  2.54553252  1.32768205  3.82082732  1.63615472  2.51214622  2.43302504
  3.03884504  2.54807899  2.92380364  7.33408171  2.24197278  8.36410506
  3.36285979  4.00144149  3.25030457  6.57203402  2.07871176  1.51044869
  2.08416303  1.50631558  1.8359936   1.59246242  1.03277813  2.9790952
  1.31762109  2.69768303 11.29906458  3.75738402  3.94345404  2.95240353
 10.84676499  2.20757431  3.83950696  3.12182143  1.62820439  2.55962181
  1.85370987  4.07154264  1.26736922  2.39634073  0.22598777  3.37301704
  1.90689735  4.65012643  4.00208218  3.11631208  0.84419106  1.93861143
  0.13902856  0.7073627 ]
mAP score regular 7.13, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [75.75304582 97.22450607 90.71181204 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.37362035
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 89.96686349 96.39235618 96.16314124 96.78102499
 90.13379176 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [63.44520019 97.22450607 89.78498642 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.37362035
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 89.71273389 96.39235618 96.16314124 96.78102499
 90.13379176 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [91.10102724  6.74020654 54.239938    4.62331525  1.50180014  3.98060163
  3.07749874  6.29693062  2.24803703  4.25977138  1.4906364   1.39496556
  0.93125534  5.38583446  1.97269853  4.57511298  4.11506423  2.15617581
  0.78739352  1.21123667  1.13266394  0.47077     0.88591797  1.26793738
  6.5222195   4.87575805 15.32937129  3.45666928  4.20239265  1.40463096
  1.40910394  0.78584326  2.77510623  1.25459476  1.52481056  1.79034992
  2.43110999  1.98526071  2.23433084 25.99120204  5.96489899 23.46220919
  4.16941733  6.77574942  6.25260297 15.82105794  2.37276578  2.00334608
  2.49230133  1.84028307  1.16316241  1.3748175   1.34460747  3.36201079
  1.7420091   3.38235777 45.93028383  9.84756825 10.6890454   4.92274127
 36.18464612  3.05950677 15.91325483  8.72102902  5.93604893  6.01323608
  6.12422356  7.08064967  4.16207353  7.59288497  0.98580515 10.07023095
  6.83741775 18.17013034  6.43669411  9.92913203  1.4951709   2.27739883
  0.2185403   0.88102615]
Accuracy th:0.5 is [45.28240775 97.22450607 71.91369559 96.96290206 97.90716795 77.50952986
 77.63659466 76.3684381  79.00939283 96.41976231 79.26352244 98.5325261
 99.34972718 78.73533149 79.0866283  96.31262924 96.21047911 78.59331789
 98.78167277 98.34068316 79.73939258 79.91379525 98.31327703 78.67304482
 78.89727683 96.52938685 94.3393876  78.60079229 97.81747515 79.2062187
 97.52597354 98.67204823 96.39983058 98.18870369 86.55853701 78.82751576
 78.72038269 91.64611207 97.0276802  75.72065675 78.76772056 92.37362035
 77.93557067 77.574308   97.03764606 94.02795426 98.18621222 98.77668984
 86.64822981 87.4504821  85.38505618 98.55993223 98.87385704 77.97543414
 98.6969629  78.62321549 72.26748387 94.21481426 96.16314124 96.78102499
 90.13379176 97.04761193 90.29075417 78.51608242 98.32075143 79.37563844
 98.13139996 77.686424   79.88638912 97.53095647 80.39464833 96.07843137
 79.57744724 95.44559882 77.6191544  83.54386227 87.31843436 79.25355657
 80.4644094  99.15040985]
Accuracy th:0.7 is [45.58636669 97.22450607 71.91369559 96.96290206 97.90716795 77.50952986
 77.63659466 76.39335277 79.00939283 96.41976231 79.26352244 98.5325261
 99.34972718 79.1613723  79.0866283  96.31262924 96.21047911 78.59331789
 98.78167277 98.34068316 80.05830032 79.91379525 98.31327703 78.67553629
 79.23860777 96.52938685 94.3393876  78.60079229 97.81747515 79.2062187
 97.52597354 98.67204823 96.39983058 98.18870369 86.88741062 78.82751576
 78.72038269 91.80307447 97.0276802  75.72065675 79.06171363 92.37362035
 77.93557067 77.574308   97.03764606 94.02795426 98.18621222 98.77668984
 88.43959439 87.77188131 85.52457832 98.55993223 98.87385704 77.97543414
 98.6969629  78.62321549 72.26748387 94.4664524  96.16314124 96.78102499
 90.13379176 97.04761193 90.4751227  78.51608242 98.32075143 79.37563844
 98.13139996 77.686424   79.88638912 97.53593941 80.39464833 96.07843137
 79.57744724 95.44559882 77.6191544  83.60614894 87.4355333  79.25355657
 80.4644094  99.15040985]
Avg Prec: is [53.41587498  3.69491925 14.90311409  4.53604339  1.47081491  4.42757295
 14.56824709  8.72606673  8.63551843  5.33809901  3.3132004   5.99442769
  2.3517253   5.81128222  3.03039914  3.71618677 17.03299024  6.49038363
  1.5676911   2.8171142   3.48046028  1.54932534  1.11723088  5.11785122
  5.52767501  7.91153587  7.73293844  4.57904034  3.85003255  4.55758057
  2.14294419  0.84033693  2.97109894  1.10388138  1.63572951  2.12477464
  1.95963129  2.18102749  2.24650204  6.18119169  1.72839929  6.00406981
  2.16118525  2.69057442  2.36737094  4.82766309  1.6775215   1.01955772
  1.37233842  1.15434056  1.19660371  0.97092604  0.73129028  2.29077405
  0.84038243  1.81476983  9.80298549  2.84032291  3.67918562  2.64438915
  7.74191738  2.07563085  3.0741871   2.50146162  1.34044122  1.81504422
  1.49161444  3.42064012  1.10423532  2.29033003  0.19065171  3.30841145
  1.61018257  3.84702759  3.54275561  2.30394479  0.5959269   1.48621776
  0.12765123  0.59233719]
mAP score regular 7.46, mAP score EMA 4.21
Train_data_mAP: current_mAP = 7.13, highest_mAP = 7.13
Val_data_mAP: current_mAP = 7.46, highest_mAP = 7.46
tensor([0.3338, 0.6087, 0.6397, 0.5270, 0.4290, 0.4780, 0.4789, 0.4578, 0.4741,
        0.4854, 0.4831, 0.4988, 0.5139, 0.4412, 0.4993, 0.5304, 0.5530, 0.4285,
        0.5713, 0.4318, 0.5003, 0.4452, 0.4571, 0.4973, 0.4603, 0.4519, 0.5110,
        0.4355, 0.3951, 0.4318, 0.4505, 0.4425, 0.3855, 0.5429, 0.3749, 0.5351,
        0.4908, 0.5948, 0.4319, 0.5995, 0.4296, 0.5394, 0.4771, 0.4694, 0.4694,
        0.4360, 0.5135, 0.5349, 0.5114, 0.4844, 0.4042, 0.5678, 0.4130, 0.4857,
        0.5025, 0.4653, 0.7414, 0.5082, 0.5260, 0.5316, 0.4617, 0.5585, 0.5532,
        0.4786, 0.4879, 0.5027, 0.4822, 0.5331, 0.4243, 0.4797, 0.4276, 0.5541,
        0.6261, 0.4849, 0.5619, 0.6813, 0.4819, 0.5978, 0.4836, 0.4536],
       device='cuda:0')
Max Train Loss:  tensor([24.0141, 13.2127, 17.2062, 15.0062, 13.5527, 14.7347, 12.9851, 17.3421,
         7.1709, 14.5615,  9.5435, 14.6504,  7.9860, 12.6076, 13.7472, 10.9690,
        10.9378, 11.6921,  8.1007,  8.2255, 13.2390,  7.7427, 10.5742, 14.9760,
        16.8573, 12.7286, 15.0040,  9.4617,  7.8567,  9.1133, 14.3246, 12.1792,
        12.2629,  8.5262,  6.9577,  9.6607, 15.7524, 11.5196, 14.2491, 15.0498,
         8.7036, 17.2276, 16.2292, 10.5126, 10.2408, 15.4614, 13.6785, 10.5079,
        11.9322, 10.3052,  8.7251, 11.6176,  7.0756, 13.0836, 11.6259, 11.2844,
        23.1145, 13.0629, 10.1559, 12.0281, 17.4675,  9.7161, 14.9916, 13.5352,
        10.3098, 12.7804, 10.9183, 16.2481,  7.0338, 12.0468,  6.2551, 12.0235,
         9.8354, 12.0572, 11.8833, 16.8765, 10.6242, 12.8879,  9.9180, 12.6693],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [6/80], Step [000/642], LR 1.0e-04, Loss: 24.0
Max Train Loss:  tensor([20.3335, 17.6248, 21.4729, 20.5798, 11.1374, 14.3684, 14.2444, 15.4195,
        23.6319, 13.0704, 10.6510, 11.4525, 23.3130, 11.4170, 15.8314, 16.9743,
        18.8282, 11.0199, 21.7222, 10.3091, 22.4986, 11.4383, 12.7600, 23.1577,
        13.4139, 11.4313, 15.1068, 13.2498, 10.6664,  9.9284, 13.3764, 10.9578,
        12.9017, 12.6442, 16.7087, 21.5756, 15.8328, 23.6405, 15.7072, 18.9080,
        10.0581, 20.4274, 14.5673, 13.6214, 12.0895, 10.6855, 13.9171, 10.6667,
        12.7648, 12.1548,  9.0048, 17.5377,  8.6116, 15.8375, 12.1882, 12.0011,
        23.5375, 22.3544, 19.5421, 24.3794, 17.1231, 18.2264, 11.8689, 13.1144,
        12.4729, 23.6434, 12.1234, 16.8656,  8.0510, 11.4908,  8.0800, 18.1400,
        17.3312, 13.6893, 16.1770, 22.2282,  8.2141, 12.3888, 10.7111,  9.9640],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [6/80], Step [100/642], LR 1.0e-04, Loss: 24.4
Max Train Loss:  tensor([20.6268, 19.9900, 23.6402, 10.9039,  9.1779, 15.1634, 11.9018, 15.6808,
        13.4476, 12.6158, 11.4200, 11.4265,  8.9880,  8.5656, 14.6968, 14.3085,
        23.2523,  9.3349, 10.9003,  7.9548, 11.7093,  8.3769, 14.7566, 11.2528,
        14.2522, 19.0789, 14.6851, 15.9043,  8.2894, 14.6727, 10.6989,  8.8669,
        10.8612, 12.8181, 16.5826, 14.0154, 13.4308, 12.8937,  8.4650, 19.0029,
        10.2701, 16.2669, 14.2867, 14.8601, 10.3638, 16.3553, 10.8702, 10.5096,
        14.6015, 11.3090,  7.4353, 16.7075,  6.5799, 16.1283,  9.6916, 11.2890,
        30.3762, 10.7942, 18.6887, 12.7849, 16.0073, 17.0795,  9.5023, 13.4158,
        12.1032, 11.4217, 11.5174, 16.4320,  8.0564, 14.0950,  7.2662, 15.0619,
        13.9999, 15.8631, 16.3993, 14.1898,  8.1125, 11.7572,  9.9097,  7.2143],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [6/80], Step [200/642], LR 1.0e-04, Loss: 30.4
Max Train Loss:  tensor([21.5921, 10.6158, 16.9853,  8.1366,  9.6799, 13.3549, 15.7749, 12.8343,
        11.0647, 13.0218, 11.8021, 13.6056,  9.3797, 11.0131, 15.4151, 13.6228,
        14.7864,  8.0118, 12.8615,  7.6059, 11.2008, 11.3618, 11.7130, 13.5985,
        12.8919, 11.4475,  8.8358, 10.3751,  7.4196, 17.2323, 10.5566, 11.4364,
         5.0076,  9.8229, 15.7461, 11.2245, 10.2959, 15.4667, 10.9378, 15.9838,
         9.6898, 15.8213, 14.1752, 13.6746, 12.2072, 12.6441, 10.1211, 11.1094,
        13.1837, 11.2945,  7.5811, 20.0583,  7.7380, 15.1201, 11.6185, 11.8440,
        23.8031, 11.9380, 12.1626, 10.6643, 19.3674, 16.5002, 15.1764, 11.9400,
        12.0355, 13.8833, 12.8061, 12.8449,  7.2223, 10.1421,  6.7723, 12.1720,
        13.1306, 13.2615, 12.7729, 19.5874,  7.1660, 13.3795, 10.9578,  9.2891],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [6/80], Step [300/642], LR 1.0e-04, Loss: 23.8
Max Train Loss:  tensor([21.3733, 13.1233, 22.8714, 14.1192,  8.6253, 14.6040, 17.0364, 15.0627,
        11.9655, 13.8063, 13.3428, 10.9430,  8.7067, 16.5179, 15.8668, 18.1993,
        15.1459, 12.2206, 15.9352, 10.4184, 10.4880, 11.6221, 11.5490, 12.5624,
        14.7582, 13.9175, 12.0296,  9.5397, 11.2165, 13.6458, 13.3055, 11.3446,
         8.1301, 14.8343, 15.9469, 10.3444, 11.5565, 10.2084, 15.2986, 18.1367,
         7.5689, 14.5425, 13.4506, 12.4871, 13.6609, 13.3626, 12.1326, 11.7976,
        10.9404, 11.8258,  8.3444, 16.7897,  6.3931, 14.5019, 10.1822, 12.3109,
        18.8015, 12.6081, 15.0775, 13.7550, 13.7860, 19.6287, 14.3779, 13.3319,
        12.2637, 12.9170, 11.1783, 12.5086,  7.6868, 10.9226,  7.0575, 13.8663,
        14.2529, 15.0247, 12.8098, 13.7937,  8.1478, 16.8302, 11.0191,  8.2712],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [6/80], Step [400/642], LR 1.0e-04, Loss: 22.9
Max Train Loss:  tensor([18.6641,  9.3595, 23.2742, 11.7718, 12.7809, 13.3568, 11.3230, 16.3150,
        10.8726, 10.9651, 10.4119, 11.3697,  9.5989, 10.9528, 17.5380, 17.4110,
        14.5960, 11.4348, 12.4959,  8.7348, 12.3165,  8.8535, 13.9327, 14.2585,
        13.8969, 11.8595, 13.9572,  8.9786,  7.1191, 11.1792, 13.8833, 12.6284,
         6.6200, 12.3181, 16.5625, 10.8514, 14.8470, 10.9808, 11.7775, 19.8043,
         8.3562, 16.3787, 11.5653, 13.0793, 11.3729, 10.7979, 13.8956, 11.2868,
        14.9269, 12.8419,  6.2543, 16.5996,  6.3972, 15.2901, 10.9362, 14.3035,
        20.0488, 11.7542, 13.1541, 15.2674, 18.6147, 19.0338, 12.4769, 12.2826,
        10.3196, 13.7249, 11.0428, 13.3900, 10.5093, 13.9474,  8.4873, 14.7999,
        14.3424, 13.3802, 12.2462, 12.4542,  8.2077, 12.9331, 10.4394,  8.6000],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [6/80], Step [500/642], LR 1.0e-04, Loss: 23.3
Max Train Loss:  tensor([15.1350,  9.0594, 20.1790, 14.0983, 12.0356, 13.3832, 15.5280, 14.8073,
        12.1937, 12.5703, 11.0290, 11.7305,  8.5053, 13.7557, 18.5705, 13.6498,
        16.6965,  8.4579, 12.4531, 10.3596, 11.4586,  8.6431, 12.5803, 11.6364,
        15.6426, 14.3720, 13.3538, 10.7152,  6.7871, 11.8136, 13.9951,  9.0397,
        13.5852, 15.1159, 18.4790, 18.0828, 13.5560, 11.1222, 10.9548, 15.3898,
         8.9441, 19.0990, 12.4588, 12.9179, 12.2204, 14.2028, 10.2532, 10.9032,
        16.4398, 10.8355, 10.6722,  6.9481,  7.1696, 17.6114,  9.9892,  9.9723,
        24.0818, 12.1315, 10.7942, 14.4398, 18.6364, 12.2297, 16.8126, 11.3920,
        12.2503, 11.4726, 12.6943, 14.1180,  7.8956, 11.5495,  6.8783, 12.2647,
        15.0924, 12.8446,  8.6973,  9.7213,  7.2802, 12.1653, 10.2182,  7.4651],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [6/80], Step [600/642], LR 1.0e-04, Loss: 24.1
Max_Val Meta Model:  tensor([ 23.0268,  41.8248,  57.2397,  26.3638,   6.4307,  12.0068,  11.9691,
         15.0391,   9.6705,  12.8009,  10.8442,  11.5540,   9.2604,  11.5836,
         13.7852,  12.9598, 175.9295,   6.2059,  10.9867,   7.5187,   8.6770,
          8.4440,   8.5008,  10.0844,  18.2527,  11.5859,  16.6755,   7.0591,
          7.4227,   8.8031,   8.7568,   8.0751,   3.8529,   9.7190,  15.4253,
          8.7623,  10.2196,   9.5847,   7.6723,  28.3468,   8.7530,  15.0481,
         11.4556,  13.5770,  11.9856,  11.9966,  10.0287,  10.8905,  10.5377,
         10.5851,   5.8814,   2.8342,   7.2626,  14.1928,   9.7803,   9.0118,
         20.0605,   9.9482,  14.2455,   9.7213,  10.2585,  16.4180,   9.5953,
         10.5072,   9.8918,   9.8928,  10.0285,   5.8577,   8.5151,  15.3432,
          6.6783,  14.0956,  17.6715,  11.4804,  10.5331,  10.7394,   7.0480,
         11.7705,  10.0124,   7.2448], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 21.6522,  39.4141,  49.5923,  26.8968,   7.2486,  12.9305,  12.8105,
         15.6228,  10.5222,  13.5229,  11.7925,  12.6108,  10.1690,  12.2275,
         15.0375,  14.1181, 167.4236,   7.0088,  12.1992,   8.4031,   9.6456,
          9.3899,   9.4542,  11.1174,  18.7502,  12.0384,  18.7654,   7.9173,
          8.0839,   9.3763,   9.7238,   9.0053,   4.2700,  10.8640,  16.2755,
          9.7513,  11.2926,  10.4513,   8.5655,  28.2653,   9.5717,  15.8856,
         12.4932,  14.7047,  12.8631,  12.8716,  11.1417,  11.9547,  11.6523,
         11.6312,   6.6490,   3.4025,   7.9151,  15.3612,  10.8257,  10.0000,
         21.0365,  10.9658,  14.4255,  10.7963,  11.1791,  16.7639,  10.6484,
         11.5751,  10.9573,  10.9346,  11.0732,   6.8641,   9.3114,  16.3426,
          7.5103,  14.9551,  19.0000,  12.3609,  11.2888,  11.6835,   7.9419,
         13.0968,  11.1298,   8.1417], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 64.8735,  64.7557,  77.5276,  51.0366,  16.8975,  27.0499,  26.7483,
         34.1248,  22.1936,  27.8605,  24.4110,  25.2844,  19.7872,  27.7168,
         30.1177,  26.6165, 302.7803,  16.3557,  21.3545,  19.4587,  19.2796,
         21.0917,  20.6817,  22.3534,  40.7377,  26.6417,  36.7260,  18.1789,
         20.4579,  21.7130,  21.5835,  20.3505,  11.0764,  20.0097,  43.4105,
         18.2244,  23.0066,  17.5709,  19.8331,  47.1514,  22.2789,  29.4519,
         26.1831,  31.3287,  27.4045,  29.5244,  21.6986,  22.3491,  22.7855,
         24.0105,  16.4515,   5.9923,  19.1663,  31.6239,  21.5453,  21.4920,
         28.3731,  21.5781,  27.4255,  20.3076,  24.2119,  30.0152,  19.2503,
         24.1830,  22.4595,  21.7496,  22.9663,  12.8751,  21.9477,  34.0662,
         17.5640,  26.9876,  30.3454,  25.4942,  20.0905,  17.1501,  16.4791,
         21.9099,  23.0133,  17.9506], device='cuda:0')
Outer loop valEpocw Maximum [6/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 175.9
model_train val_loss valEpocw [6/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 167.4
model_train val_loss  valEpocw [6/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 302.8
Max_Val Meta Model:  tensor([35.7581, 40.1740, 49.5543, 44.9808, 36.0103, 47.6010, 44.2117, 39.6509,
        39.1878, 41.7649, 41.3072, 50.7159, 42.4403, 37.0385, 40.2899, 38.5568,
        44.2746, 37.8439, 46.6614, 36.8463, 42.6346, 37.1131, 38.0323, 40.2390,
        39.5265, 38.0357, 40.7266, 37.8195, 36.0719, 37.6042, 39.1580, 38.6480,
        33.4008, 46.8843, 33.9393, 41.2935, 39.9525, 45.8935, 37.7415, 43.2095,
        38.7264, 43.3262, 39.5550, 38.8854, 39.4313, 36.6717, 41.4907, 41.7313,
        43.6211, 40.8750, 34.9373, 43.1119, 35.2251, 41.3378, 40.8911, 39.0324,
        41.2772, 43.6800, 43.9841, 42.3990, 42.5834, 45.7985, 42.5086, 41.1920,
        39.9448, 43.5150, 41.2011, 41.0184, 35.9826, 40.5404, 38.0051, 42.3866,
        45.4496, 40.8334, 41.3586, 46.7638, 38.3194, 41.4868, 40.3803, 38.0935],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([28.4230,  8.9924, 33.1032,  7.3408,  5.0929, 19.7097, 42.0271, 26.9686,
        16.6793, 18.6175, 11.9600, 77.4157,  8.5044,  6.8914, 13.0554,  9.2835,
         6.9981,  9.2655,  8.7296, 11.5892,  6.9492,  6.6396,  6.7798,  8.6798,
        15.0508, 10.8808, 18.5410,  9.1546,  7.5523,  5.6702,  7.1762,  6.5976,
         1.7670,  7.9483, 14.4763,  6.3326,  9.0164,  6.0279,  6.3032,  3.9737,
         5.5797,  4.6266,  8.4288,  7.7494,  7.4979,  6.1079,  7.8142,  7.8050,
         8.6673,  9.0564,  4.7002,  1.7993,  4.7913, 12.1219,  7.7537,  8.0886,
         9.3147,  7.0161,  7.8311,  7.4491,  5.0025,  4.8953,  6.0478,  8.1720,
         7.9157,  8.2221,  8.2905,  7.6737,  5.0042,  8.8372,  5.5974,  5.7761,
         9.5273,  8.3944,  5.9463,  4.0931,  5.2213,  7.9711,  7.9944,  5.6200],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 73.7923,  17.2808,  55.7614,  14.0047,  11.4711,  37.4306,  84.7081,
         57.5505,  35.4686,  36.5446,  24.2569, 152.5624,  16.6453,  15.4556,
         26.9435,  20.1940,  13.6744,  19.9686,  15.5355,  26.1424,  13.5958,
         14.9406,  14.6441,  18.0962,  32.2152,  23.9572,  38.0984,  19.7562,
         17.4463,  12.3666,  15.3466,  14.2868,   4.2573,  14.0220,  36.4370,
         12.6054,  18.8405,  10.8190,  13.8729,   7.7667,  11.7051,   8.6768,
         17.6718,  16.7418,  15.4941,  13.4705,  15.4759,  14.8572,  16.4989,
         18.1791,  11.0632,   3.2486,  11.0991,  24.6424,  15.4733,  16.9702,
         13.4999,  13.3989,  16.1190,  14.3839,   9.8310,   8.7987,  11.9543,
         16.5658,  16.2113,  15.6497,  16.6325,  14.8793,  11.4421,  18.2913,
         12.0100,  10.8210,  16.7604,  17.2135,  11.7302,   6.5098,  11.0803,
         14.5635,  16.5752,  12.2585], device='cuda:0')
Outer loop valEpocw Maximum [6/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 50.7
model_train val_loss valEpocw [6/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 77.4
model_train val_loss  valEpocw [6/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 152.6
Max_Val Meta Model:  tensor([34.3788, 37.0413, 37.9566, 43.3389, 35.0070, 43.4006, 39.9713, 37.2396,
        33.0345, 39.5601, 40.0505, 36.3384, 40.8789, 36.0801, 37.2247, 36.7434,
        39.6056, 32.4601, 41.5211, 35.6897, 36.0850, 36.0980, 39.7842, 38.9461,
        37.9169, 36.7593, 38.2011, 39.2063, 34.4507, 36.1439, 45.2802, 37.6904,
        32.6687, 38.4452, 33.0240, 38.8862, 38.5401, 39.8049, 36.2211, 40.5733,
        38.1891, 44.4004, 39.1991, 39.1646, 39.4849, 38.9744, 45.6092, 44.0711,
        47.6986, 40.9890, 34.0953, 39.7357, 38.3961, 39.9732, 40.7866, 41.3010,
        68.6395, 41.8545, 38.2645, 40.7710, 51.0948, 41.7844, 40.7629, 39.9327,
        38.6486, 41.9313, 39.9684, 39.2347, 34.8417, 39.1765, 37.1465, 40.3911,
        39.7260, 39.6800, 39.2330, 39.2714, 36.1587, 36.0780, 39.0330, 36.6693],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([16.9254,  7.0473,  3.8044,  9.0554,  8.9909, 13.6960, 12.5372, 13.8763,
        10.6209, 13.2476, 13.0385, 12.2554, 10.5709, 14.5632, 15.1459, 12.7957,
         7.8140,  8.5722, 13.0280, 10.2086, 11.3280, 10.9717, 11.5264, 12.1849,
        12.7406, 10.7772, 16.6057, 11.9223,  8.4901,  9.7246, 14.4474, 11.6875,
         6.6120, 12.5882, 18.4163, 10.3998, 15.5859, 11.6515, 10.4822, 21.5917,
        16.5656, 41.0973, 35.5232, 33.7262, 27.4311, 35.7080, 14.9266, 15.9955,
        24.4932, 14.6160, 16.4093, 25.6838, 24.0190, 20.1862, 30.3867, 41.3734,
        68.5479, 13.5597, 11.7516, 12.1464, 93.5575,  9.5420, 12.7024, 15.0683,
        14.6865, 13.0893, 14.4881,  9.2442,  9.4912, 12.8530,  9.8216, 12.1668,
        14.5738, 20.3664,  8.6155, 12.9071, 10.5773, 12.1312, 12.8181,  9.7529],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 43.5133,  13.8819,   6.8340,  17.4359,  20.0916,  26.8205,  26.2130,
         29.8970,  23.8990,  26.0345,  26.3848,  25.2761,  20.7239,  32.5735,
         31.8373,  28.5383,  15.9243,  19.5078,  24.2074,  22.8349,  22.3585,
         24.5641,  24.0918,  25.4268,  27.2706,  23.6973,  34.4662,  24.7234,
         19.6808,  21.3016,  27.2082,  25.1139,  15.5992,  23.5690,  46.7760,
         21.3074,  32.8150,  21.3432,  23.2404,  43.9774,  34.5170,  77.9716,
         75.8917,  73.4000,  56.5133,  78.1468,  28.1697,  29.2498,  45.5235,
         29.2120,  38.6860,  48.4439,  55.0508,  41.5461,  61.2784,  85.7331,
        101.4755,  26.1614,  25.6356,  23.5928, 170.0013,  17.6611,  25.5014,
         30.6205,  30.2433,  25.0363,  29.1306,  17.9519,  21.6310,  26.6790,
         20.8067,  23.2332,  26.7559,  41.9553,  17.2055,  21.4094,  22.9500,
         23.4219,  26.6466,  21.3289], device='cuda:0')
Outer loop valEpocw Maximum [6/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 68.6
model_train val_loss valEpocw [6/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 93.6
model_train val_loss  valEpocw [6/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 170.0
Max_Val Meta Model:  tensor([33.7176, 41.2249, 41.4764, 44.8314, 35.9922, 44.1619, 40.8146, 38.4455,
        34.7543, 40.4800, 41.2855, 37.7655, 42.5879, 37.2227, 38.6732, 38.4787,
        41.8694, 32.8612, 43.2352, 36.4423, 37.7633, 36.9536, 38.1711, 40.8796,
        39.1481, 38.1895, 39.8992, 38.8774, 35.3586, 36.7315, 34.5722, 38.3329,
        39.5928, 38.5576, 33.1610, 41.0195, 39.8804, 46.0441, 37.5572, 43.4857,
        37.9078, 39.2305, 38.6534, 39.3016, 39.2279, 36.0230, 37.7786, 37.7266,
        38.7401, 40.8461, 34.4479, 44.8797, 35.4214, 40.5899, 37.4405, 39.3191,
        40.7078, 42.8924, 41.4788, 34.4547, 37.0874, 43.9909, 42.5859, 40.5612,
        39.6256, 42.4020, 40.7430, 53.2846, 35.6106, 40.4171, 37.9256, 43.5573,
        43.2757, 40.7955, 47.3336, 43.7077, 37.3738, 39.0118, 40.7608, 40.7898],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 22.2696,   3.8969,  13.5306,   5.0296,   4.9688,  11.0423,   8.6539,
         11.2474,   6.4891,  13.5626,   8.9060,   8.4980,   6.2880,   9.8028,
         12.2135,   8.4332,   4.4865,   4.5810,   8.1990,   5.8516,   6.7720,
          6.4707,   6.6204,   7.8099,  13.6521,   6.3434,  13.3230,   5.7518,
          7.2007,   5.4092,   6.7604,   6.4009,   1.8544,   7.4118,  13.9983,
          6.0722,   7.7911,   6.1109,   6.1335,   4.9160,   5.3307,   4.4527,
          7.9311,   7.6765,   7.3070,   5.1708,   7.2718,   7.2800,   8.0449,
          7.9806,   4.5198,   1.8133,   4.6995,  11.7384,   7.2100,   7.2234,
          5.9055,   5.9515,   9.3222,   6.6502,   6.8754,   5.0204,   5.9001,
          7.8793,   7.6717,   7.8465,   8.0378,   5.1102,   4.8321,   7.9019,
          5.4541,   5.9203,   8.2985,   8.6244, 188.6037,   6.0386,   4.9336,
          8.0617,   7.8444,   5.6679], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 59.2924,   7.2971,  23.2904,   9.6005,  11.1912,  21.4214,  18.0688,
         24.2889,  14.4701,  27.0680,  18.0437,  17.6706,  12.2612,  21.9464,
         25.7568,  18.3762,   8.8306,  10.7595,  15.0292,  13.2577,  13.3329,
         14.6192,  14.3293,  16.0129,  29.3428,  13.8783,  27.5060,  12.2381,
         16.9272,  12.0717,  15.0343,  13.9699,   4.0945,  13.7882,  36.0806,
         12.2021,  16.2961,  10.3515,  13.5609,   9.6478,  11.4183,   8.6380,
         17.0156,  16.4056,  15.1743,  11.5926,  15.1571,  14.5369,  16.1249,
         16.0154,  10.7833,   3.1420,  10.8199,  24.3496,  15.1530,  15.0303,
          8.5249,  11.5671,  19.1023,  14.0950,  14.1129,   9.1222,  11.6814,
         16.2176,  15.8330,  15.3227,  16.3039,   8.9372,  11.1579,  16.4013,
         11.7211,  10.9465,  14.8306,  17.7244, 359.6607,   9.4048,  10.7217,
         15.3504,  16.1069,  11.9727], device='cuda:0')
Outer loop valEpocw Maximum [6/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 53.3
model_train val_loss valEpocw [6/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 188.6
model_train val_loss  valEpocw [6/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 359.7
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [79.08407549 97.2137279  90.817607   97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.05906361
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 90.45820592 96.13796128 96.24273583 96.9067141
 89.79300934 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [75.02223413 97.2137279  90.48013548 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.05906361
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 89.70041788 96.13796128 96.24273583 96.9067141
 89.79300934 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [90.16317723  3.25944278 50.5484899   3.28636495  1.8714782   2.85401541
  2.20604419  4.69165109  1.87761591  3.10527762  1.27890858  1.1359054
  0.66855762  4.35421051  1.87409585  4.83289082  4.05788084  1.97643631
  0.83719769  1.12532509  1.22158441  0.46795734  0.84886483  1.25194664
  5.45435839  3.66199499 18.65160667  4.09726447  3.47644732  1.40355781
  1.61261549  0.87895178  2.66360887  1.31303896  1.6075362   1.6479184
  2.54561564  2.76264059  2.0064818  24.34506972  7.37372601 27.81819317
  6.89341908  9.87763711  8.53629684 15.6232895   2.63517487  2.1167116
  3.39106723  2.22132252  1.44450861  1.61879675  1.39922858  4.27662358
  2.09885175  5.06860656 50.16847621 13.75794917 10.42532689  5.52656085
 32.89541199  4.87614235 16.43129232  9.96997733  7.05266473  7.38654259
  7.18631194  7.73269717  4.10529239  7.23749531  0.6947815  10.24410549
  7.33766581 16.49834062  5.07885027  9.08035964  1.64195983  2.6115013
  0.26419815  1.15070737]
Accuracy th:0.5 is [45.60860613 97.2137279  72.92430648 97.02489005 97.26733349 78.00099901
 78.36527333 77.08239422 79.31555415 96.40964413 79.61403979 98.52097319
 99.41399349 80.58746848 79.16813879 96.56680596 96.29512311 78.91351226
 98.65376884 98.30776915 80.60696142 80.13182101 98.38695922 78.96711785
 80.74706692 96.65086926 94.0778012  78.6686322  98.01293844 79.48246245
 97.30875598 98.57457877 96.36213009 98.02024829 86.03574518 79.12549798
 78.85868837 89.80275581 97.11504489 76.01150083 79.59332854 92.05906361
 78.37136487 77.86942167 96.9627563  93.87434364 98.02877645 98.57336046
 86.18559715 87.49527905 86.78622336 98.55508583 98.99976852 78.51147038
 98.70615611 78.85503344 73.25568646 93.00325288 96.24273583 96.9067141
 89.79300934 97.17717864 90.6226776  78.90742072 98.42838172 79.32773724
 98.20786784 77.99490747 80.00877182 97.55485435 80.57406708 95.99054592
 79.63596935 95.45083515 78.25684385 82.35157954 85.5228372  79.51535678
 80.65082053 99.14718388]
Accuracy th:0.7 is [45.67926804 97.2137279  72.92430648 97.02489005 97.26733349 78.00099901
 78.36527333 77.20178848 79.31555415 96.47299619 79.61403979 98.52097319
 99.41399349 81.02118639 79.16813879 96.56680596 96.29512311 78.91351226
 98.65376884 98.30776915 81.06260889 80.13182101 98.38695922 79.00001218
 81.2599749  96.65086926 94.0778012  78.6686322  98.01293844 79.48246245
 97.30875598 98.57457877 96.36213009 98.02024829 86.28184354 79.12549798
 78.85868837 90.10124146 97.11504489 76.01150083 80.20979277 92.05906361
 78.37136487 77.86942167 96.9627563  93.87434364 98.02877645 98.57336046
 88.15194747 88.32007407 86.95434997 98.55508583 98.99976852 78.51147038
 98.70615611 78.85503344 73.25568646 93.44549896 96.24273583 96.9067141
 89.79300934 97.17717864 90.80298729 78.90742072 98.42838172 79.32773724
 98.20786784 77.99490747 80.00877182 97.55972759 80.57406708 95.99054592
 79.63840596 95.45083515 78.25684385 82.43564284 85.63370329 79.51535678
 80.65082053 99.14718388]
Avg Prec: is [55.88030758  3.02236057 11.17526191  3.23684571  2.29229607  3.80957148
  3.38728835  5.63709043  2.36581557  3.76095975  1.56459722  1.5416406
  0.6367761   5.04343819  2.66508835  3.21145996  3.76511456  2.66291161
  1.31888637  1.7310359   1.94150776  0.83783498  1.88239596  2.45980732
  5.0787681   3.64610327  6.55282315  3.42633345  2.02647272  1.90514825
  2.62494721  1.31100136  3.675198    1.70647692  2.38466511  2.40169905
  3.05110461  2.62056742  2.91476841  7.43741355  2.40440906  8.47844843
  3.44361334  4.12229018  3.26898082  6.51665331  1.99228776  1.51069517
  2.2095759   1.58609603  1.86265318  1.58936304  1.08425903  3.04924381
  1.31011356  2.65632003 11.43985755  3.83673075  3.94278068  2.87937786
 10.83973985  2.15122512  3.88484966  3.03843184  1.59364906  2.61670984
  1.79253391  4.2099405   1.26189145  2.39815994  0.19319508  3.43814389
  1.97413407  4.51529047  3.88207654  3.17542581  0.77604217  1.90651154
  0.11630299  0.72307805]
mAP score regular 7.62, mAP score EMA 3.78
starting validation
Accuracy th:0.5 is [81.93437477 97.22450607 91.42437153 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.37362035
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.31566883 96.39235618 96.16314124 96.78102499
 90.13379176 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [80.20280539 97.22450607 91.09798939 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.37362035
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 89.91703416 96.39235618 96.16314124 96.78102499
 90.13379176 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [92.32822144  3.57832875 56.15225091  3.47058246  1.44340862  2.78498343
  2.27109463  4.73354719  2.00800808  3.09279554  1.3227223   1.08744284
  0.67470752  4.63892191  1.89168093  5.08357703  4.1429668   2.01357781
  0.74715324  1.12425145  1.07148508  0.4686194   0.86565025  1.16230988
  5.86885641  4.15189308 18.15395821  3.81149952  3.88431138  1.535352
  1.44453948  0.81619715  2.73343509  1.23996757  1.52139753  1.57851187
  2.3523451   4.10642129  2.05561717 25.9182269   8.66241674 28.23823428
  6.79097922  9.48290482  8.41859702 14.56691133  2.39611334  1.87930125
  3.58387965  1.87029587  1.34513479  1.70111324  1.59077197  4.83699778
  2.07469865  4.92543367 49.66938303 12.53803224  9.51992321  5.82637193
 34.20309635  4.53335761 17.54966143 12.23631966  8.87456253  7.74801657
  8.27337004  9.32788714  3.96792202  7.2725371   0.90666996  9.5722647
  6.50910695 17.28852439  5.20372709  8.93420001  1.82267156  2.71690439
  0.25919214  1.01990834]
Accuracy th:0.5 is [45.27991629 97.22450607 71.80407106 96.96290206 97.90716795 77.38495652
 77.50205546 76.25881356 78.88481949 96.41727085 79.13396617 98.5325261
 99.34972718 78.66058749 78.9670379  96.31262924 96.21047911 78.47372748
 98.78167277 98.34068316 79.64720831 79.77925605 98.31327703 78.54099708
 78.83000723 96.52938685 94.3393876  78.47621895 97.81747515 79.0716795
 97.52597354 98.67204823 96.39983058 98.18870369 86.53362234 78.69795949
 78.59082642 91.6211974  97.0276802  75.60106635 78.64813015 92.37362035
 77.80103147 77.44475173 97.03764606 94.02795426 98.18621222 98.77668984
 86.64573835 87.34833196 85.33024391 98.55993223 98.87385704 77.84587787
 98.6969629  78.48867628 72.17779107 94.17993373 96.16314124 96.78102499
 90.13379176 97.04761193 90.36051524 78.38652615 98.32075143 79.2510651
 98.13139996 77.56185066 79.75683285 97.53095647 80.26010913 96.07843137
 79.44789097 95.44559882 77.49458106 83.47908414 87.33338316 79.12898323
 80.32987019 99.15040985]
Accuracy th:0.7 is [45.59633256 97.22450607 71.80407106 96.96290206 97.90716795 77.38495652
 77.50205546 76.2787453  78.88481949 96.41976231 79.13396617 98.5325261
 99.34972718 79.10656003 78.9670379  96.31262924 96.21047911 78.47372748
 98.78167277 98.34068316 79.96362459 79.77925605 98.31327703 78.54099708
 79.17133817 96.52938685 94.3393876  78.47621895 97.81747515 79.0716795
 97.52597354 98.67204823 96.39983058 98.18870369 86.84505568 78.69795949
 78.59082642 91.800583   97.0276802  75.60106635 78.96952936 92.37362035
 77.80103147 77.44475173 97.03764606 94.02795426 98.18621222 98.77668984
 88.46201759 87.71457757 85.48471485 98.55993223 98.87385704 77.84587787
 98.6969629  78.48867628 72.17779107 94.4515036  96.16314124 96.78102499
 90.13379176 97.04761193 90.52246057 78.38652615 98.32075143 79.2510651
 98.13139996 77.56185066 79.75683285 97.53593941 80.26010913 96.07843137
 79.44789097 95.44559882 77.49458106 83.57126841 87.4504821  79.12898323
 80.32987019 99.15040985]
Avg Prec: is [53.50829095  3.69757811 14.86861095  4.54167305  1.47348913  4.39323889
 14.52225902  8.69559373  8.65238822  5.33312053  3.03425666  6.19761465
  2.36083426  5.79777186  2.97436428  3.67210042 18.08788779  6.46021466
  1.55679099  2.74366452  3.48509114  1.5545302   1.23071917  5.11712743
  5.52375486  8.00823293  7.75619248  4.58891456  3.84819804  4.65526487
  2.13714683  0.84136351  2.88416291  1.1377435   1.70066659  2.14340247
  1.95178055  2.19079058  2.19226383  6.20544582  1.69830529  5.96764919
  2.148968    2.68035707  2.3364303   4.87857601  1.65017659  1.01359145
  1.39171225  1.15425     1.19474462  0.96721998  0.73800316  2.25104954
  0.85521619  1.82358876  9.95507316  2.99620288  3.80784951  2.7879247
  7.77124024  2.09234387  3.28167283  2.51864875  1.35730444  1.93054615
  1.52225439  3.52516064  1.0962135   2.2585575   0.1935736   3.30420225
  1.63971172  3.96853405  3.18887839  2.26321197  0.57776214  1.43167406
  0.11969808  0.60839727]
mAP score regular 7.84, mAP score EMA 4.23
Train_data_mAP: current_mAP = 7.62, highest_mAP = 7.62
Val_data_mAP: current_mAP = 7.84, highest_mAP = 7.84
tensor([0.3674, 0.5570, 0.6057, 0.5265, 0.4373, 0.5180, 0.4763, 0.4645, 0.4502,
        0.5036, 0.4918, 0.4751, 0.5161, 0.4460, 0.4723, 0.4784, 0.5243, 0.4159,
        0.5591, 0.4391, 0.5157, 0.4438, 0.4684, 0.4927, 0.4634, 0.4586, 0.4956,
        0.4667, 0.4163, 0.4474, 0.4543, 0.4491, 0.4441, 0.5398, 0.3905, 0.5164,
        0.4869, 0.6113, 0.4492, 0.5355, 0.4580, 0.5450, 0.4784, 0.4675, 0.4813,
        0.4478, 0.4908, 0.5132, 0.5053, 0.4906, 0.4111, 0.6030, 0.4280, 0.4911,
        0.4810, 0.4745, 0.7189, 0.5215, 0.5119, 0.4676, 0.4845, 0.5529, 0.5200,
        0.4869, 0.4889, 0.5175, 0.4970, 0.5392, 0.4319, 0.4818, 0.4524, 0.5550,
        0.5991, 0.4844, 0.5364, 0.6849, 0.4753, 0.5576, 0.4826, 0.4791],
       device='cuda:0')
Max Train Loss:  tensor([21.6256, 11.7509, 16.4150, 13.3087,  7.9355, 15.6577, 17.0537, 10.6366,
        14.2571, 13.5718, 10.4977, 11.8128,  8.6698, 12.9476, 19.6868, 12.5421,
        12.5220,  9.4302, 12.4621,  9.0787, 13.2443,  9.8046, 12.3497, 12.9848,
        12.4061, 12.7480, 15.0964, 12.0559,  7.0190, 10.4847, 13.1099,  8.5257,
         4.7378, 12.7476, 16.1964,  8.8085, 15.3435, 16.2428, 10.1638, 12.9359,
         8.8104, 17.7648, 12.6144, 12.6156, 12.5298, 13.5984, 11.3403, 12.5386,
        12.1443, 11.9483,  9.1120,  8.5508,  7.9452, 17.6763, 11.5637, 11.8826,
        27.2207, 15.1071, 10.6212, 10.1306, 21.0178, 10.4189, 13.1111, 12.5200,
        10.9075, 14.0352, 11.8740,  8.7595,  6.8048, 11.6676,  7.4034, 12.3604,
        11.9946, 14.5967, 14.5919, 13.3132,  9.4244, 12.4895, 10.3152,  9.9535],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [7/80], Step [000/642], LR 1.0e-04, Loss: 27.2
Max Train Loss:  tensor([24.0402, 21.3178, 17.1204, 20.7390, 12.3085, 12.2474, 12.4405, 13.0527,
        10.3325, 21.6552, 10.3379, 11.3913, 19.2060, 14.2139,  9.6138, 15.3333,
        22.3872, 12.0554, 15.5990,  9.3234, 18.6304,  8.9238, 12.7729, 14.4689,
        14.5656, 11.7872, 20.2628, 11.4824,  8.4284,  9.2921, 15.2326, 12.6381,
        22.0382, 17.0728, 11.8707, 13.8268, 10.0693, 23.2482, 12.6674, 17.1198,
         9.5477, 20.1852, 13.3384, 12.2210, 12.0666, 14.4608, 12.5163, 12.0635,
        19.4095, 12.1501,  8.5837, 22.4333, 10.3916, 11.0379, 10.9101, 12.4416,
        24.7606, 17.5124, 11.6496, 10.6772, 22.6881, 23.0145, 14.7247, 14.3061,
        11.4626, 17.7837, 22.0183, 18.4798,  8.4559, 12.2764,  9.1768, 17.6138,
        11.2690, 11.3342, 18.0802, 18.5417,  9.4891, 22.7929, 10.2812, 11.6776],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [7/80], Step [100/642], LR 1.0e-04, Loss: 24.8
Max Train Loss:  tensor([25.0578, 13.4827, 25.0106, 16.2433,  9.5646, 16.3433, 11.9613, 12.8721,
        12.7836, 14.2364, 11.4305, 10.8657, 10.7946, 11.8964, 10.2736, 17.3372,
        16.7622, 11.2903, 11.7239,  8.0446, 13.1711,  6.6910, 11.6960, 13.7097,
        11.6843, 13.1225, 15.4680, 12.4877,  8.4091, 12.1177,  9.3067,  9.3247,
        12.1356, 14.4933, 12.6221, 14.8498, 11.0919, 11.5687,  9.2907, 16.3451,
        12.5953, 17.6722, 13.6028, 10.9286, 12.8324, 10.7996, 13.2239, 12.1437,
        18.2864, 10.1915,  6.4529,  9.4487,  6.0208, 10.3576,  9.4358, 12.7357,
        22.5306, 14.9276, 12.1876, 12.5841, 19.6405, 13.6445, 11.0301, 11.8994,
        11.3264, 18.8566, 10.9518, 15.3682,  7.0318,  9.5988,  8.2698, 14.5868,
        10.7390, 13.9669, 18.3316, 17.4720, 10.0634, 10.0001,  9.6582, 11.8503],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [7/80], Step [200/642], LR 1.0e-04, Loss: 25.1
Max Train Loss:  tensor([21.3349, 14.2311, 18.1372, 10.3175, 11.7230, 13.2677, 14.8953, 12.2469,
        11.3859, 10.5648, 12.4070, 10.1804,  9.9653, 10.7976, 12.8805, 14.0533,
        12.0381,  8.2551, 14.1932, 11.5406, 11.2954,  8.5009, 11.5242, 14.2065,
        19.6093, 11.1027, 13.3366,  9.2868,  7.3455, 10.7839, 21.6320, 12.6556,
        10.7153, 13.2039, 10.0421,  9.3342, 12.1251, 14.8121,  9.8649, 11.7466,
         9.9675, 10.7503,  9.6703, 13.4745, 13.1728,  9.2951, 11.0367, 10.4047,
        18.4474,  9.6468,  5.7523, 10.4390,  6.0690, 10.8762, 10.8606, 11.4756,
        16.7463, 12.4855, 11.8209, 12.0072, 15.5901, 12.7048, 12.9387, 10.2146,
        10.3171, 15.9042,  8.2340, 15.6236,  7.9633, 11.1608,  7.6030, 13.0395,
        11.5959, 13.0452, 15.3457, 12.3496,  8.7959, 10.9396,  9.6980, 11.9003],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [7/80], Step [300/642], LR 1.0e-04, Loss: 21.6
Max Train Loss:  tensor([25.4059,  8.0489, 19.9161,  9.0606, 11.7088, 14.5699, 14.9254, 15.5158,
        11.7014, 17.5897, 11.6324, 13.0686, 10.7578,  8.6691,  9.0150, 14.1906,
        16.6486,  9.6062, 17.6656,  8.5908, 10.5644,  8.0087, 13.3103, 12.2686,
        14.4330, 15.6064, 15.0665,  9.8634,  9.3058, 10.6851, 11.4533, 11.5475,
        14.2581, 16.9367, 11.7552, 10.5816, 13.1018,  7.2693,  9.6904, 16.0104,
         9.4378, 20.7723, 10.9395, 14.5857, 12.0217, 10.3585, 11.9197, 11.4893,
         9.3480, 11.1045,  5.7850, 11.9701,  7.2138,  8.3485, 12.5807, 12.7766,
        28.0092, 12.4070, 12.7597,  9.3519, 16.7810, 12.2453, 11.8051, 12.9174,
        11.6318, 13.8825,  9.7255, 13.1531,  7.0355, 11.0577,  7.6431, 16.8465,
        10.8626, 14.2105, 13.3177, 12.3941,  9.6333, 11.4007,  9.7514, 11.7453],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [7/80], Step [400/642], LR 1.0e-04, Loss: 28.0
Max Train Loss:  tensor([24.1885, 17.2291, 22.4666, 10.8617, 10.2220, 14.0834, 12.2307, 16.1195,
        15.0046, 15.4184, 12.3904, 12.1256, 10.2343, 16.9056, 16.9669, 18.3271,
        16.6848,  8.2520, 14.9474,  7.7319, 12.2533,  6.9814, 11.8826, 11.0130,
        17.0816, 14.0460, 19.4373, 12.3060,  6.1869,  9.8595, 13.0338,  9.7443,
        15.0690, 14.9380,  9.8093,  8.4698, 14.4151, 12.0593, 12.6517, 18.1612,
        11.3986, 18.2740, 12.8832, 11.2835, 11.2564, 12.2155, 13.6058, 11.8346,
         6.2709, 11.3290,  6.7616, 12.8659,  7.2286,  9.5144,  9.7345, 11.8122,
        24.3571, 15.6597, 14.7015, 11.5715, 16.2129, 15.5852, 13.3269, 10.6477,
         9.9164,  7.7342,  8.5020, 11.4296,  9.8498, 12.2563,  7.8479, 13.2077,
        13.7643, 12.6007, 11.9937, 17.5153,  9.0504, 11.6415, 10.9195, 11.9143],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [7/80], Step [500/642], LR 1.0e-04, Loss: 24.4
Max Train Loss:  tensor([15.1631, 13.6668, 15.7628, 10.5157,  9.9386, 10.1324, 13.9056, 12.8325,
         9.8676, 11.9686, 10.0441, 10.3773, 10.2026,  8.5789, 13.7119, 15.9235,
        14.6776,  7.8704, 12.0451,  8.0488, 12.6939,  7.8362, 12.5123, 12.2216,
        13.6319, 14.0488, 14.8388, 13.8761,  8.7748, 11.2559, 12.6936, 10.3512,
        13.0245, 14.2977, 10.8495,  9.0828, 15.0899, 16.5753, 10.8251, 14.1094,
        12.4452, 17.0428, 13.4465, 12.5560, 14.1113, 14.0732, 16.6597, 14.7339,
         6.0675, 14.1073,  7.9627, 12.3950,  6.2644, 11.0947,  9.7117, 10.2430,
        21.5778, 14.5500, 14.1553, 13.3604, 17.3519, 14.0115, 13.3207, 12.7031,
        11.1455,  7.3617, 10.1091, 11.7869,  7.2269, 12.7325,  7.8195, 14.8297,
        15.7289, 16.3088, 10.3150, 13.1843, 10.2299, 14.8662,  9.9468, 12.1545],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [7/80], Step [600/642], LR 1.0e-04, Loss: 21.6
Max_Val Meta Model:  tensor([ 23.8426,  35.5328,  52.2531,  25.6132,   6.2185,  11.2585,  11.7908,
         14.5593,   7.7402,  10.7774,  10.7589,  11.0604,  10.8667,  10.3393,
          9.2227,  14.2468, 136.1372,   5.6152,  11.8502,   6.4644,  10.5784,
          6.7519,   9.0072,  10.1965,  18.8583,  12.0824,  16.8196,   8.4656,
          7.6493,   9.2164,   8.0447,   7.9453,   9.8007,  11.3518,   8.8053,
          6.9558,   9.5746,   8.9259,   7.3230,  26.6298,  10.3519,  15.5983,
         10.9981,  13.4682,  12.3023,  12.1506,   9.6523,  10.7944,   4.8966,
         10.3425,   5.7557,   9.5540,   7.2660,   6.5715,   9.5254,   9.2671,
         20.8872,  11.8569,  14.3752,   8.6018,  10.1014,  17.6908,  10.2718,
         10.2189,   9.7037,   3.9678,   8.2633,   9.4651,   8.7122,  15.3221,
          7.6283,  15.5667,  16.5289,  11.3247,   9.5860,  11.9697,   8.8325,
          8.8861,   9.7544,  11.1796], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 21.4044,  33.9440,  47.7856,  26.3158,   6.6351,  11.6820,  12.1391,
         15.0840,   8.0684,  11.0087,  11.2405,  11.5347,  11.2142,  10.3791,
          9.9554,  14.7487, 133.0651,   6.0017,  12.4248,   6.8928,  11.0848,
          7.1927,   9.5087,  10.7693,  19.1872,  12.3326,  17.6971,   8.9536,
          8.0153,   9.5420,   8.5645,   8.4204,  10.2379,  11.9961,   9.2463,
          7.4678,  10.1048,   9.1156,   7.7806,  26.7417,  10.7705,  15.8578,
         11.5542,  14.0470,  12.7048,  12.5463,  10.1802,  11.3141,   5.2667,
         10.9136,   6.1487,  10.1112,   7.5966,   7.0500,  10.0403,   9.7750,
         21.8435,  12.4717,  14.3264,   9.0952,  10.9859,  17.8239,  10.7858,
         10.7531,  10.2411,   4.3466,   8.7393,  10.3151,   9.1881,  15.9551,
          8.0905,  15.8869,  17.1068,  11.8347,   9.8513,  12.2520,   9.3295,
          9.4516,  10.3112,  11.7284], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 58.2560,  60.9446,  78.8932,  49.9829,  15.1718,  22.5510,  25.4855,
         32.4742,  17.9210,  21.8615,  22.8564,  24.2805,  21.7286,  23.2710,
         21.0772,  30.8307, 253.7978,  14.4323,  22.2244,  15.6992,  21.4953,
         16.2084,  20.3004,  21.8594,  41.4076,  26.8907,  35.7107,  19.1853,
         19.2529,  21.3260,  18.8532,  18.7476,  23.0556,  22.2223,  23.6776,
         14.4618,  20.7554,  14.9119,  17.3217,  49.9418,  23.5160,  29.0950,
         24.1542,  30.0449,  26.3994,  28.0148,  20.7425,  22.0446,  10.4239,
         22.2436,  14.9572,  16.7670,  17.7510,  14.3547,  20.8722,  20.6007,
         30.3831,  23.9162,  27.9876,  19.4506,  22.6755,  32.2395,  20.7438,
         22.0859,  20.9472,   8.3997,  17.5846,  19.1301,  21.2733,  33.1126,
         17.8829,  28.6250,  28.5550,  24.4313,  18.3650,  17.8893,  19.6272,
         16.9504,  21.3675,  24.4785], device='cuda:0')
Outer loop valEpocw Maximum [7/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 136.1
model_train val_loss valEpocw [7/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 133.1
model_train val_loss  valEpocw [7/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 253.8
Max_Val Meta Model:  tensor([36.5166, 41.0374, 48.1344, 45.7583, 37.7986, 44.6662, 47.7050, 40.0290,
        38.0328, 42.0163, 42.6016, 48.1831, 49.9742, 38.1876, 41.1345, 34.6500,
        37.5387, 36.9381, 46.0359, 38.2893, 43.6197, 37.9807, 39.6366, 40.3189,
        40.1708, 39.4113, 40.6355, 40.4350, 37.7802, 38.9442, 39.0643, 40.0572,
        39.8111, 43.5841, 33.7387, 40.8490, 41.0863, 46.9051, 39.2970, 39.9774,
        41.4645, 43.8523, 39.5998, 39.9342, 41.1363, 37.5059, 40.4800, 42.6819,
        42.4071, 42.0910, 36.5746, 47.0647, 37.0726, 40.8742, 40.1896, 40.9586,
        42.6903, 47.0602, 35.8541, 39.6346, 43.9639, 42.0659, 35.8153, 42.0675,
        40.4389, 43.8012, 42.2274, 42.7822, 36.6984, 41.8592, 40.1632, 46.2806,
        45.9368, 41.6025, 43.1365, 45.6926, 38.5742, 45.5003, 42.0272, 40.7301],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([29.7105,  9.5989, 29.1059,  7.1979,  5.1069, 18.9632, 42.9456, 26.5479,
        15.7061, 17.2801, 12.2701, 74.9693,  9.9102,  5.8514, 10.0895, 10.8108,
         8.6126,  8.3356,  9.5945, 11.6829,  8.6579,  5.3883,  7.3746,  8.7803,
        14.7168, 11.3919, 17.7274,  9.9789,  7.6486,  5.9188,  6.3253,  6.6603,
         7.5061,  9.0276,  7.3783,  5.0518,  8.7864,  5.3586,  6.0281,  4.1985,
         7.0871,  5.2885,  7.8056,  7.8814,  7.9490,  5.9683,  7.7324,  8.1077,
         3.6611,  8.7847,  4.7782,  6.8457,  4.9596,  5.1146,  7.7953,  8.6737,
        10.1893,  9.7979,  7.6306,  7.0437,  4.3826,  6.0833,  6.4282,  7.9294,
         7.8665,  2.9092,  6.7274,  9.4640,  5.0635,  8.8912,  6.4896,  7.6210,
         9.1727,  8.6444,  6.0082,  5.2587,  6.9307,  6.2828,  8.1297,  9.1891],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 73.8131,  19.7658,  54.0653,  13.5570,  11.2419,  36.2097,  84.2690,
         56.7591,  34.9111,  34.0603,  24.6714, 152.8339,  19.1662,  12.9650,
         21.0635,  25.9137,  19.2423,  18.7611,  17.7668,  25.8269,  17.0281,
         12.0746,  15.6634,  18.6931,  31.4652,  24.8211,  37.2529,  20.6795,
         17.2580,  12.7903,  13.8497,  14.2476,  16.1602,  17.3285,  18.6722,
         10.4243,  18.2334,   9.6464,  13.0352,   8.9227,  14.3032,   9.9452,
         16.6815,  16.9493,  16.1153,  13.1996,  16.0635,  15.7461,   7.2200,
         17.4999,  11.0312,  12.4413,  11.2105,  10.5399,  16.2015,  17.7505,
         15.1491,  17.3559,  17.9726,  14.9170,   8.4960,  11.6336,  14.7257,
         16.0788,  16.2616,   5.5113,  13.3980,  18.4433,  11.6561,  18.2081,
         13.5476,  14.3657,  16.3213,  17.7925,  11.7199,   8.5500,  15.0674,
         11.3916,  16.5568,  19.3863], device='cuda:0')
Outer loop valEpocw Maximum [7/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 50.0
model_train val_loss valEpocw [7/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 75.0
model_train val_loss  valEpocw [7/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 152.8
Max_Val Meta Model:  tensor([35.7558, 37.6875, 36.7296, 36.1832, 35.7412, 40.9623, 38.7517, 37.7170,
        36.2012, 39.2604, 40.5161, 36.8109, 40.8469, 36.3337, 37.4132, 32.0983,
        37.3782, 38.9687, 40.6855, 36.4945, 35.4091, 35.9733, 35.3168, 40.3601,
        38.4369, 37.0646, 38.2806, 40.9554, 36.1503, 38.2788, 37.5796, 38.4866,
        40.4649, 41.2570, 32.2395, 40.1829, 38.9692, 40.2173, 37.2918, 36.7068,
        40.5097, 45.7743, 39.3531, 39.2833, 40.9432, 37.9631, 44.4582, 40.5644,
        44.3350, 40.3491, 35.6325, 38.8140, 36.8224, 38.5346, 39.4602, 45.3901,
        65.4276, 39.5655, 32.8891, 37.5325, 51.0278, 39.6106, 32.1213, 41.3705,
        38.4310, 40.5918, 40.3657, 40.3715, 35.1965, 39.8357, 39.0134, 40.4052,
        38.1022, 40.1473, 39.9529, 38.3808, 36.2597, 37.4236, 39.9660, 38.6924],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([12.9129,  8.2273,  4.3798,  7.9166,  8.4158, 11.2302, 12.0714, 13.0078,
         8.7775,  9.6695, 12.3731, 11.8996, 12.5532, 12.3530, 10.3903, 13.6270,
         9.9512,  8.2383, 13.3859,  8.7458, 11.7517,  8.7950, 10.8728, 12.4633,
        12.2285, 11.0099, 15.7066, 13.2922,  8.4126,  9.8736, 10.9329, 11.2632,
        13.4257, 13.5608, 10.8677,  8.5413, 14.7342,  9.8250,  9.6619, 17.8554,
        17.4229, 41.2480, 34.8931, 33.8789, 27.7875, 36.1238, 14.0512, 14.5789,
        22.1501, 13.7693, 16.7658, 22.5758, 25.3646, 13.8222, 28.7714, 43.1273,
        65.8122, 14.8827, 10.8099, 10.8558, 77.8841,  9.9075, 11.8678, 14.9233,
        13.8986,  6.1268, 12.2499, 12.9247,  9.1889, 12.1752, 10.5098, 13.1333,
        13.3468, 19.3779,  6.9645, 13.3995, 11.9122,  9.4532, 12.3279, 13.3451],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 31.6747,  17.7635,   8.7337,  15.9090,  18.5331,  21.7985,  25.1659,
         27.8529,  19.6316,  18.8899,  24.9334,  24.7189,  23.2303,  27.3798,
         22.0939,  33.5113,  22.0712,  17.7203,  25.6858,  19.1392,  24.7990,
         19.7133,  24.0307,  25.7917,  26.0305,  24.0751,  33.2696,  26.5236,
         18.7347,  20.5519,  23.7795,  23.9135,  27.8892,  26.2579,  27.5845,
         17.3935,  30.9616,  18.6965,  20.9008,  39.5548,  34.7521,  74.9581,
         74.9423,  73.7821,  55.8398,  79.1550,  27.5603,  28.4974,  42.0209,
         27.3749,  38.0309,  45.2011,  56.6539,  28.6751,  60.3152,  87.1711,
        101.4708,  28.4235,  26.4420,  23.1364, 145.9494,  19.2531,  27.6236,
         28.9385,  29.0122,  11.6306,  24.2950,  25.3433,  20.9045,  25.0148,
         21.4705,  25.9336,  24.8903,  40.0347,  13.7658,  22.6987,  26.3564,
         18.4453,  25.2286,  28.4313], device='cuda:0')
Outer loop valEpocw Maximum [7/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 65.4
model_train val_loss valEpocw [7/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 77.9
model_train val_loss  valEpocw [7/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 145.9
Max_Val Meta Model:  tensor([35.9879, 37.8457, 38.9297, 35.4634, 34.7348, 40.6775, 37.9223, 39.5869,
        34.7923, 38.7305, 39.7698, 36.6037, 40.6241, 35.0845, 36.6451, 31.4051,
        36.0274, 37.9959, 40.1934, 35.2874, 34.2850, 35.0291, 31.5488, 39.7107,
        38.0087, 36.2369, 37.7876, 39.8172, 35.6008, 37.9002, 35.6922, 33.9621,
        34.9712, 40.6178, 31.5990, 39.6668, 38.1767, 40.3849, 36.2670, 36.0854,
        38.7246, 38.7936, 36.3401, 37.2045, 38.6791, 34.3604, 39.8384, 38.8318,
        35.7943, 39.4743, 34.0149, 39.8197, 35.0763, 35.6203, 39.2690, 37.0735,
        38.4932, 38.6861, 32.0761, 40.0141, 36.9958, 39.6670, 33.3697, 40.8093,
        37.4545, 37.4118, 38.9278, 51.4702, 33.7626, 38.9196, 37.6784, 40.0461,
        39.3926, 38.8900, 47.6128, 39.2258, 35.2121, 38.1612, 39.1645, 39.3608],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 21.0134,   5.5880,  11.6079,   4.8285,   5.2558,  10.1245,   9.1934,
         11.1935,   5.4931,  12.4874,   9.2775,   9.0757,   8.7995,   9.2595,
          9.4176,  10.2241,   7.1813,   5.0899,   9.3662,   5.4557,   8.1606,
          5.5554,   6.9761,   8.4246,  13.9532,   7.3557,  14.1425,   7.5001,
          7.8473,   6.3200,   6.2861,   6.6622,   7.5392,   9.2399,   7.5955,
          5.0248,   7.8315,   5.0135,   6.2007,   5.4989,   7.3493,   5.6641,
          7.8643,   8.1042,   8.2582,   5.4153,   8.0617,   8.1777,   3.8026,
          8.4120,   4.9804,   6.7995,   5.4049,   5.0861,   8.1987,   7.8218,
          5.3734,   8.3667,   8.5735,   7.5665,   6.9377,   6.5209,   6.7180,
          8.5078,   8.0316,   3.0798,   6.9183,   8.3207,   5.2116,   8.2386,
          6.7733,   7.7261,   8.0597,   9.1346, 184.5244,   7.0217,   6.9969,
          6.9338,   8.3356,   9.5405], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 52.0138,  11.8872,  22.9435,   9.7140,  11.5483,  19.6080,  19.1688,
         24.0208,  12.3776,  24.6359,  18.5880,  18.7413,  16.3490,  20.5866,
         20.1226,  25.2213,  15.9081,  10.9093,  18.0521,  11.9781,  17.4930,
         12.4102,  16.1171,  17.4292,  29.7081,  16.0532,  29.9831,  15.1195,
         17.4457,  13.1497,  13.5534,  14.6536,  16.4462,  17.8838,  19.2139,
         10.3268,  16.5005,   9.2481,  13.4010,  12.0291,  14.7133,  10.5312,
         17.0729,  17.4391,  16.5798,  12.0030,  16.5239,  16.1994,   7.6458,
         16.6249,  11.3271,  12.8535,  11.5148,  10.8169,  16.6725,  16.4065,
          8.2938,  16.0525,  21.1063,  15.3456,  13.9991,  12.5150,  15.1609,
         16.5425,  16.6958,   5.7681,  13.8077,  14.9034,  11.9739,  16.8905,
         13.9312,  15.1342,  14.8298,  18.7776, 349.2377,  11.6841,  15.4739,
         13.3310,  16.9723,  19.9641], device='cuda:0')
Outer loop valEpocw Maximum [7/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 51.5
model_train val_loss valEpocw [7/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 184.5
model_train val_loss  valEpocw [7/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 349.2
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [80.55579245 97.2137279  91.04908566 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.05906361
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 90.65557194 96.13796128 96.24273583 96.9067141
 89.79300934 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [75.3523958  97.2137279  90.25353005 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.05906361
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 90.05372742 96.13796128 96.24273583 96.9067141
 89.79300934 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [91.98062889  3.59935416 53.14210644  3.81423403  1.99709541  3.41456763
  2.28949776  5.49899424  2.0911248   3.89949587  1.40808535  1.21121589
  0.93799965  4.71377265  1.93157506  4.73367032  3.97441307  1.93239093
  0.82687892  1.11032323  1.19024642  0.46149139  0.84390408  1.2432708
  5.35563057  3.97775619 16.68728792  3.4287587   3.35832206  1.31739752
  1.75833859  0.86809332  2.66893346  1.38766681  1.63196894  1.83490071
  2.49776231  6.44040828  1.99340649 24.10001145  6.31521586 27.5933278
  6.33659348  9.36234475  7.69355774 14.87656055  2.62603383  2.05738698
  4.08387967  2.26595541  1.46634913  1.7196612   1.41434261  4.48424894
  2.10016308  4.34304047 52.02177311 11.93382605 10.88701789  5.73293098
 35.90901653  3.87040474 16.04330036  9.6607024   6.63207231  6.89914328
  6.64850012  6.66547474  4.32505756  7.30187774  0.68049816  9.51563975
  7.44840308 16.55330453  5.25713373 10.22091954  1.56829187  2.69809864
  0.2611243   1.04241013]
Accuracy th:0.5 is [45.53063437 97.2137279  73.1496936  97.02489005 97.26733349 78.01196379
 78.32750576 77.17985892 79.2119979  96.41695398 79.56408913 98.52097319
 99.41399349 80.44858128 79.1279346  96.56680596 96.29512311 78.88549116
 98.65376884 98.30776915 80.5765037  80.11110976 98.38695922 78.91473057
 80.72026413 96.65086926 94.0778012  78.60406184 98.01293844 79.44956811
 97.30875598 98.57457877 96.36213009 98.02024829 86.15635774 79.06092762
 78.87452638 89.9124036  97.11504489 75.95180371 79.52510325 92.05906361
 78.30923113 77.84871042 96.9627563  93.87434364 98.02877645 98.57336046
 86.72408962 87.42583546 86.73627271 98.55508583 98.99976852 78.55898442
 98.70615611 78.86843484 73.20817242 93.08000634 96.24273583 96.9067141
 89.79300934 97.17717864 90.553234   78.83797712 98.42838172 79.32651893
 98.20786784 77.94983005 79.99293381 97.55607266 80.58259524 95.99054592
 79.59332854 95.45083515 78.20445657 82.44051608 85.60446388 79.46540612
 80.61548958 99.14718388]
Accuracy th:0.7 is [45.60007797 97.2137279  73.1496936  97.02489005 97.26733349 78.01196379
 78.32750576 77.27976024 79.2119979  96.47177788 79.56408913 98.52097319
 99.41399349 80.91519353 79.1279346  96.56680596 96.29512311 78.88549116
 98.65376884 98.30776915 81.01022161 80.11110976 98.38695922 78.94153336
 81.24170027 96.65086926 94.0778012  78.60406184 98.01293844 79.44956811
 97.30875598 98.57457877 96.36213009 98.02024829 86.36590685 79.06092762
 78.87452638 90.20845263 97.11504489 75.95180371 80.20248291 92.05906361
 78.30923113 77.84871042 96.9627563  93.87434364 98.02877645 98.57336046
 88.72211596 88.19946151 86.903181   98.55508583 98.99976852 78.55898442
 98.70615611 78.86843484 73.20817242 93.46133697 96.24273583 96.9067141
 89.79300934 97.17717864 90.73110708 78.83797712 98.42838172 79.32651893
 98.20786784 77.94983005 79.99293381 97.55972759 80.58259524 95.99054592
 79.59698347 95.45083515 78.20445657 82.51848784 85.72873138 79.46540612
 80.61548958 99.14718388]
Avg Prec: is [55.9186933   3.05047917 11.26877016  3.28770945  2.20945296  3.818828
  3.3107645   5.45380936  2.45229653  3.78741205  1.54552465  1.59347332
  0.62789347  5.15292902  2.72428212  3.30548352  3.63068986  2.71959339
  1.46397625  1.71764198  1.91282112  0.90540258  1.80064145  2.4626633
  5.05274164  3.53471525  6.59460928  3.34834284  1.99070004  1.95973274
  2.67709448  1.32765917  3.6414811   1.67608609  2.35908288  2.33679654
  2.99893845  2.58878469  2.73523609  7.63510803  2.40837945  8.39193787
  3.34043032  4.25255546  3.34089368  6.5233594   2.06968083  1.45082482
  2.12714742  1.51858033  1.77666262  1.64132593  1.07500416  3.05782948
  1.41281218  2.71448545 11.41136873  3.7882204   3.96923517  2.79172986
 11.02255935  2.15374395  3.89555061  2.95045729  1.56268067  2.48915885
  1.75111601  4.17383438  1.3022228   2.45781182  0.17115347  3.3992956
  1.97356639  4.54729863  3.90563864  3.16870841  0.82358975  1.88279328
  0.13669423  0.73326652]
mAP score regular 7.70, mAP score EMA 3.78
starting validation
Accuracy th:0.5 is [83.80546628 97.22450607 91.67102673 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.37362035
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.3704811  96.39235618 96.16314124 96.78102499
 90.13628323 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [80.86802701 97.22450607 90.97839898 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.37362035
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.2284675  96.39235618 96.16314124 96.78102499
 90.13379176 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [93.88006798  4.0125848  58.04607295  3.96582279  1.56274859  3.37614169
  2.42168286  5.57991897  2.26852     4.0424916   1.46497495  1.20404124
  0.86434691  4.95598405  1.96451149  5.28628689  3.94103935  1.90418189
  0.73876787  1.07958102  1.05121121  0.4663078   0.86648183  1.16702583
  5.36423082  4.24473357 16.37750847  3.06975938  3.99744661  1.34330754
  1.5438361   0.78776705  2.6207865   1.31882646  1.42662925  1.6673576
  2.21191991 12.83697192  2.03017203 24.58285258  6.65071077 28.16751925
  6.51575887  9.01919223  8.2648811  14.53758671  2.60239966  2.12187736
  4.40344904  2.19522916  1.52006577  2.03066642  1.71548273  4.72756485
  2.20092982  4.3603111  50.71723518 10.74751009  9.97695865  6.22763199
 37.70431428  3.62617048 17.28420034 10.64219511  8.26607486  6.72983983
  8.21592433  7.54147073  4.0399316   7.671774    0.88805643  9.01955782
  6.94994571 18.22961345  5.59304961 10.44985324  1.47492195  3.18074103
  0.24434688  0.93907409]
Accuracy th:0.5 is [45.31479682 97.22450607 71.69693799 96.96290206 97.90716795 77.27782345
 77.38993946 76.15666343 78.77270349 96.41727085 79.0268331  98.5325261
 99.34972718 78.62819842 78.85990483 96.31262924 96.21047911 78.36161148
 98.78167277 98.34068316 79.58492164 79.66714005 98.31327703 78.42888108
 78.77021202 96.52938685 94.3393876  78.36410295 97.81747515 78.96952936
 97.52597354 98.67204823 96.39983058 98.18870369 86.55355408 78.58584349
 78.49365922 91.57136806 97.0276802  75.48895035 78.55345442 92.37362035
 77.6938984  77.33761866 97.03764606 94.02795426 98.18621222 98.77668984
 87.15150609 87.32092583 85.27792311 98.55993223 98.87385704 77.7387448
 98.6969629  78.37656028 72.0856068  94.15751053 96.16314124 96.78102499
 90.13379176 97.04761193 90.4078531  78.28935895 98.32075143 79.1389491
 98.13139996 77.45970053 79.64969978 97.53095647 80.14799312 96.07843137
 79.34574084 95.44559882 77.40737972 83.4367292  87.3533149  79.01686723
 80.21775419 99.15040985]
Accuracy th:0.7 is [45.59384109 97.22450607 71.69693799 96.96290206 97.90716795 77.27782345
 77.38993946 76.17659516 78.77270349 96.41976231 79.0268331  98.5325261
 99.34972718 79.05423923 78.85990483 96.31262924 96.21047911 78.36161148
 98.78167277 98.34068316 79.90133792 79.66714005 98.31327703 78.42888108
 79.1090515  96.52938685 94.3393876  78.36410295 97.81747515 78.96952936
 97.52597354 98.67204823 96.39983058 98.18870369 86.85253008 78.58584349
 78.49365922 91.77566834 97.0276802  75.48895035 78.89976829 92.37362035
 77.6938984  77.33761866 97.03764606 94.02795426 98.18621222 98.77668984
 89.05498667 87.70212024 85.43488552 98.55993223 98.87385704 77.7387448
 98.6969629  78.37656028 72.0856068  94.4290804  96.16314124 96.78102499
 90.13379176 97.04761193 90.57976431 78.28935895 98.32075143 79.1389491
 98.13139996 77.45970053 79.64969978 97.53593941 80.14799312 96.07843137
 79.34574084 95.44559882 77.40737972 83.52393054 87.45546503 79.01686723
 80.21775419 99.15040985]
Avg Prec: is [53.51521961  3.70710268 14.95041479  4.53638406  1.43917925  4.36177424
 14.53725659  8.72278167  8.63407528  5.32781882  2.91048154  5.52077056
  2.36154429  5.82602607  3.00063407  3.67609129 18.91991372  6.47947147
  1.535484    2.74922666  3.47387951  1.55174534  1.19085217  5.10928295
  5.5579328   8.07863655  7.76988588  4.62608598  3.82788806  4.73071155
  2.15466316  0.84093378  3.04396074  1.10030849  1.64015271  2.13989954
  1.95048097  2.18444924  2.24912053  6.19279082  1.73031812  6.00573132
  2.190305    2.71355948  2.37126026  4.82118438  1.63770585  0.98970192
  1.40566974  1.12456414  1.15156987  0.95369521  0.72470488  2.28955476
  0.84058643  1.81583603  9.86925307  2.87338201  3.68012832  2.75945148
  7.76385643  2.07393943  3.07702927  2.49981376  1.33862812  1.82005253
  1.48831218  3.46154336  1.09355918  2.27852658  0.19089952  3.29275757
  1.59637708  3.85765252  3.5444564   2.30775086  0.59577214  1.49784819
  0.1256768   0.58895462]
mAP score regular 7.99, mAP score EMA 4.23
Train_data_mAP: current_mAP = 7.70, highest_mAP = 7.70
Val_data_mAP: current_mAP = 7.99, highest_mAP = 7.99
tensor([0.3980, 0.4926, 0.5319, 0.4968, 0.4525, 0.5185, 0.4778, 0.4664, 0.4443,
        0.5098, 0.5008, 0.4839, 0.5457, 0.4490, 0.4693, 0.4181, 0.4647, 0.4570,
        0.5253, 0.4547, 0.4645, 0.4469, 0.4329, 0.4856, 0.4668, 0.4579, 0.4763,
        0.4985, 0.4417, 0.4796, 0.4715, 0.4486, 0.4438, 0.5120, 0.3918, 0.5010,
        0.4811, 0.5566, 0.4593, 0.4765, 0.4916, 0.5465, 0.4667, 0.4652, 0.4955,
        0.4489, 0.4990, 0.5104, 0.4961, 0.5027, 0.4331, 0.5483, 0.4679, 0.4723,
        0.4956, 0.4709, 0.6755, 0.5280, 0.4280, 0.4929, 0.4884, 0.5231, 0.4530,
        0.5183, 0.4842, 0.5378, 0.5002, 0.5299, 0.4340, 0.4888, 0.4792, 0.5235,
        0.5829, 0.4856, 0.5335, 0.6401, 0.4608, 0.5508, 0.4896, 0.4805],
       device='cuda:0')
Max Train Loss:  tensor([20.6258, 11.4123, 11.6620,  9.7307, 11.9954, 16.9565, 12.5607, 12.2078,
        12.1114, 12.1314, 12.2927, 14.1230, 11.0262, 12.1561, 12.6093, 13.4218,
        13.1290,  9.7709, 16.3679,  8.1095, 11.9199,  8.4705, 12.3069, 11.6254,
        14.1189, 11.3705, 13.6548, 13.9511,  9.0112,  9.1383, 13.8043,  8.3198,
        13.1313, 12.4426, 12.0240, 11.6307, 13.5078, 12.6969, 11.2276, 14.4986,
        11.6886, 16.1790, 13.6067, 13.5110, 14.2611, 14.6455, 13.0213, 10.9770,
         7.4578, 10.3530,  7.2596, 12.1845,  7.0272,  8.4006, 11.6643, 12.6971,
        15.0904, 13.4570, 13.8169, 13.8854, 15.7763, 13.5339, 12.3641, 15.3714,
        12.5601,  6.4994, 12.2207, 10.4335,  8.0611, 12.9600,  8.4791, 14.2852,
        12.5443, 13.5645, 12.0668, 13.4287, 10.4006, 10.4892, 10.3158, 12.3622],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [8/80], Step [000/642], LR 1.0e-04, Loss: 20.6
Max Train Loss:  tensor([19.8405, 13.7532, 22.1350, 18.1594, 11.3819, 13.6057, 13.6757, 13.7631,
        12.3837, 18.9459, 12.5444, 11.6458, 15.7867,  9.9494, 11.4822, 12.6838,
        10.2329, 11.3959, 17.0108,  9.8876, 11.8173,  8.5225,  9.4174, 16.4072,
        16.8556,  9.7462, 20.3499, 14.3118, 10.3788, 11.6070, 14.8789,  9.2971,
        13.9818, 19.7017,  5.1125, 20.5332, 12.7876, 18.7435, 14.0790, 15.8412,
        12.2711, 18.1048, 11.1882, 14.1689, 12.7524, 12.9832, 12.5486, 12.7326,
        23.3640, 12.9919,  9.8896, 17.1691, 14.7934, 12.9435, 11.5998, 10.9139,
        22.0812, 21.7019, 11.2392, 11.6192, 19.8346, 18.2135, 10.9435, 21.4042,
        11.5007, 19.5333, 21.2163, 19.5668,  8.6619, 11.6568,  9.8429, 20.1811,
        21.9460, 12.3282, 24.2998, 15.4267,  7.9650, 22.6020, 10.0799, 18.1347],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [8/80], Step [100/642], LR 1.0e-04, Loss: 24.3
Max Train Loss:  tensor([21.0770, 12.6183, 17.8730, 19.3551, 11.0698, 13.2898, 11.3865, 14.3692,
        12.6749, 13.6296, 12.5871, 14.8309, 14.3497, 13.4718, 11.3570, 13.8037,
         9.5659, 12.7424, 17.3555,  8.0592, 12.3130, 10.8228,  8.6543, 11.7709,
        11.0319, 10.3168, 13.9128, 10.1964,  9.3123, 13.1173, 13.8804,  8.9882,
        13.2104, 11.9529,  8.8509, 11.9375, 13.4246, 13.1054, 12.9282, 11.9687,
        10.3593, 15.9068,  8.9011, 12.8186, 12.5139, 11.8921, 13.2083, 14.3750,
        10.4553, 13.0278,  6.6224, 12.0746, 13.0281, 10.0112, 11.8739,  9.5823,
        22.5707, 10.6786, 10.9693, 10.2132, 14.8404, 12.7155,  9.3294,  9.9992,
         9.5021, 12.9711,  8.0894, 11.7398,  9.4827, 14.8432,  9.1859, 16.3737,
        11.4619, 12.8546, 10.0705, 13.9831,  7.8461, 13.1456,  9.5167, 17.9106],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [8/80], Step [200/642], LR 1.0e-04, Loss: 22.6
Max Train Loss:  tensor([18.7260, 11.6982, 17.9375, 13.9389,  8.0741, 12.7590, 18.0001, 12.1794,
         8.4282, 13.7411, 11.4003, 11.6953, 13.6868, 10.2530, 11.9889, 11.8225,
         9.9049, 18.4604, 10.7592, 13.0349, 15.6425,  9.0906,  9.2007,  9.5419,
        13.5420, 14.5372,  9.0127, 15.6552,  7.8262, 12.3146,  7.7273,  8.7204,
        13.6457, 12.9510,  3.6063,  9.0024, 11.2260,  9.3288, 16.0755, 17.0123,
        13.4679, 16.4548, 10.8341, 15.1334, 14.6490, 13.9391, 10.4767, 11.4154,
        11.5313, 11.6200,  6.8149, 10.3894, 13.7363, 10.0287, 10.1126, 12.7566,
        16.7946, 15.2866,  7.6573,  9.4484, 17.3148, 13.3304, 14.1132,  9.8967,
        11.6383, 12.4115, 10.4384, 14.5300,  8.4429, 12.5696,  9.1730, 15.8529,
        10.5608, 14.2030, 10.0759, 14.9744,  7.1996, 10.6883,  9.5014,  7.0312],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [8/80], Step [300/642], LR 1.0e-04, Loss: 18.7
Max Train Loss:  tensor([20.5442, 10.5991, 19.9490, 15.3813, 12.6566, 16.2000, 12.8101, 14.8466,
         8.2132, 14.0139, 11.3466, 11.2513, 17.2985, 11.0631, 10.0100, 12.7136,
        11.8962, 10.4615,  8.1884, 10.3078, 11.2954,  7.5556,  8.9917,  9.4187,
        15.7167, 12.6130, 12.8855, 11.9186,  6.8865, 10.1420, 14.0081, 11.3508,
        10.5234,  8.9223,  8.2120, 14.5183, 11.2328, 17.1480, 12.6518, 14.3461,
        10.2746, 16.8397,  8.0954, 13.4853, 10.9498, 10.4188, 12.7101, 12.4764,
         9.7521, 11.0703,  7.5536, 11.4291, 13.5835, 12.6246, 10.4746,  8.6751,
        23.2277, 13.5763, 11.5304, 15.2362, 15.0316,  9.7269,  8.2744, 13.1734,
        12.9123, 13.4960,  8.7684,  9.4444,  8.1507, 11.4901,  9.7350, 11.5429,
        10.2749, 12.9907,  8.5013, 13.7365,  6.5083, 10.6935,  9.4706,  5.7908],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [8/80], Step [400/642], LR 1.0e-04, Loss: 23.2
Max Train Loss:  tensor([20.4870, 12.7849, 19.4565, 12.9715,  9.2555, 15.3259, 15.1794, 15.4494,
         6.3550, 15.0450, 12.1092, 11.4978, 14.0329, 12.3117, 11.4749, 13.3086,
        11.3370, 10.7160, 10.2302, 12.3744, 10.4247, 10.9002,  9.9882,  8.2159,
        16.6941, 13.1298, 16.4491, 13.0063, 12.6081, 13.9175, 13.3998,  9.2258,
        13.8622, 10.9850,  6.5649,  8.1524, 12.9140, 13.8118, 14.3420, 15.5393,
        12.2967, 14.5573,  9.0765, 10.4983, 11.0405, 10.4747, 11.7991, 11.6822,
        10.0027, 10.1973,  9.5885, 13.0965, 14.8551,  9.7025, 11.1559,  8.9681,
        15.0563, 14.5944,  8.0408, 13.9731, 12.6252, 15.2973,  9.4757, 10.8599,
        10.3184, 12.8716,  8.0367, 11.9399,  6.2002, 10.8856,  9.4422, 14.1064,
        10.8767, 14.1461,  9.6141, 10.0840,  6.7973, 13.9722,  9.7826,  4.7506],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [8/80], Step [500/642], LR 1.0e-04, Loss: 20.5
Max Train Loss:  tensor([26.1145, 10.0414, 21.0819,  8.8621, 11.1629, 13.6669, 10.8076, 13.0939,
         9.0510, 15.8230, 15.5468, 13.3942, 14.1348, 13.0977, 14.1944, 15.0811,
         9.6610,  9.7452,  7.4389,  9.3111, 12.7193,  7.8994, 10.9278, 11.5048,
        14.0332, 10.8564, 15.6749, 15.4952, 11.1123,  9.4573,  8.0213,  6.9801,
        15.0068,  4.4265, 10.1963, 16.4972, 12.8575, 14.3046, 13.0552, 11.3541,
        12.3950, 14.9753,  8.9624, 11.6304, 11.6882,  9.0265, 11.1909, 13.6852,
        10.0326, 12.4827,  7.3673, 12.0095, 13.3661, 10.3815, 10.4084, 11.4032,
        15.6449, 16.1677, 10.7551, 15.0561, 15.1295, 11.1920, 13.9025, 12.3157,
        11.0432, 12.3073,  8.2612, 13.7792,  6.9418,  9.9823,  9.5083, 14.8858,
         9.8733, 14.0030, 18.4777, 16.1889,  8.2191, 13.3686,  9.8518,  6.1814],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [8/80], Step [600/642], LR 1.0e-04, Loss: 26.1
Max_Val Meta Model:  tensor([ 21.5511,  30.1039,  48.4635,  23.5875,   7.0929,  10.1071,   8.6347,
         12.1661,   7.1269,  14.4616,  11.3808,  11.1931,  14.5966,  11.3543,
         10.1433,  11.5765, 128.8089,   7.8123,   5.4639,   8.1561,   9.4781,
          7.6991,   5.9487,   7.0395,  18.2137,  11.9172,  16.8774,   9.3960,
          8.5981,  11.4642,   7.8243,   6.7854,   7.5448,   4.2701,   2.3543,
          8.0045,   5.1315,   9.3302,   8.6187,  24.2313,  11.9110,  15.4496,
          8.8480,  13.5383,  12.7515,  12.6047,   9.8576,  11.5296,   8.9569,
         10.6867,   6.0050,  10.5397,  13.9699,   8.4368,   9.5412,   8.8211,
         18.3783,  11.1655,  10.4389,   9.5715,   7.6329,  18.1959,   7.6032,
          8.9616,   9.6106,  10.1119,   7.1971,   9.3287,   8.5542,  15.3163,
          9.2919,  16.1942,  15.4905,  11.4690,   8.6118,  10.2375,   6.6526,
         10.4485,   9.6257,   4.6224], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 20.3905,  28.6493,  42.5738,  23.7173,   7.9785,  10.8977,   9.1496,
         12.6680,   7.7879,  15.4251,  12.3859,  12.1171,  15.7074,  12.0814,
         11.2889,  12.5323, 120.6597,   8.7425,   6.2314,   9.1128,  10.5130,
          8.6273,   6.7430,   7.8734,  18.1433,  12.4520,  18.0818,  10.4564,
          9.5173,  12.1961,   8.6532,   7.6607,   8.3266,   4.7978,   2.7802,
          8.9698,   5.9122,   9.7871,   9.6080,  23.9322,  12.9308,  16.2228,
          9.7765,  14.5374,  13.5162,  13.1515,  10.9446,  12.6843,   9.9675,
         11.8040,   6.8071,  11.6900,  15.0346,   9.4326,  10.6048,   9.8217,
         18.2462,  12.1238,  10.9692,  10.6386,   8.5418,  18.5966,   8.4747,
          9.9860,  10.6669,  11.2607,   8.1233,  10.6975,   9.3577,  16.4193,
         10.3283,  16.8924,  16.6067,  12.4014,   9.2521,  11.1329,   7.5189,
         11.6133,  10.7194,   5.3214], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 51.2386,  58.1585,  80.0407,  47.7399,  17.6331,  21.0196,  19.1505,
         27.1637,  17.5278,  30.2559,  24.7316,  25.0382,  28.7856,  26.9084,
         24.0560,  29.9735, 259.6554,  19.1316,  11.8615,  20.0408,  22.6349,
         19.3039,  15.5773,  16.2127,  38.8640,  27.1963,  37.9606,  20.9741,
         21.5478,  25.4288,  18.3533,  17.0758,  18.7627,   9.3703,   7.0966,
         17.9022,  12.2876,  17.5828,  20.9192,  50.2300,  26.3048,  29.6842,
         20.9461,  31.2494,  27.2773,  29.2988,  21.9319,  24.8515,  20.0917,
         23.4833,  15.7182,  21.3217,  32.1324,  19.9708,  21.3991,  20.8556,
         27.0097,  22.9599,  25.6268,  21.5823,  17.4880,  35.5522,  18.7073,
         19.2669,  22.0320,  20.9399,  16.2401,  20.1884,  21.5604,  33.5942,
         21.5545,  32.2674,  28.4919,  25.5390,  17.3407,  17.3938,  16.3187,
         21.0861,  21.8923,  11.0745], device='cuda:0')
Outer loop valEpocw Maximum [8/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 128.8
model_train val_loss valEpocw [8/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 120.7
model_train val_loss  valEpocw [8/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 259.7
Max_Val Meta Model:  tensor([40.3213, 32.6747, 42.8128, 43.4728, 34.5337, 46.3230, 47.6139, 39.3412,
        36.9407, 43.8955, 42.7143, 43.2277, 45.0197, 43.6326, 43.4538, 38.8510,
        36.6620, 39.0798, 46.2352, 44.5032, 40.0795, 37.4004, 35.8529, 42.8843,
        38.8236, 38.2570, 39.2813, 41.7371, 38.6266, 40.3433, 39.1706, 38.7960,
        37.9615, 40.0693, 31.7904, 40.5726, 38.9678, 43.9633, 39.4006, 42.4830,
        41.4945, 44.3392, 38.3417, 38.8298, 41.2662, 37.8289, 40.6725, 42.3981,
        42.7020, 41.5185, 36.8738, 43.6475, 39.6277, 39.1979, 40.6151, 39.4177,
        39.1433, 44.3144, 31.5757, 40.1363, 43.6399, 41.3033, 35.5764, 42.7405,
        38.9258, 43.9550, 41.1057, 40.4178, 35.9812, 40.8537, 41.0004, 41.6381,
        40.3160, 40.7893, 42.7431, 43.9237, 37.0544, 44.8016, 40.8447, 39.4475],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([33.4766, 10.1330, 26.4084,  7.4026,  6.5941, 19.0699, 43.8956, 24.4977,
        16.0313, 19.6243, 13.6827, 70.2921, 14.5506, 10.0117, 12.2602, 10.9188,
         8.4993, 10.9898,  5.5747, 14.2375,  9.2854,  7.3152,  5.6859,  7.8980,
        14.2576, 11.8785, 18.6586, 11.9104,  9.4132,  8.9395,  7.2541,  6.6370,
         6.3313,  3.8547,  2.1875,  7.3497,  6.1481,  7.3655,  8.4237,  6.1000,
         9.3709,  5.4928,  6.9120,  9.0932,  9.1797,  6.6213,  9.2912, 10.3501,
         8.8193, 10.2759,  5.8415,  9.4283, 12.8468,  8.0163,  9.1066,  9.3629,
         8.3531,  9.6425,  6.1701,  9.0162,  3.0497,  8.4732,  6.1435,  7.6333,
         8.9944,  9.3596,  6.7556, 10.0293,  5.7455, 10.0266,  9.1920,  9.8579,
         9.2975,  9.9519,  6.0506,  5.0881,  6.1936,  8.9590,  9.1420,  4.3071],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 79.5649,  24.4273,  52.3640,  14.0931,  14.7845,  35.8643,  84.8672,
         51.7930,  35.7171,  38.1687,  26.9149, 143.4929,  27.2802,  19.4980,
         24.7970,  26.6668,  19.6093,  22.9354,  10.1429,  28.3137,  19.4471,
         16.2870,  12.8969,  15.9179,  30.4886,  26.0217,  39.7154,  23.4264,
         20.3008,  18.3007,  15.4247,  14.2308,  13.8314,   7.6071,   5.4921,
         14.9369,  12.8116,  13.8708,  17.8035,  13.3325,  18.5161,  10.2135,
         14.7844,  19.6759,  18.1236,  14.3064,  18.7893,  20.1015,  17.0899,
         20.3057,  13.0109,  18.1264,  27.0913,  16.9013,  18.3003,  19.4473,
         12.7862,  17.8236,  16.0596,  18.4570,   5.8874,  16.4994,  14.2496,
         14.7916,  18.8923,  17.6051,  13.4233,  20.1682,  13.1109,  20.5773,
         18.4321,  19.8859,  17.6593,  20.4339,  11.5022,   8.5323,  13.5709,
         16.6245,  18.7143,   8.9150], device='cuda:0')
Outer loop valEpocw Maximum [8/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 47.6
model_train val_loss valEpocw [8/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 70.3
model_train val_loss  valEpocw [8/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 143.5
Max_Val Meta Model:  tensor([40.0460, 30.5017, 39.3573, 41.7097, 33.9422, 40.4671, 38.7884, 38.0520,
        37.7706, 42.5053, 42.4881, 42.4597, 38.5695, 43.0831, 41.7957, 38.3774,
        35.5478, 36.1433, 38.5179, 41.4883, 39.8143, 37.0571, 40.4992, 40.5993,
        38.2258, 38.0049, 38.2889, 35.6499, 38.5223, 40.8712, 38.8356, 38.5184,
        37.8458, 39.4222, 31.3805, 38.4187, 38.5297, 39.7158, 38.8398, 41.1812,
        41.8824, 46.7582, 42.4483, 39.0781, 42.0733, 39.4202, 44.1120, 45.3801,
        45.0847, 41.3647, 37.0897, 39.3169, 40.1126, 38.8600, 44.3824, 41.4238,
        62.3709, 39.9672, 30.5600, 39.5572, 51.0212, 40.8324, 34.7389, 42.1953,
        38.6600, 39.5685, 40.8162, 40.0201, 36.0196, 40.4466, 40.8842, 40.9892,
        39.2654, 40.7878, 41.8077, 38.9966, 36.2762, 38.8146, 40.4975, 38.9594],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([11.9487,  9.8202,  4.5554,  7.3806,  9.7906,  8.8256,  7.5533, 11.1982,
         9.2423, 15.6113, 14.4039, 13.4169, 16.1290, 14.8567, 13.0387, 13.5473,
        10.0643, 10.9809,  7.4714, 12.1337, 13.1221, 10.7800,  9.2294,  9.4586,
         9.8229, 11.4540, 16.8753, 13.6240, 10.5317, 13.1256, 11.6419, 10.9087,
        11.4098,  6.2065,  4.0251, 11.0904, 11.5107, 10.9953, 12.0908, 17.9364,
        19.1791, 37.7880, 35.1749, 33.0523, 28.6216, 34.3194, 15.1063, 17.5965,
        23.4686, 14.9792, 18.3975, 23.7258, 27.4607, 15.6408, 29.7012, 41.5723,
        67.9456, 14.8104,  8.6335, 12.7880, 75.5409, 12.3422, 11.2723, 14.2578,
        14.9211, 13.1610, 12.0525, 14.1922,  9.6958, 13.1617, 13.1934, 15.3252,
        13.0943, 20.6130,  7.5815, 12.4577, 10.7609, 12.4536, 13.0580,  7.0896],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 28.2785,  24.4286,   9.3986,  14.2973,  22.1247,  17.2091,  15.8258,
         22.7119,  20.2067,  29.7543,  28.1584,  26.7206,  31.6903,  29.0461,
         26.7720,  33.5792,  23.6270,  23.7134,  14.6755,  24.7436,  27.4323,
         23.9982,  19.9042,  19.4732,  21.0270,  24.9730,  36.2687,  28.5616,
         22.5137,  26.1940,  24.7514,  23.3415,  24.7980,  12.2940,  10.0905,
         22.5948,  24.0707,  21.4467,  25.6932,  40.3548,  37.6545,  72.6414,
         75.2147,  72.3596,  56.4962,  73.5177,  29.4582,  32.7159,  44.6287,
         29.4935,  40.6548,  47.9920,  58.0237,  33.1227,  60.0949,  85.8663,
        107.4917,  28.6715,  23.0522,  26.3328, 142.1345,  24.4223,  26.5625,
         27.7856,  31.3970,  26.0023,  23.9216,  28.5307,  21.8917,  27.0310,
         26.3025,  31.3003,  25.2524,  42.3895,  14.5657,  21.7927,  23.9129,
         24.1516,  26.7290,  14.6751], device='cuda:0')
Outer loop valEpocw Maximum [8/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 62.4
model_train val_loss valEpocw [8/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 75.5
model_train val_loss  valEpocw [8/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 142.1
Max_Val Meta Model:  tensor([39.4662, 27.7754, 40.2991, 40.9169, 30.6080, 38.9606, 36.9145, 36.3519,
        34.9694, 40.7393, 39.7274, 40.8875, 36.0563, 35.0065, 40.6341, 40.2892,
        33.9111, 32.1879, 36.8357, 39.8195, 37.3320, 34.6532, 32.5293, 38.3542,
        36.4380, 35.8625, 37.2485, 31.9037, 35.8803, 39.7701, 36.1104, 35.5515,
        38.4860, 38.0944, 28.0452, 36.3973, 35.8432, 26.7872, 36.4070, 36.2763,
        38.6073, 38.9528, 35.3969, 36.3424, 38.2727, 34.6836, 37.0644, 38.7530,
        37.7512, 38.5772, 33.8523, 40.4446, 42.9183, 42.1181, 39.9762, 36.5777,
        39.0399, 37.3509, 29.5451, 32.3953, 39.2384, 39.9948, 32.2921, 45.4516,
        36.2577, 36.6433, 37.7812, 45.2622, 33.0246, 37.9720, 38.0321, 39.9105,
        35.7993, 38.1551, 58.9280, 36.7501, 34.0305, 38.3171, 38.0509, 43.6309],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 20.7354,   6.8792,  12.9337,   4.7067,   6.0563,   9.8895,   7.2691,
         10.9021,   5.5255,  15.8126,  10.2457,   9.8682,  11.8825,   9.7688,
         10.8231,   9.9954,   6.5249,   6.9315,   4.8649,   7.7943,   8.7280,
          6.8509,   5.2433,   6.5852,  12.4694,   7.3827,  15.4129,   7.9716,
          9.1003,   8.7084,   6.7891,   6.1544,   6.0744,   3.6872,   1.9640,
          6.9574,   4.3177,   6.2189,   7.8619,   6.4475,   8.8028,   5.8047,
          6.4572,   8.5876,   8.5978,   6.1736,   8.7352,   9.8378,   8.2505,
          9.0330,   5.4312,   9.0919,  13.2556,   7.7513,   8.6662,   8.0337,
          5.8613,   7.8811,   7.0897,   8.0739,   4.6013,   7.9937,   5.6673,
          7.7380,   8.4507,   8.4234,   6.3263,   7.9949,   5.3402,   8.6839,
          8.6094,   9.5622,   7.7407,   9.7591, 193.8842,   6.4379,   5.7575,
          8.8937,   8.5841,   4.2878], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 49.6579,  16.5175,  25.7534,   8.9990,  13.8028,  19.2142,  15.1135,
         21.9174,  12.2098,  30.3974,  20.2012,  19.7240,  23.4243,  20.7836,
         22.1629,  24.1935,  15.0212,  15.1516,   9.3808,  15.9310,  18.3016,
         15.2414,  11.9823,  13.4815,  26.6233,  16.0589,  32.7041,  16.8181,
         19.8112,  17.2018,  14.4468,  13.2548,  12.8686,   7.0586,   4.9491,
         14.0152,   8.8717,  12.5690,  16.7131,  14.3908,  17.4144,  10.8519,
         13.8002,  18.5279,  17.0374,  13.4506,  17.6769,  18.9595,  16.0819,
         17.9025,  12.0870,  16.9811,  25.8144,  15.8689,  17.2093,  16.7040,
          8.9160,  15.1442,  18.4260,  17.3550,   8.7849,  15.5166,  13.2726,
         13.9753,  17.7705,  16.5891,  12.5662,  15.1242,  12.1836,  17.8543,
         17.3319,  19.1422,  14.5643,  20.0161, 376.4153,  10.6556,  12.6294,
         16.6057,  17.5623,   8.2373], device='cuda:0')
Outer loop valEpocw Maximum [8/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 58.9
model_train val_loss valEpocw [8/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 193.9
model_train val_loss  valEpocw [8/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 376.4
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [81.90811515 97.2137279  91.11974757 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.10292272
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 90.9016703  96.13796128 96.24273583 96.9067141
 89.83321353 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [80.83722177 97.2137279  90.09880484 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.05906361
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 90.13779072 96.13796128 96.24273583 96.9067141
 89.79300934 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [92.94822278  3.76717798 53.56280547  5.66882242  1.98251986 11.26911454
  3.20437567 16.61951896  2.07028146  4.44353996  1.46790083  1.27039006
  1.01583996  6.05787879  1.88551045  4.531492    3.8770403   1.96840046
  0.89057072  1.13502138  1.23094192  0.48091113  0.86993172  1.34509308
  7.32769486  4.2194277  14.98990218  3.75120359  3.50006056  1.25855525
  1.57472382  0.83561221  2.44412495  1.38164009  1.47003497  1.44986592
  2.65669866  3.33212285  1.95482851 24.04506614  7.08785554 31.06323104
  8.5944492  10.82990862  8.7124128  20.34644405  2.76031438  2.14546646
  4.29132773  2.34648543  1.55966567  1.91764377  1.63098078  6.57533373
  2.29838578  5.39910769 54.48531575  9.48923999  9.11733493  4.74041992
 43.26466452  2.71551801 10.60692441  8.04207936  4.58294385  5.75856315
  4.83758341  7.75523635  2.62984268  5.11838386  0.40002056  6.68262162
  5.03791111 12.30576264  4.92183855  8.4107654   1.41413934  2.83839309
  0.23075253  0.98323518]
Accuracy th:0.5 is [45.47093724 97.2137279  73.02055287 97.02489005 97.26733349 77.9437385
 78.38354796 77.13234488 79.24367393 96.40964413 79.54459619 98.52097319
 99.41399349 80.40837709 79.07920225 96.56680596 96.29512311 78.79777293
 98.65376884 98.30776915 80.57406708 80.09649005 98.38695922 78.96589954
 80.6459473  96.65086926 94.0778012  78.59918861 98.01293844 79.45687796
 97.30875598 98.57457877 96.36213009 98.02024829 86.20996333 79.07798394
 78.84528697 89.96722749 97.11504489 76.03708532 79.58114545 92.05906361
 78.26780863 77.90962586 96.9627563  93.87434364 98.02877645 98.57336046
 87.14684275 87.41730729 86.74358256 98.55508583 98.99976852 78.49075913
 98.70615611 78.88549116 73.34949623 93.12021052 96.24273583 96.9067141
 89.79300934 97.17717864 90.64460716 78.86477991 98.42838172 79.30215275
 98.20786784 77.92302725 79.95394793 97.55485435 80.54604598 95.99054592
 79.60794825 95.45083515 78.17765378 82.50508644 85.67268917 79.47758921
 80.60574311 99.14718388]
Accuracy th:0.7 is [45.48312033 97.2137279  73.02055287 97.02489005 97.26733349 77.9437385
 78.38354796 77.24321097 79.24367393 96.47055957 79.54459619 98.52097319
 99.41399349 80.84574993 79.07920225 96.56680596 96.29512311 78.79777293
 98.65376884 98.30776915 80.99803852 80.09649005 98.38695922 79.00732204
 81.12596094 96.65086926 94.0778012  78.59918861 98.01293844 79.45687796
 97.30875598 98.57457877 96.36213009 98.02024829 86.44631523 79.07798394
 78.84528697 90.24012865 97.11504489 76.03708532 80.25974342 92.05906361
 78.26780863 77.90962586 96.9627563  93.87434364 98.02877645 98.57336046
 89.11684799 88.2128629  86.90683593 98.55508583 98.99976852 78.49075913
 98.70615611 78.88549116 73.34949623 93.53199888 96.24273583 96.9067141
 89.79300934 97.17717864 90.86146611 78.86477991 98.42838172 79.30215275
 98.20786784 77.92302725 79.95394793 97.55972759 80.54604598 95.99054592
 79.61647641 95.45083515 78.17765378 82.57087511 85.77259049 79.47758921
 80.60574311 99.14718388]
Avg Prec: is [55.52378276  3.14090574 11.23017408  3.46826948  2.27118905  3.87411034
  3.29104699  5.59402204  2.41822156  3.8830105   1.59787126  1.65306397
  0.67954346  4.99046377  2.64897768  3.10710664  3.66879371  2.66297244
  1.3224313   1.72700649  1.89826461  0.87686668  1.77652318  2.46088376
  5.1594106   3.62870683  6.42842004  3.3008207   2.14255183  1.97278822
  2.60189689  1.3730499   3.71899138  1.63841633  2.22072432  2.33756833
  2.98880461  2.56071215  2.81367157  7.39109714  2.38257966  8.2920572
  3.45239207  4.06847157  3.25606988  6.56568107  2.10705778  1.52048551
  2.1117352   1.719671    1.91173925  1.60194362  1.08940618  2.98586472
  1.30298717  2.63496691 11.21748112  3.81350708  4.03231868  2.85619813
 10.85589272  2.14127395  3.81310479  3.03554928  1.58055564  2.48629364
  1.81377668  4.10016172  1.27738595  2.40111843  0.19364048  3.41059897
  1.96627972  4.69843302  3.94500007  3.16901945  0.81879357  1.93257653
  0.12754308  0.75255105]
mAP score regular 7.92, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [84.7771383  97.22450607 91.57136806 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.33624835
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.54239231 96.39235618 96.16314124 96.78102499
 90.36549817 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [84.42584149 97.22450607 90.66696564 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.37362035
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.09641976 96.39235618 96.16314124 96.78102499
 90.13379176 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [94.71769184  4.28187427 57.58826221  6.84620555  1.51102551 12.7163872
  3.54731996 20.73824337  2.18771269  4.65252483  1.60311981  1.26698356
  1.00495616  6.33631684  1.92338973  5.06188645  3.91439448  2.01370146
  0.83778636  1.1758385   1.13251845  0.50073088  0.89644426  1.31097269
  7.86732136  4.69643207 15.44064565  3.41586681  3.90130305  1.29170494
  1.37517476  0.75638922  2.37754783  1.28892463  1.26329835  1.2728284
  2.42849651  4.19060302  1.98285132 24.12388793  7.19733116 31.8070132
  8.7559234  10.70763429  8.41294379 20.56620515  2.80327949  1.95965048
  4.51882782  2.1722725   1.57705688  2.23087679  1.9774569   7.90562933
  2.47201493  5.31729426 51.8785184   8.72749292  8.45735091  5.05901183
 46.60038149  2.45082201 10.78077825  8.81443759  4.95864724  5.68948188
  5.07285492  8.73805777  2.37738308  4.90449885  0.44899031  5.83988603
  4.4182223  13.3105954   5.06661434  7.95936709  1.38600338  3.23587726
  0.19206416  0.86052653]
Accuracy th:0.5 is [45.36711762 97.22450607 71.56738172 96.96290206 97.90716795 77.14328425
 77.26038319 76.03209009 78.64813015 96.41477938 78.89229389 98.5325261
 99.34972718 78.57338615 78.73034856 96.31262924 96.21047911 78.22707228
 98.78167277 98.34068316 79.51266911 79.53260084 98.31327703 78.29434188
 78.70792536 96.52938685 94.3393876  78.23952961 97.81747515 78.83997309
 97.52597354 98.67204823 96.39983058 98.18870369 86.60338341 78.46127015
 78.38403468 91.53648753 97.0276802  75.38929168 78.44632135 92.37362035
 77.56932506 77.21802825 97.03764606 94.02795426 98.18621222 98.77668984
 87.6149189  87.25614769 85.25549991 98.55993223 98.87385704 77.6042056
 98.6969629  78.25198694 71.96601639 94.11266413 96.16314124 96.78102499
 90.13379176 97.04761193 90.44024217 78.16478561 98.32075143 79.01437576
 98.13139996 77.34011012 79.53010938 97.53095647 80.01345392 96.07843137
 79.2211675  95.44559882 77.27782345 83.39437427 87.3682637  78.88731096
 80.08321499 99.15040985]
Accuracy th:0.7 is [45.63370456 97.22450607 71.56738172 96.96290206 97.90716795 77.14328425
 77.26038319 76.05949623 78.64813015 96.41976231 78.89229389 98.5325261
 99.34972718 78.99195256 78.73034856 96.31262924 96.21047911 78.22707228
 98.78167277 98.34068316 79.81662805 79.53260084 98.31327703 78.29434188
 79.05922216 96.52938685 94.3393876  78.23952961 97.81747515 78.83997309
 97.52597354 98.67204823 96.39983058 98.18870369 86.92229115 78.46127015
 78.38403468 91.74577074 97.0276802  75.38929168 78.81754989 92.37362035
 77.56932506 77.21802825 97.03764606 94.02795426 98.18621222 98.77668984
 89.60310935 87.64730797 85.40498792 98.55993223 98.87385704 77.6042056
 98.6969629  78.25198694 71.96601639 94.39669133 96.16314124 96.78102499
 90.13379176 97.04761193 90.61215337 78.16478561 98.32075143 79.01437576
 98.13139996 77.34011012 79.53010938 97.53593941 80.01345392 96.07843137
 79.2211675  95.44559882 77.27782345 83.47659267 87.4729053  78.88731096
 80.08321499 99.15040985]
Avg Prec: is [53.55085585  3.69879675 14.8350017   4.54257829  1.47622447  4.35089165
 14.52165962  8.67530024  8.59408535  5.32092898  2.8387491   5.07476738
  2.35698178  5.8130596   2.97136526  3.6560248  19.99466013  6.48237799
  1.55320662  2.76617852  3.48454864  1.55565142  1.23358183  5.10656702
  5.57014569  8.1508751   7.7952005   4.58991197  3.83516     4.85830029
  2.15434923  0.84486866  2.87890235  1.14017364  1.66256756  2.14896215
  1.95265269  2.19097016  2.19471895  6.19568627  1.70473522  6.0296853
  2.15311738  2.68227215  2.3330781   4.84519225  1.65962746  1.01667343
  1.42936873  1.15309682  1.19399354  0.98407896  0.73908699  2.24888441
  0.86458035  1.84559484  9.99713752  2.99714924  3.8060495   2.85935915
  7.79613381  2.0906957   3.31039747  2.51280223  1.35187481  1.93427438
  1.52195601  3.53921753  1.08547148  2.25405078  0.19419672  3.3051429
  1.62786098  3.97724096  3.17573916  2.30202741  0.56547006  1.47883939
  0.12020758  0.6065301 ]
mAP score regular 8.16, mAP score EMA 4.25
Train_data_mAP: current_mAP = 7.92, highest_mAP = 7.92
Val_data_mAP: current_mAP = 8.16, highest_mAP = 8.16
tensor([0.4137, 0.4210, 0.5117, 0.5226, 0.4386, 0.5151, 0.4780, 0.4965, 0.4533,
        0.5228, 0.5063, 0.5008, 0.5052, 0.4683, 0.4877, 0.4183, 0.4385, 0.4524,
        0.5187, 0.4892, 0.4728, 0.4488, 0.4355, 0.4881, 0.4644, 0.4605, 0.4706,
        0.4741, 0.4553, 0.5036, 0.4715, 0.4614, 0.4662, 0.5094, 0.3950, 0.4979,
        0.4916, 0.4974, 0.4697, 0.4527, 0.4997, 0.5338, 0.4654, 0.4643, 0.5051,
        0.4585, 0.4991, 0.5191, 0.5083, 0.5039, 0.4468, 0.5409, 0.5172, 0.4855,
        0.5022, 0.4788, 0.6700, 0.5208, 0.3923, 0.4619, 0.5398, 0.5183, 0.4296,
        0.5614, 0.4765, 0.5078, 0.5015, 0.5248, 0.4357, 0.4861, 0.4924, 0.5066,
        0.5455, 0.4872, 0.5601, 0.6242, 0.4574, 0.5469, 0.4896, 0.5199],
       device='cuda:0')
Max Train Loss:  tensor([19.0960,  9.4046, 11.6242,  6.6786, 14.7660,  9.0407,  8.7041, 17.2360,
         8.7792, 15.0493, 10.9300, 14.2733, 13.6844, 15.1924, 14.9354, 13.5013,
        10.5767, 12.1075,  6.5771, 12.4028, 11.7166, 10.4456,  7.2702,  8.1386,
        12.3263, 13.2735, 19.9008, 11.5011,  8.8915, 11.9234, 13.0883,  8.3472,
        15.0272,  6.2771, 10.1509, 15.6054,  9.2970,  9.4868, 12.6994, 18.3048,
        12.4437, 15.9812, 10.8234, 12.9749, 13.5848, 11.8884, 11.8454, 13.2815,
        10.4089, 12.0831,  8.8071, 11.7546, 16.1418, 13.4359, 11.2684, 10.4959,
        18.5064, 14.3160,  8.2891, 11.8183, 19.7337, 14.1440, 11.9337, 12.2079,
        10.6888, 13.3991,  8.7035, 14.2351,  7.8991, 12.2660,  9.8283, 14.0055,
        10.9408, 14.4205, 16.3147,  9.6156,  7.8764, 10.6368,  9.9091,  6.3734],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [9/80], Step [000/642], LR 1.0e-04, Loss: 19.9
Max Train Loss:  tensor([23.7272,  9.5292, 23.4734, 18.4085, 13.4539, 19.6710, 17.6049, 22.6213,
         9.5825, 15.8007, 12.8902, 21.9505,  9.3026, 19.7756, 17.7802,  9.1158,
         9.6356,  9.7301, 20.7555, 10.2053, 14.9971,  9.0610,  9.3440, 16.9621,
        16.2156, 11.4374, 10.6771, 10.3566,  9.1443, 22.1259, 14.1667, 12.7344,
        15.3425, 22.2823, 18.0175, 12.5018, 21.5092, 12.0398, 13.5418, 15.0478,
        12.0293, 20.4344, 14.1314, 15.6046, 13.3694, 15.0688, 10.8808, 15.3131,
        15.0323, 19.9367,  9.3479, 17.7374, 10.8980, 16.4448, 10.8732, 13.2155,
        23.3317, 15.1567,  8.4989,  9.4748, 23.3879, 12.9210, 10.5334, 20.8642,
        10.6351, 17.0313, 20.9942, 19.7772,  8.5351, 15.4725, 10.3494, 16.6950,
        13.9801, 12.2088, 21.0531, 16.7811, 10.6182, 17.4899, 10.3680, 19.8893],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [9/80], Step [100/642], LR 1.0e-04, Loss: 23.7
Max Train Loss:  tensor([19.3604,  9.7118, 18.4258, 14.8262, 11.1735, 18.0738, 14.7843, 19.9652,
        10.7432, 15.2382, 11.2784, 10.8176,  9.9392, 14.8577, 16.6332,  6.2345,
        12.2894, 11.4243,  9.9071, 15.1789, 13.0548,  7.1592,  8.4224, 10.0406,
        13.7229, 11.7467, 11.1501, 12.7579, 10.9298, 11.5755, 13.2312, 14.3983,
        12.9321, 10.4762, 16.9113, 13.3378, 14.1111, 14.1831, 12.3712, 11.0794,
        12.2724, 13.8835, 11.0846, 13.0990, 13.4165, 14.3462, 13.0620, 15.9588,
        14.5179, 17.5828,  9.9326, 15.5698, 11.5503, 12.4003, 10.9203,  9.6216,
        20.6458, 12.1418,  6.9202,  8.7037, 16.0403, 17.2441,  9.9727,  8.8304,
         9.6750, 14.8243, 10.2828, 12.8950,  7.5940, 10.7767,  9.9767, 14.7483,
        12.1086, 12.8940, 18.2417, 15.0498,  8.0288, 15.1197, 10.6340, 10.5820],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [9/80], Step [200/642], LR 1.0e-04, Loss: 20.6
Max Train Loss:  tensor([20.6007,  8.6537, 17.7935, 14.9723, 13.1661, 15.9994, 18.1972, 13.0293,
        11.4282, 15.4508, 12.0070, 10.7891, 10.5691, 14.7787, 14.1481,  7.5773,
        12.4356, 10.3287, 13.9337, 12.4346, 14.8068,  8.6214,  7.5775,  8.3454,
        13.2010,  9.7763, 18.2643,  9.8427,  9.7543, 11.1030, 12.9147,  8.7183,
        14.1776, 10.2249, 17.5206, 14.0081, 11.9410, 10.4928, 11.0883, 15.4280,
        14.5392, 21.6421, 11.2380, 11.8579, 16.3439, 15.7227, 15.2117, 14.7840,
        14.9728,  8.5191,  8.5038, 17.7523, 10.4282, 11.2088, 11.8103, 13.2927,
        22.4461, 11.9810,  7.8051, 11.1091, 16.0480, 12.1041,  7.1263, 10.6234,
         9.4818, 13.1324, 10.0824, 12.8141,  8.9440, 14.5401,  9.7755, 14.9184,
        12.0254, 12.1255, 11.9503, 11.8641,  7.8449, 12.8118, 10.4868, 11.2460],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [9/80], Step [300/642], LR 1.0e-04, Loss: 22.4
Max Train Loss:  tensor([19.2427,  9.4359, 12.5401, 11.6143,  7.6204, 10.3868,  8.1843, 15.1448,
        10.5410, 12.8723, 12.4422,  8.9416,  9.0671, 14.7327, 14.9485, 12.6607,
        10.7984, 12.3921, 15.2823,  9.8518, 13.0246,  7.1822, 10.7451, 11.2887,
        14.1774, 12.9168, 10.5391, 12.7662,  8.5777, 15.9669, 15.0137,  8.9338,
        15.9573, 12.8023, 18.1748, 12.4649, 15.1949, 10.8781, 11.4631, 18.4214,
        14.0345, 16.6195, 11.1329, 12.4100, 14.2359, 14.3018, 11.5122, 14.3967,
        13.3329,  7.9056,  9.1891, 16.6582,  9.2943, 12.4926, 11.5226, 10.1752,
        15.4644, 14.1070,  6.8344, 11.4784, 17.8890, 10.1766, 10.8848, 13.3501,
        10.5090, 12.8154, 12.0625, 11.2906,  8.1898, 11.6287, 10.7469, 15.7356,
        11.3181, 12.0063, 17.7163, 12.9358,  8.8798, 11.2123, 10.0378, 11.5801],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [9/80], Step [400/642], LR 1.0e-04, Loss: 19.2
Max Train Loss:  tensor([21.4835,  6.7541, 18.2258, 14.2818, 10.9935, 12.0843, 11.0675, 16.7390,
        13.8777, 12.6217, 10.3229,  8.6886,  9.6533, 10.8607, 10.9517, 10.2487,
        10.0417, 10.0776,  8.9075, 13.6995, 13.2500, 12.6762, 10.3533,  9.6482,
        13.7878, 11.5576, 12.1233, 12.3467, 12.2799, 11.7718, 11.1578, 11.8159,
        10.3746, 14.3226,  6.5979, 13.5594, 10.7370, 12.4833,  9.0043, 11.8874,
        10.0209, 16.0221,  9.0300, 13.2341, 12.1737, 15.7002, 11.3513, 15.0209,
        14.6048,  7.9525,  9.5389,  5.0017, 10.2015, 13.3638, 10.2630, 11.3254,
        19.9585,  8.8919,  7.6126,  7.7353, 13.3257, 12.7923,  7.1245, 10.3574,
        10.5089, 13.6632,  9.9735, 12.3776,  8.2767, 12.4116,  9.6642,  7.9866,
        11.2467, 10.7477, 10.7010, 12.5438,  9.4443, 15.0232, 11.3269, 10.2649],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [9/80], Step [500/642], LR 1.0e-04, Loss: 21.5
Max Train Loss:  tensor([21.0506, 10.1888, 13.8075, 15.0359, 12.0863, 12.8124, 10.0290, 15.4898,
         8.9488,  8.8677, 12.2311,  7.4837,  8.9854, 14.5848, 13.5614,  9.4700,
        15.6234,  6.8112, 10.9904, 10.6342, 12.6025,  7.1013, 11.3974, 14.5913,
         8.2329, 11.0978, 13.4802, 12.3200, 10.5652, 12.0596,  8.3452, 11.2204,
        18.0153, 10.9201, 13.7417, 15.6263, 17.9048, 10.9987, 10.5242, 11.2610,
        10.5907, 17.8486, 12.3714, 11.3818, 10.7350, 10.6051, 10.7860, 15.7573,
        15.7501,  6.0713,  7.7028,  3.0705,  8.5003, 12.0755, 11.5872, 10.3739,
        19.1442, 12.5387,  8.7760, 12.6742, 16.2619, 13.5239,  8.5091, 17.3755,
        11.2130, 14.5322, 12.4709, 15.9429,  6.6663, 10.5417,  9.9071, 10.1984,
        12.5359, 13.6286, 15.8765, 10.7895,  9.1357, 11.8836, 11.1246, 10.5141],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [9/80], Step [600/642], LR 1.0e-04, Loss: 21.1
Max_Val Meta Model:  tensor([ 23.4387,  27.3253,  40.4192,  24.8702,   6.0198,   9.5549,   8.2250,
         13.2137,   8.5025,   9.5345,  11.1848,   9.3791,   9.5402,  13.1868,
         10.5065,   6.7203, 129.8381,   6.5502,   7.8519,   9.4750,   9.6009,
          6.8351,   6.1435,   6.7529,  18.2091,  12.1954,  13.6749,   8.9743,
          9.6629,  10.8702,   8.0788,   8.5732,   7.4562,   9.1660,   3.4709,
          9.4928,  10.0467,   9.4626,   6.2615,  24.0801,  11.9084,  15.4472,
          9.7076,  13.0903,   8.9252,  11.9793,   9.4479,  14.6729,  11.1940,
          5.7375,   7.4391,   2.8696,   9.4625,   9.4594,   9.4980,   9.2827,
         17.5714,   8.5025,   9.4998,   6.8788,   8.9050,  18.0424,   6.9637,
          8.5318,   9.3381,  10.9650,   9.9432,   6.4982,   8.8698,  15.3146,
          9.6260,  12.0376,  15.2565,  11.3891,  10.8640,  11.2124,   7.7040,
         11.5250,   9.6574,  10.2398], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 22.9095,  26.2110,  37.3104,  25.7908,   6.7819,  10.5349,   8.6885,
         13.5274,   9.2409,  10.0608,  12.0825,  10.0756,  10.3875,  13.5507,
         11.6523,   7.5169, 122.7796,   7.3534,   8.7665,  10.4605,  10.5752,
          7.6638,   6.9123,   7.5363,  18.1489,  12.8034,  14.1707,   9.9121,
         10.5200,  11.6322,   8.6623,   9.4965,   8.1725,  10.1866,   4.0038,
         10.4800,  11.0252,  10.3570,   7.0134,  23.8734,  12.8124,  16.3344,
         10.6876,  14.0479,   9.4940,  12.7430,  10.4384,  15.7810,  12.2181,
          6.3955,   8.2929,   3.4013,  10.1993,  10.4427,  10.4893,  10.2380,
         18.7414,   9.1399,   9.9550,   7.7132,   9.7536,  18.9228,   7.7324,
          9.5413,  10.2913,  11.9938,  10.9293,   7.5073,   9.5914,  16.1577,
         10.6144,  12.3737,  16.2377,  12.1661,  11.6514,  11.8288,   8.5727,
         12.6407,  10.6554,  11.2537], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 55.3785,  62.2552,  72.9189,  49.3539,  15.4642,  20.4517,  18.1754,
         27.2430,  20.3860,  19.2451,  23.8652,  20.1171,  20.5630,  28.9364,
         23.8926,  17.9713, 280.0150,  16.2537,  16.9019,  21.3841,  22.3662,
         17.0757,  15.8722,  15.4390,  39.0774,  27.8040,  30.1151,  20.9087,
         23.1036,  23.0967,  18.3716,  20.5807,  17.5289,  19.9966,  10.1355,
         21.0470,  22.4283,  20.8238,  14.9310,  52.7333,  25.6423,  30.5978,
         22.9657,  30.2589,  18.7977,  27.7917,  20.9129,  30.4001,  24.0380,
         12.6928,  18.5591,   6.2887,  19.7201,  21.5079,  20.8860,  21.3812,
         27.9740,  17.5494,  25.3755,  16.6984,  18.0673,  36.5072,  17.9972,
         16.9942,  21.5986,  23.6204,  21.7926,  14.3048,  22.0118,  33.2370,
         21.5546,  24.4240,  29.7683,  24.9702,  20.8023,  18.9505,  18.7421,
         23.1154,  21.7654,  21.6458], device='cuda:0')
Outer loop valEpocw Maximum [9/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 129.8
model_train val_loss valEpocw [9/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 122.8
model_train val_loss  valEpocw [9/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 280.0
Max_Val Meta Model:  tensor([44.2438, 35.4997, 45.1782, 46.0089, 39.8790, 50.1646, 44.0700, 43.2068,
        37.7430, 43.5964, 42.8900, 50.4259, 42.5468, 39.4824, 47.1471, 39.6889,
        33.3801, 38.2134, 41.2198, 45.9100, 40.4480, 37.5300, 35.9835, 37.5127,
        38.7506, 38.6273, 38.4439, 39.3083, 39.5649, 41.8118, 39.5076, 39.6449,
        39.7177, 41.4396, 32.5095, 40.7577, 40.8478, 40.2213, 39.4795, 41.4959,
        41.9301, 43.2285, 38.2928, 38.8890, 40.6751, 37.7938, 40.8075, 42.9704,
        42.9542, 40.9731, 37.6993, 41.8942, 42.9403, 40.6952, 40.9446, 40.0484,
        39.6958, 43.1269, 32.3021, 38.1714, 43.5302, 41.7650, 35.4321, 45.8867,
        38.9635, 42.2363, 42.8592, 40.1099, 36.1006, 40.6682, 41.8396, 39.0721,
        45.2476, 40.9311, 45.8561, 41.9041, 37.2352, 45.0780, 41.3303, 43.2052],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([27.8047,  8.5125, 27.4184, 11.6389,  6.1484, 19.5046, 45.8845, 26.8133,
        17.3741, 17.5046, 13.2949, 79.6831, 10.1986, 10.4627, 12.6569,  6.7599,
         7.2776, 10.0185,  7.4189, 15.5087,  9.4288,  6.5763,  5.9367,  7.1007,
        13.9805, 12.0949, 16.4522, 11.0898, 10.7193,  8.2355,  7.8532,  8.4419,
         6.1663,  8.8278,  3.3012,  8.9851, 10.6612,  7.5210,  5.8372,  4.5341,
         9.3280,  6.1408,  8.0258,  8.8342,  3.8582,  6.1795,  9.0430, 13.6019,
        10.9281,  5.4652,  7.2671,  2.5004,  7.9414,  9.2157,  9.1305,  9.8357,
         6.6143,  6.3497,  6.1599,  6.6619,  3.3286,  9.1706,  5.9191,  7.0471,
         8.9980, 10.4296, 10.1062,  8.5733,  6.1530, 10.0069,  9.5738,  3.7005,
        10.7467,  9.9398,  9.3433,  6.3968,  7.3590, 10.3041,  9.3818,  9.8319],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 64.7743,  20.5590,  54.1750,  21.7554,  13.1731,  35.4593,  91.6690,
         50.8876,  38.3837,  33.3215,  26.2290, 158.2853,  20.2135,  22.2204,
         25.0208,  16.0622,  17.8000,  21.4370,  14.5504,  29.8019,  19.7001,
         14.6463,  13.5422,  15.3269,  30.0891,  26.5090,  35.1673,  23.3102,
         22.8475,  16.3080,  16.6845,  17.9706,  12.9277,  17.5118,   8.3020,
         18.3967,  21.9276,  15.5340,  12.2513,  10.1577,  18.3524,  11.6479,
         17.3991,  19.1903,   7.6181,  13.3217,  18.3304,  26.4552,  21.3036,
         10.7662,  16.0565,   4.8490,  15.2681,  18.8935,  18.3190,  20.2813,
         10.2266,  12.2514,  16.1976,  14.3220,   6.1253,  18.3141,  14.0554,
         12.7437,  19.0022,  20.6335,  19.2381,  17.2561,  14.1352,  20.7827,
         18.9495,   7.6955,  20.0290,  20.4848,  16.9130,  11.0559,  16.2568,
         19.1819,  19.1152,  19.1640], device='cuda:0')
Outer loop valEpocw Maximum [9/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 50.4
model_train val_loss valEpocw [9/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 79.7
model_train val_loss  valEpocw [9/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 158.3
Max_Val Meta Model:  tensor([36.9839, 35.6409, 39.7305, 40.3934, 38.2327, 38.8648, 38.8558, 37.4246,
        41.3914, 41.8161, 41.2167, 37.0506, 40.6673, 38.2496, 40.9036, 39.9970,
        32.0873, 32.1975, 39.7462, 39.6264, 40.9737, 35.8686, 43.4440, 37.3749,
        36.6530, 36.6090, 35.2851, 38.5580, 41.3903, 41.7417, 38.3839, 38.0575,
        39.5211, 39.5322, 30.7534, 38.1350, 39.3085, 38.9594, 37.4050, 41.2630,
        40.9529, 46.0360, 39.2946, 38.9930, 40.0964, 38.5670, 42.2855, 41.4988,
        45.1347, 43.2001, 36.3846, 40.3703, 45.4883, 39.0838, 40.4489, 41.5626,
        65.3876, 39.6282, 31.3069, 36.2091, 55.2546, 40.1883, 33.6385, 40.7672,
        37.6154, 40.3368, 41.4755, 38.1255, 34.6661, 38.9285, 39.9345, 36.8150,
        39.9086, 39.6607, 41.1711, 38.9286, 35.9414, 37.4194, 39.5603, 41.5314],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([11.4744,  8.5446,  5.3122, 14.2032,  9.7805,  8.3319,  8.1281, 12.3241,
        11.8136,  7.8200, 14.6891, 10.6108, 12.3919, 15.3890, 13.6762,  9.2137,
         9.3078,  9.7450, 11.6869, 13.7283, 14.2951, 10.3984, 10.1781,  9.6832,
         8.8034, 12.4393, 12.4889, 14.5194, 13.4267, 12.9465, 11.0556, 13.3639,
        11.6441, 13.0967,  5.9101, 13.3699, 16.8151, 13.1000, 10.0428, 16.6701,
        19.4186, 36.6933, 33.6009, 32.5550, 28.0053, 35.4092, 15.1283, 20.2652,
        24.9709, 10.7023, 18.6168, 24.2267, 29.8859, 17.0178, 29.3416, 41.6729,
        69.8388, 12.9904,  8.9853, 10.4596, 60.9783, 13.5409, 11.4155, 14.4043,
        15.2681, 15.3760, 16.0540, 10.4202, 10.4921, 13.6680, 14.0066,  9.0020,
        14.8968, 20.5494,  9.7866, 15.7067, 12.5597, 14.2925, 13.8017, 14.1949],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 26.9184,  21.0091,  10.8140,  27.5899,  21.1791,  16.2478,  16.8672,
         24.3604,  24.7411,  14.8011,  28.8195,  21.6443,  24.5100,  32.5306,
         27.7966,  21.7943,  22.6823,  22.0559,  22.8688,  27.7330,  28.8062,
         23.0864,  21.6703,  20.2576,  18.8580,  27.4676,  27.0159,  30.2526,
         26.3410,  24.7157,  23.2899,  28.4608,  24.1120,  25.9947,  14.7790,
         27.3438,  34.7070,  26.9606,  21.1387,  37.7079,  37.9914,  70.1849,
         72.4102,  70.7073,  55.2450,  75.7766,  29.8269,  39.5785,  47.1198,
         19.2809,  41.2266,  46.4159,  57.6615,  35.1238,  59.4842,  85.9156,
        109.0452,  25.0039,  23.9831,  22.5803, 109.8219,  27.1499,  27.4683,
         26.9146,  32.1665,  30.6321,  30.5236,  20.8234,  23.9279,  28.4172,
         27.8626,  18.7934,  28.5126,  42.5403,  17.6845,  27.2797,  27.6029,
         28.1633,  28.1785,  27.6052], device='cuda:0')
Outer loop valEpocw Maximum [9/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 65.4
model_train val_loss valEpocw [9/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 69.8
model_train val_loss  valEpocw [9/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 109.8
Max_Val Meta Model:  tensor([41.0386, 35.1891, 38.8397, 40.5576, 38.9598, 40.1176, 39.8412, 38.3354,
        41.5465, 42.4873, 42.2238, 38.1356, 40.9456, 38.9570, 41.7424, 36.4655,
        33.0567, 33.0193, 40.2717, 40.3977, 40.9953, 36.6939, 38.0558, 38.1197,
        37.7994, 37.4714, 37.1390, 38.8473, 42.1904, 42.7123, 36.9122, 36.9506,
        37.0811, 44.5200, 31.5166, 38.2684, 38.9356, 43.3563, 38.4545, 35.3289,
        41.2499, 37.2469, 37.2478, 38.2530, 39.5852, 36.8051, 40.8726, 41.8420,
        35.6312, 36.4311, 36.6849, 42.6183, 35.1647, 39.9468, 41.0586, 39.3954,
        39.4877, 40.2588, 31.3427, 42.6440, 42.1963, 41.2339, 34.0944, 46.5977,
        37.9717, 41.0415, 42.0753, 45.8057, 35.4460, 39.7554, 40.6762, 37.8390,
        41.3458, 39.9736, 57.5326, 42.5324, 36.2490, 37.8343, 40.4294, 40.5248],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 19.4749,   5.2461,  15.2313,   9.5956,   5.9982,   9.8686,   6.3377,
         11.4562,   7.5245,  12.5916,  10.6499,   7.8126,   8.2571,  11.8741,
         11.4890,   5.5470,   5.7662,   6.0080,   7.2200,   9.1525,   9.4536,
          6.4878,   5.9586,   6.3313,  12.3741,   8.2676,  13.6058,   8.6427,
         11.6002,   8.3749,   7.2145,   8.2706,   5.3495,   8.9865,   3.1510,
          8.7119,   9.2732,   7.5907,   5.5031,   5.4130,   9.2560,   7.8924,
          7.8759,   8.7637,   4.1592,   6.1901,   8.9646,  13.3422,  10.0819,
          4.3303,   7.1342,   2.5836,   7.2138,   8.9546,   9.0731,   9.0550,
          4.7410,   5.4451,   7.2332,   6.7980,   5.8766,   9.0290,   5.7357,
          7.2392,   8.8443,  10.3537,   9.9969,   6.0320,   6.0967,   9.1830,
          9.3869,   4.0199,   9.2963,  10.1111, 159.5059,   8.4698,   7.2276,
         10.0519,   9.2530,   9.5812], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 45.6787,  12.8733,  31.4523,  18.7303,  12.9530,  19.2891,  13.0983,
         22.7524,  15.9111,  24.1346,  20.8459,  15.9404,  16.5449,  25.2364,
         23.3268,  13.4121,  14.0526,  13.6913,  14.2714,  18.5194,  19.4492,
         14.4143,  13.3181,  13.3407,  26.7143,  18.2089,  29.3721,  18.0960,
         22.8549,  16.0475,  15.5985,  17.7264,  11.5096,  17.2391,   7.9181,
         18.1331,  19.4760,  15.0596,  11.5482,  12.6165,  18.1118,  15.6211,
         17.1611,  18.9426,   8.1788,  13.3374,  18.0889,  26.2053,  21.1769,
          8.8110,  15.8193,   4.8261,  15.0381,  18.6341,  18.0802,  18.5490,
          7.3539,  10.5841,  19.4210,  14.0939,  10.8000,  18.0446,  13.8252,
         12.7463,  18.7615,  20.6593,  19.0409,  11.4419,  13.9075,  19.0756,
         18.7062,   8.3413,  17.7035,  20.8907, 292.8028,  14.2650,  16.0213,
         20.0806,  18.8624,  18.9850], device='cuda:0')
Outer loop valEpocw Maximum [9/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 57.5
model_train val_loss valEpocw [9/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 159.5
model_train val_loss  valEpocw [9/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 292.8
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [83.43465601 97.2137279  91.01619132 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 91.97743692
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 90.67506487 96.13796128 96.24273583 96.9067141
 90.23038218 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [79.058491   97.2137279  90.2462202  97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.05906361
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 89.76986148 96.13796128 96.24273583 96.9067141
 89.79300934 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [93.83734738  3.33859513 52.95187962  3.57346292  1.93595471 13.70207478
  2.73681613 15.82472527  1.99401807  9.7862578   1.48899216  1.37310417
  0.94352601  5.71649751  1.91558051  4.54189989  3.8003766   1.92190589
  0.84026192  1.14378513  1.19251657  0.47186351  0.86503932  1.33754772
  6.44140686  4.05065491 17.31429105  3.51191288  3.23088478  1.24489302
  1.83922041  0.86911109  3.08239489  1.26359259  1.54245183  1.45103206
  2.47414792  1.64129364  2.18147508 23.29906378  7.95586665 30.52781011
  9.66292988 11.61169091 12.1373141  20.98663662  2.8190753   2.27561208
  4.95356519  2.58289677  1.64306458  2.0329891   1.77573339  6.92894423
  2.49320858  6.00262867 55.1290286  10.9852018   9.1890224   4.48496201
 47.9694443   2.33833356  9.90338661  7.78524694  4.07288761  5.41505377
  4.41774669  6.33373519  2.45189146  5.10021357  0.46372106  7.70278811
  4.30252836 12.11795647  5.12145422 10.6518783   1.47873607  2.80760148
  0.22304283  0.92428156]
Accuracy th:0.5 is [45.41733166 97.2137279  72.8292784  97.02489005 97.26733349 77.78170344
 78.16790731 77.00442246 79.0304699  96.39624274 79.4069273  98.52097319
 99.41399349 80.41203202 79.00001218 96.56680596 96.29512311 78.70639978
 98.65376884 98.30776915 80.53020797 79.94907469 98.38695922 78.79655462
 80.68493318 96.65086926 94.0778012  78.38598458 98.01293844 79.27047672
 97.30875598 98.57457877 96.36213009 98.02024829 86.08569584 78.81848418
 78.64426603 89.80275581 97.11504489 75.87261364 79.51901171 92.05906361
 78.11795665 77.71591477 96.9627563  93.87434364 98.02877645 98.57336046
 87.31375105 87.31375105 86.721653   98.55508583 98.99976852 78.33116068
 98.70615611 78.63573787 73.18989778 93.00081627 96.24273583 96.9067141
 89.79300934 97.17717864 90.69333951 78.68325191 98.42838172 79.10356843
 98.20786784 77.81459778 79.80653257 97.55363604 80.34258842 95.99054592
 79.46296951 95.45083515 78.04485813 82.2772627  85.6203019  79.26925842
 80.4242151  99.14718388]
Accuracy th:0.7 is [45.48921188 97.2137279  72.8292784  97.02489005 97.26733349 77.78170344
 78.16790731 77.13356319 79.0304699  96.4754328  79.4069273  98.52097319
 99.41399349 80.82625699 79.00001218 96.56680596 96.29512311 78.70639978
 98.65376884 98.30776915 80.9663625  79.94907469 98.38695922 78.85016021
 81.17347498 96.65086926 94.0778012  78.38598458 98.01293844 79.27047672
 97.30875598 98.57457877 96.36213009 98.02024829 86.30499141 78.81848418
 78.64426603 90.07565697 97.11504489 75.87261364 80.18908152 92.05906361
 78.11795665 77.71591477 96.9627563  93.87434364 98.02877645 98.57336046
 89.30934077 88.12758129 86.91414578 98.55508583 98.99976852 78.33116068
 98.70615611 78.63573787 73.18989778 93.38092859 96.24273583 96.9067141
 89.79300934 97.17717864 90.87974074 78.68325191 98.42838172 79.10356843
 98.20786784 77.81459778 79.80653257 97.55972759 80.34258842 95.99054592
 79.47758921 95.45083515 78.04485813 82.36741755 85.72873138 79.26925842
 80.4242151  99.14718388]
Avg Prec: is [55.69034219  3.07197182 11.04199529  3.41092346  2.25640922  3.89196832
  3.14874902  5.43382413  2.52213053  3.80752892  1.48132477  1.54289525
  0.57850416  5.03784641  2.6802313   3.20376076  3.68268418  2.75792917
  1.36707309  1.78651725  2.04852713  0.88431894  1.84215392  2.45670732
  5.06732765  3.54658132  6.42584705  3.30110471  1.98958492  1.85734904
  2.6020735   1.31969641  3.65156162  1.61591486  2.39946619  2.43056067
  3.06048942  2.61025936  2.8685925   7.41155759  2.25242635  8.10470078
  3.30195499  3.97646872  3.24901245  6.38788827  2.21525518  1.67716277
  2.15114408  1.68370818  1.86994755  1.61569643  1.12667537  2.92494338
  1.36150468  2.70438053 11.27301319  3.70896814  4.03043418  2.82285383
 10.79430252  2.22835921  3.84749595  2.9124646   1.62594998  2.5447043
  1.85231873  4.1607282   1.24937811  2.4265904   0.22690012  3.47694565
  1.91870146  4.57692616  3.94519907  3.17329116  0.80713093  1.97933891
  0.12052986  0.73489778]
mAP score regular 8.13, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [86.18232554 97.22450607 91.19266512 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.01734061
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.53242644 96.39235618 96.16314124 96.78102499
 91.11542965 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [83.32959613 97.22450607 91.12041259 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.37362035
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 89.87717069 96.39235618 96.16314124 96.78102499
 90.1462491  97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [95.4600287   3.54122442 57.25674557  3.64312321  1.49800672 14.39612811
  2.92335918 18.21891627  2.08764329 10.52603157  1.56803637  1.34142319
  0.97395894  5.77301969  1.94195728  5.02864043  3.76242805  1.86389077
  0.76095043  1.12989326  1.07086006  0.48031554  0.88430228  1.26413378
  6.17115881  4.3091739  17.0491573   3.17286095  3.5394334   1.25660342
  1.63084686  0.7938464   3.07184114  1.18096154  1.33454188  1.2970299
  2.16943903  1.87185186  2.2436863  23.81151279  8.64457805 32.14777112
 10.69910237 12.02496615 13.40357382 21.21939388  2.98587106  2.27166442
  5.62297199  2.71208763  1.73901146  2.69557376  2.2728642   8.50419367
  2.89293021  6.23803024 51.35446264  9.75642611  8.704555    4.68814656
 50.69641024  2.3530796  10.01479988  8.62263233  4.82532113  5.20414047
  5.09405238  7.00167372  2.42189444  5.28976559  0.49471127  7.26211287
  4.08509367 12.79783569  5.48415573 11.25506264  1.4679066   3.10610035
  0.18343918  0.82514283]
Accuracy th:0.5 is [45.38455789 97.22450607 71.46772305 96.96290206 97.90716795 77.04362558
 77.15574159 75.93741436 78.55345442 96.41477938 78.78765229 98.5325261
 99.34972718 78.52106535 78.63567282 96.31262924 96.21047911 78.12243067
 98.78167277 98.34068316 79.43792511 79.42795924 98.31327703 78.19468321
 78.65809602 96.52938685 94.3393876  78.13488801 97.81747515 78.74031442
 97.52597354 98.67204823 96.39983058 98.18870369 86.61334928 78.35662855
 78.29434188 91.49662406 97.0276802  75.29959887 78.36161148 92.37362035
 77.46966639 77.12335252 97.03764606 94.02795426 98.18621222 98.77668984
 87.73949224 87.21628423 85.20317911 98.55993223 98.87385704 77.49956399
 98.6969629  78.15731121 71.89625533 94.09024092 96.16314124 96.78102499
 90.13379176 97.04761193 90.49007151 78.06014401 98.32075143 78.93464883
 98.13139996 77.24045145 79.43045071 97.53095647 79.90881232 96.07843137
 79.12649177 95.44559882 77.17816479 83.34205347 87.3607893  78.78266936
 79.97857339 99.15040985]
Accuracy th:0.7 is [45.6312131  97.22450607 71.46772305 96.96290206 97.90716795 77.04362558
 77.15574159 75.97478636 78.55345442 96.41976231 78.78765229 98.5325261
 99.34972718 78.94710616 78.63567282 96.31262924 96.21047911 78.12243067
 98.78167277 98.34068316 79.73690111 79.42795924 98.31327703 78.19468321
 79.01437576 96.52938685 94.3393876  78.13488801 97.81747515 78.74031442
 97.52597354 98.67204823 96.39983058 98.18870369 86.92478262 78.35662855
 78.29434188 91.72085607 97.0276802  75.29959887 78.75277176 92.37362035
 77.46966639 77.12335252 97.03764606 94.02795426 98.18621222 98.77668984
 89.76754615 87.60993597 85.36014152 98.55993223 98.87385704 77.49956399
 98.6969629  78.15731121 71.89625533 94.3692852  96.16314124 96.78102499
 90.13379176 97.04761193 90.62959364 78.06014401 98.32075143 78.93464883
 98.13139996 77.24045145 79.43045071 97.53593941 79.90881232 96.07843137
 79.12649177 95.44559882 77.17816479 83.414306   87.47041383 78.78266936
 79.97857339 99.15040985]
Avg Prec: is [53.63614396  3.6988633  14.83682271  4.54200332  1.4774407   4.32506035
 14.51057475  8.65418646  8.5497634   5.31102344  2.73982136  4.98475374
  2.35232567  5.82253472  2.96378099  3.66899756 20.98336466  6.49249656
  1.54838861  2.76030589  3.48968862  1.55710889  1.23492466  5.10259614
  5.57806589  8.28043641  7.80712322  4.59006288  3.84996528  5.0565413
  2.16069257  0.84571021  2.88873625  1.14127718  1.62260235  2.15467833
  1.94737996  2.20808985  2.19274881  6.25383869  1.71164924  6.03293154
  2.15660013  2.69217367  2.33383686  4.83994146  1.66428058  1.01551314
  1.41268152  1.1547583   1.18435289  0.98533744  0.74121595  2.2498599
  0.86983089  1.85503333 10.04907015  2.995867    3.79196719  2.84690589
  7.81194221  2.09227291  3.37372108  2.50897142  1.35168156  1.93152138
  1.52337242  3.53398874  1.09202779  2.24490875  0.19422621  3.30831433
  1.65563502  3.98019874  3.18645679  2.30261556  0.55589952  1.47660243
  0.12189978  0.60709642]
mAP score regular 8.39, mAP score EMA 4.27
Train_data_mAP: current_mAP = 8.13, highest_mAP = 8.13
Val_data_mAP: current_mAP = 8.39, highest_mAP = 8.39
tensor([0.4314, 0.3997, 0.4673, 0.5033, 0.4676, 0.5074, 0.4818, 0.4959, 0.4692,
        0.5203, 0.5130, 0.4926, 0.4942, 0.4683, 0.4924, 0.4048, 0.4069, 0.4456,
        0.4952, 0.4920, 0.4790, 0.4490, 0.4480, 0.4684, 0.4622, 0.4571, 0.4584,
        0.4776, 0.5069, 0.5124, 0.4597, 0.4678, 0.4622, 0.5092, 0.3980, 0.4646,
        0.4739, 0.4889, 0.4782, 0.4216, 0.5114, 0.4959, 0.4539, 0.4626, 0.5096,
        0.4663, 0.4906, 0.5023, 0.4714, 0.4930, 0.4563, 0.5176, 0.4791, 0.4733,
        0.4946, 0.4935, 0.6299, 0.5075, 0.3643, 0.4801, 0.5521, 0.4978, 0.4143,
        0.5688, 0.4707, 0.5019, 0.5208, 0.5211, 0.4383, 0.4830, 0.5045, 0.4790,
        0.5248, 0.4854, 0.5630, 0.5795, 0.4468, 0.4841, 0.4926, 0.4970],
       device='cuda:0')
Max Train Loss:  tensor([22.8329,  8.9385, 16.2776, 14.6247, 12.4961,  9.4728,  7.4569, 11.9806,
        11.3949,  8.9913, 13.8006,  8.3217, 11.4824, 13.2302, 12.7940,  9.0791,
        10.0891,  7.9716,  8.6158, 11.5814, 12.2657,  9.4743, 12.6858,  9.3910,
        12.0005,  8.9613, 11.1826, 11.9044, 11.0296, 10.2923, 10.8732,  8.9470,
        10.9788, 10.2189,  8.3769, 12.1125, 14.0314, 10.6907, 12.2414,  9.6583,
        13.4130, 14.9448, 11.1179, 14.2994,  9.7939, 11.6939, 10.5735, 14.9990,
        13.3051, 10.4038,  9.4965,  6.6990,  7.8533, 12.2994, 10.3884, 11.6059,
        13.3632, 18.2526,  9.2243, 12.8554, 16.5934, 16.1587, 11.9205, 12.4936,
        11.8762, 16.3122, 13.0566, 20.4443,  8.1343, 11.8395, 10.8948, 13.7834,
        12.2963, 17.3253, 11.9434, 13.2315,  9.0918, 10.7285,  9.9824, 11.6115],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [10/80], Step [000/642], LR 1.0e-04, Loss: 22.8
Max Train Loss:  tensor([21.2435,  8.0247, 18.1480, 11.8348, 11.9891, 22.0018, 15.2434, 18.1664,
        12.5888, 21.3953, 13.5365, 18.5481, 10.8356,  9.7945, 13.9150, 10.1132,
        11.3935, 10.2412, 11.9032, 18.8857, 13.3381, 10.8543, 11.8054, 13.2054,
        18.5095, 13.0720, 16.8685, 12.9295, 17.3761, 16.8269, 14.0086, 11.4345,
        19.1916,  9.9112,  9.1805, 12.7758, 10.0970, 11.8084, 16.6649, 13.2886,
        18.4169, 18.4863,  7.8666, 11.9392, 16.7928, 11.7572, 12.2505,  8.0809,
        15.9048, 23.0225,  8.5920, 21.3515, 12.1989, 13.3432, 10.7033, 12.8461,
        20.4797, 17.6275,  8.4509, 11.1210, 20.8275, 15.1202,  8.6945, 14.5057,
         8.0665, 11.7218, 12.2779, 15.8771,  7.7876, 10.6528, 15.2465, 17.7949,
        19.9440, 14.1082, 11.3839, 20.1986,  7.6325, 11.2613, 21.2567, 19.3976],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [10/80], Step [100/642], LR 1.0e-04, Loss: 23.0
Max Train Loss:  tensor([19.1368,  6.3704, 15.5166, 13.8694, 15.0126, 14.2123, 14.2449, 20.1308,
        14.0499, 13.2666, 11.3750, 10.6491,  9.9183, 11.0860, 13.7266, 10.4653,
         8.7173, 15.2075, 12.8583, 12.4637, 12.0310,  9.6727, 12.3531, 13.0181,
        15.6383,  8.3375, 12.1147, 11.0796, 12.3509, 15.2120, 15.0126, 11.7100,
        14.8469, 15.0343,  8.0559, 12.1544, 12.5303, 14.3782, 10.9478, 12.2422,
        17.5319, 15.1860,  8.5303, 11.1172, 12.6688, 15.8035, 11.2092,  7.4210,
        14.0458, 12.9770,  8.8949, 10.2298, 10.0796, 12.9148, 11.3903, 10.9997,
        16.2128, 14.9462, 10.4849, 12.9713, 17.5769, 12.8391, 12.5817, 14.3929,
         9.9125, 10.3237, 14.7303, 17.4766,  7.0586, 11.1307, 15.0277, 17.2928,
        10.1106, 14.1908, 10.4318, 15.4158,  8.3497, 12.5656,  9.8078,  6.9087],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [10/80], Step [200/642], LR 1.0e-04, Loss: 20.1
Max Train Loss:  tensor([18.5632,  5.6022, 15.8199, 13.3999, 10.5578, 15.3231, 13.8393, 11.9317,
        10.6645, 12.0478, 10.5243, 13.1995, 10.5446, 15.2931, 10.2715,  8.2720,
        10.0614,  8.6650, 14.4805, 11.0623, 11.4128,  9.2454,  9.2057, 15.0400,
        13.2593, 11.0022, 13.2451, 13.0939, 12.7660, 15.9643, 14.6946, 11.9497,
        13.6280, 12.0764,  6.8897, 11.6086, 12.1438, 10.7953, 15.0150, 16.1615,
         8.9850, 19.1808,  9.4137, 13.5128, 13.5030, 15.2225, 12.0687,  9.5461,
        13.9891, 12.2435,  8.9470, 11.5233, 12.1223, 12.7046, 12.2629, 12.7213,
        18.7543, 13.5242,  7.0700,  9.7925, 15.1357, 13.6320,  9.1580, 12.5389,
         8.8606, 14.0343, 12.7684, 12.2985,  8.5910, 13.0224, 14.9829, 18.8891,
        11.4392, 12.4738, 10.0490, 13.1001,  6.6783, 10.4212,  8.2471,  7.7402],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [10/80], Step [300/642], LR 1.0e-04, Loss: 19.2
Max Train Loss:  tensor([22.8397,  6.8356, 16.2068, 11.2164, 12.8307, 12.8338, 13.1250, 12.6345,
         8.7352, 12.4595, 11.3923, 11.4372,  9.6766, 16.5957,  8.1005, 11.9745,
        11.1545, 13.3689,  9.3826, 11.5284, 10.4977,  9.4533, 15.1224, 11.5501,
        11.8023, 12.1502, 13.2646, 11.2592, 12.2504, 14.9018, 13.2377, 10.7146,
        11.0320, 13.8730,  8.6946, 10.5441, 11.9651, 10.3961, 13.9149, 14.8302,
         6.8723, 17.1839,  8.5418, 11.6365, 11.6271, 11.5848, 10.2227,  7.4503,
        14.5753, 10.9044,  8.8342, 10.3837, 10.7475, 11.6123, 12.6161, 12.1035,
        22.4274, 13.4431,  9.9654, 11.2754, 21.7540, 15.6918,  9.0026, 14.5327,
         8.9274,  9.5169, 13.3806, 12.3189,  7.6505, 12.8617, 14.8679, 14.4928,
        10.2180, 13.7316, 15.5376, 15.0281,  9.1444, 11.5386,  8.0700,  9.7868],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [10/80], Step [400/642], LR 1.0e-04, Loss: 22.8
Max Train Loss:  tensor([15.8295, 10.9053, 13.5786, 10.2045,  9.6283, 13.8283, 11.8447, 12.6132,
         8.5354,  6.8269, 11.7462, 13.6748, 10.4075, 11.8346, 11.1483,  9.9253,
         7.0843, 12.1210, 10.1462, 12.4298, 12.8158,  9.3701,  9.1508,  7.6692,
        15.4659, 11.0086, 16.4472, 15.2207, 13.9543, 12.8624, 14.3071, 12.1210,
        10.8915, 10.2460,  7.5113, 10.3753, 10.3097, 12.5939, 10.5238, 15.3454,
         5.5717, 12.5561,  8.4843, 12.6530, 12.1858, 15.9709, 10.1110,  9.7108,
        15.0190,  9.8373,  8.9043,  9.1951,  9.6126, 10.8488, 10.0856, 15.2553,
        16.3028, 14.0771,  8.7183, 10.3263, 18.0748, 14.6253, 10.6642, 11.1665,
         9.4478, 10.1353, 14.4603,  9.7787,  6.6204, 10.6280, 14.6938, 16.7816,
        10.8805, 13.5859, 10.6144, 16.0577,  9.3119, 11.7380,  7.8608,  6.2704],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [10/80], Step [500/642], LR 1.0e-04, Loss: 18.1
Max Train Loss:  tensor([19.7574,  6.1762, 14.5895, 15.5703, 13.8908, 14.4791, 10.8535, 12.9685,
        12.2861,  7.6548, 10.7054, 11.9528, 10.1450, 10.7732,  7.6993,  7.9799,
        12.1805,  7.5464, 12.9762, 13.5520, 15.8480,  7.6387,  7.8045,  9.0248,
        11.9624, 11.7023, 12.4962, 13.6950, 11.8076,  7.5727,  8.0627, 11.0854,
        10.7851, 14.7154,  6.7866,  8.4648, 14.8184, 12.7938, 10.8547, 13.8091,
         9.5888, 15.0721,  8.3188,  9.7881, 13.2153, 10.6674, 11.1628,  8.5929,
        15.4094,  9.8059,  7.7555,  9.2359, 11.6717,  9.2808,  9.4425, 11.5185,
        17.1652, 15.1457,  6.9201, 15.5310, 13.5734, 10.8854,  7.0763, 11.2452,
         7.0738, 11.9973, 12.4193, 10.1847,  7.5533, 11.0561, 15.2892, 13.9412,
        10.7618, 11.0476, 13.1252, 10.6707,  8.1190, 12.4855,  7.8950,  8.3046],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [10/80], Step [600/642], LR 1.0e-04, Loss: 19.8
Max_Val Meta Model:  tensor([ 26.5384,  24.6669,  40.6919,  23.5923,   7.9361,  12.5145,  11.2658,
         12.0732,   9.2351,   9.5073,  10.8629,  11.7169,  10.0285,  11.2296,
          6.1417,   6.1502, 123.5440,   6.2946,   9.0003,   9.6227,   9.3154,
          7.4444,   6.8010,   5.1869,  16.1641,  10.9176,  14.1273,   8.9453,
         11.9528,   8.8967,   4.1054,   8.9456,   7.9080,   9.1625,   5.5930,
          5.9659,   9.3120,   7.9097,   5.1230,  20.8683,   8.8551,  15.2806,
          8.1849,  12.4729,  13.1458,  12.9581,   9.2827,   7.1810,  13.0561,
          9.7010,   7.5609,   9.0496,   9.9763,   9.0898,   9.2459,   9.1419,
         17.9957,  12.8590,   8.9036,   8.6165,   9.3400,  15.5649,   6.4550,
          7.2549,   6.8708,   7.2577,  11.5494,   7.4109,   9.0317,  15.3760,
         14.5488,  15.3265,  13.4961,   7.6690,   9.7446,  11.6910,   6.1654,
          9.8922,   7.6943,   6.1147], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 23.3334,  23.5718,  36.7442,  24.4729,   9.0109,  13.6438,  12.1887,
         13.0244,  10.2773,  10.2232,  11.9903,  12.7838,  11.1514,  11.9150,
          6.8994,   6.9167, 115.5892,   7.2411,  10.1609,  10.7825,  10.4900,
          8.4836,   7.7897,   5.8847,  16.5088,  11.3285,  15.6512,  10.0813,
         13.0844,   9.3050,   4.8981,  10.0892,   8.8534,  10.3794,   6.4475,
          6.9505,  10.4672,   8.8080,   6.0881,  20.9635,   9.5569,  15.7592,
          9.1049,  13.3514,  13.9954,  13.3766,  10.4578,   8.1154,  14.2480,
         10.8043,   8.6117,  10.1657,  10.8794,  10.2358,  10.4202,  10.3139,
         19.1671,  14.0628,   9.3975,   9.7452,  10.0725,  15.6302,   7.3379,
          8.1900,   7.8879,   8.3149,  12.8285,   8.6440,   9.8949,  16.1716,
         15.8735,  16.1466,  14.3788,   8.7040,  10.3687,  12.7947,   7.1081,
         11.0493,   8.7764,   7.0738], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 54.0866,  58.9664,  78.6333,  48.6269,  19.2703,  26.8877,  25.2977,
         26.2643,  21.9024,  19.6482,  23.3739,  25.9511,  22.5622,  25.4452,
         14.0106,  17.0847, 284.0747,  16.2499,  20.5183,  21.9152,  21.9008,
         18.8953,  17.3868,  12.5646,  35.7173,  24.7862,  34.1464,  21.1079,
         25.8131,  18.1582,  10.6554,  21.5687,  19.1560,  20.3850,  16.1981,
         14.9594,  22.0873,  18.0146,  12.7324,  49.7269,  18.6870,  31.7768,
         20.0593,  28.8643,  27.4631,  28.6879,  21.3183,  16.1559,  30.2246,
         21.9161,  18.8714,  19.6415,  22.7100,  21.6250,  21.0688,  20.9012,
         30.4300,  27.7094,  25.7942,  20.2996,  18.2440,  31.3958,  17.7119,
         14.3986,  16.7571,  16.5683,  24.6304,  16.5865,  22.5736,  33.4797,
         31.4621,  33.7069,  27.4011,  17.9329,  18.4164,  22.0773,  15.9087,
         22.8267,  17.8159,  14.2341], device='cuda:0')
Outer loop valEpocw Maximum [10/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 123.5
model_train val_loss valEpocw [10/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 115.6
model_train val_loss  valEpocw [10/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 284.1
Max_Val Meta Model:  tensor([40.6309, 35.1860, 44.3628, 38.7943, 43.5982, 43.8694, 46.0675, 47.3899,
        39.4261, 41.6262, 42.9301, 47.0481, 41.9661, 44.8478, 42.0164, 39.3914,
        35.1859, 36.5053, 43.6979, 46.6612, 40.7767, 37.5680, 37.1336, 40.5260,
        38.9675, 38.0694, 38.4431, 39.4884, 43.1780, 41.6873, 38.1593, 39.6281,
        38.6687, 41.3079, 33.1136, 38.6814, 39.6660, 39.9141, 39.4803, 33.8727,
        41.9668, 41.0200, 37.5061, 38.9445, 42.1034, 37.8755, 40.5433, 40.6837,
        40.2914, 40.5263, 37.8762, 43.1820, 40.1758, 39.6525, 40.7271, 40.1488,
        43.3335, 43.2115, 29.3514, 39.9910, 43.9067, 38.3192, 34.0027, 45.1528,
        38.0653, 41.3130, 43.3114, 41.0200, 36.0527, 40.7711, 42.6503, 37.0051,
        42.9954, 40.0071, 45.7332, 45.0193, 36.6388, 40.2137, 41.0409, 41.5439],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([29.6146,  8.1779, 26.9389,  9.8806,  7.9196, 19.0727, 40.1828, 23.7429,
        17.4971, 16.5828, 12.6308, 70.8923, 10.7476, 10.2463,  9.0036,  6.0789,
         6.8244,  9.0793,  8.8025, 15.2239,  8.9084,  7.0185,  6.4601,  6.2175,
        13.9157, 10.7949, 15.9211, 10.9903, 12.4137,  5.7316,  3.8603,  8.5017,
         5.9230,  8.5910,  5.2861,  5.3248,  9.7911,  6.4727,  4.8673,  6.7692,
         5.0573,  5.5836,  6.2167,  7.6073,  9.3376,  5.6153,  8.7918,  5.6842,
        12.6275,  9.2312,  7.1172,  8.3615,  8.3916,  8.6568,  8.7879,  9.2867,
         5.7072, 11.0203,  5.2689,  8.2337,  3.4418,  3.7991,  5.3098,  5.5982,
         6.4186,  6.6895, 10.9691,  8.6532,  6.0400,  9.5055, 14.0800,  9.5810,
         8.4217,  5.4377,  7.2536,  7.5293,  5.8183,  8.6043,  7.2479,  5.7945],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 67.9768,  20.2099,  56.7771,  20.2792,  16.0266,  36.9110,  78.7295,
         47.7310,  37.1397,  32.1172,  24.7908, 143.7696,  21.5611,  19.9266,
         18.2749,  14.4967,  16.7391,  20.3004,  17.2341,  28.8587,  18.4154,
         15.6380,  14.2964,  13.0917,  29.9833,  23.8199,  34.7114,  22.9955,
         24.2791,  11.2327,   8.3212,  18.0983,  12.7425,  17.0510,  13.2203,
         11.3371,  20.6919,  13.4219,  10.1474,  16.7295,   9.7570,  11.2201,
         13.6559,  16.4170,  18.1728,  12.0549,  17.9148,  11.3907,  26.3990,
         18.7254,  15.6314,  16.2898,  17.2975,  18.1821,  17.6975,  19.0430,
          9.2211,  21.6011,  14.9334,  16.9711,   6.3191,   7.8720,  13.0058,
          9.9414,  13.7366,  13.3597,  21.0783,  17.1282,  13.8809,  19.6354,
         27.6283,  21.1684,  16.2528,  11.2736,  12.7660,  13.1750,  12.9707,
         17.9210,  14.7425,  11.5646], device='cuda:0')
Outer loop valEpocw Maximum [10/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 47.4
model_train val_loss valEpocw [10/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 70.9
model_train val_loss  valEpocw [10/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 143.8
Max_Val Meta Model:  tensor([36.3633, 35.3002, 41.4194, 37.3948, 38.7517, 38.3868, 41.1592, 42.4700,
        38.7671, 39.0839, 34.7937, 42.3612, 37.1896, 39.9880, 38.6614, 40.6284,
        34.9407, 40.2783, 37.9863, 41.6023, 36.9301, 36.5634, 39.8716, 38.7318,
        37.3054, 40.6977, 37.2358, 40.3730, 37.3552, 40.9560, 43.3467, 38.5552,
        37.1685, 40.2435, 31.9992, 40.9404, 39.0432, 43.4281, 37.7192, 33.1982,
        40.9262, 42.4112, 38.5045, 39.7609, 41.9358, 44.4180, 42.9044, 39.6774,
        39.9340, 42.9680, 36.8149, 40.7846, 40.2290, 38.8360, 41.0477, 41.7691,
        64.1108, 42.3590, 29.0591, 38.6246, 51.4313, 37.1072, 32.6660, 42.1586,
        37.1294, 39.9327, 42.6063, 39.8974, 34.7479, 39.6122, 41.3882, 35.6463,
        42.4625, 38.8925, 42.8418, 39.8470, 35.9517, 40.0813, 39.5978, 40.0901],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([13.1576,  7.6272,  4.4992, 12.1800, 11.1817, 13.5177, 12.9572, 10.5184,
        11.7419,  6.9547, 12.3777, 13.5018, 12.4056, 12.1740,  5.3748,  8.1344,
         8.0200,  9.2354, 12.3346, 13.1446, 12.4290, 10.4174,  9.9911,  6.6657,
         8.9061, 10.4334, 14.9459, 13.6811, 12.9952,  8.1616,  8.0598, 12.9603,
        10.7575, 12.4441,  8.0959,  9.0069, 15.4229, 10.4170,  7.4079, 19.4155,
        16.3689, 36.0751, 34.8701, 34.2765, 28.8310, 36.9742, 14.4668, 12.1869,
        24.0316, 14.0474, 18.2747, 23.8525, 27.7154, 16.2247, 29.8624, 41.3469,
        56.6800, 16.6203,  7.8140, 11.8346, 47.9598,  7.0907, 10.2109, 12.5504,
        12.1939, 10.5465, 16.4125, 11.7971,  9.7171, 12.5866, 18.0947, 14.7799,
        12.2849, 18.2831,  7.9229, 14.5792, 10.4208, 12.7044, 10.5961,  8.7959],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([30.3916, 18.7525,  9.2805, 24.8553, 23.4071, 26.8433, 26.2779, 20.0591,
        24.4054, 13.5037, 26.1733, 26.6317, 25.1304, 24.6049, 11.0333, 18.9866,
        19.5133, 20.1771, 24.7326, 25.6767, 26.2352, 23.0909, 21.4285, 14.0386,
        19.1663, 21.8780, 32.3941, 28.2862, 27.1383, 15.3425, 15.5913, 27.5537,
        23.2435, 24.5709, 20.2240, 18.2066, 32.3141, 21.0796, 15.5129, 47.6399,
        31.7434, 72.1006, 76.3791, 73.6849, 56.4740, 70.9818, 28.3321, 24.1908,
        50.2009, 26.9510, 40.4570, 46.7644, 57.0886, 34.0269, 59.9827, 85.0435,
        90.7069, 32.3839, 21.7494, 24.4964, 87.7687, 14.7107, 25.1534, 22.4395,
        25.9744, 21.0394, 31.2738, 23.1208, 22.4138, 25.9629, 35.8106, 32.6529,
        23.4359, 38.0095, 13.8428, 25.4411, 22.9351, 25.7804, 21.6028, 17.5107],
       device='cuda:0')
Outer loop valEpocw Maximum [10/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 64.1
model_train val_loss valEpocw [10/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 56.7
model_train val_loss  valEpocw [10/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 90.7
Max_Val Meta Model:  tensor([39.3206, 35.0869, 38.7914, 36.6140, 37.9786, 37.7789, 40.6081, 35.9904,
        38.0237, 38.5666, 33.4937, 36.0273, 35.7196, 38.8889, 38.0450, 37.0162,
        34.4402, 38.4200, 36.3138, 41.0691, 35.9238, 35.9514, 37.2041, 37.7966,
        37.1292, 40.0566, 36.6847, 40.0330, 37.2404, 39.4412, 35.8640, 38.3776,
        39.5849, 42.7945, 31.4522, 39.1005, 38.0522, 37.5834, 37.1077, 32.3283,
        39.5611, 39.1281, 35.5646, 37.1448, 40.1250, 34.8800, 40.7393, 38.1938,
        38.8620, 38.4122, 36.3117, 38.3067, 38.6218, 37.9381, 39.7885, 38.5526,
        38.7664, 41.4223, 28.3734, 34.7010, 38.9714, 37.1861, 32.1503, 41.8704,
        36.4082, 39.1726, 41.8229, 47.5238, 34.4095, 39.0389, 32.8488, 34.9615,
        40.6986, 38.0557, 57.5010, 38.2186, 34.7354, 38.2220, 39.0735, 41.1318],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 16.2912,   5.0346,  12.7984,   8.7064,   7.8571,  12.0511,   9.7231,
         11.6908,   8.2853,  12.1715,   9.4274,  10.5801,   8.9048,  11.0245,
          8.4037,   5.2362,   5.3138,   6.3182,   8.6117,   9.7905,   8.8836,
          7.2694,   6.7605,   5.7111,  12.1474,   7.1421,  12.5917,   8.8532,
         12.0643,   5.8768,   3.9448,   8.9445,   5.6611,   8.9668,   5.4764,
          5.2930,   9.0942,   6.1630,   4.8855,   7.6926,   5.3030,   6.5620,
          6.4090,   7.8397,   9.6116,   6.1930,   9.2239,   5.8394,  12.9446,
          8.8787,   7.3831,   8.3863,   8.8060,   8.9110,   9.1447,   8.9088,
          4.8590,  10.6337,   6.5991,   8.2886,   5.8757,   4.1852,   5.4709,
          6.0705,   6.6710,   6.9694,  11.3538,   7.2039,   6.2647,   9.0664,
         12.4492,   9.9560,   7.5205,   6.3594, 184.5899,   8.6348,   6.0067,
          9.4122,   7.4911,   6.1280], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 37.3275,  12.4895,  27.4405,  17.8882,  16.5466,  24.2065,  19.7871,
         23.9749,  17.4072,  23.8655,  20.2735,  22.0479,  18.4434,  22.6051,
         17.3147,  12.6662,  13.0550,  13.7401,  17.7248,  19.2213,  18.9881,
         16.1540,  14.7767,  12.1377,  26.0800,  15.0868,  27.4439,  18.2836,
         25.0780,  11.3558,   8.3205,  18.6671,  11.9730,  17.5909,  13.7013,
         11.0626,  19.1610,  12.8199,  10.0535,  18.9737,  10.2318,  13.2846,
         14.1209,  16.9509,  18.8164,  13.2016,  18.4738,  11.7956,  27.1720,
         17.8795,  16.1446,  16.9183,  17.8441,  18.7514,  18.2491,  18.0819,
          7.8903,  20.8897,  18.6597,  17.5109,  10.9892,   8.6796,  13.4557,
         10.8225,  14.2029,  13.9552,  21.7451,  13.5091,  14.3520,  18.7300,
         28.3202,  21.9537,  14.5863,  13.1031, 329.7692,  15.5290,  13.4170,
         19.7943,  15.2485,  12.0456], device='cuda:0')
Outer loop valEpocw Maximum [10/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 57.5
model_train val_loss valEpocw [10/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 184.6
model_train val_loss  valEpocw [10/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 329.8
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [83.27262095 97.2137279  90.45455099 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.12850721
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 90.6092762  96.13796128 96.24273583 96.9067141
 91.04299412 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [78.46882957 97.2137279  89.55422083 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.07002839
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 90.74572678 96.13796128 96.24273583 96.9067141
 90.13048087 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [94.36090104  4.15929377 52.08239513  4.65423414  2.04275122  4.77674265
  2.5333613  27.31899005  2.44180557  7.67946092  1.62480589  1.32992077
  0.97712753  7.56734382  2.29010588  3.69307888  3.66731587  2.03810153
  0.84104033  1.17809577  1.23269145  0.4616373   0.8527407   1.53411734
 11.10106113  5.08427092 19.91910795  3.84186088  3.40806473  1.38142636
  1.72833786  0.9067995   4.12353117  1.40374775  1.53840396  2.02838939
  2.69643983  2.41069268  2.233545   25.52251248  7.95100984 34.72492619
  7.84720012 10.59909322  7.94297999 26.03899554  2.71509558  1.98290869
  3.89906419  2.04840332  1.44216928  1.71092928  1.50316126  5.4720211
  2.16599533  5.62505536 54.91934189  7.32868978  8.59092626  3.82644477
 52.34240782  2.91684729  8.63792823  7.06746351  3.32207831  4.9824901
  3.67653612  7.24279449  2.37978291  4.91074604  0.40480322  5.07681135
  4.36424538 11.72182656  5.37475031  5.96951748  1.3570661   2.48372813
  0.18540504  0.85839495]
Accuracy th:0.5 is [45.51966959 97.2137279  72.78541928 97.02489005 97.26733349 77.803633
 78.15816084 76.9130493  79.1133149  96.40598921 79.38012451 98.52097319
 99.41399349 80.51193333 78.84163205 96.56680596 96.29512311 78.6406111
 98.65376884 98.30776915 80.51436995 79.88328602 98.38695922 78.75878705
 80.82138376 96.65086926 94.0778012  78.31288605 98.01293844 79.20225143
 97.30875598 98.57457877 96.36213009 98.02024829 86.0394001  78.8209208
 78.63695618 89.60051656 97.11504489 75.86530379 79.52875818 92.05906361
 78.08628063 77.64768948 96.9627563  93.87434364 98.02877645 98.57336046
 87.35882847 87.37953972 86.83008248 98.55508583 98.99976852 78.25562554
 98.70615611 78.68447022 73.1923344  92.91066142 96.24273583 96.9067141
 89.79300934 97.17717864 90.8456281  78.70518147 98.42838172 79.2107796
 98.20786784 77.79997807 79.75049037 97.55485435 80.33771518 95.99054592
 79.44591318 95.45083515 77.99368916 82.29675564 85.57644278 79.30580768
 80.36086305 99.14718388]
Accuracy th:0.7 is [45.62200753 97.2137279  72.78541928 97.02489005 97.26733349 77.803633
 78.15816084 77.0653379  79.1133149  96.47177788 79.38012451 98.52097319
 99.41399349 81.01387654 78.84163205 96.56680596 96.29512311 78.6406111
 98.65376884 98.30776915 80.96027095 79.88328602 98.38695922 78.79777293
 81.29774247 96.65086926 94.0778012  78.31288605 98.01293844 79.20225143
 97.30875598 98.57457877 96.36213009 98.02024829 86.275752   78.8209208
 78.63695618 89.85026986 97.11504489 75.86530379 80.21344769 92.05906361
 78.08628063 77.64768948 96.9627563  93.87434364 98.02877645 98.57336046
 89.44457304 88.20798967 87.00917386 98.55508583 98.99976852 78.25562554
 98.70615611 78.68447022 73.1923344  93.34072441 96.24273583 96.9067141
 89.79300934 97.17717864 91.0405575  78.70518147 98.42838172 79.2107796
 98.20786784 77.79997807 79.75049037 97.55972759 80.33771518 95.99054592
 79.46053289 95.45083515 77.99368916 82.38081895 85.71289336 79.30580768
 80.36086305 99.14718388]
Avg Prec: is [56.07229714  3.07296384 11.34433459  3.30066246  2.26834847  3.787707
  3.36142864  5.65126802  2.45789704  3.91039711  1.55976505  1.67072943
  0.65267689  5.01991856  2.7226536   3.10319833  3.72866919  2.6048283
  1.37651071  1.74213654  1.89166263  0.87228031  1.85220262  2.41158724
  5.18621754  3.57042082  6.56861912  3.53289877  2.09667094  2.00367997
  2.5753723   1.30641152  3.69571818  1.72283327  2.39168484  2.38167613
  3.0083492   2.55260278  2.83628068  7.27036805  2.23965305  8.19810094
  3.26382487  4.18065919  3.30142922  6.38936138  2.13137655  1.53133998
  2.08607356  1.60576982  1.80054432  1.5669995   1.14382747  2.95122263
  1.33183055  2.70156769 11.29754429  3.65089006  4.01586942  2.81979765
 10.75813857  2.21864372  3.69631077  2.88414153  1.51806758  2.4808435
  1.76127768  4.21316846  1.25723112  2.3603919   0.18028889  3.33799481
  1.87211717  4.66651754  4.00072994  3.13054131  0.83166943  1.76634744
  0.16948557  0.78004919]
mAP score regular 8.18, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [86.25457807 97.22450607 91.24747739 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.25901288
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.3106859  96.39235618 96.16314124 96.78102499
 91.44430326 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [83.63604654 97.22450607 89.74761442 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.41846675
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.60467897 96.39235618 96.16314124 96.78102499
 90.89618058 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [95.58975294  4.61952775 56.19185724  4.90380982  1.58886256  4.62516982
  2.65144203 30.2144768   2.69135442  8.28491202  1.69149142  1.3096636
  0.93267323  7.84453698  2.46415251  4.04626904  3.65873563  2.05308485
  0.76241777  1.18348595  1.09887727  0.47224255  0.87906534  1.56198276
 10.98081472  5.30861976 20.28950125  3.22817059  3.72144948  1.43099339
  1.546719    0.81566087  4.06676343  1.381747    1.29204133  1.72318951
  2.36385105  3.06299922  2.20999073 26.14271825  8.56331185 35.48972955
  8.48390157 10.85850312  8.31231312 26.76976149  2.76793891  2.00291492
  4.35448741  2.01014766  1.45647873  2.06850314  2.03958622  6.16670311
  2.77984074  5.80185357 52.55547965  6.87820923  8.46777101  4.00940338
 54.51917799  2.89172549  8.75728672  8.17917566  4.15193395  4.90490196
  4.24067939  8.08146273  2.40692753  5.13118265  0.43372214  4.79258698
  4.25953951 12.34239062  5.9746436   5.81037304  1.27362602  2.63525965
  0.18497534  0.79959983]
Accuracy th:0.5 is [45.35964322 97.22450607 71.30328624 96.96290206 97.90716795 76.86922291
 76.97635598 75.75802875 78.38901761 96.41477938 78.60826669 98.5325261
 99.34972718 78.44382988 78.47123602 96.31262924 96.21047911 77.948028
 98.78167277 98.34068316 79.29342004 79.2435907  98.31327703 78.02277201
 78.52853975 96.52938685 94.3393876  77.96048534 97.81747515 78.56092882
 97.52597354 98.67204823 96.39983058 98.18870369 86.63328101 78.17226001
 78.11495627 91.44928619 97.0276802  75.16505967 78.20714054 92.37362035
 77.29028079 76.94894985 97.03764606 94.02795426 98.18621222 98.77668984
 87.71956051 87.09918529 85.12594364 98.55993223 98.87385704 77.31519546
 98.6969629  77.97294267 71.76171612 94.05536039 96.16314124 96.78102499
 90.13379176 97.04761193 90.48758004 77.90069014 98.32075143 78.75526322
 98.13139996 77.08099758 79.2660139  97.53095647 79.72444378 96.07843137
 78.97202083 95.44559882 76.99877918 83.27478387 87.3906869  78.61324962
 79.79420485 99.15040985]
Accuracy th:0.7 is [45.62872163 97.22450607 71.30328624 96.96290206 97.90716795 76.86922291
 76.97635598 75.80536662 78.38901761 96.41976231 78.60826669 98.5325261
 99.34972718 78.84246456 78.47123602 96.31262924 96.21047911 77.948028
 98.78167277 98.34068316 79.62229364 79.2435907  98.31327703 78.02526347
 78.87983656 96.52938685 94.3393876  77.96048534 97.81747515 78.56092882
 97.52597354 98.67204823 96.39983058 98.18870369 86.93474849 78.17226001
 78.11495627 91.65607793 97.0276802  75.16505967 78.62321549 92.37362035
 77.29028079 76.94894985 97.03764606 94.02795426 98.18621222 98.77668984
 89.78747789 87.47788823 85.27044871 98.55993223 98.87385704 77.31519546
 98.6969629  77.97294267 71.76171612 94.32693026 96.16314124 96.78102499
 90.13379176 97.04761193 90.65201684 77.90069014 98.32075143 78.75526322
 98.13139996 77.08099758 79.2660139  97.53593941 79.72444378 96.07843137
 78.97700376 95.44559882 76.99877918 83.34454493 87.4953285  78.61324962
 79.79420485 99.15040985]
Avg Prec: is [53.6995246   3.69907512 14.84349388  4.54096513  1.48268086  4.27568285
 14.44785452  8.6679079   8.39058337  5.28994169  2.52964493  5.0808283
  2.37373842  5.82679238  2.95253407  3.66459488 22.50438073  6.49449002
  1.55093801  2.71449269  3.49199648  1.54220445  1.23817679  5.12181904
  5.61134113  8.52257697  7.79687563  4.5793057   3.88398699  5.391092
  2.18640659  0.84304814  2.90695557  1.14379869  1.66518316  2.1854438
  1.9617885   2.27485274  2.19294299  6.24597243  1.69237041  6.03586701
  2.16284538  2.69248422  2.33799428  4.85158629  1.66377813  1.01583844
  1.40550766  1.1569658   1.1905216   0.9871914   0.73950801  2.25447703
  0.87669031  1.85928579 10.07841973  2.99702254  3.83172935  2.76029507
  7.82333725  2.08449334  3.32645007  2.50746343  1.34676277  1.93521814
  1.52335405  3.52491923  1.08363502  2.23430967  0.19405358  3.28540437
  1.61210145  3.98259949  3.18086443  2.31304596  0.55542233  1.46222203
  0.12083387  0.60833992]
mAP score regular 8.43, mAP score EMA 4.29
Train_data_mAP: current_mAP = 8.18, highest_mAP = 8.18
Val_data_mAP: current_mAP = 8.43, highest_mAP = 8.43
tensor([0.4362, 0.4012, 0.4625, 0.4830, 0.4755, 0.4930, 0.4903, 0.4815, 0.4745,
        0.5086, 0.4655, 0.4808, 0.4773, 0.4855, 0.4859, 0.4101, 0.4061, 0.4572,
        0.4805, 0.5081, 0.4616, 0.4498, 0.4562, 0.4679, 0.4630, 0.4750, 0.4568,
        0.4823, 0.4785, 0.5089, 0.4688, 0.4777, 0.4679, 0.4994, 0.3988, 0.4710,
        0.4736, 0.4739, 0.4850, 0.4035, 0.5170, 0.4875, 0.4491, 0.4651, 0.5118,
        0.4668, 0.4968, 0.4899, 0.4703, 0.4949, 0.4579, 0.4898, 0.4876, 0.4713,
        0.4981, 0.4948, 0.6162, 0.5058, 0.3518, 0.4708, 0.5381, 0.4830, 0.4049,
        0.5630, 0.4679, 0.4993, 0.5186, 0.5306, 0.4353, 0.4838, 0.4373, 0.4552,
        0.5208, 0.4847, 0.5748, 0.5557, 0.4455, 0.4687, 0.4932, 0.5058],
       device='cuda:0')
Max Train Loss:  tensor([21.5231,  7.0259, 13.5936, 12.7152, 12.7242, 14.6478, 13.8720, 10.7387,
        11.7938, 11.4760, 12.7939, 10.9927,  9.3488, 12.0000,  9.5513,  8.2495,
         7.7424, 10.9112, 11.7051, 11.6997, 11.8249,  7.8210,  8.8131,  9.4975,
        12.9287, 10.1338, 10.2972, 11.3476, 12.0443, 10.0921, 15.0438,  9.5160,
         9.1432, 11.8377, 10.2795, 10.8098, 12.6441, 10.8697,  8.8393, 14.8621,
        14.6937, 15.0878, 11.0925, 13.3322, 14.2306, 13.2074, 10.4929, 10.4984,
        16.2783, 10.0983,  7.9524, 11.0311, 10.1947, 13.9240, 10.4413, 12.8226,
        17.0507, 16.8682,  8.5150, 11.8439, 19.2217,  7.0629,  7.0926, 13.5723,
         7.1978, 10.4049, 13.4029, 11.8715,  8.0104, 10.4461, 12.8999, 13.8542,
         9.0461,  8.9823, 12.5451,  9.5482,  7.5880,  9.2639,  8.0884,  6.6041],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [11/80], Step [000/642], LR 1.0e-04, Loss: 21.5
Max Train Loss:  tensor([21.1049, 10.0493, 20.1952, 11.3467, 15.6899, 12.3255, 12.4349, 18.2690,
        12.2175, 16.6936, 10.3775, 15.9828, 10.7940, 17.1716, 19.0698, 10.4501,
        11.5825, 14.4492, 10.8841, 14.7031, 10.0827,  8.0851, 10.6097, 17.2094,
        15.4780, 14.7550, 16.4013, 12.2548, 19.7543, 15.4453, 20.7460,  9.9755,
        12.5400, 12.7634,  9.4907, 11.4540, 13.2295, 17.2283, 14.3800, 15.8585,
        16.9392, 23.0497,  9.6696, 12.5749, 17.0671, 18.2612, 18.5071, 17.2863,
         9.5156, 11.0063,  9.5653, 10.6931, 10.6400, 11.5069, 20.6280, 19.7789,
        19.2405, 10.3697,  8.5714, 13.9502, 21.9453, 22.3257,  9.2675, 17.6364,
        11.6900, 11.5841, 12.2129, 20.0098,  9.7208, 21.8583,  9.0620, 10.8083,
        11.7480, 20.6092, 22.1032, 17.4423,  9.3987, 13.5477, 17.9562, 15.2534],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [11/80], Step [100/642], LR 1.0e-04, Loss: 23.0
Max Train Loss:  tensor([17.7510,  8.5515, 14.8437, 14.6193, 12.6405, 14.4540, 13.5988, 14.6597,
        10.9756, 14.2421, 10.2667, 15.4318, 10.4244, 10.9013, 11.1592,  8.5108,
        11.7844,  8.1783, 10.9840, 11.7000,  9.7648,  7.2002, 12.1556, 11.1135,
        13.0106, 11.2443, 13.1462, 12.0725, 10.9810, 13.3439,  8.9416, 13.1570,
        11.3479, 13.3601,  7.1591, 11.2911, 14.2183, 13.9277, 15.4098, 16.0567,
        12.6842, 16.2311, 10.9779, 12.1106, 15.6473, 12.3575,  7.0991, 16.3349,
        10.0596, 11.1772,  8.6912, 10.1617, 10.5810, 10.3009, 10.7929, 11.3815,
        23.6015, 10.7171,  8.0343, 10.9237, 19.5527, 15.1987, 12.4255, 15.5399,
         9.5032, 13.7218, 12.9394, 12.7386,  7.2196,  9.9634,  8.6889, 14.4404,
        11.4017, 16.4458, 10.6108, 13.1408,  9.3192, 10.2174, 10.4500, 11.4446],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [11/80], Step [200/642], LR 1.0e-04, Loss: 23.6
Max Train Loss:  tensor([16.9046, 10.5513, 16.7274, 11.8058, 10.6775, 11.5302, 13.9619, 10.6302,
        10.8969, 15.2458,  9.3485, 15.8775,  9.4954, 12.5972, 10.5167, 11.8048,
        10.6622, 10.8300, 12.7522, 17.4597,  8.5646, 10.0803, 11.1063, 11.1821,
         9.8616, 12.7788, 13.2173, 17.9071,  7.6476, 12.1103,  8.9810, 12.7159,
        13.7362, 10.4514, 10.1206, 11.1518, 14.3446, 10.5202, 14.2277, 13.0637,
        12.7009, 15.7906, 11.0148, 14.6104, 16.4946, 12.7669, 10.7098,  7.8924,
         9.1962, 11.8977,  7.7984, 10.3004, 10.2360, 13.0105, 10.8721, 11.4094,
        20.5434,  6.8354,  8.8832, 11.9177, 17.1697, 10.9268, 10.3064, 13.6830,
        10.7046, 11.5143, 10.8027, 11.3251,  8.0878, 10.3589, 10.2679, 10.5725,
        12.5293, 13.7157, 13.7563, 15.4757,  7.5286,  8.0540,  9.7602, 12.1631],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [11/80], Step [300/642], LR 1.0e-04, Loss: 20.5
Max Train Loss:  tensor([15.2721,  8.3503, 13.9585, 11.5372, 12.5924, 12.7410, 13.5701, 11.7012,
        14.8894, 11.8401,  9.5989, 16.8291, 10.4962, 10.8209, 13.4348, 10.6441,
         7.9828, 12.7962, 10.6481,  9.3341, 12.4574,  7.3004,  7.5928,  8.3139,
        16.1047, 13.2205, 15.3901, 12.2857,  8.0655, 13.8360, 10.3309, 11.7983,
        12.6610, 11.4002,  7.7761, 11.7669, 11.9067, 10.8972, 12.8437, 10.4281,
        12.5927, 19.5150, 11.9337, 17.5969, 16.2417, 14.6424,  8.6684, 11.6375,
        11.4044, 13.2981,  9.0471, 10.6692, 12.7994, 13.1229, 11.9318, 12.5753,
        21.6821, 11.2526,  7.2939, 12.1568, 21.1997,  9.8379,  8.9756, 14.7566,
         8.3952, 13.3308, 10.7930, 12.7624,  8.4924, 10.2062,  9.4696, 10.9257,
        11.4111, 15.9650,  8.0392, 10.7929,  8.1668, 10.4367,  9.9064, 11.9409],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [11/80], Step [400/642], LR 1.0e-04, Loss: 21.7
Max Train Loss:  tensor([19.1728, 10.6850, 13.5580,  9.2580, 14.2891, 12.2347, 11.4021, 12.3456,
        11.0190, 11.3054,  9.7151, 15.8058, 11.1556, 10.7817, 20.9617,  8.1148,
         9.7534, 11.4205,  9.3942,  9.0983,  9.5197,  8.8999, 10.4344,  9.6509,
         9.4698, 15.1249, 18.9614, 12.9445,  8.7513, 16.6345,  8.7448,  9.4103,
        13.0896, 14.6229,  7.7276, 11.5283, 13.4897, 11.7407, 12.8771, 15.4747,
        13.0165, 13.5883,  9.1625, 12.9929,  8.3397, 13.0665,  6.7725,  5.7723,
         9.2466, 11.3146,  7.7585,  9.3869,  9.3896, 13.3628,  9.7683, 12.9607,
        17.1154,  7.8750, 10.7209,  9.3736, 14.0523, 10.5899, 10.2007, 11.9248,
        10.0339, 10.6850, 11.8358, 14.3105,  8.3498, 11.1604,  8.6173, 12.3616,
        11.5864, 14.0721, 13.0435,  9.9862,  6.6612,  8.8576,  9.7183, 10.6396],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [11/80], Step [500/642], LR 1.0e-04, Loss: 21.0
Max Train Loss:  tensor([17.1752,  7.2222, 12.4066,  9.5744, 10.7004, 14.7096, 11.3014, 12.5215,
         9.8739,  9.1645, 12.9654,  2.4388,  9.4804, 14.1816, 12.3137,  7.4844,
         9.9371,  8.2073,  9.4226,  5.1784, 13.2181,  9.4233, 13.4610, 13.6381,
        12.3152, 14.6285, 16.1006,  8.2837,  9.3715, 12.9676,  8.8475,  9.4381,
        11.4867, 14.8273,  8.1950, 10.4476, 11.7237, 12.5458, 15.0841, 11.0906,
        12.2207, 10.0812,  8.6081, 12.2566,  8.3902, 11.8630,  5.5132,  7.0086,
         7.5109,  9.3652,  7.7809, 10.7263, 11.3900, 12.9329,  9.6651, 11.5307,
        17.3688,  9.2746,  6.5126, 10.6597, 15.5369, 10.3127, 11.6922,  9.3026,
         8.9150, 12.2365, 11.3464, 14.5978,  8.4602,  9.2991,  8.6423,  8.5160,
        11.8988, 10.5840,  9.9898, 15.0201,  7.5249, 12.5323,  9.7476, 10.6730],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [11/80], Step [600/642], LR 1.0e-04, Loss: 17.4
Max_Val Meta Model:  tensor([ 25.2785,  25.1119,  38.8359,  22.3921,   8.6663,  12.1896,   7.4233,
         12.4662,   9.7486,  10.3005,   9.2945,   5.3226,  10.1127,  12.2701,
          7.8905,   6.4571, 119.1373,   7.2223,   9.3576,   3.6399,   7.5361,
          7.0820,   7.3758,   6.8819,  17.9780,  12.7937,  14.9572,   3.9833,
          8.1094,  13.0659,   7.7255,   9.3742,   8.1391,   9.2368,   5.4245,
          7.8724,   9.4402,   8.2496,   7.7995,  21.2517,  13.2549,  14.0888,
          8.4558,  13.1554,   8.6788,  14.6824,   4.1623,   6.7395,   7.4465,
         10.0302,   7.7112,   9.3538,  10.5307,   9.1591,   7.5002,   9.5492,
         16.3343,   7.5694,   8.4335,   8.3240,   9.2711,  15.4939,   6.4466,
          6.7539,   8.1797,   9.2401,   9.5896,   7.6264,   8.9314,  14.8825,
          8.5875,  12.4225,  14.9304,   7.4430,   9.7396,   9.2917,   6.6080,
          8.6188,   9.6916,  10.6234], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 23.2678,  24.6757,  34.0160,  23.1072,   8.7299,  12.2052,   7.3868,
         13.2779,   9.8092,  10.4505,   9.3103,   5.1934,  10.2023,  12.9261,
          8.1063,   6.5139, 118.4986,   7.2790,   9.4235,   3.6767,   7.5947,
          7.1384,   7.4335,   6.9910,  18.2583,  12.7175,  15.8430,   4.0628,
          8.2321,  12.9999,   7.9073,   9.4407,   8.3486,   9.3112,   5.4712,
          7.9288,   9.5051,   8.3169,   7.8516,  21.1538,  13.2792,  14.2031,
          8.4771,  13.2538,   8.5714,  14.1535,   4.1864,   6.8594,   7.4568,
         10.1046,   7.7706,   9.4063,  10.6203,   9.2240,   7.5490,   9.5881,
         16.8675,   7.7601,   8.4762,   8.3858,   9.4224,  15.5042,   6.5172,
          6.7832,   8.2406,   9.2964,   9.6430,   8.1104,   8.9797,  14.8678,
          8.6469,  12.2903,  15.2027,   7.6136,   9.6233,   9.2887,   6.6620,
          8.7024,   9.7428,  10.6691], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 53.3445,  61.4987,  73.5482,  47.8435,  18.3596,  24.7589,  15.0655,
         27.5790,  20.6722,  20.5470,  19.9993,  10.8027,  21.3733,  26.6248,
         16.6843,  15.8840, 291.7832,  15.9220,  19.6099,   7.2360,  16.4514,
         15.8692,  16.2929,  14.9422,  39.4385,  26.7752,  34.6806,   8.4242,
         17.2025,  25.5435,  16.8657,  19.7613,  17.8442,  18.6441,  13.7203,
         16.8354,  20.0699,  17.5512,  16.1900,  52.4281,  25.6852,  29.1355,
         18.8754,  28.4971,  16.7477,  30.3207,   8.4259,  14.0005,  15.8546,
         20.4176,  16.9701,  19.2028,  21.7811,  19.5725,  15.1553,  19.3765,
         27.3742,  15.3412,  24.0905,  17.8106,  17.5098,  32.1009,  16.0942,
         12.0488,  17.6116,  18.6195,  18.5938,  15.2854,  20.6296,  30.7326,
         19.7744,  27.0003,  29.1911,  15.7073,  16.7426,  16.7164,  14.9526,
         18.5676,  19.7539,  21.0952], device='cuda:0')
Outer loop valEpocw Maximum [11/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 119.1
model_train val_loss valEpocw [11/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 118.5
model_train val_loss  valEpocw [11/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 291.8
Max_Val Meta Model:  tensor([38.6999, 34.2098, 42.6006, 42.9595, 39.7149, 39.4724, 47.9033, 41.5494,
        39.1793, 45.6666, 39.2125, 48.9314, 40.4989, 45.5236, 45.3770, 33.4964,
        35.0429, 37.8484, 42.7093, 47.0159, 38.6643, 37.2879, 37.3653, 42.4086,
        39.1502, 39.5554, 37.8959, 39.5365, 39.7540, 41.9417, 38.9577, 40.4283,
        38.7842, 40.8498, 32.8961, 38.7481, 38.8004, 38.0795, 39.7319, 38.1843,
        42.2235, 39.7771, 36.9433, 38.4301, 40.1967, 38.9211, 39.3450, 40.0031,
        38.6000, 40.4549, 37.6404, 40.0219, 39.6822, 38.9314, 40.0182, 40.4788,
        37.8721, 41.2775, 27.8303, 38.6663, 46.1188, 36.1319, 32.1986, 43.8340,
        38.1153, 41.0044, 42.5787, 40.3183, 35.8588, 39.9950, 37.0650, 40.6808,
        43.4673, 39.4599, 41.4086, 44.2979, 36.2218, 38.4966, 40.8502, 41.6080],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 30.6940,   8.1746,  24.4339,   8.5973,   7.9616,  17.7262,  46.8770,
         23.4920,  17.2676,  17.6119,  11.1021, 112.1527,  10.4283,  12.2855,
          9.7814,   5.8916,   6.7243,   9.9068,   8.7401,  13.4353,   6.8255,
          6.3553,   6.6606,   7.3701,  15.0440,  12.0348,  16.8503,   6.7781,
          8.8881,  10.0442,   7.2233,   8.6252,   6.2670,   8.4338,   4.8486,
          7.0935,   9.5854,   5.7779,   6.7354,   6.0270,   9.8571,   4.6129,
          6.2535,   7.9553,   2.8111,   7.6392,   3.5508,   4.9953,   6.6339,
          9.1913,   6.9270,   8.2803,   8.4235,   8.3389,   6.6937,   9.4439,
          5.7272,   4.8192,   5.2072,   7.5762,   3.0833,   5.8098,   5.0343,
          4.4753,   7.4274,   8.3061,   8.6990,   8.6760,   5.7470,   8.3914,
          8.0882,   6.0434,   9.9149,   4.6659,   5.9116,   4.8616,   5.9241,
          7.1392,   8.7802,   9.5762], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 69.2024,  20.3961,  52.3558,  17.2950,  16.4875,  36.8937,  91.0017,
         48.3122,  36.4233,  33.2403,  23.5529, 233.0929,  21.4909,  23.6319,
         19.2577,  14.6197,  16.6242,  21.1694,  17.7093,  25.1976,  14.6242,
         14.0688,  14.4968,  15.3175,  31.9505,  25.4000,  36.8661,  14.0917,
         18.3978,  19.7072,  15.3111,  17.8093,  13.3282,  16.7645,  12.0358,
         15.0189,  20.4967,  12.4294,  13.9256,  14.7834,  19.0660,   9.4274,
         13.7980,  17.2444,   5.4711,  16.0329,   7.1641,  10.0553,  14.0691,
         18.4875,  15.1322,  17.2482,  17.4059,  17.6448,  13.4541,  19.0026,
          9.6004,   9.4821,  15.2296,  15.9548,   5.6170,  12.4873,  12.7798,
          8.0703,  15.7709,  16.6511,  16.7216,  17.2060,  13.1103,  17.3914,
         17.8552,  13.1542,  19.3052,   9.6035,  10.7914,   8.8713,  13.2183,
         15.2880,  17.8427,  19.1577], device='cuda:0')
Outer loop valEpocw Maximum [11/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 48.9
model_train val_loss valEpocw [11/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 112.2
model_train val_loss  valEpocw [11/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 233.1
Max_Val Meta Model:  tensor([34.0015, 34.6561, 41.3133, 41.9189, 39.1512, 38.0469, 36.7870, 39.3713,
        39.8861, 40.6236, 38.2725, 40.9598, 40.2128, 41.9511, 42.2466, 33.7009,
        34.6720, 35.6927, 40.9830, 40.1377, 38.2033, 39.2969, 40.1194, 39.9303,
        37.9760, 42.0951, 37.2107, 40.1874, 36.8743, 40.2952, 40.5682, 39.5381,
        38.0064, 40.8882, 32.0144, 38.9400, 38.7770, 38.8006, 39.1683, 38.4021,
        41.8992, 42.4484, 39.9240, 39.5330, 40.4846, 39.9188, 42.8435, 42.0922,
        40.3089, 39.7718, 36.6397, 41.1549, 41.9238, 39.1212, 40.3911, 43.0889,
        59.1322, 40.7017, 27.3057, 37.8565, 51.9423, 35.2265, 31.4432, 41.5272,
        37.6051, 40.0555, 39.6257, 40.1898, 34.8642, 38.9390, 35.5140, 40.7677,
        41.9882, 39.1649, 39.8831, 40.7140, 36.1307, 38.7299, 39.8692, 41.3254],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 9.2831,  7.2064,  4.1074,  8.4677, 10.9128, 11.8745,  4.7222, 10.6423,
        11.3310,  8.2229, 10.7195,  3.3128, 11.8700, 11.7921,  7.3140,  7.3883,
         7.7664,  9.0637, 11.8974,  4.5146,  9.7192,  9.4276,  9.5710,  8.9441,
         9.8716, 11.6547, 14.9579,  8.3973,  7.7881, 13.1105,  9.7395, 12.4378,
        10.5568, 11.7673,  7.0608, 10.1406, 14.6030, 11.5211,  9.6796, 15.7137,
        19.2898, 33.0909, 34.5635, 33.8948, 31.4736, 34.4386,  8.3088, 11.4161,
        20.9416, 12.7408, 17.8387, 23.0770, 27.8383, 15.2316, 30.6894, 43.1087,
        73.0028,  9.3751,  7.0783, 10.4524, 52.1303,  8.9940,  9.5383, 10.7979,
        12.5011, 11.4576, 13.0648, 11.2501,  8.9320, 10.3215, 10.6732, 10.3721,
        12.9712, 17.9504,  6.0090, 11.6920, 10.2251, 10.2510, 11.6842, 12.7084],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 21.1882,  17.6291,   8.6620,  17.0304,  22.5689,  24.5370,   9.8369,
         21.5960,  23.2685,  15.8553,  22.7211,   6.5683,  24.0994,  23.2060,
         14.5855,  17.8949,  18.9958,  19.9273,  23.9383,   8.7719,  20.5874,
         19.9448,  20.3564,  18.6862,  20.9837,  23.5648,  32.5383,  17.0488,
         16.3756,  25.5000,  19.7395,  25.7312,  22.5230,  22.8709,  17.5529,
         20.8792,  30.8313,  23.8326,  19.8929,  37.9657,  37.4924,  66.7845,
         75.8629,  73.0248,  61.9263,  72.6329,  15.7689,  21.6730,  43.5355,
         25.5451,  39.4226,  46.2787,  57.1995,  32.0710,  61.4266,  87.1302,
        119.7554,  18.1631,  20.5609,  21.9911,  97.2930,  19.3562,  24.1892,
         19.2658,  26.4071,  23.0116,  25.7170,  21.9848,  20.4604,  21.4559,
         24.0869,  22.5270,  25.0572,  37.1352,  10.7814,  20.9858,  22.4304,
         21.3253,  23.8285,  25.0961], device='cuda:0')
Outer loop valEpocw Maximum [11/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 59.1
model_train val_loss valEpocw [11/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 73.0
model_train val_loss  valEpocw [11/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 119.8
Max_Val Meta Model:  tensor([29.0651, 33.8676, 38.9454, 39.2325, 39.5311, 38.3488, 37.4280, 39.0492,
        39.7860, 38.3128, 38.4201, 41.0320, 39.4756, 41.1046, 40.1002, 33.1650,
        34.3308, 36.5649, 39.6340, 40.1050, 37.8135, 39.3758, 38.6650, 39.5944,
        38.4880, 38.6306, 37.1468, 38.7290, 37.5403, 38.9360, 37.6387, 41.2225,
        43.2184, 42.5339, 32.2910, 37.8491, 38.1617, 39.5752, 42.2024, 37.5440,
        35.4882, 38.5125, 36.0400, 38.1024, 36.3677, 38.1565, 39.5673, 37.4081,
        38.3766, 39.9529, 37.2804, 39.3381, 39.3808, 40.2579, 40.8931, 35.4479,
        37.6747, 35.7899, 27.0706, 39.9302, 40.0060, 36.3492, 31.4720, 41.0590,
        37.5052, 40.4438, 37.7442, 47.8351, 35.2278, 39.4967, 36.4442, 38.6505,
        40.5042, 38.7684, 55.8115, 38.2813, 35.6964, 37.6753, 40.4597, 39.6162],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 12.9189,   5.0650,  12.6966,   6.5599,   8.3170,  11.5811,   5.8626,
         12.3960,   8.5917,  12.3345,   8.7423,   3.9636,   8.9637,  11.0491,
          9.6383,   5.2076,   5.5860,   6.8049,   8.8681,   3.5506,   7.0746,
          6.9252,   6.9734,   6.4740,  12.9878,   8.5524,  13.8772,   3.3618,
          8.6969,  10.2613,   6.9163,   9.2385,   6.0958,   8.9164,   5.0682,
          7.3387,   8.7887,   6.1427,   7.2057,   7.2555,   9.1333,   5.4592,
          6.4732,   8.3221,   2.9128,   8.9519,   3.9214,   5.2464,   6.9741,
          8.8491,   7.2625,   8.6013,   8.9127,   8.8033,   7.1242,   8.2538,
          2.8645,   3.5543,   6.3540,   8.0045,   5.9009,   6.4404,   5.2434,
          5.0171,   7.7233,   8.6735,   8.5792,   6.5436,   5.9984,   7.9060,
          8.3764,   6.2645,   8.9766,   5.4835, 185.5716,   6.4035,   6.1993,
          8.1800,   9.1661,   9.8638], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 36.7712,  12.7504,  27.9834,  13.6394,  17.2290,  24.2981,  12.2897,
         25.7628,  17.8492,  24.7526,  18.6851,   7.8863,  18.7421,  22.1238,
         19.7115,  12.9745,  13.8939,  14.8312,  18.4467,   6.9384,  15.3298,
         14.7577,  15.1910,  13.7214,  27.6978,  18.0514,  30.4983,   6.8918,
         18.3564,  20.6177,  14.7856,  18.5895,  12.4975,  17.5101,  12.6625,
         15.7380,  18.9013,  12.9829,  14.2486,  17.9477,  19.9223,  11.2992,
         14.4768,  18.0143,   5.9893,  18.9412,   7.8045,  10.6580,  14.8227,
         17.8305,  15.8472,  18.0472,  18.1705,  18.4154,  14.1513,  18.0981,
          4.7892,   7.4010,  18.9455,  16.6857,  11.1438,  13.9002,  13.4340,
          9.1076,  16.4938,  17.4526,  17.5235,  12.2375,  13.7685,  16.4078,
         18.6259,  13.8161,  17.8052,  11.3267, 339.0239,  12.1823,  13.8758,
         17.7228,  18.6311,  19.9972], device='cuda:0')
Outer loop valEpocw Maximum [11/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 55.8
model_train val_loss valEpocw [11/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 185.6
model_train val_loss  valEpocw [11/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 339.0
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [84.33620448 97.2137279  91.01131809 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.0992678
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 90.96624066 96.13796128 96.24273583 96.9067141
 90.97476883 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [81.11743278 97.2137279  89.71869251 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.05906361
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 90.288861   96.13796128 96.24273583 96.9067141
 90.04519925 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [94.78972018  3.43165356 53.48549765  4.65395834  2.1244565   4.58812823
  4.08832597 27.84897059  2.27753568  8.63423835  1.53820715  1.43864539
  0.84363477  8.88097304  2.04115268  4.03418362  3.58428687  1.8868664
  0.79443047  1.14569558  1.14742502  0.4481352   0.85957856  1.26137609
  6.40712585  4.28923873 15.9717325   3.86045507  3.1083942   1.18429001
  1.88040242  0.86005193  3.0938249   1.40529173  1.43241515  1.41304325
  2.42102943  1.73629767  2.06510146 21.4477659   7.48137579 33.0614137
  8.06308999 10.53466795  9.9158862  25.79602807  2.71020006  2.13004586
  4.55958318  2.29980763  1.53650854  1.78872978  1.59759472  5.30599879
  2.30221878  5.61179701 56.2849978  12.00176435 10.30121078  5.18403588
 52.44999989  3.16067582 13.42245283  8.7886327   4.99667314  5.56220225
  5.36780262  7.25828276  3.82921776  6.96121717  0.64335394  6.67671981
  5.79739101 19.55891402  6.05426606  9.84256226  1.4450332   2.39432076
  0.23396031  0.88672515]
Accuracy th:0.5 is [45.44169784 97.2137279  72.56368709 97.02489005 97.26733349 77.3723517
 77.8316541  76.55121161 78.65035757 96.38649627 78.95615307 98.52097319
 99.41399349 80.0757788  78.54680133 96.56680596 96.29512311 78.25075231
 98.65376884 98.30776915 80.21710262 79.49830046 98.38695922 78.43349862
 80.42543341 96.65086926 94.0778012  77.97663284 98.01293844 78.88549116
 97.30875598 98.57457877 96.36213009 98.02024829 86.06985782 78.52365346
 78.21298473 89.54081943 97.11504489 75.49250131 79.24976548 92.05906361
 77.79632314 77.36504185 96.9627563  93.87434364 98.02877645 98.57336046
 87.4185256  86.99211754 86.51454051 98.55508583 98.99976852 77.98028776
 98.70615611 78.29461142 72.87070089 92.97888671 96.24273583 96.9067141
 89.79300934 97.17717864 90.83100839 78.30313958 98.42838172 78.6820336
 98.20786784 77.3857531  79.38743436 97.53901634 79.95272962 95.99054592
 79.06092762 95.45083515 77.64525286 82.19685433 85.58740756 78.90863903
 80.00755351 99.14718388]
Accuracy th:0.7 is [45.5208879  97.2137279  72.56368709 97.02489005 97.26733349 77.3723517
 77.8316541  76.72177483 78.65035757 96.46934126 78.95615307 98.52097319
 99.41399349 80.5204615  78.54680133 96.56680596 96.29512311 78.25075231
 98.65376884 98.30776915 80.64472899 79.49830046 98.38695922 78.48832251
 80.92615831 96.65086926 94.0778012  77.97663284 98.01293844 78.88549116
 97.30875598 98.57457877 96.36213009 98.02024829 86.29402663 78.52365346
 78.21298473 89.82712199 97.11504489 75.49250131 79.92714514 92.05906361
 77.79632314 77.36504185 96.9627563  93.87434364 98.02877645 98.57336046
 89.59442502 87.84371535 86.71068822 98.55508583 98.99976852 77.98028776
 98.70615611 78.29461142 72.87070089 93.37240043 96.24273583 96.9067141
 89.79300934 97.17717864 91.00157162 78.30313958 98.42838172 78.6820336
 98.20786784 77.3857531  79.38743436 97.55972759 79.95272962 95.99054592
 79.08163887 95.45083515 77.64525286 82.29310072 85.70680182 78.90863903
 80.00755351 99.14718388]
Avg Prec: is [55.69538527  3.11291226 11.25631212  3.33304634  2.30850975  3.72499816
  3.32875254  5.54291149  2.42887496  3.82664096  1.64812366  1.60423437
  0.60002911  5.15570531  2.61027065  3.20341197  3.59894837  2.65059198
  1.41000911  1.70817505  1.96225751  0.91226754  1.8692692   2.37139084
  5.04708079  3.48139057  6.62399518  3.25629552  2.15854354  1.86397378
  2.55128884  1.3502386   3.65915436  1.62579113  2.3629097   2.34468102
  3.02436814  2.55460927  2.8048941   7.53171757  2.25399027  8.30188902
  3.36984361  4.01220973  3.14811529  6.4285358   2.02261344  1.58246885
  2.15045085  1.6342665   1.82289145  1.64440946  1.07213758  2.99186785
  1.35971932  2.71393352 11.29377501  3.7328941   3.95036837  2.91788378
 10.84467805  2.19794253  3.7934852   2.92120251  1.62113119  2.50086573
  1.77714737  4.13447974  1.24770133  2.31419993  0.16403849  3.45969519
  1.87116629  4.74197418  3.9671006   3.04959952  0.83992392  1.86832761
  0.17025646  0.78287322]
mAP score regular 8.53, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [86.97212049 97.22450607 91.576351   96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.49819369
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.57228991 96.39235618 96.16314124 96.78102499
 91.8304806  97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [85.31280365 97.22450607 90.1985699  96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.37362035
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.33061763 96.39235618 96.16314124 96.78102499
 90.90863792 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [95.97708028  3.9714699  57.99261718  5.32894368  1.72985076  5.04899163
  4.78967364 30.41264684  2.55347476 10.8576872   1.6824215   1.51365527
  1.03582095  9.57573534  2.13158631  4.39357362  3.57379651  1.87402635
  0.71087057  1.14961507  1.0351587   0.45409482  0.8844426   1.19569501
  6.39701768  4.56958634 16.11289919  3.43067253  3.58498695  1.21872003
  1.70551493  0.78678672  3.14247023  1.3079603   1.25210602  1.26996999
  2.1537907   1.94217832  2.12236705 21.52317735  7.66638222 35.28506888
  7.97117348 10.09642369  9.90994933 25.899653    2.66448693  2.08445362
  4.80938473  2.16715171  1.45612386  1.97045716  1.76434689  5.70630809
  2.3497625   5.44625179 52.77190826 10.38650114  9.6257258   5.42385174
 56.19118966  3.12242829 13.46987194  9.51118077  5.56863296  5.02965897
  5.52752986  8.05391209  3.26162101  6.67079522  0.70906128  5.98812343
  5.20568971 20.7097837   6.99134738  9.39315756  1.32504269  2.42139746
  0.19225218  0.82131575]
Accuracy th:0.5 is [45.37957496 97.22450607 71.17622144 96.96290206 97.90716795 76.73717518
 76.83434238 75.62099808 78.26195281 96.41477938 78.47123602 98.5325261
 99.34972718 78.37656028 78.33918828 96.31262924 96.21047911 77.81099733
 98.78167277 98.34068316 79.1987443  79.1015771  98.31327703 77.89072427
 78.48867628 96.52938685 94.3393876  77.82345467 97.81747515 78.42389815
 97.52597354 98.67204823 96.39983058 98.18870369 86.63078955 78.03522934
 77.98789147 91.40942273 97.0276802  75.05294367 78.10249894 92.37362035
 77.15823305 76.82188504 97.03764606 94.02795426 98.18621222 98.77668984
 87.98863891 87.00450955 85.04372524 98.55993223 98.87385704 77.17318185
 98.6969629  77.835912   71.63465132 94.02047986 96.16314124 96.78102499
 90.13379176 97.04761193 90.51996911 77.7686424  98.32075143 78.62819842
 98.13139996 76.95393278 79.1240003  97.53095647 79.58243018 96.07843137
 78.83000723 95.44559882 76.87171438 83.22495453 87.3906869  78.48120188
 79.65219124 99.15040985]
Accuracy th:0.7 is [45.6685851  97.22450607 71.17622144 96.96290206 97.90716795 76.73717518
 76.83434238 75.67082742 78.26195281 96.41976231 78.47123602 98.5325261
 99.34972718 78.76273762 78.33918828 96.31262924 96.21047911 77.81099733
 98.78167277 98.34068316 79.51266911 79.1015771  98.31327703 77.89321574
 78.82751576 96.52938685 94.3393876  77.82345467 97.81747515 78.42389815
 97.52597354 98.67204823 96.39983058 98.18870369 86.91979969 78.03522934
 77.98789147 91.59379126 97.0276802  75.05294367 78.52853975 92.37362035
 77.15823305 76.82188504 97.03764606 94.02795426 98.18621222 98.77668984
 90.08147096 87.40314423 85.19072178 98.55993223 98.87385704 77.17318185
 98.6969629  77.835912   71.63465132 94.31198146 96.16314124 96.78102499
 90.13379176 97.04761193 90.68689738 77.7686424  98.32075143 78.62819842
 98.13139996 76.95393278 79.1240003  97.53593941 79.58243018 96.07843137
 78.83748163 95.44559882 76.87171438 83.28225827 87.5102773  78.48120188
 79.65219124 99.15040985]
Avg Prec: is [53.82127291  3.71035683 14.93094686  4.54018473  1.44863443  4.25187633
 14.27529313  8.71883551  8.2645934   5.26709147  2.40872847  4.69285555
  2.36440593  5.8496324   2.98213894  3.6429692  23.71160359  6.47760772
  1.54583216  2.74155177  3.5002423   1.52473003  1.19706796  5.15651841
  5.60574839  8.7948336   7.82070378  4.57806018  3.87234379  5.48969207
  2.20613438  0.8513125   3.06277092  1.10150422  1.65864917  2.19600723
  1.98043895  2.26214102  2.25028972  6.1845915   1.72842873  6.01309425
  2.1905906   2.71302946  2.37211545  4.82972748  1.65024646  0.99230294
  1.40225214  1.13108109  1.16251437  0.95417753  0.72871207  2.3028818
  0.9366808   1.82502486  9.99397227  2.88290302  3.7731189   2.76814541
  7.8070896   2.06217016  3.07825802  2.50220653  1.34010045  1.82352355
  1.49768371  3.45962185  1.08503701  2.25490022  0.19163646  3.25890919
  1.58670598  3.89186143  3.54923663  2.31022096  0.59839258  1.48617635
  0.12617379  0.58219147]
mAP score regular 8.73, mAP score EMA 4.30
Train_data_mAP: current_mAP = 8.53, highest_mAP = 8.53
Val_data_mAP: current_mAP = 8.73, highest_mAP = 8.73
tensor([0.3428, 0.4055, 0.4627, 0.4841, 0.4837, 0.4765, 0.4775, 0.4826, 0.4836,
        0.4986, 0.4651, 0.5045, 0.4805, 0.5017, 0.4887, 0.4062, 0.4046, 0.4532,
        0.4873, 0.5141, 0.4579, 0.4734, 0.4591, 0.4742, 0.4672, 0.4759, 0.4554,
        0.4888, 0.4718, 0.5024, 0.4684, 0.4976, 0.4888, 0.5121, 0.3989, 0.4729,
        0.4685, 0.4831, 0.5083, 0.4088, 0.4522, 0.4849, 0.4463, 0.4652, 0.4844,
        0.4696, 0.5057, 0.4939, 0.4668, 0.4957, 0.4555, 0.4884, 0.4899, 0.4790,
        0.5052, 0.4547, 0.6149, 0.4808, 0.3394, 0.4793, 0.5385, 0.4680, 0.3885,
        0.5586, 0.4680, 0.4974, 0.4877, 0.5334, 0.4330, 0.4813, 0.4419, 0.4564,
        0.5145, 0.4833, 0.5747, 0.5458, 0.4486, 0.4709, 0.4932, 0.4955],
       device='cuda:0')
Max Train Loss:  tensor([18.3037, 10.3739, 13.6167, 11.0024, 11.2483, 12.0954,  6.8489,  9.5691,
        10.9915, 13.2063, 10.8150,  5.3288, 10.3896, 17.2972, 12.0254,  7.7790,
        10.0771, 15.2818, 10.6892,  7.8906, 11.1750,  7.5758,  8.9848, 11.7334,
        13.1758, 12.0254, 12.7155,  9.1186,  7.4546, 13.3181, 10.9643, 14.9224,
        16.6330,  9.5885,  7.5264, 10.4187, 13.9995, 11.0881, 12.9279, 14.9612,
        10.3920, 11.0675,  9.6417, 10.9619,  6.5729, 11.8741,  7.5372,  7.5446,
         8.9941, 10.4104, 10.0561, 10.9215,  9.5103,  9.4193, 11.7669, 12.7707,
        21.3158, 10.5676,  8.7340, 12.3799, 14.8950, 13.8151,  7.5451, 10.6651,
         8.2925, 12.4636,  9.7006, 14.4296,  7.7895,  9.7740,  8.7783, 15.1057,
        13.5278,  9.5786, 15.0031,  9.5194,  8.3504, 12.0204,  9.7966, 13.3429],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [12/80], Step [000/642], LR 1.0e-04, Loss: 21.3
Max Train Loss:  tensor([23.9503, 10.7101, 15.4777, 10.5810, 12.0106, 18.7161, 18.0823, 15.4096,
        10.1335, 17.6470, 11.8752, 17.1307, 10.3368, 18.6506, 14.8591,  7.4594,
        10.9957, 12.2756, 20.3520, 15.5063, 11.8110, 10.0930,  9.4087, 11.3884,
        13.4045, 12.4439, 14.0056, 18.4703, 11.6973, 10.8421, 11.0175, 12.0961,
        17.3243, 13.7053,  9.0526, 12.9167, 12.9055, 14.6211, 15.6098, 12.2207,
        10.2409, 19.2857,  9.9689, 13.0748, 21.6881, 16.0712, 16.9312, 14.1584,
        14.3120, 18.4168,  8.8360, 17.8154, 19.5458, 10.7269, 15.0148, 10.4488,
        20.2106, 14.0617,  7.9875, 11.0379, 21.8607, 11.0457,  3.9356, 13.4974,
         8.4585, 22.0267, 13.1204, 22.3688,  7.9882,  9.2419,  7.1104,  9.1563,
         9.6691, 18.4981, 14.8363, 14.3256,  8.3597, 10.8697, 14.7484, 10.6018],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [12/80], Step [100/642], LR 1.0e-04, Loss: 24.0
Max Train Loss:  tensor([13.7311,  8.3124, 11.0191, 15.4719,  8.4093, 12.8974,  8.7096, 13.7272,
        12.4590, 13.2331,  8.5863, 10.6267,  8.7043,  9.7009,  7.8067, 11.1348,
        11.7764,  7.6864, 11.5826, 13.8630, 10.3873, 11.5192, 10.1891, 10.9982,
        11.9060, 15.2451, 20.0686, 10.5848, 10.5533,  8.4987, 10.4787,  9.1800,
         6.8205, 13.0286,  5.1780,  8.5156, 13.8845, 10.0523, 11.9743, 12.4225,
        10.4297, 13.2960, 10.6440, 14.3202,  8.7445, 15.9007, 12.6523, 11.8656,
         9.5211, 11.1666, 10.9713, 11.8923,  8.7934, 10.1862, 10.1381,  9.8131,
        19.6959, 11.4588,  8.0150, 14.0155, 15.0771, 10.1494,  9.7677, 15.0702,
         8.6382,  7.3834, 11.3976, 18.7415,  9.0289, 12.6470,  5.9248, 10.1682,
        15.5371, 16.5665, 12.3902, 17.5580,  7.6512, 12.7626, 14.5233, 10.1359],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [12/80], Step [200/642], LR 1.0e-04, Loss: 20.1
Max Train Loss:  tensor([14.3376, 11.9000, 14.9009, 12.6051,  8.7627,  9.1603, 13.9185,  9.8703,
        14.5620, 12.9195, 11.6870,  9.4050, 10.2788,  9.2811, 10.4369, 10.1959,
        14.5392,  7.6062, 11.5259, 11.8012,  6.8945, 10.0845,  8.8944,  9.9630,
        18.5361, 13.5982, 11.7583, 14.7987, 10.9247,  6.9820, 17.9492, 10.3842,
        10.7342, 10.9566,  5.8602, 10.0192, 10.6936, 11.7592,  9.7327, 10.2763,
         6.8078, 14.9912,  8.1954,  9.9409,  7.9788, 13.8027,  9.6285, 11.8271,
         9.0408,  6.5782,  9.2854, 10.8644,  9.4656, 10.9672, 10.8866,  8.2719,
        12.0290,  9.8402,  6.6772, 11.5057, 14.5479, 10.3014,  4.6564, 13.5235,
         8.2337,  7.7385, 10.4824,  7.4004,  7.4768, 11.9713,  5.8836, 10.0261,
        10.4015, 12.3630, 16.2920, 14.3395,  8.4238, 10.4859, 13.9034,  9.1112],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [12/80], Step [300/642], LR 1.0e-04, Loss: 18.5
Max Train Loss:  tensor([19.4605,  8.9854, 16.9086, 11.8083, 11.4904, 11.0577, 11.5620, 13.7790,
        12.6344, 11.2692, 10.4821, 11.4384,  9.4099, 12.6010,  8.4671,  7.8569,
         8.1515, 12.8568, 12.4572,  9.1601, 12.1040,  9.4055, 11.4017, 11.2954,
        11.2626, 12.2873,  9.9666, 14.5896,  9.5726,  5.4022, 15.4382,  7.4817,
        10.2958, 12.2332,  8.0972,  9.6556, 10.8019,  7.9323,  9.8325,  9.8132,
         6.9625, 12.6197,  6.6162,  8.5875,  5.4288,  8.5269,  8.5709, 12.2293,
         8.8599,  9.2799,  8.2402,  8.8939,  8.6832,  8.5173, 10.3847,  7.3636,
        15.1444, 10.3347,  4.8228, 12.6128, 14.2982, 11.7986,  6.2683, 10.7653,
         9.8502,  7.2874, 11.7950, 11.0933,  7.6504, 10.0839,  6.5576, 11.6979,
         9.9388, 13.9597, 10.5455, 10.1207,  8.4506,  9.3856, 13.9288, 10.3030],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [12/80], Step [400/642], LR 1.0e-04, Loss: 19.5
Max Train Loss:  tensor([15.6612,  6.8365, 14.2914, 10.5908, 10.2426, 10.8283,  9.5304, 11.0618,
        11.7442, 11.5139,  8.5905, 10.5921,  9.4957, 12.1012,  7.2271,  9.8333,
        11.5905, 10.6091, 10.1733, 13.3643,  8.9886, 10.4192, 11.5277,  9.3316,
        12.2541, 10.8154, 12.5184,  8.5000,  9.7631,  6.9775, 10.5508,  8.0820,
        11.9110,  8.9324,  8.0149, 11.3372, 11.2677, 10.0038, 14.8643, 11.9302,
         8.3119, 15.6286,  9.6780, 11.1635,  9.4122, 12.9638,  8.2012, 12.0215,
        10.0231,  7.8631,  8.2299, 10.5225,  9.8419, 10.0692, 10.1644,  9.2753,
        17.4270,  8.5402,  6.9323,  9.8341, 14.5338,  9.1236,  5.9283,  9.4618,
         8.6578,  9.7258, 12.7292, 12.8549,  7.9652, 13.4039,  6.8506, 11.5592,
        12.3962, 17.3375, 11.3606, 13.7212,  6.8323, 12.1433, 14.5752,  9.4750],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [12/80], Step [500/642], LR 1.0e-04, Loss: 17.4
Max Train Loss:  tensor([16.2944,  8.5210, 12.7269, 10.9956,  8.5848, 10.6845, 10.7382, 13.0836,
         8.8244,  6.2796,  8.8530,  5.6159,  8.9802, 12.5646, 11.6994,  7.5270,
        14.2107, 14.8290,  7.9415, 10.3019,  9.1022, 10.0195, 11.9818, 10.3779,
         8.2837,  9.6491, 12.8429,  7.7866,  8.4178,  7.3916,  7.5375,  8.4541,
        10.2319,  4.9995, 12.2046, 12.7240,  7.0499, 11.3533,  9.8959, 13.4533,
         7.7934, 15.8271, 10.7858, 16.1468, 10.4432, 16.7371, 11.9546, 13.2647,
         9.2531, 10.2116, 13.8950, 13.5102, 10.3119, 10.0868, 10.5301, 10.1026,
        15.4872,  9.6987,  6.0662,  9.2497, 16.6667, 11.9781,  5.6777, 11.8567,
         9.3947,  9.2283, 11.4467, 11.3358,  9.0759,  9.6901,  6.1551, 13.4482,
        11.3411, 13.9469,  7.9192, 10.1680,  8.1275,  9.4599, 14.2586, 10.3079],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [12/80], Step [600/642], LR 1.0e-04, Loss: 16.7
Max_Val Meta Model:  tensor([ 20.5019,  25.6274,  40.5281,  23.6582,   8.8089,   9.1308,   8.3585,
         12.5902,   9.1875,   8.2142,   9.1060,   7.2866,   9.1916,  11.5456,
          6.9335,   5.8742, 123.4964,   6.2525,   7.3968,   5.8450,   6.7301,
          8.2129,   4.3779,   4.1523,  15.6702,   9.4144,  13.5211,   4.7394,
          9.5816,   7.0717,   5.3722,   5.5692,   8.5829,   4.4740,   4.9122,
          8.2399,   5.3257,   7.7430,   4.9041,  21.2778,   8.5931,  13.3502,
          8.0114,  12.9217,   8.5958,  13.0587,   6.7105,  11.6240,   7.9635,
          7.1959,   6.9512,   8.7070,   9.8406,   8.3257,   9.1558,   6.0210,
         16.2522,   8.0463,   8.1295,   4.0158,  11.5450,  14.4899,   2.7927,
          7.2044,   7.3626,   5.0080,  10.3167,   6.9775,   8.4827,  13.1576,
          5.6386,  11.5614,  14.1523,   9.3187,  10.2502,   9.4379,   6.5569,
          8.8975,  13.7997,   7.8365], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 18.2758,  25.0271,  34.6209,  23.8117,   8.0802,   9.6429,   8.3162,
         13.2169,   9.1150,   8.3860,   9.0416,   7.2885,   9.1620,  11.6638,
          6.9704,   5.9045, 123.3555,   6.2136,   7.3628,   5.7784,   6.7009,
          8.1734,   4.2218,   4.0736,  15.5257,   9.2173,  14.2473,   4.7701,
          9.6576,   6.7156,   5.4658,   5.5059,   9.0877,   4.4394,   4.8761,
          8.1991,   5.3817,   7.6438,   4.9309,  21.1449,   8.4443,  13.1994,
          7.9193,  12.9164,   8.4843,  12.0143,   6.6526,  11.7017,   7.9078,
          7.2401,   6.9115,   8.6293,   9.7537,   8.2857,   9.0833,   5.9838,
         14.9074,   8.0922,   7.9332,   3.8875,  10.5710,  14.4516,   2.7885,
          7.1206,   7.3237,   4.9106,  10.2309,   7.5187,   8.3783,  13.3140,
          5.5998,  11.5595,  14.3916,   9.2289,  10.3601,   8.9465,   6.5185,
          8.8748,  13.7588,   7.7834], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 53.3078,  61.7223,  74.8213,  49.1828,  16.7050,  20.2391,  17.4174,
         27.3866,  18.8484,  16.8206,  19.4420,  14.4462,  19.0676,  23.2490,
         14.2635,  14.5349, 304.8717,  13.7105,  15.1085,  11.2393,  14.6337,
         17.2640,   9.1961,   8.5903,  33.2320,  19.3689,  31.2841,   9.7588,
         20.4700,  13.3668,  11.6683,  11.0645,  18.5936,   8.6695,  12.2230,
         17.3370,  11.4878,  15.8239,   9.7000,  51.7268,  18.6736,  27.2229,
         17.7439,  27.7642,  17.5136,  25.5848,  13.1559,  23.6927,  16.9388,
         14.6064,  15.1727,  17.6689,  19.9094,  17.2982,  17.9795,  13.1602,
         24.2447,  16.8306,  23.3717,   8.1100,  19.6300,  30.8825,   7.1771,
         12.7477,  15.6483,   9.8719,  20.9759,  14.0945,  19.3503,  27.6653,
         12.6707,  25.3269,  27.9710,  19.0964,  18.0269,  16.3917,  14.5322,
         18.8456,  27.8987,  15.7093], device='cuda:0')
Outer loop valEpocw Maximum [12/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 123.5
model_train val_loss valEpocw [12/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 123.4
model_train val_loss  valEpocw [12/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 304.9
Max_Val Meta Model:  tensor([31.9841, 31.9265, 39.4902, 40.0051, 36.6256, 42.3799, 43.4407, 41.5470,
        42.2849, 42.4050, 43.7938, 44.0956, 39.8742, 42.7585, 41.7510, 32.9532,
        34.6559, 37.6254, 41.6108, 45.5800, 38.4992, 39.0611, 36.9839, 40.2887,
        39.1865, 39.0357, 37.9083, 39.0941, 39.7719, 39.6780, 38.0435, 40.7934,
        40.0625, 40.4261, 32.9791, 38.0491, 37.7798, 37.9593, 41.1349, 32.8571,
        37.5090, 40.0766, 36.5600, 38.5765, 38.8451, 40.8400, 40.4305, 39.9035,
        38.7050, 40.0673, 37.8373, 38.6437, 39.9618, 39.3185, 40.3083, 37.3553,
        40.6842, 39.5930, 27.9911, 38.4687, 42.0366, 36.6487, 38.7886, 44.2312,
        38.0909, 40.9848, 40.4498, 41.0837, 36.0104, 41.4615, 37.3442, 37.9339,
        43.7837, 40.0147, 43.3655, 42.2195, 36.0807, 37.3681, 41.4993, 40.5315],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([22.5623,  7.3422, 22.8162,  8.5913, 10.5820, 17.6624, 42.2329, 24.0066,
        17.2748, 16.3555, 11.6382, 84.3723,  9.1869, 10.0532,  8.9352,  4.9859,
         6.0800,  9.2480,  6.3503, 12.7956,  5.9909,  6.9444,  3.9512,  4.6789,
        13.4524, 10.0947, 15.0022,  7.4369,  9.5752,  3.0282,  4.4646,  4.5408,
         5.4532,  3.6479,  4.0907,  6.8574,  5.7810,  5.3557,  3.8098,  4.9817,
         5.0108,  4.2662,  5.3959,  7.1372,  3.2355,  3.9229,  5.4732,  9.3394,
         6.8161,  6.2836,  5.9044,  7.0120,  7.0860,  7.0748,  7.6987,  6.1879,
         5.8475,  5.3580,  4.4087,  3.0267,  3.1120,  4.5710,  1.5160,  4.5311,
         6.2613,  3.9894,  8.9886,  7.4700,  4.9330,  3.9634,  4.8691,  4.6380,
         8.5724,  7.4006,  7.6888,  4.5254,  5.4571,  6.7555, 12.3181,  6.5435],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 61.8464,  18.7742,  50.8106,  17.8302,  22.6650,  36.2544,  85.6863,
         48.9091,  36.1980,  32.6410,  23.1267, 168.4961,  19.2644,  19.6410,
         18.1859,  12.6527,  15.0594,  19.9199,  13.0620,  24.0368,  12.9123,
         14.8088,   8.5834,   9.8736,  28.4382,  21.3229,  33.1092,  15.3185,
         20.0805,   6.1239,   9.6651,   9.1809,  11.2113,   7.2046,  10.1696,
         14.8893,  12.5385,  11.5916,   7.5393,  12.6882,  10.8155,   8.8330,
         12.0810,  15.4716,   6.6267,   8.2600,  10.9902,  19.3202,  14.5447,
         12.6853,  12.8697,  15.1658,  14.5662,  14.8516,  15.5411,  13.4222,
          9.8910,  11.1816,  13.2349,   6.3022,   5.9953,  10.0008,   3.6813,
          8.3680,  13.3369,   7.8391,  18.3493,  14.8611,  11.2503,   7.8895,
         10.5884,  10.2540,  16.8856,  15.3685,  14.1560,   8.7470,  12.2971,
         15.0023,  25.0580,  13.3978], device='cuda:0')
Outer loop valEpocw Maximum [12/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 45.6
model_train val_loss valEpocw [12/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 84.4
model_train val_loss  valEpocw [12/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 168.5
Max_Val Meta Model:  tensor([30.6216, 32.2574, 41.9456, 40.7830, 37.3051, 42.7168, 41.0666, 41.6859,
        41.5824, 42.7749, 42.6191, 42.2367, 40.8402, 43.2355, 40.2896, 33.7122,
        35.0722, 37.0522, 42.0417, 39.8206, 39.0067, 39.6233, 41.2467, 43.1931,
        40.4645, 40.7645, 37.9775, 39.9222, 41.1781, 39.1480, 40.8877, 41.7855,
        42.9582, 41.6621, 33.2036, 38.9717, 39.1972, 40.0814, 41.8687, 33.3024,
        37.5120, 41.5947, 38.3669, 39.8776, 40.1589, 40.3214, 43.0681, 43.1823,
        40.7581, 40.7127, 38.0950, 40.9529, 43.7365, 40.0074, 42.0752, 39.4206,
        57.3605, 40.3887, 28.0580, 37.9767, 50.3930, 37.1134, 38.0385, 43.2772,
        38.6420, 41.4066, 40.9654, 41.1882, 36.3212, 42.1466, 36.8847, 38.2272,
        41.7623, 40.5328, 42.5647, 40.8999, 36.8643, 38.9741, 41.8249, 41.3051],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 7.5278,  5.4104,  2.9263,  8.5764,  4.8198,  5.2156,  5.1725,  8.8436,
         9.1198,  4.0252,  9.7165,  4.5803,  9.2507,  9.5039,  4.5538,  5.5856,
         5.8323,  6.8971,  7.9114,  5.5091,  6.8306,  8.9156,  4.1565,  3.8954,
         6.7337,  5.2257, 13.1946,  7.5223,  8.8121,  3.5770,  5.9206,  6.9300,
         9.0845,  4.6066,  5.4847,  9.0477,  9.4702,  8.7462,  5.2711, 16.0734,
        14.1883, 34.6152, 38.0572, 34.9619, 31.4821, 34.7347,  9.9227, 14.2829,
        21.2057,  8.9177, 17.5520, 22.0979, 26.7583, 14.4100, 30.0836, 45.4286,
        79.5862,  9.5326,  5.6946,  4.5186, 42.7663,  6.4395,  5.9035, 10.0497,
        10.6113,  5.9579, 12.5471, 12.1703,  7.3029,  3.9950,  6.3558,  8.1005,
        10.4529, 17.8778,  4.4031, 10.8364,  8.9046,  8.8946, 14.4435,  8.3799],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 21.3503,  13.7038,   6.3110,  17.6507,  10.2807,  10.5903,  10.6818,
         17.6966,  18.6304,   7.9759,  19.4511,   8.8766,  19.0812,  18.2801,
          9.3070,  13.9348,  14.3616,  15.1999,  16.0137,  10.8043,  14.6713,
         18.9065,   8.7218,   7.9381,  13.5795,  10.7601,  28.9653,  15.4110,
         18.0888,   7.0918,  12.2332,  13.8478,  18.1833,   8.9192,  13.6647,
         18.9504,  20.1292,  18.2462,  10.3541,  40.6471,  31.2149,  71.0546,
         84.7421,  75.6248,  64.8751,  73.2215,  18.9947,  28.0848,  44.5752,
         17.8977,  38.7407,  45.9701,  54.6576,  30.1437,  60.2113,  98.8130,
        132.5550,  19.7021,  16.9027,   9.3995,  81.3065,  14.1710,  14.4115,
         18.3490,  22.5556,  11.5727,  25.5434,  23.8941,  16.6967,   7.9096,
         14.1191,  17.9065,  20.6765,  37.2593,   7.9588,  20.2564,  19.8599,
         19.1037,  29.3834,  16.9893], device='cuda:0')
Outer loop valEpocw Maximum [12/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 57.4
model_train val_loss valEpocw [12/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 79.6
model_train val_loss  valEpocw [12/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 132.6
Max_Val Meta Model:  tensor([33.3352, 30.3439, 37.3367, 39.4449, 34.9231, 37.8793, 39.4312, 39.7974,
        40.1393, 40.8492, 41.3094, 40.5256, 38.3739, 37.8284, 38.4117, 32.2571,
        33.9467, 35.2814, 40.4684, 37.9548, 36.8492, 37.5150, 38.7704, 39.1076,
        38.8121, 38.8555, 36.7551, 37.5854, 39.9763, 36.6218, 38.2143, 39.5150,
        40.0189, 44.3941, 31.3384, 37.0217, 39.9235, 38.0203, 38.1319, 31.7151,
        35.7463, 38.9576, 35.1060, 37.0955, 36.7977, 37.6020, 39.5682, 37.9865,
        37.9586, 36.4466, 36.8949, 37.5087, 40.8333, 41.1298, 43.2523, 35.4996,
        38.3312, 37.7798, 26.7260, 39.4430, 39.9410, 35.6127, 38.8248, 42.9154,
        36.5752, 39.5245, 39.1464, 46.4093, 34.5283, 40.4583, 35.6481, 37.0683,
        40.7409, 38.3766, 55.7553, 37.8002, 34.6174, 36.0995, 40.4776, 40.6320],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 14.0336,   4.4072,  13.0240,   7.3457,  11.1208,   9.9210,   7.1883,
         11.1352,   7.7055,  10.9360,   9.0524,   6.7776,   7.7740,  10.2793,
          9.5202,   4.5368,   4.7711,   5.7995,   6.8729,   5.6325,   6.6839,
          7.5377,   4.9199,   4.3437,  12.5171,   4.3454,  12.6702,   4.0040,
         10.3896,   3.4126,   4.4384,   4.9926,   4.8119,   4.0945,   4.4842,
          7.5206,   4.9509,   5.7228,   4.0237,   7.2867,   5.4859,   6.4702,
          5.9199,   7.7385,   3.6645,   5.8029,   6.2763,  10.0442,   7.4636,
          5.7517,   6.7077,   7.7449,   7.9505,   7.8439,   8.6948,   5.5586,
          3.8939,   4.7553,   5.9656,   3.7553,   8.3018,   5.5627,   1.7089,
          5.4049,   6.8265,   4.5951,   9.8473,   5.7587,   5.4274,   2.9045,
          5.3482,   5.5236,   7.9331,   8.4229, 159.0774,   6.5352,   5.9740,
          8.1248,  13.1511,   7.2286], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 38.9564,  11.2652,  29.0223,  15.2220,  23.8117,  20.7313,  14.8766,
         22.6642,  15.8884,  21.8411,  18.3187,  13.2411,  16.2968,  20.8221,
         19.5514,  11.4700,  11.8421,  12.6695,  14.1605,  11.1261,  14.4393,
         16.1216,  10.4495,   9.1277,  25.5230,   9.0423,  27.9790,   8.2397,
         21.3091,   6.9386,   9.3137,   9.9942,   9.7541,   7.8479,  11.2144,
         16.2064,  10.3231,  12.1367,   7.9219,  18.5369,  11.8947,  13.4164,
         13.2425,  16.8179,   7.5172,  12.2511,  12.3618,  20.7510,  15.8571,
         11.7849,  14.0799,  16.6312,  15.8688,  16.1623,  16.9658,  12.1365,
          6.5634,   9.9180,  17.8559,   7.6944,  15.8630,  12.2442,   4.1616,
          9.7649,  14.5603,   8.9869,  20.0955,  10.9139,  12.3657,   5.7570,
         11.6577,  12.2059,  15.8088,  17.5506, 289.2463,  12.6057,  13.4674,
         18.0137,  26.7045,  14.6596], device='cuda:0')
Outer loop valEpocw Maximum [12/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 55.8
model_train val_loss valEpocw [12/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 159.1
model_train val_loss  valEpocw [12/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 289.2
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [84.64199998 97.2137279  91.26472631 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 91.64483864
 96.90915072 96.22689782 96.9627563  93.65870299 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.05030397 96.13796128 96.24273583 96.9067141
 90.78593097 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [81.46708739 97.2137279  90.69090289 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.13216213
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 90.05616403 96.13796128 96.24273583 96.9067141
 90.87852244 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [94.77589243  3.7207611  55.433262    4.33932626  8.4916989  24.00254093
  4.33843756 24.90548733  2.26993424 17.55823529  1.59385237  2.01132623
  0.8243809  10.22513921  2.49938279  3.95524917  3.85697541  1.97305923
  0.87277421  1.28652464  1.2307585   0.46122474  1.14766481  1.51824182
 11.91271741  7.72100228 21.03564956  4.67044068  3.55492961  1.48913525
  2.80790866  0.93012408 20.23784203  1.55597554  1.61266165  1.56901845
  3.05469072  2.66787582  2.7085767  20.35917144  6.93478086 34.59767968
  5.76544761  8.10361018  7.54089785 27.59623994  2.87975252  1.99070101
  3.02499022  2.23804264  1.27970427  1.43634596  1.33776283  3.6914533
  1.91822633  4.60462734 58.26739782 12.90023767 10.34288338  5.60364324
 51.30914388  3.75811107 11.9137477   8.2386166   4.04184764  6.03459748
  4.68187139  9.0656321   2.96356026  6.97219773  0.44002458  8.32748643
  4.66592678 11.37022826  7.79892403 12.90918828  1.33756401  2.39656371
  0.16880469  0.85025624]
Accuracy th:0.5 is [45.50748651 97.2137279  72.62216591 97.02489005 97.26733349 77.54047831
 77.87551321 76.76928887 78.76975183 96.38405965 79.13158953 98.52097319
 99.41399349 80.2012646  78.64670265 96.56680596 96.29512311 78.35065362
 98.65376884 98.30776915 80.26218004 79.63475104 98.38695922 78.47004788
 80.81894714 96.65086926 94.0778012  78.07409754 98.01293844 78.94396998
 97.30875598 98.57457877 96.36213009 98.02024829 86.28184354 78.60406184
 78.39573105 89.6480306  97.11504489 75.58509277 79.36915973 92.05906361
 77.85723858 77.43570376 96.9627563  93.87434364 98.02877645 98.57336046
 87.65122257 87.05425129 86.51941375 98.55508583 98.99976852 78.00952717
 98.70615611 78.36283671 72.99984162 92.95695715 96.24273583 96.9067141
 89.79300934 97.17717864 90.90776185 78.39329443 98.42838172 78.86477991
 98.20786784 77.48809103 79.46053289 97.54267126 80.05750417 95.99054592
 79.17666695 95.45083515 77.70860491 82.18467124 85.72507645 79.03290652
 80.11476468 99.14718388]
Accuracy th:0.7 is [45.51114143 97.2137279  72.62216591 97.02489005 97.26733349 77.54047831
 77.87551321 76.90330284 78.76975183 96.46934126 79.13158953 98.52097319
 99.41399349 80.61792619 78.64670265 96.56680596 96.29512311 78.35065362
 98.65376884 98.30776915 80.69589795 79.63475104 98.38695922 78.53827317
 81.34038328 96.65086926 94.0778012  78.07409754 98.01293844 78.94396998
 97.30875598 98.57457877 96.36213009 98.02024829 86.52306868 78.60406184
 78.39573105 89.9538261  97.11504489 75.58509277 80.02826476 92.05906361
 77.85723858 77.43570376 96.9627563  93.87434364 98.02877645 98.57336046
 89.7454953  87.8644266  86.6790122  98.55508583 98.99976852 78.00952717
 98.70615611 78.36283671 72.99984162 93.37361874 96.24273583 96.9067141
 89.79300934 97.17717864 91.11609264 78.39329443 98.42838172 78.86477991
 98.20786784 77.48809103 79.46053289 97.55972759 80.05750417 95.99054592
 79.2107796  95.45083515 77.70860491 82.24680499 85.83594254 79.03290652
 80.11476468 99.14718388]
Avg Prec: is [55.680676    3.09926424 11.09697541  3.26637507  2.32018961  3.71502192
  3.30593683  5.64001277  2.43090833  3.714754    1.52075051  1.56928654
  0.58973283  5.10043945  2.59163577  3.1918745   3.65917648  2.74120009
  1.33469599  1.81035244  1.93756048  0.86792215  1.82231686  2.50423616
  5.08298766  3.56388275  6.42676499  3.30827539  2.08114804  1.97761714
  2.60891367  1.31459464  3.75339343  1.66104497  2.36972121  2.35187856
  3.06780899  2.61229132  2.8328075   7.52348332  2.27936209  8.22063546
  3.35272679  4.02745696  3.22301874  6.51770926  2.02537215  1.56188098
  2.05528149  1.56089939  1.88141721  1.63183476  1.14450586  2.97524498
  1.27500696  2.64190543 11.21160514  3.69256591  3.95795482  2.8183681
 10.90167672  2.18340536  3.82542466  3.02431769  1.59637905  2.48878552
  1.84564935  4.14511851  1.35691795  2.50500628  0.19376251  3.4466263
  1.92025915  4.54113753  3.89946898  3.18871759  0.78679256  1.87298777
  0.12199529  0.75608529]
mAP score regular 9.28, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [87.00949249 97.22450607 91.73082193 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 91.27239206
 97.07750953 96.48703192 97.03764606 93.41007051 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.64205098 96.39235618 96.16314124 96.78102499
 90.60717044 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [85.05119964 97.22450607 91.60874007 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.44587289
 97.07750953 96.48703192 97.03764606 94.02546279 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.04160749 96.39235618 96.16314124 96.78102499
 91.33717019 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [95.88542454  4.15895418 60.6878356   4.48410584  8.68517744 23.00353975
  4.7151792  27.19163103  2.40383659 18.58167215  1.62574307  2.16782432
  0.84522701 11.21377188  2.6351894   4.56947408  3.98199249  1.9847598
  0.77513122  1.29643556  1.09978732  0.46756584  1.16338431  1.44556512
 12.61206116  8.28169757 20.86488413  4.23817543  4.13674085  1.54204066
  2.6393699   0.83872833 24.24064677  1.47179984  1.40365926  1.39925722
  2.87165828  3.15371434  2.80908813 20.44453384  7.45238943 35.6546169
  5.55497473  8.10421672  7.50020763 28.08517968  3.02109648  2.06147412
  3.13823339  2.09394745  1.26015978  1.58258226  1.72277185  3.95009565
  2.12387487  4.54532351 54.2972456  12.79880683 10.16423473  6.35008565
 51.78351508  3.82228825 13.56735943  9.16298506  4.77483334  6.15224998
  5.41846725 10.18819033  2.75184875  7.5996641   0.5566118   8.67649519
  4.71829343 12.15448476  9.79970339 13.05816927  1.3783651   2.62974812
  0.17863691  0.80743286]
Accuracy th:0.5 is [45.36711762 97.22450607 71.04168224 96.96290206 97.90716795 76.5926701
 76.68485437 75.47400154 78.11744774 96.41477938 78.31676508 98.5325261
 99.34972718 78.29185041 78.19468321 96.31262924 96.21047911 77.6565264
 98.78167277 98.34068316 79.06669656 78.95208909 98.31327703 77.73625333
 78.38403468 96.52938685 94.3393876  77.67396666 97.81747515 78.27441015
 97.52597354 98.67204823 96.39983058 98.18870369 86.69307621 77.89072427
 77.83342053 91.36955926 97.0276802  74.89847273 77.9779256  92.37362035
 77.01372798 76.67239704 97.03764606 94.02795426 98.18621222 98.77668984
 88.09577198 86.88990209 84.98393004 98.55993223 98.87385704 77.01871092
 98.6969629  77.686424   71.53000972 93.98061639 96.16314124 96.78102499
 90.13379176 97.04761193 90.63208511 77.63908613 98.32075143 78.48369335
 98.13139996 76.80444478 78.9745123  97.53095647 79.42795924 96.07843137
 78.68550216 95.44559882 76.74215811 83.12031293 87.41809303 78.33171388
 79.49772031 99.15040985]
Accuracy th:0.7 is [45.67356803 97.22450607 71.04168224 96.96290206 97.90716795 76.5926701
 76.68485437 75.53379675 78.11744774 96.41976231 78.31676508 98.5325261
 99.34972718 78.69048509 78.19468321 96.31262924 96.21047911 77.6565264
 98.78167277 98.34068316 79.39806164 78.95208909 98.31327703 77.74123627
 78.74031442 96.52938685 94.3393876  77.67396666 97.81747515 78.27441015
 97.52597354 98.67204823 96.39983058 98.18870369 86.98457782 77.89072427
 77.83342053 91.52901313 97.0276802  74.89847273 78.41891522 92.37362035
 77.01372798 76.67239704 97.03764606 94.02795426 98.18621222 98.77668984
 90.2434163  87.29601116 85.11846924 98.55993223 98.87385704 77.01871092
 98.6969629  77.686424   71.53000972 94.2795924  96.16314124 96.78102499
 90.13379176 97.04761193 90.81147071 77.63908613 98.32075143 78.48369335
 98.13139996 76.80444478 78.9745123  97.53593941 79.42795924 96.07843137
 78.69297656 95.44559882 76.74215811 83.20751426 87.54266637 78.33171388
 79.49772031 99.15040985]
Avg Prec: is [53.85585563  3.71506303 14.87572792  4.53376576  1.4771652   4.24813292
 14.02254777  8.6608608   8.23258932  5.26484524  2.35745067  4.71394517
  2.33967011  5.84799488  2.97848779  3.67215147 24.01697756  6.48424539
  1.56359552  2.76215153  3.51528399  1.51522657  1.13126391  5.17334461
  5.62606945  8.96005786  7.82940223  4.55021743  3.87718452  5.55558724
  2.22614159  0.85798166  3.01808794  1.10853806  1.66077946  2.20953246
  1.98711088  2.22652864  2.24103236  6.18228626  1.72508872  5.99843174
  2.16981675  2.69817914  2.37424658  4.84470659  1.70291304  1.0244704
  1.32526978  1.14489378  1.20576508  0.9765507   0.73044082  2.30877977
  0.86607144  1.83837193  9.99864388  2.89932505  3.83763288  2.78578041
  7.81602716  2.05070598  3.09692059  2.49484857  1.33828523  1.8267208
  1.55527092  3.43322512  1.08929626  2.25956308  0.19237938  3.25470775
  1.58926586  3.90462336  3.53431056  2.31962978  0.59715803  1.508501
  0.1290769   0.59036433]
mAP score regular 9.61, mAP score EMA 4.30
Train_data_mAP: current_mAP = 9.28, highest_mAP = 9.28
Val_data_mAP: current_mAP = 9.61, highest_mAP = 9.61
tensor([0.3626, 0.3866, 0.4436, 0.4770, 0.4673, 0.4720, 0.4812, 0.4835, 0.4819,
        0.4952, 0.4930, 0.5131, 0.4704, 0.4917, 0.4852, 0.3895, 0.3988, 0.4579,
        0.4785, 0.5028, 0.4559, 0.4694, 0.4689, 0.4733, 0.4848, 0.4786, 0.4510,
        0.4822, 0.4854, 0.4832, 0.4727, 0.4967, 0.4902, 0.5107, 0.3976, 0.4556,
        0.4763, 0.4641, 0.5051, 0.3878, 0.4624, 0.4762, 0.4419, 0.4625, 0.4880,
        0.4735, 0.5004, 0.4774, 0.4657, 0.4873, 0.4672, 0.4587, 0.4992, 0.4826,
        0.5101, 0.4611, 0.5890, 0.4733, 0.3285, 0.4856, 0.5198, 0.4552, 0.4074,
        0.5494, 0.4656, 0.5089, 0.4874, 0.5220, 0.4382, 0.5037, 0.4572, 0.4535,
        0.5014, 0.4797, 0.5548, 0.5116, 0.4432, 0.4435, 0.4933, 0.4910],
       device='cuda:0')
Max Train Loss:  tensor([13.6846,  8.8564, 17.4483, 12.3015, 10.8101, 10.6743,  8.1717, 14.2453,
        13.2032,  9.8387, 10.6606,  6.3459,  9.9661, 11.7511, 13.5607,  7.4529,
        11.3022,  7.1161,  8.2015,  5.6829,  7.7041,  9.9621,  8.1298,  6.8628,
        14.7887, 14.6039, 12.4841, 10.6646, 10.8891,  7.7009,  8.6656,  8.0021,
        10.6622,  8.1460, 10.1202, 11.2437,  8.3793, 11.7876,  5.9973, 16.4418,
         9.8031, 16.4548,  9.1767, 14.2172,  8.3050, 11.3178,  6.5782, 10.4205,
        11.8799,  7.3734,  8.8925,  9.6778,  9.5411, 14.7728,  9.1794, 10.1007,
        17.5169,  8.3650,  4.9603,  6.2121, 15.5767,  7.6958,  6.8956, 10.2108,
        11.2252,  9.8577, 13.7898, 15.2935,  6.9992,  6.0327,  5.7918,  7.5064,
        10.5958, 12.8242, 10.0626,  6.7866,  7.5441,  9.4982, 13.7298,  7.7122],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [13/80], Step [000/642], LR 1.0e-04, Loss: 17.5
Max Train Loss:  tensor([20.6002,  5.9970, 16.9365, 12.0908, 12.8638, 14.4058, 12.2185, 15.3214,
        19.1920, 21.1776, 12.2815, 22.7217, 18.7361, 23.4842, 14.2057,  8.8389,
        10.6364, 11.0071,  8.7990, 15.1934, 10.5005, 21.5105, 11.3623, 10.9518,
        17.7066, 12.2720, 15.2872, 11.9755, 17.9263, 20.6740, 16.3102, 18.3685,
        16.8885, 15.7253,  7.0373,  9.4614, 13.9108, 14.2125, 16.8294, 11.7132,
         8.2435, 18.7179,  9.6632, 14.6447, 19.0123, 16.5055, 14.3993,  8.5154,
        11.5094,  8.6423, 10.0512,  8.9020, 13.8182, 17.6110, 13.6950,  9.0209,
        18.3302, 10.9076,  7.6943, 20.7922, 18.7992, 10.0621, 17.4855, 19.5975,
        10.3085, 15.8913, 10.5634, 19.4989,  9.3769, 16.9331,  8.7698, 13.4454,
         6.6124, 11.4062, 19.4872, 12.7951,  6.8434,  7.6374,  6.2211, 20.0076],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [13/80], Step [100/642], LR 1.0e-04, Loss: 23.5
Max Train Loss:  tensor([16.7614,  5.1735, 15.4337, 10.5886, 14.6064, 14.2489,  8.4760, 18.0437,
         8.3802, 12.3947, 12.4374,  9.2189,  5.8289,  8.3095,  8.3310, 12.8159,
        10.3142,  7.9016,  9.2916, 11.5695,  9.2984,  8.2069,  9.3507, 11.4529,
        13.9929, 12.1334, 14.7959, 11.4412,  9.8620, 11.3693, 13.0840, 11.1559,
        12.5220, 10.2462,  5.6751,  9.8772, 11.7711, 11.7658, 13.2490, 11.3331,
         8.9201, 15.2070,  7.5241,  9.9727,  9.8227, 13.1461,  9.4073,  6.8007,
        10.5559,  7.8142,  8.9977,  9.9968, 12.8818, 14.8205,  9.4729,  8.4911,
        17.8159, 13.1359,  5.9693, 10.3105, 11.7872, 11.6709,  9.4619, 16.6102,
         8.7496, 14.9748, 10.2591, 15.0085,  7.8874, 12.4974,  7.0140, 12.7284,
         6.1795,  9.7976, 11.2109, 13.4858,  5.5210,  6.4863,  4.6192,  9.6003],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [13/80], Step [200/642], LR 1.0e-04, Loss: 18.0
Max Train Loss:  tensor([16.2323,  7.9048, 15.6465, 11.7551,  9.3403, 12.6295,  7.9789, 10.8434,
         7.4899, 13.1926, 11.6158, 12.0195,  5.6511, 12.3655,  8.4331, 10.4864,
        10.9056, 13.6259,  9.0923,  9.7623, 10.1055,  7.1687, 10.3603,  7.2609,
        12.2628, 13.2489, 13.9093,  9.6115,  9.8969, 11.9705, 11.5050,  9.9723,
        10.0799, 13.9553,  5.7915,  7.7551,  6.7460,  8.1185, 10.4452, 13.4846,
         9.8237, 14.2313, 11.3380, 13.6159, 10.3789,  8.6956, 14.4309,  9.6421,
         8.8552, 11.1522,  9.0706,  9.8684, 11.2515,  7.8523,  9.3390,  8.0280,
        16.3618, 12.6899,  4.4295, 13.7074, 16.3809,  9.8268,  9.6812, 11.7307,
         9.5544, 14.5739, 12.4103, 11.7843,  7.2855, 13.2247,  6.8387, 14.9593,
         8.9113,  9.7204,  8.9062, 14.7454,  6.3815,  8.6985,  4.4625,  9.7993],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [13/80], Step [300/642], LR 1.0e-04, Loss: 16.4
Max Train Loss:  tensor([16.2850,  5.4452, 12.8257, 10.5902,  8.6512, 12.6347, 12.6877, 10.1065,
         9.4205,  9.3108, 12.2116, 12.2561,  6.4108, 11.3006, 11.7042, 10.2582,
         9.1509, 11.2327,  9.1063,  9.0923,  8.1429,  8.6978,  7.9817,  7.1373,
        15.3150,  8.6752,  9.6340, 11.7641,  6.0003, 12.6804,  9.0113, 11.1423,
         7.9081,  8.1213,  9.1825,  8.8754, 11.0790,  7.9857, 10.3060, 16.5764,
        13.9473, 13.4523, 12.0196, 14.3593, 13.6292, 13.0044, 10.4111, 12.9969,
        10.2435, 12.0026, 10.4342,  9.8570, 12.1411,  8.6202, 10.4481,  7.1190,
        15.0998, 10.7126, 10.6717,  9.0665, 16.5589, 12.0896,  9.4835, 10.1175,
         7.6773, 12.0357,  8.8888, 12.4728,  8.2936, 12.7200,  7.8142, 11.1427,
         9.9991, 11.5111, 15.1764, 10.2523,  5.4189,  7.8751,  4.5184,  7.0326],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [13/80], Step [400/642], LR 1.0e-04, Loss: 16.6
Max Train Loss:  tensor([15.7448,  5.2400, 12.8672,  9.4772,  6.9514, 13.4814, 10.0572, 11.5285,
        12.4544,  9.0908, 13.5826, 13.3525,  5.9747, 11.7599,  7.5491,  7.4805,
         7.5910, 10.9979, 12.7996,  7.8687,  9.8436,  9.1223,  6.5751, 14.0048,
        11.2911,  7.8493, 12.8666, 11.3259, 11.9809,  7.5486,  7.7563, 13.0664,
        11.5265,  8.1144,  7.3459,  8.1070,  8.5226, 12.9975, 13.1150,  9.7582,
         8.1250, 11.0803, 10.6246, 11.1148, 13.3521,  9.8053, 11.6357,  9.8851,
         9.6540,  8.9475,  8.7050,  9.4487, 11.6026,  9.9851,  6.3255,  6.6937,
        13.1180,  7.3521,  5.4489, 14.2037, 13.7044, 11.1784, 10.3825, 11.3743,
        10.1244, 10.6323, 11.4538, 13.5926,  7.8655, 10.9392,  7.1625,  9.9629,
         4.7320, 11.3958,  9.6224, 11.5263,  5.6674,  9.5784,  4.7797,  8.3328],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [13/80], Step [500/642], LR 1.0e-04, Loss: 15.7
Max Train Loss:  tensor([15.3627,  9.6902, 13.3369, 10.3266, 10.7388,  7.2857, 11.5896,  9.6238,
         8.6043,  8.7212,  9.9292,  5.5406,  6.6552, 14.5353, 12.1662, 11.0770,
        11.4490, 14.3475, 10.0898,  8.6925,  9.6068,  9.7650,  9.3904,  7.3640,
        14.0628,  5.9326,  8.2730,  9.6399,  8.5281, 11.4632, 11.1208,  9.0131,
        11.1397, 10.8004,  5.7214,  6.4900,  7.6646, 11.3545, 12.2339, 10.3042,
         9.7462, 12.4464,  7.4233,  6.8069, 10.6244, 11.5523, 10.5738, 10.1283,
         9.0866,  9.9197,  8.8100, 10.4151, 13.2641,  9.7675,  4.6616, 11.1130,
        16.4286, 12.3485,  5.6350, 12.1029, 16.5598,  7.9804,  9.6016,  8.0786,
         7.7586, 12.1591,  8.0337, 10.5152,  6.6699, 12.1408,  7.7226, 10.3572,
         7.2697, 12.1683, 10.9697, 11.0994,  7.1917, 11.2602,  4.6071,  9.5034],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [13/80], Step [600/642], LR 1.0e-04, Loss: 16.6
Max_Val Meta Model:  tensor([ 19.5288,  30.2732,  38.1620,  22.9608,   5.7231,   6.9704,   7.7680,
         11.0726,   7.5959,   9.2604,  10.6958,   7.7237,   6.6590,  11.7254,
          7.3891,   5.4476, 124.3075,   6.7620,   7.9617,   6.5263,   6.4883,
          7.3168,   6.3807,   6.7660,  14.5979,   9.1007,  12.5746,   6.2437,
          7.4188,   9.3696,   5.5152,   9.0325,   7.9181,   5.5116,   4.8475,
          5.5847,   6.6832,   7.2500,   7.0244,  19.1862,   9.8666,  12.3256,
          7.3916,  10.1437,   8.9898,  12.7316,   9.3878,   7.6295,   8.0567,
          8.5060,   7.8980,   8.0826,  12.6031,   4.6720,   3.4896,   4.1990,
         14.9140,   8.7214,   7.8761,   6.1300,   8.5808,  13.4329,   7.6035,
          6.4780,   7.7598,   5.2455,   8.0384,   6.4987,   8.4517,  15.4153,
          6.9747,  11.2193,  10.7825,   8.3197,   8.9153,  11.0635,   5.4771,
          6.4221,   4.5638,   7.1173], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 16.7901,  29.5332,  34.6949,  23.3670,   5.6528,   7.1749,   7.8233,
         12.3071,   7.7390,   9.6064,  10.9371,   7.7758,   6.8130,  12.4435,
          7.4889,   5.5468, 121.6993,   6.9998,   8.2262,   6.7342,   6.4586,
          7.5405,   6.5956,   6.5535,  14.7189,   8.8659,  13.1541,   6.8595,
          7.6702,   9.3532,   5.9047,   9.2919,   8.8750,   5.6562,   5.0339,
          5.9431,   7.3456,   7.2718,   7.4860,  18.8352,   9.9215,  12.3457,
          7.5251,  10.1893,   8.8505,  12.2395,   9.6232,   7.9196,   8.3162,
          8.8255,   8.1610,   8.3357,  12.8645,   4.7864,   3.5367,   4.2923,
         14.7049,   8.7556,   7.7204,   6.2979,   8.7657,  13.0364,   7.8279,
          6.5301,   8.0182,   5.4839,   8.2561,   7.1386,   8.6737,  15.5795,
          7.2173,  11.1483,  11.0247,   8.3622,   9.0882,  11.1131,   5.6851,
          6.6593,   4.7514,   7.3451], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 46.3005,  76.3891,  78.2177,  48.9920,  12.0972,  15.2017,  16.2587,
         25.4543,  16.0590,  19.3980,  22.1831,  15.1550,  14.4843,  25.3051,
         15.4355,  14.2410, 305.1273,  15.2858,  17.1899,  13.3946,  14.1676,
         16.0635,  14.0649,  13.8478,  30.3614,  18.5246,  29.1652,  14.2259,
         15.8015,  19.3550,  12.4912,  18.7056,  18.1042,  11.0747,  12.6617,
         13.0437,  15.4218,  15.6687,  14.8220,  48.5754,  21.4562,  25.9255,
         17.0293,  22.0303,  18.1363,  25.8497,  19.2326,  16.5874,  17.8565,
         18.1115,  17.4679,  18.1743,  25.7709,   9.9175,   6.9336,   9.3079,
         24.9643,  18.4999,  23.5038,  12.9698,  16.8646,  28.6396,  19.2157,
         11.8854,  17.2199,  10.7750,  16.9376,  13.6768,  19.7919,  30.9292,
         15.7847,  24.5831,  21.9860,  17.4307,  16.3813,  21.7207,  12.8281,
         15.0160,   9.6327,  14.9604], device='cuda:0')
Outer loop valEpocw Maximum [13/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 124.3
model_train val_loss valEpocw [13/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 121.7
model_train val_loss  valEpocw [13/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 305.1
Max_Val Meta Model:  tensor([37.5510, 33.1066, 38.3452, 44.1665, 38.4431, 42.7630, 44.6625, 41.5344,
        45.2851, 41.5992, 42.3426, 50.6857, 40.6037, 43.1469, 40.4581, 32.1357,
        33.7411, 38.3625, 41.9445, 46.1022, 39.3667, 39.6957, 39.2710, 42.1289,
        40.9667, 40.5194, 38.5551, 40.3436, 41.1682, 40.4171, 39.9581, 42.3579,
        41.0699, 41.8450, 33.9161, 38.2879, 40.3298, 38.7535, 42.9375, 36.8527,
        39.1357, 39.4674, 37.4291, 39.2585, 40.2952, 39.5212, 42.0834, 39.7110,
        39.9647, 40.9410, 39.3122, 38.8054, 42.3030, 40.6973, 41.5657, 38.3633,
        40.9073, 40.7623, 27.0965, 40.6010, 45.0121, 37.7760, 33.8159, 43.4743,
        39.1861, 42.3862, 41.1364, 40.9596, 37.2020, 43.1530, 39.0418, 41.3367,
        39.5818, 40.0273, 42.5521, 43.1725, 36.9534, 37.1043, 41.4935, 41.8403],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([25.9101,  6.5993, 23.2792,  8.9390,  5.4303, 16.1658, 46.7647, 21.0325,
        16.8966, 15.7636, 12.7772, 88.3358,  7.3812, 10.4478,  8.8939,  4.8916,
         5.9033,  9.6814,  7.3867, 13.2433,  6.4385,  6.5995,  5.7403,  7.6286,
        13.2279,  9.8922, 15.7377,  8.6604,  8.0688,  6.5768,  5.0249,  8.2624,
         5.1910,  4.8699,  4.3685,  4.9438,  7.5975,  5.4521,  6.3055,  3.6560,
         6.6941,  3.5950,  5.1725,  3.8038,  3.5575,  4.5731,  8.4529,  6.0663,
         7.4180,  7.9735,  7.1131,  7.2447, 10.4630,  3.9307,  2.5975,  4.1764,
         6.7714,  5.6102,  4.4243,  5.2227,  3.5800,  4.5315,  6.0562,  4.0498,
         7.1161,  4.3904,  7.2646,  7.9442,  5.3124,  8.8725,  6.4193,  4.9004,
         4.5160,  5.1471,  6.5082,  7.4306,  4.8959,  4.9511,  3.9924,  6.4323],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 69.4145,  17.0055,  53.2587,  18.4762,  11.7346,  33.3512,  93.8972,
         42.4822,  35.0163,  31.8141,  25.8665, 172.6931,  15.4621,  20.8412,
         18.4653,  12.8528,  15.0136,  20.9854,  15.1432,  25.4237,  13.8976,
         14.1381,  12.2068,  15.8625,  27.2789,  20.6708,  34.6822,  17.8549,
         16.5938,  13.6589,  10.6314,  16.6985,  10.6514,   9.6092,  10.8407,
         10.8366,  15.9335,  11.8689,  12.4004,   9.3458,  14.2630,   7.5445,
         11.5742,   8.2293,   7.2390,   9.5584,  16.8317,  12.7663,  15.6965,
         16.2350,  15.3214,  15.9667,  20.9097,   8.0758,   5.1102,   8.9896,
         11.5918,  11.7170,  13.7074,  10.7079,   6.6660,   9.9684,  15.1334,
          7.4039,  15.1083,   8.6465,  14.8281,  15.8541,  12.0304,  17.5581,
         13.7514,  10.6960,   9.4042,  10.6369,  12.2154,  14.5263,  11.0059,
         11.2601,   8.0990,  13.0545], device='cuda:0')
Outer loop valEpocw Maximum [13/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 50.7
model_train val_loss valEpocw [13/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 88.3
model_train val_loss  valEpocw [13/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 172.7
Max_Val Meta Model:  tensor([31.4541, 32.2547, 37.6567, 40.7289, 37.4242, 41.9183, 39.3321, 39.0874,
        38.5600, 41.2707, 37.6032, 39.6615, 39.2355, 42.6081, 38.3024, 31.2848,
        32.6170, 41.0319, 41.5727, 41.8295, 43.2394, 39.6980, 41.1342, 39.6122,
        37.3287, 38.8464, 37.5808, 38.9422, 39.0814, 39.5809, 39.5509, 40.3074,
        42.3177, 40.6632, 32.8587, 37.2095, 39.4747, 38.2815, 41.8588, 36.2971,
        38.4040, 40.5198, 37.6196, 38.9493, 40.2679, 40.5220, 42.9926, 38.6891,
        40.5118, 42.7942, 38.4529, 38.1399, 43.4304, 39.4756, 41.4425, 39.6264,
        54.4064, 39.5261, 26.0964, 38.7216, 50.2959, 37.5953, 32.5613, 40.1402,
        38.3662, 41.0296, 40.0567, 39.6288, 36.1043, 42.1576, 37.8881, 41.4028,
        37.9843, 40.1522, 40.8869, 38.5946, 35.9169, 36.0326, 40.0356, 40.5714],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 8.7705,  2.5810,  3.9581,  9.8539,  5.9295,  4.3940,  5.7016,  7.1354,
         7.7898,  5.9493, 11.6341,  5.9363,  7.4159,  9.3917,  7.1831,  5.7925,
         6.3468,  8.7499,  9.9852,  7.6675,  6.7972,  9.1733,  8.2752,  6.3865,
         5.7561,  6.1351, 12.1903,  9.7930,  6.6942,  9.3720,  7.7161, 11.4954,
         7.6714,  6.4043,  6.3601,  6.8886, 10.5256,  8.5769,  8.1389, 19.3247,
        15.8139, 32.6793, 36.0849, 34.3304, 27.8310, 32.6417, 13.5740, 10.9179,
        21.6196, 11.7549, 18.0905, 20.9184, 28.8415, 12.6070, 32.3708, 44.9599,
        59.0291, 13.1139,  5.8456,  8.5268, 43.8038,  6.9620, 10.4546, 10.8645,
        12.3067,  7.5646, 11.4091,  9.8373,  8.3029, 11.1554,  8.9295,  9.9542,
         6.0721, 19.3182,  3.7858, 12.8816,  8.6529,  7.1770,  6.0429,  8.7504],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 23.8038,   6.6958,   9.0416,  20.6326,  12.8149,   9.0916,  11.8171,
         14.5776,  16.2478,  11.9118,  24.5218,  11.6878,  15.5664,  18.7550,
         15.0349,  15.2469,  16.1880,  18.6291,  20.4726,  15.1306,  13.9761,
         19.2200,  17.1833,  13.4735,  12.2533,  12.8476,  26.8115,  20.2650,
         13.7733,  19.2765,  16.0842,  23.4234,  15.4501,  12.6116,  15.8082,
         15.0987,  22.0307,  18.4984,  15.9853,  49.5732,  33.8732,  68.7757,
         81.0546,  74.3173,  56.7249,  68.2006,  26.3552,  22.9651,  45.5753,
         23.0122,  39.0646,  46.0688,  57.2956,  25.9897,  64.0797,  96.7530,
        102.5979,  27.4686,  18.2742,  17.5435,  82.7075,  15.3915,  26.2809,
         20.2634,  26.1028,  14.9023,  23.3364,  19.5175,  18.8335,  22.0185,
         19.1827,  21.7015,  12.6359,  39.8726,   7.0741,  25.8057,  19.5026,
         16.3169,  12.2878,  17.8098], device='cuda:0')
Outer loop valEpocw Maximum [13/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 54.4
model_train val_loss valEpocw [13/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 59.0
model_train val_loss  valEpocw [13/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 102.6
Max_Val Meta Model:  tensor([33.5849, 32.5264, 37.8027, 40.9008, 37.6880, 42.5201, 39.7402, 39.8829,
        38.9227, 41.9252, 38.0071, 39.9416, 39.5707, 40.0668, 38.8428, 31.3912,
        32.8952, 38.3093, 41.3728, 42.0756, 39.8759, 39.8743, 40.0599, 39.9232,
        38.4902, 39.3038, 38.4804, 39.4575, 40.4529, 39.6427, 38.7003, 38.5613,
        41.5177, 44.3743, 33.2664, 37.2468, 38.2727, 39.3658, 41.3708, 35.7331,
        38.5057, 38.4943, 36.6036, 38.3181, 39.3325, 40.8130, 39.2650, 38.8955,
        39.8112, 38.8492, 38.8032, 37.7842, 40.9391, 41.7960, 41.2320, 37.3125,
        38.8722, 41.1894, 26.5166, 40.6431, 40.1470, 37.7191, 33.1297, 42.3940,
        38.5337, 41.1566, 40.5405, 45.0566, 36.5826, 42.5500, 38.6516, 39.0590,
        38.8709, 39.3619, 53.0479, 38.6621, 36.1579, 35.9988, 40.6571, 42.1628],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 19.1881,   1.8561,  12.8704,   8.3077,   6.5070,   7.9306,   6.5024,
         12.6362,   6.8717,  12.2285,  10.5606,   7.3013,   6.1467,  10.0974,
          9.7730,   4.6280,   5.1148,   7.1429,   8.4180,   7.1256,   7.9190,
          7.8070,   6.7918,   8.6021,  10.2629,   5.1471,  12.0186,   6.1631,
          8.6013,   7.5889,   5.5477,   9.1086,   4.7770,   5.9508,   5.1327,
          5.4446,   6.6827,   6.1739,   6.7497,   5.7538,   7.7269,   5.1865,
          6.0289,   4.9540,   4.6507,   6.1461,   9.5924,   7.0179,   8.5227,
          8.0704,   8.2017,   8.2430,  11.7374,   4.9085,   3.4097,   4.1829,
          3.7716,   6.3800,   5.9927,   6.3578,   6.4897,   6.3247,   7.0382,
          5.6018,   8.1716,   5.2478,   8.4887,   6.4965,   6.2115,   9.4251,
          7.4775,   6.6616,   3.6135,   7.2541, 167.4077,   9.2262,   5.7264,
          6.5741,   4.7814,   7.5663], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 51.5063,   4.8096,  29.6206,  17.4060,  14.0825,  16.4190,  13.5036,
         25.9439,  14.3915,  24.4329,  22.3900,  14.4491,  12.9552,  20.7777,
         20.5204,  12.2605,  13.0970,  15.4257,  17.4134,  14.1166,  16.9045,
         16.4461,  14.3006,  18.2093,  21.9738,  10.8303,  26.5399,  12.7611,
         17.7270,  15.7461,  11.7672,  19.0862,   9.6850,  11.3219,  12.7639,
         12.0763,  14.3639,  13.2871,  13.4176,  14.8447,  16.4805,  10.9434,
         13.5657,  10.7705,   9.4919,  12.3489,  19.5079,  14.8375,  18.0471,
         16.6891,  17.6357,  18.3887,  23.5309,   9.9306,   6.7030,   9.0402,
          6.5714,  13.1523,  18.6671,  12.9048,  12.5218,  13.9590,  17.6180,
         10.2663,  17.3848,  10.2853,  17.3170,  12.4957,  14.0706,  18.6338,
         15.9313,  14.7302,   7.5238,  14.9753, 312.7869,  18.6693,  12.9335,
         15.1632,   9.7110,  15.2358], device='cuda:0')
Outer loop valEpocw Maximum [13/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 53.0
model_train val_loss valEpocw [13/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 167.4
model_train val_loss  valEpocw [13/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 312.8
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [84.40686639 97.2137279  91.18919116 97.02489005 97.26733349 96.5997003
 96.99808726 94.74299777 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72426018 97.84237521 92.33927462
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.52178945 96.13796128 96.24273583 96.9067141
 91.527881   97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [84.04746531 97.2137279  90.5386143  97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.06515515
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 90.88826891 96.13796128 96.24273583 96.9067141
 90.83831825 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [94.80854905  4.26809028 54.51039826  4.19920964  2.62159986 16.1161323
  3.12991662 31.88421187  2.34046521 17.63082353  1.5892493   1.76545602
  1.00942576  9.62050177  2.05099941  3.4137673   3.47896948  2.07543565
  0.79697667  1.14357658  1.30836859  0.44080388  0.84910657  1.94342768
 16.09259739  5.94682264 23.98539046  6.67584897  3.29167526  1.36967895
  1.96509669  0.88643305 20.14613119  1.56862625  1.64779474  2.16676639
  4.69302347  7.19659375  3.55520822 24.09852222  8.17273081 38.99484828
  7.85636622 20.50013432 14.27829712 29.54489452  2.51157503  1.90294315
  3.31477594  1.88322988  1.37691026  1.59508666  1.39722352  6.61987514
  2.93533575  8.53958713 60.60894296 15.85725351  9.27887044  3.81302839
 57.28580833  5.8183142   9.95864937  9.45368417  3.47773261  6.0272948
  3.8717878   8.20706282  2.74891987  5.32018021  0.53667298 12.69516322
  4.50438219 17.67036156  9.35919184  6.83158936  1.26998966  2.1561622
  0.21085219  0.86998507]
Accuracy th:0.5 is [45.53063437 97.2137279  72.27860284 97.02489005 97.26733349 77.07752099
 77.456415   76.41597934 78.36283671 96.38649627 78.67594206 98.52097319
 99.41399349 80.04532109 78.23491429 96.56680596 96.29512311 78.04607644
 98.65376884 98.30776915 80.04653939 79.23027254 98.38695922 78.11917496
 80.50827841 96.65086926 94.0778012  77.62575992 98.01293844 78.56873089
 97.30875598 98.57457877 96.36213009 98.02024829 85.81157637 78.21663966
 77.9571399  89.46406598 97.11504489 75.14650163 78.9464066  92.05906361
 77.50149243 77.0239154  96.9627563  93.87434364 98.02877645 98.57336046
 87.82787734 86.88977961 86.48164618 98.55508583 98.99976852 77.61479514
 98.70615611 77.94617512 72.69770105 92.78395731 96.24273583 96.9067141
 89.79300934 97.17717864 91.07710676 78.04242151 98.42838172 78.48223097
 98.20786784 77.10310547 79.04874453 97.54267126 79.62865949 95.99054592
 78.8075194  95.45083515 77.27976024 81.77288288 85.34618243 78.58944214
 79.68104677 99.14718388]
Accuracy th:0.7 is [45.58423996 97.2137279  72.27860284 97.02489005 97.26733349 77.07752099
 77.456415   76.56095808 78.36283671 96.47299619 78.67594206 98.52097319
 99.41399349 80.48878547 78.23491429 96.56680596 96.29512311 78.04607644
 98.65376884 98.30776915 80.50096856 79.23027254 98.38695922 78.19958334
 81.06260889 96.65086926 94.0778012  77.62575992 98.01293844 78.56873089
 97.30875598 98.57457877 96.36213009 98.02024829 86.02965363 78.21663966
 77.9571399  89.74062207 97.11504489 75.14650163 79.68713831 92.05906361
 77.50149243 77.0239154  96.9627563  93.87434364 98.02877645 98.57336046
 90.00864999 87.78645484 86.67413896 98.55508583 98.99976852 77.61479514
 98.70615611 77.94617512 72.69770105 93.20427383 96.24273583 96.9067141
 89.79300934 97.17717864 91.28421925 78.04242151 98.42838172 78.48223097
 98.20786784 77.10310547 79.04874453 97.55972759 79.62865949 95.99054592
 78.85381513 95.45083515 77.27976024 81.87034758 85.46923161 78.58944214
 79.68104677 99.14718388]
Avg Prec: is [55.80074573  2.99097521 11.08028704  3.28831196  2.2371633   3.91249221
  3.27609682  5.56155489  2.43126025  3.80388261  1.69226754  1.64230813
  0.63548682  5.13085767  2.58607011  3.18936376  3.62783422  2.72155967
  1.33656247  1.72354272  1.97370988  0.8767937   1.87012424  2.43208115
  4.9218077   3.56629834  6.59828902  3.3310784   1.98245824  1.85391985
  2.51532028  1.32928537  3.6736043   1.69265917  2.3312036   2.32999006
  3.13211281  2.48198556  2.78513378  7.41361711  2.29017332  8.17056433
  3.43659361  4.05015451  3.2288186   6.42727373  2.059019    1.48809536
  2.08756285  1.67550002  1.83049827  1.58082933  1.06799893  2.96963894
  1.40423977  2.73684798 11.11393462  3.65420621  3.97676827  2.80252609
 10.83340614  2.25418876  3.82309219  3.0220326   1.56649122  2.46777499
  1.87097941  4.24218532  1.24668905  2.38431611  0.19095182  3.43717014
  1.89728636  4.56036575  3.98056694  3.21937671  0.90495063  1.87502314
  0.13540652  0.74107639]
mAP score regular 9.97, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [86.79522635 97.22450607 91.48167526 96.96290206 97.90716795 96.63651992
 96.80843112 94.75047961 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.73986596 97.82744101 92.66013902
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.76413285 96.39235618 96.16314124 96.78102499
 91.94010514 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [87.01447542 97.22450607 91.24000299 96.96290206 97.90716795 96.63651992
 96.80843112 94.87505294 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.39853502
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.68938884 96.39235618 96.16314124 96.78102499
 91.6211974  97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [96.02695143  4.54608284 58.93500501  4.13801773  2.30468566 17.02410461
  3.20540893 32.71021403  2.62104436 19.49374741  1.67968712  1.76761773
  1.17307826 10.07393904  2.20552202  3.61152318  3.50047963  2.01386536
  0.70823948  1.13468503  1.15950341  0.45130932  0.87022941  1.86550399
 16.25565566  6.22437139 23.50935396  5.82142891  3.73725599  1.44809855
  1.80744305  0.81279262 25.26137028  1.72630048  1.44108146  1.99111652
  4.58513863 12.19617935  3.70654355 24.56524862  9.57213139 39.42324075
  8.32064582 22.26091678 16.37440657 30.30196726  2.48352565  1.78499714
  3.89941608  1.97223134  1.45076621  1.79038108  1.85644394  8.46180813
  3.52944863  8.7661072  55.3571588  14.72754401  8.72328821  4.0474723
 59.06017578  6.50929208  9.96407219 10.13638051  3.80989479  5.66456388
  4.41026997  9.08661997  2.45482902  5.50130127  0.48031166 12.97064548
  3.99421928 18.71476458 11.5844648   6.50654667  1.32926303  2.23005794
  0.17811364  0.80803873]
Accuracy th:0.5 is [45.39701522 97.22450607 70.9245833  96.96290206 97.90716795 76.47058824
 76.5627725  75.35939408 77.99536587 96.41477938 78.18970028 98.5325261
 99.34972718 78.21710641 78.08755014 96.31262924 96.21047911 77.53942746
 98.78167277 98.34068316 79.0044099  78.82502429 98.31327703 77.60918853
 78.32423948 96.52938685 94.3393876  77.55188479 97.81747515 78.14734534
 97.52597354 98.67204823 96.39983058 98.18870369 86.62829808 77.7686424
 77.7163216  91.32969579 97.0276802  74.7963226  77.90567307 92.37362035
 76.88666318 76.55031517 97.03764606 94.02795426 98.18621222 98.77668984
 88.47447492 86.84754715 84.92164337 98.55993223 98.87385704 76.89164611
 98.6969629  77.56434213 71.43782545 93.95071879 96.16314124 96.78102499
 90.13379176 97.04761193 90.68440591 77.52198719 98.32075143 78.37157735
 98.13139996 76.68236291 78.84744749 97.52846501 79.30089444 96.07843137
 78.55843735 95.44559882 76.62007624 83.04806039 87.41809303 78.20464908
 79.3706555  99.15040985]
Accuracy th:0.7 is [45.69349976 97.22450607 70.9245833  96.96290206 97.90716795 76.47058824
 76.5627725  75.43164661 77.99536587 96.41976231 78.18970028 98.5325261
 99.34972718 78.62321549 78.08755014 96.31262924 96.21047911 77.53942746
 98.78167277 98.34068316 79.35819817 78.82502429 98.31327703 77.6266288
 78.73533149 96.52938685 94.3393876  77.55188479 97.81747515 78.14734534
 97.52597354 98.67204823 96.39983058 98.18870369 86.92478262 77.7686424
 77.7163216  91.4866582  97.0276802  74.7963226  78.36908588 92.37362035
 76.88666318 76.55031517 97.03764606 94.02795426 98.18621222 98.77668984
 90.66447418 87.22625009 85.07860578 98.55993223 98.87385704 76.89164611
 98.6969629  77.56434213 71.43782545 94.24720333 96.16314124 96.78102499
 90.13379176 97.04761193 90.86628298 77.52198719 98.32075143 78.37157735
 98.13139996 76.68236291 78.84744749 97.53593941 79.30089444 96.07843137
 78.56591175 95.44559882 76.62007624 83.14024466 87.53519197 78.20464908
 79.3706555  99.15040985]
Avg Prec: is [53.87284901  3.71678135 14.85405561  4.53548326  1.48332803  4.25529171
 13.82962224  8.66039003  8.14018895  5.24990028  2.33439252  4.74738243
  2.30010509  5.847455    2.92322107  3.62967309 24.08565109  6.43957602
  1.55812157  2.73597881  3.51767726  1.51457121  1.23901344  5.19832998
  5.63393868  9.1136928   7.84676543  4.59357678  3.94884431  5.51166107
  2.22082291  0.85448759  2.91947973  1.15277265  1.6970872   2.25042032
  1.98606818  2.17144257  2.19164619  6.18981365  1.7156303   6.03237447
  2.17142184  2.69611969  2.33738149  4.87636041  1.68697442  1.01751796
  1.41583536  1.1376244   1.21088114  1.00864319  0.74479336  2.26342259
  0.88169233  1.85212001 10.14147549  3.03826272  3.96249754  2.91299896
  7.8479753   2.07268778  3.26607224  2.49934128  1.34881735  1.92510644
  1.52634211  3.52185685  1.07456581  2.23357323  0.19495173  3.24590484
  1.60100433  4.00932031  3.19624051  2.30628302  0.56947013  1.45053809
  0.12113601  0.60865973]
mAP score regular 10.36, mAP score EMA 4.31
Train_data_mAP: current_mAP = 9.97, highest_mAP = 9.97
Val_data_mAP: current_mAP = 10.36, highest_mAP = 10.36
tensor([0.3836, 0.3734, 0.4156, 0.4666, 0.4619, 0.4707, 0.4775, 0.4695, 0.4693,
        0.4885, 0.4668, 0.4978, 0.4608, 0.4729, 0.4725, 0.3667, 0.3839, 0.4676,
        0.4648, 0.4966, 0.4592, 0.4701, 0.4696, 0.4627, 0.4576, 0.4669, 0.4493,
        0.4764, 0.4824, 0.4598, 0.4595, 0.4660, 0.4881, 0.5076, 0.3998, 0.4318,
        0.4554, 0.4420, 0.4919, 0.3798, 0.4777, 0.4620, 0.4367, 0.4599, 0.4940,
        0.4949, 0.4749, 0.4614, 0.4637, 0.4804, 0.4676, 0.4268, 0.4927, 0.4811,
        0.4994, 0.4692, 0.5485, 0.4718, 0.3112, 0.4862, 0.5008, 0.4513, 0.3952,
        0.5229, 0.4657, 0.4973, 0.4861, 0.5052, 0.4424, 0.5045, 0.4758, 0.4500,
        0.4753, 0.4807, 0.5234, 0.4598, 0.4354, 0.4137, 0.4937, 0.4872],
       device='cuda:0')
Max Train Loss:  tensor([19.7887,  7.2468, 14.4560, 11.1712, 12.8591, 11.1885, 15.2930, 11.7997,
         7.3556,  8.7911, 10.1985,  7.1167,  5.6682, 15.2855, 13.2221,  5.0926,
        11.1457,  8.7399,  9.0678,  8.8599,  8.5896,  7.3215, 10.9487, 10.2621,
        10.1368,  9.9224, 11.4389,  8.4299,  8.9097, 11.4902,  7.7942, 10.3128,
        10.8403,  7.7634,  6.5415,  6.9945,  9.2177, 16.5124, 11.7517, 11.2514,
         9.7317, 12.6364,  7.1003,  5.9757,  5.6076, 11.8168, 10.9497,  9.5810,
         8.8189,  8.8261,  7.9012,  7.5143, 12.8427, 11.3264,  6.8372,  8.3939,
        18.6025, 10.2256,  6.0499,  8.2162, 11.6588,  7.4615,  8.7422,  7.4321,
         8.7916,  8.6504,  9.0711, 15.8741,  7.6403,  9.7581,  8.0628,  7.8568,
         4.6871, 10.8791, 11.1207,  8.2470,  7.5414,  5.2425,  4.5952,  8.4496],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [14/80], Step [000/642], LR 1.0e-04, Loss: 19.8
Max Train Loss:  tensor([19.3331,  6.0189, 13.2975, 10.2216,  8.5031, 18.2953, 12.0459, 16.6177,
        10.0527, 13.9161,  6.6350, 13.8822,  8.1292, 13.8415, 10.9158,  8.0596,
         7.6774, 10.9429, 10.4160, 16.2730, 11.2905, 19.6892,  9.2411, 13.5061,
        20.7480, 10.0280, 15.2606, 12.1609, 18.7290, 12.3179,  9.9886, 11.8789,
        19.5479, 15.5774,  8.6597, 12.5595,  9.4475, 13.5527, 14.1443,  9.1884,
        18.6608, 19.8497, 11.9911, 11.4120, 15.7292, 20.0845,  9.8745,  9.4630,
        11.7568, 16.7100, 15.8511,  7.3450,  7.3444, 13.8410, 20.9137, 10.9484,
        11.7465, 11.3510,  6.9250,  9.7915, 20.1858, 10.4863,  7.4290, 17.1587,
         8.3190, 12.6811, 13.8502, 15.7737, 10.9046, 12.1572, 20.4251, 13.7075,
        15.1847, 15.0305, 14.3890, 10.6055,  7.3571,  6.3527, 18.1214, 15.1545],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [14/80], Step [100/642], LR 1.0e-04, Loss: 20.9
Max Train Loss:  tensor([16.8945,  9.4058, 16.6405,  6.9167,  6.9048, 13.7427, 10.4065, 14.3758,
        10.8795, 10.6807,  6.3104, 10.3042,  7.5701, 12.1030,  9.4138,  3.8404,
         5.7808, 12.7623,  8.6203, 11.6558,  9.8948,  7.5687, 10.5124, 14.4769,
        12.7217, 12.7427, 11.6469, 10.9292,  9.8610,  9.7003, 11.3718, 12.2729,
        12.2416, 11.6117,  8.2252, 11.8388, 11.8960,  9.3332, 12.6037, 13.2287,
        10.9087, 13.5915, 10.4999, 14.0250, 12.2559,  9.4532, 10.0686,  8.9252,
         9.2999,  8.0166, 11.6521,  7.1995,  7.1943, 11.9867,  9.1992,  8.9113,
        16.3733, 11.2298,  5.3869,  9.5066, 15.5103,  9.7905, 10.0600, 12.0091,
         8.3664, 13.8770, 12.9895, 17.4077,  7.1896, 12.0346,  7.6295, 13.4855,
        13.7525,  8.3456, 11.6177, 11.5903,  9.8178,  6.4749,  8.2390, 10.3627],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [14/80], Step [200/642], LR 1.0e-04, Loss: 17.4
Max Train Loss:  tensor([15.8873,  7.9471, 12.2292, 12.1320,  9.2173, 11.5191, 12.9701, 11.5894,
         9.5626,  8.7849,  6.2393, 10.2020,  7.4838, 10.0230, 10.5110,  5.6030,
         9.8392, 12.3674,  7.5057, 10.5010, 10.4330,  8.6749, 10.6503, 14.0872,
        15.2429, 17.6914, 13.2166, 11.4352, 12.4112,  8.4955, 13.9036, 11.2150,
        10.3703, 12.3612,  4.9897,  7.9708, 12.4148, 11.0924,  8.9809, 13.1657,
         7.9862, 11.8200, 10.6410, 10.0018,  9.7304, 11.0284, 11.2937,  7.5703,
        10.4905,  8.1377, 11.4368,  6.5130,  8.0756,  9.8867,  4.2209,  8.6545,
        13.9470, 10.4420,  6.6083, 10.9405, 10.6979,  9.5817,  8.0506,  8.6027,
         8.9673, 11.9351,  5.3683, 10.5232,  7.2303,  8.9754,  8.3134, 11.1805,
        10.2602,  9.1042,  9.5237, 10.1474, 10.5272,  7.6861,  8.1437,  9.3227],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [14/80], Step [300/642], LR 1.0e-04, Loss: 17.7
Max Train Loss:  tensor([14.6598,  7.8047, 13.4414, 11.7726,  9.2291, 15.6506, 12.7030, 12.2106,
         9.6532,  9.1996,  9.7883, 13.5408,  9.0988, 11.6471, 10.4612,  7.8814,
        13.8581,  9.4134,  6.8366,  6.6785, 13.3254,  8.3430, 11.4745,  5.9256,
        12.8827,  9.0399, 12.4585,  7.4843,  9.8555,  7.5388, 10.0016,  6.1608,
        12.4929,  7.9483, 10.9118, 12.2405, 11.1638, 10.6073,  7.8748, 12.1265,
         8.8073, 13.5577,  9.4875, 12.7814, 12.4079, 13.0199, 10.4767,  7.4548,
        12.5937,  5.9314,  7.2261,  6.1201,  6.8706, 10.7066,  4.0650,  9.3107,
        15.1650, 12.2771,  5.1914,  9.0372, 10.5999,  8.7856,  7.9903,  9.5288,
         8.4100, 10.5472,  5.3610,  9.9076,  8.9142, 10.6904,  7.4274, 10.3853,
         8.0980, 10.4025,  9.2124,  9.1600,  5.6128,  8.7385,  8.0255,  9.2003],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [14/80], Step [400/642], LR 1.0e-04, Loss: 15.7
Max Train Loss:  tensor([16.3698,  9.8216, 14.7502, 14.9301, 12.4223, 10.2057, 10.0383, 15.8308,
        12.0713,  7.8620,  8.8759, 11.7012,  9.5910, 16.8344, 12.0320,  8.7381,
         7.9540, 12.7651,  8.2520,  9.8439,  8.9492, 10.4281,  9.9133, 12.2235,
        13.3846, 10.2506, 16.9517,  8.0165,  8.3899,  9.4997,  7.5428,  5.1788,
        10.5267,  7.4149,  5.1596,  6.0670,  7.9578,  9.2354, 10.5736, 10.9392,
         8.4635, 13.3480,  6.9232,  9.5029,  9.4939,  7.8470, 10.4259,  8.5734,
        10.3240,  9.5456,  6.6516,  6.7248,  6.2589,  7.9986,  5.3401,  7.7551,
        16.5371, 11.9178,  9.6425,  6.0071, 16.6951,  7.6250,  8.1988, 10.3427,
         8.6040, 14.4011,  6.8786,  9.1611, 10.4389, 13.5496,  7.7632, 11.1959,
        11.3362,  9.7775,  9.0468, 13.8998,  7.0511,  6.5181,  9.1884, 11.1121],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [14/80], Step [500/642], LR 1.0e-04, Loss: 17.0
Max Train Loss:  tensor([15.6136,  8.7013, 12.4831,  7.2840, 10.1867,  9.2190,  8.9357,  9.5083,
        12.8688,  9.0789,  7.1358, 10.8779,  7.2722, 12.7170,  9.0544,  8.9942,
         9.8755,  5.3836,  7.5769, 11.4912, 10.4233, 10.5587,  8.4672,  5.8794,
        13.4965, 12.7470, 13.8796, 12.1443,  9.6368,  8.5009,  6.6936, 10.9609,
         7.7541,  9.2772,  5.6797,  6.6016,  8.5276, 10.5655,  7.6329, 13.0304,
         6.2711, 13.0748, 12.3808, 13.3307, 12.9070, 13.6106, 11.2200,  8.3545,
        13.7838,  8.9229,  8.6583,  7.4583,  6.8845,  8.3949,  6.6930, 11.2342,
        14.1867,  9.6178,  4.6613,  9.3333, 14.8554,  7.2620,  4.6380,  9.3577,
         7.5548, 10.6323,  5.4012, 12.1336,  7.1719,  8.7717,  7.3356, 11.2608,
         8.7414,  9.8066, 12.3239, 11.9878,  6.5119,  7.7958,  9.6952,  9.9479],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [14/80], Step [600/642], LR 1.0e-04, Loss: 15.6
Max_Val Meta Model:  tensor([ 21.9134,  23.5488,  38.6180,  24.6618,   5.4329,   7.6892,   7.3708,
          9.6409,   8.4067,   9.6395,   7.0223,  10.9002,   8.0720,  10.8759,
          7.2864,   2.7528, 113.7995,   4.1152,   5.4214,   5.5651,   7.7641,
          7.3946,   6.4712,   6.0087,  15.2438,   9.8502,  13.6209,   5.5623,
          8.8063,   9.6344,   4.7491,   3.3219,   7.6402,   5.7348,   4.9773,
          5.7718,   5.5073,   8.1491,   5.5436,  19.2333,   9.4408,  12.2366,
          7.5950,  11.5348,  11.8623,  12.8762,   8.4504,   8.4108,   9.1361,
          6.7409,   6.3403,   5.3580,   7.6529,   6.4092,   4.0686,   6.7432,
         14.9963,   8.2686,   7.5691,   4.7525,   8.5203,  14.4018,   5.4575,
          6.8968,   7.6685,   9.8639,   4.3512,   7.0842,   8.8790,  14.6129,
          7.4524,  11.1541,  11.7041,   7.9129,   9.0015,  10.2047,   5.6216,
          5.9844,   8.0555,   8.1818], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 17.8580,  23.0802,  32.9854,  21.9186,   5.0866,   7.8793,   7.3127,
         10.6924,   8.4568,  10.2136,   7.0895,  11.0647,   8.2089,  11.5265,
          7.3861,   2.7628, 114.3185,   4.2263,   5.3976,   5.6501,   7.8625,
          7.4933,   6.3399,   6.0232,  15.7264,   9.7967,  14.2565,   5.8608,
          9.0014,   9.6208,   5.0208,   3.3968,   8.2925,   5.7389,   5.0684,
          5.8549,   5.8821,   8.6727,   5.9931,  19.1381,   9.3709,  12.2723,
          7.6845,  11.8108,  12.1092,  12.1936,   8.5131,   8.5672,   9.2491,
          6.8804,   6.3785,   5.4515,   7.5501,   6.3376,   4.1499,   6.8278,
         14.4081,   7.9568,   7.3007,   4.6934,   8.1309,  14.0645,   5.5349,
          6.9314,   7.7792,  10.0119,   4.3381,   8.1900,   9.0420,  14.7637,
          7.5765,  11.1338,  12.0858,   7.7554,   9.0358,  10.1305,   5.7158,
          6.0667,   8.1729,   8.2393], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 46.5546,  61.8137,  79.3683,  46.9795,  11.0117,  16.7386,  15.3144,
         22.7737,  18.0182,  20.9072,  15.1881,  22.2256,  17.8148,  24.3745,
         15.6327,   7.5348, 297.7974,   9.0377,  11.6129,  11.3779,  17.1236,
         15.9408,  13.5020,  13.0170,  34.3647,  20.9827,  31.7289,  12.3014,
         18.6578,  20.9244,  10.9259,   7.2886,  16.9876,  11.3062,  12.6765,
         13.5604,  12.9174,  19.6227,  12.1824,  50.3858,  19.6174,  26.5645,
         17.5959,  25.6828,  24.5143,  24.6373,  17.9245,  18.5672,  19.9448,
         14.3213,  13.6419,  12.7735,  15.3228,  13.1720,   8.3091,  14.5519,
         26.2680,  16.8653,  23.4608,   9.6539,  16.2355,  31.1640,  14.0070,
         13.2561,  16.7032,  20.1306,   8.9242,  16.2107,  20.4386,  29.2670,
         15.9238,  24.7442,  25.4252,  16.1347,  17.2630,  22.0313,  13.1280,
         14.6660,  16.5551,  16.9099], device='cuda:0')
Outer loop valEpocw Maximum [14/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 113.8
model_train val_loss valEpocw [14/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 114.3
model_train val_loss  valEpocw [14/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 297.8
Max_Val Meta Model:  tensor([33.7996, 32.1636, 39.1955, 45.5424, 43.3799, 43.4205, 43.0961, 40.6734,
        40.5609, 40.8251, 41.0054, 50.6629, 41.3523, 44.4890, 44.7077, 32.0159,
        34.1812, 39.0650, 38.0024, 43.7105, 40.7426, 40.9122, 40.0644, 46.8010,
        47.7226, 39.4497, 46.2020, 40.5143, 48.9272, 40.7417, 40.1842, 41.0693,
        42.0818, 41.9506, 34.8251, 38.3288, 39.9102, 39.3581, 42.8720, 38.0127,
        40.0223, 39.6288, 37.9893, 39.8244, 41.3883, 41.8197, 41.4697, 39.8382,
        40.6279, 40.9989, 39.8466, 38.2351, 41.9760, 41.7015, 42.1986, 39.5809,
        46.6457, 41.5218, 35.4591, 40.0713, 45.4551, 42.8696, 35.1036, 46.2086,
        39.6180, 43.1458, 41.3016, 41.1797, 37.8862, 43.6034, 40.4631, 41.2107,
        41.6890, 41.6847, 43.0850, 41.4731, 37.2961, 36.9318, 42.4685, 42.5743],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([22.2546,  7.2997, 23.3869,  5.1619,  5.1396, 17.2997, 40.6801, 20.9534,
        17.2002, 16.8977,  9.9503, 77.0268,  8.7681,  8.2903,  9.0685,  2.7101,
         6.0053,  8.1510,  4.7627, 12.2932,  6.8904,  6.4174,  6.7472,  6.5583,
        13.1969,  9.8585, 15.4355,  7.8469,  9.0998,  6.7248,  4.0864,  2.8294,
         5.1650,  4.9036,  4.2356,  5.0731,  6.3956,  6.6743,  4.5920,  4.1393,
         5.3280,  2.8471,  5.1015,  5.6769,  7.3417,  3.1652,  7.3943,  6.5115,
         8.0975,  5.9593,  5.2087,  4.6708,  4.9175,  5.1356,  3.3590,  6.5950,
         6.3572,  4.5804,  4.1999,  3.5875,  2.5192,  4.4849,  4.0307,  4.6620,
         6.6371,  8.5288,  3.4199,  7.8387,  5.4191,  7.0482,  6.4216,  4.2978,
         6.2865,  4.9828,  6.4917,  6.9352,  4.8084,  4.5841,  6.8974,  6.9662],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 58.1043,  19.3384,  53.1508,  10.5213,  10.7435,  34.8232,  83.1345,
         43.2142,  36.2966,  35.1617,  20.9237, 153.2382,  18.3534,  16.5242,
         18.6342,   7.2238,  15.3589,  17.4845,  10.2173,  24.0898,  14.6309,
         13.5294,  14.2381,  13.3538,  28.2905,  21.0473,  34.0373,  16.3264,
         18.7719,  14.0714,   8.7453,   5.9150,  10.5436,   9.5571,  10.4264,
         11.3051,  13.7351,  14.5637,   9.1523,  10.5462,  11.2347,   6.0402,
         11.4304,  12.3326,  14.9140,   6.3239,  15.1308,  13.8921,  17.1131,
         12.2491,  11.1921,  10.5496,   9.9456,  10.5018,   6.6519,  14.0729,
         11.0512,   9.5208,  12.5036,   7.3642,   4.8045,   9.6213,  10.0072,
          8.6373,  14.1288,  16.9247,   7.0030,  16.0135,  12.2480,  13.9526,
         13.4639,   9.3583,  13.0061,  10.2642,  12.2632,  14.3266,  10.8884,
         10.6426,  14.0111,  14.1222], device='cuda:0')
Outer loop valEpocw Maximum [14/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 50.7
model_train val_loss valEpocw [14/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 77.0
model_train val_loss  valEpocw [14/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 153.2
Max_Val Meta Model:  tensor([33.2490, 32.7567, 41.2247, 42.7069, 42.5949, 43.5669, 41.7898, 40.2743,
        41.5360, 39.9680, 38.0213, 40.2446, 41.0581, 43.2882, 42.2775, 32.1295,
        34.1335, 37.8736, 38.5128, 42.5719, 40.2831, 41.0411, 42.5032, 42.4819,
        39.6900, 38.5217, 38.2472, 42.6831, 38.9444, 41.7573, 41.5151, 40.3502,
        41.5972, 41.3327, 34.3049, 38.1526, 39.3391, 39.7651, 42.4266, 36.3504,
        39.4566, 40.9275, 41.3152, 39.7955, 41.6964, 43.3183, 44.0155, 39.6714,
        40.5448, 42.4725, 39.4136, 38.3419, 41.9806, 41.6095, 43.0785, 42.3661,
        53.0570, 41.1711, 33.0179, 39.3994, 50.5079, 42.8103, 35.6710, 42.8288,
        39.1504, 42.7634, 40.8974, 40.9927, 37.3445, 42.7842, 39.8469, 41.0073,
        41.5464, 41.3868, 43.6438, 41.8496, 36.9438, 36.9464, 41.9490, 42.3119],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 6.9172,  5.1208,  2.8210,  1.0950,  4.0325,  5.1753,  3.2669,  7.0869,
         8.1577,  4.4494,  6.5627,  9.0820,  8.2011,  9.6552,  4.5136,  1.8021,
         5.4820,  3.5206,  5.0509,  5.2014,  8.4961,  7.9273,  4.9920,  5.9694,
         6.8998,  5.5602, 11.8890,  8.8211,  7.3496,  8.6801,  5.4379,  4.7368,
         7.2246,  5.1604,  5.4479,  6.5100,  8.9966,  9.0757,  6.0348, 16.1742,
        14.8499, 31.7366, 38.3208, 36.7598, 29.2530, 36.0190, 11.4647, 10.6784,
        20.9964,  8.9475, 17.0116, 20.2231, 28.5409, 13.7195, 34.9830, 45.2010,
        55.6603, 12.3971,  5.2630,  5.6221, 42.9386,  5.6915,  7.5941, 10.3055,
        10.6530, 10.8982,  7.0798, 12.2262,  7.6274,  7.7008,  7.8095,  8.3354,
         8.6026, 18.3361,  4.3201, 11.7280,  8.0352,  5.9623,  8.4390,  8.8582],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([18.1913, 13.2753,  6.2479,  2.2409,  8.4607, 10.3058,  6.6982, 14.3373,
        16.8138,  9.2063, 14.1148, 18.5085, 17.0756, 19.2653,  9.4673,  4.7426,
        13.9756,  7.6324, 10.6289, 10.1168, 18.0350, 16.4048, 10.2083, 12.4026,
        14.7849, 11.8616, 26.2352, 17.7637, 15.3825, 17.6089, 11.2222,  9.9170,
        14.7411,  9.9004, 13.4478, 14.3819, 19.3857, 19.4546, 11.9866, 40.9833,
        31.5943, 66.5644, 85.5885, 80.0512, 59.5505, 70.2811, 22.7387, 22.6371,
        44.3227, 17.8373, 36.6720, 45.2058, 57.0927, 27.8542, 68.9517, 98.3522,
        97.4249, 25.6190, 15.5670, 11.4949, 82.1369, 12.1605, 18.6204, 19.3934,
        22.7380, 21.5921, 14.4376, 24.7337, 17.2761, 15.2624, 16.4283, 18.0292,
        17.6235, 37.6720,  8.0713, 23.7715, 18.1550, 13.6499, 17.1504, 17.8574],
       device='cuda:0')
Outer loop valEpocw Maximum [14/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 53.1
model_train val_loss valEpocw [14/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 55.7
model_train val_loss  valEpocw [14/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 98.4
Max_Val Meta Model:  tensor([36.8056, 32.0554, 39.4175, 40.6066, 40.9221, 40.2105, 41.2738, 39.0391,
        40.8296, 39.4624, 37.5923, 39.5766, 40.0951, 41.4867, 41.9529, 31.3152,
        33.7593, 37.8222, 37.1106, 41.6034, 39.6677, 40.0735, 40.3132, 41.7601,
        39.1853, 37.8213, 37.9841, 39.8110, 38.2064, 40.2081, 39.9342, 40.8323,
        40.9512, 43.0584, 34.1817, 36.6711, 38.9147, 37.8942, 42.4817, 35.7647,
        39.4228, 42.4015, 36.9851, 38.7559, 40.8127, 40.6604, 39.3314, 38.7176,
        39.7585, 38.9931, 39.2834, 36.3463, 39.0148, 40.7967, 41.6125, 38.3804,
        37.9511, 40.8526, 32.0517, 40.8604, 41.3045, 40.7474, 35.5790, 39.7771,
        38.7460, 38.9579, 39.9254, 43.6595, 37.2579, 39.5208, 40.3603, 40.5873,
        40.9775, 39.4934, 49.9865, 39.2210, 36.2219, 35.4526, 41.8106, 42.6710],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 16.0284,   4.5341,  10.3860,   2.4131,   6.1964,   8.5715,   7.3281,
         10.8530,   7.5411,  12.4300,   6.7972,  10.0831,   7.4263,   9.5353,
          9.7372,   1.4950,   5.5035,   4.3467,   5.6756,   5.9712,   7.8426,
          7.3963,   8.4158,   6.5452,  11.4751,   5.6890,  12.4626,   5.0176,
          9.6032,   7.6625,   4.2942,   3.3370,   4.8308,   5.6663,   4.9015,
          5.7484,   5.3529,   6.6844,   4.7246,   5.7139,   6.1711,   4.1774,
          5.8490,   6.5221,   8.2941,   4.3497,   8.3569,   7.3715,   9.1288,
          5.8085,   6.1731,   5.2529,   5.6496,   6.1764,   4.0127,   6.4900,
          3.2672,   5.4852,   5.9481,   4.6916,   5.5200,   5.8113,   4.7296,
          5.5043,   7.5527,   9.3004,   4.2692,   5.4446,   6.2558,   6.8234,
          7.4465,   5.9794,   6.2123,   6.6605, 155.4350,   8.5023,   5.5108,
          5.9552,   7.9104,   8.1365], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 41.2616,  11.9933,  24.0040,   5.0626,  13.1448,  17.7478,  15.1152,
         22.5756,  15.7773,  25.9800,  14.6989,  20.8168,  15.7543,  19.6200,
         20.4796,   4.0211,  14.1574,   9.3342,  12.3055,  11.8146,  16.8233,
         15.5727,  17.6852,  13.7840,  24.8763,  12.2598,  27.6594,  10.3815,
         20.3092,  16.1014,   9.0770,   6.9075,   9.8888,  10.9438,  12.0791,
         13.1392,  11.6146,  14.9261,   9.4188,  14.7024,  12.9664,   8.4518,
         13.2123,  14.2997,  16.8125,   8.5562,  17.4817,  15.9115,  19.4225,
         12.0921,  13.2041,  12.2400,  11.5487,  12.6673,   7.8975,  14.0178,
          5.9292,  11.3749,  17.9067,   9.4931,  10.7535,  12.5945,  11.6361,
         10.7951,  16.1686,  19.1658,   8.6655,  10.9036,  14.1203,  13.9372,
         15.3865,  13.0481,  12.9282,  13.7701, 302.3062,  18.2833,  12.6063,
         14.1475,  16.0482,  16.3868], device='cuda:0')
Outer loop valEpocw Maximum [14/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 50.0
model_train val_loss valEpocw [14/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 155.4
model_train val_loss  valEpocw [14/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 302.3
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [85.7457877  97.2137279  90.56298047 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.23328176
 96.90915072 96.22689782 96.9627563  93.88774503 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.73621179 96.13796128 96.24273583 96.9067141
 91.68747944 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [82.91321987 97.2137279  89.62853766 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.06150023
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 90.96989559 96.13796128 96.24273583 96.9067141
 90.96258574 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [95.48503478  3.99008402 54.01253116 14.74825917  3.35756587  6.63108543
  7.05497654 31.1133305   2.39331089 23.76360079  1.51798722  1.39724913
  0.96146389  9.44738414  2.44673947  3.7086173   3.85487659  2.25920656
  0.91479961  1.25619989  1.22670323  0.46234427  1.30973075  1.33973088
 14.05005781  6.06099347 23.16469817  5.35487424  3.8648927   1.28931646
  2.70395143  0.97009544 16.15842392  1.63671364  1.52695241  1.46723874
  3.49597436  2.47678807  3.71311921 25.64799933  6.56417083 37.3713978
  5.07132038  7.99167288  6.57897235 29.00909517  2.42754971  1.83586305
  2.39446857  1.99112497  1.32113739  1.25320571  1.16531351  5.16249142
  1.62934623  4.11108888 62.21086392 25.74987486 10.67845542  6.03656174
 58.24789857  3.55928949 13.53757279 10.63229598  5.46062443  5.86491408
  6.57288607 10.23718294  4.01400785  7.25237526  0.57735728 13.71093204
  7.51977728 22.14200895  8.15030018  7.84496949  1.42481616  2.23067368
  0.20333452  0.89908079]
Accuracy th:0.5 is [45.631754   97.2137279  72.44307452 97.02489005 97.26733349 77.27854193
 77.63063316 76.47811308 78.51268869 96.36456671 78.82579403 98.52097319
 99.41399349 80.11841961 78.32872407 96.56680596 96.29512311 78.10821018
 98.65376884 98.30776915 80.24634203 79.32651893 98.38695922 78.19105518
 80.63620083 96.65086926 94.0778012  77.86089351 98.01293844 78.74294904
 97.30875598 98.57457877 96.36213009 98.02024829 86.19534362 78.34943531
 78.12892143 89.45431951 97.11504489 75.33533948 79.2668218  92.05906361
 77.55631632 77.17376738 96.9627563  93.87434364 98.02877645 98.57336046
 88.29814452 86.72652624 86.48286449 98.55508583 98.99976852 77.78414006
 98.70615611 78.05704122 72.66724333 92.80710518 96.24273583 96.9067141
 89.79300934 97.17717864 90.99426177 78.16303408 98.42838172 78.56385765
 98.20786784 77.19447862 79.23758239 97.53901634 79.79069456 95.99054592
 78.85868837 95.45083515 77.38331648 82.07867838 85.79208343 78.79533631
 79.84795507 99.14718388]
Accuracy th:0.7 is [45.69876098 97.2137279  72.44307452 97.02489005 97.26733349 77.27854193
 77.63063316 76.65111293 78.51268869 96.46812295 78.82579403 98.52097319
 99.41399349 80.57041215 78.32872407 96.56680596 96.29512311 78.10821018
 98.65376884 98.30776915 80.67153178 79.32651893 98.38695922 78.2921748
 81.2173341  96.65086926 94.0778012  77.86089351 98.01293844 78.74294904
 97.30875598 98.57457877 96.36213009 98.02024829 86.42804059 78.34943531
 78.12892143 89.74427699 97.11504489 75.33533948 79.99780704 92.05906361
 77.55631632 77.17376738 96.9627563  93.87434364 98.02877645 98.57336046
 90.4557693  87.56837758 86.66804742 98.55508583 98.99976852 77.78414006
 98.70615611 78.05704122 72.66724333 93.22011184 96.24273583 96.9067141
 89.79300934 97.17717864 91.17700808 78.16303408 98.42838172 78.56385765
 98.20786784 77.19447862 79.23758239 97.55972759 79.79069456 95.99054592
 78.91351226 95.45083515 77.38331648 82.1785797  85.8992946  78.79533631
 79.84795507 99.14718388]
Avg Prec: is [56.16429865  3.22593116 11.15615034  3.39639571  2.23708085  3.77806721
  3.33014959  5.63979609  2.47633174  3.73403385  1.59012118  1.6240045
  0.62836963  5.19595542  2.70343415  3.1035849   3.59810212  2.68154017
  1.37303767  1.72442614  1.95506052  0.86508669  1.73886022  2.52705205
  5.2760757   3.61612873  6.6844056   3.3688453   2.07641062  1.85357269
  2.64804412  1.35360123  3.69819761  1.62976602  2.43988965  2.45858868
  3.08304027  2.58062142  2.89768497  7.47890862  2.27043554  8.34546444
  3.40898869  4.08669088  3.1984696   6.37008762  2.15234065  1.53149045
  2.17096133  1.58546108  1.77802285  1.54167399  1.08232998  2.89182306
  1.27804163  2.69997126 11.43205075  3.72962304  3.85366734  2.7511293
 10.75174768  2.17161438  3.8217832   2.92149415  1.52340539  2.52548521
  1.79407755  4.29158002  1.25841153  2.48438814  0.20843783  3.44889266
  1.97977563  4.64830917  3.96435448  3.0833394   0.93283542  1.85299911
  0.12788888  0.76455748]
mAP score regular 9.99, mAP score EMA 3.78
starting validation
Accuracy th:0.5 is [87.75194957 97.22450607 91.24498592 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.64020729
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.92358672 96.39235618 96.16314124 96.78102499
 92.06716994 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [86.47382714 97.22450607 89.93447443 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.37860328
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.92358672 96.39235618 96.16314124 96.78102499
 91.83297207 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [96.32444052  4.52648274 58.67872513 16.86614868  3.18559231  7.30238931
  8.10187858 32.60143774  2.65879823 26.64832229  1.64053606  1.3639036
  1.00533952 10.2286581   2.70305447  4.04709181  3.85984408  2.28555435
  0.84135241  1.25180699  1.12458617  0.48005956  1.32728701  1.26378428
 14.90220108  6.25377208 22.87684938  4.69860047  4.36086407  1.30680108
  2.48703113  0.87033202 22.09280546  1.62313372  1.3128267   1.2946345
  3.09862491  3.01024687  4.13461051 27.03972293  6.82851683 39.02042697
  4.72705503  7.61147307  5.93064889 28.9486379   2.38065111  1.91087368
  2.44075169  1.91681783  1.40412047  1.40889059  1.38326546  5.63433704
  1.62485954  3.68715764 56.84337028 25.54245285  9.64702308  6.6309288
 59.79237898  3.57377056 15.4896811  12.54239264  7.76075401  5.39977216
  8.9204332  11.34381537  3.56934507  7.81667762  0.75935155 15.25977181
  7.85791274 22.91677832  9.72687455  7.41794894  1.50878125  2.25029979
  0.19248571  0.85187714]
Accuracy th:0.5 is [45.35715176 97.22450607 70.76512943 96.96290206 97.90716795 76.31113436
 76.40331863 75.21738047 77.84587787 96.41477938 78.03024641 98.5325261
 99.34972718 78.13239654 77.9330792  96.31262924 96.21047911 77.37997359
 98.78167277 98.34068316 78.89976829 78.66557042 98.31327703 77.45970053
 78.26693575 96.52938685 94.3393876  77.41236266 97.81747515 77.99785734
 97.52597354 98.67204823 96.39983058 98.18870369 86.66068715 77.60918853
 77.56683359 91.24000299 97.0276802  74.66676633 77.78608267 92.37362035
 76.74714104 76.40082717 97.03764606 94.02795426 98.18621222 98.77668984
 88.63392879 86.73543115 84.8518823  98.55993223 98.87385704 76.73219224
 98.6969629  77.41485412 71.33318385 93.89092359 96.16314124 96.78102499
 90.13379176 97.04761193 90.72177791 77.37748212 98.32075143 78.23205521
 98.13139996 76.54284077 78.69297656 97.52597354 79.14144057 96.07843137
 78.41891522 95.44559882 76.48553704 82.97331639 87.3906869  78.06014401
 79.21120163 99.15040985]
Accuracy th:0.7 is [45.6685851  97.22450607 70.76512943 96.96290206 97.90716795 76.31113436
 76.40331863 75.29710741 77.84587787 96.41976231 78.03024641 98.5325261
 99.34972718 78.52853975 77.9330792  96.31262924 96.21047911 77.37997359
 98.78167277 98.34068316 79.26103097 78.66557042 98.31327703 77.47963226
 78.68550216 96.52938685 94.3393876  77.41236266 97.81747515 77.99785734
 97.52597354 98.67204823 96.39983058 98.18870369 86.95468022 77.60918853
 77.56683359 91.39696539 97.0276802  74.66676633 78.24700401 92.37362035
 76.74714104 76.40082717 97.03764606 94.02795426 98.18621222 98.77668984
 90.88372325 87.13904876 85.03126791 98.55993223 98.87385704 76.73219224
 98.6969629  77.41485412 71.33318385 94.18242519 96.16314124 96.78102499
 90.13379176 97.04761193 90.85133418 77.37748212 98.32075143 78.23205521
 98.13139996 76.54284077 78.69297656 97.53593941 79.14144057 96.07843137
 78.42638962 95.44559882 76.48553704 83.05304333 87.5177517  78.06014401
 79.21120163 99.15040985]
Avg Prec: is [53.6859569   3.71717407 14.85755382  4.53825851  1.48412412  4.25765545
 13.76700334  8.62565403  8.08405615  5.27065887  2.33883836  4.75117582
  2.22136547  5.80902476  2.92056746  3.63099862 24.0574068   6.45012859
  1.5585093   2.71863505  3.5239912   1.50936727  1.2594651   5.20424107
  5.62809481  9.21482441  7.8736854   4.5980104   3.98360518  5.50339727
  2.22610703  0.85888891  2.92879291  1.15631774  1.65975457  2.27699965
  1.99335865  2.21157654  2.19102484  6.19071964  1.71432102  6.01838905
  2.17096759  2.6980526   2.33958604  4.8316271   1.67851577  1.03027483
  1.40379543  1.1668368   1.19844031  0.98513901  0.74387118  2.26356432
  0.8845492   1.84516652 10.12758989  3.06571639  4.02508637  2.90560592
  7.88289762  2.05922334  3.32568145  2.50118701  1.35778444  1.92043129
  1.52542188  3.52237434  1.07368581  2.22863088  0.19508788  3.23909363
  1.58742911  4.01665271  3.21161374  2.32564348  0.55448463  1.45348214
  0.12108511  0.60179933]
mAP score regular 10.40, mAP score EMA 4.31
Train_data_mAP: current_mAP = 9.99, highest_mAP = 9.99
Val_data_mAP: current_mAP = 10.40, highest_mAP = 10.40
tensor([0.3932, 0.3675, 0.4170, 0.4670, 0.4720, 0.4723, 0.4815, 0.4676, 0.4706,
        0.4688, 0.4585, 0.4789, 0.4605, 0.4760, 0.4743, 0.3632, 0.3832, 0.4663,
        0.4462, 0.4970, 0.4589, 0.4719, 0.4711, 0.4673, 0.4563, 0.4576, 0.4492,
        0.4767, 0.4695, 0.4597, 0.4621, 0.4736, 0.4842, 0.5024, 0.4053, 0.4240,
        0.4527, 0.4344, 0.4939, 0.3809, 0.4794, 0.4826, 0.4364, 0.4566, 0.4953,
        0.5037, 0.4661, 0.4533, 0.4624, 0.4767, 0.4683, 0.4137, 0.4839, 0.4778,
        0.5010, 0.4689, 0.5327, 0.4707, 0.3250, 0.4869, 0.5016, 0.4610, 0.4007,
        0.4924, 0.4644, 0.4723, 0.4880, 0.4873, 0.4430, 0.4887, 0.4862, 0.4552,
        0.4780, 0.4780, 0.5081, 0.4413, 0.4326, 0.4074, 0.4948, 0.4875],
       device='cuda:0')
Max Train Loss:  tensor([18.1150,  9.1473,  9.7103,  8.6257,  8.2194, 12.4251, 10.9444, 10.5847,
        10.4808, 13.4103,  8.0214, 12.2360,  7.5409, 15.2538,  9.9324,  8.2512,
         7.3544,  8.6332,  8.9693,  5.7099, 12.6284,  8.5786,  7.6539,  8.7651,
        15.2403,  9.5476, 13.1867, 10.0945,  9.4951,  7.7123,  7.0556,  6.7464,
         8.1943,  8.3687,  7.2479,  7.9229, 11.1123,  8.9855, 10.0691, 14.2601,
         9.2688, 10.6672, 10.4586, 13.0841, 10.9680,  9.6026, 12.2284,  8.8878,
        10.1179,  7.2946,  9.2886,  6.3111,  7.1597,  8.3734,  7.5188,  7.6286,
        12.6885, 12.0351,  8.0873, 11.8513, 10.9521, 12.8854,  9.0230, 10.2411,
         9.7217, 12.2796,  8.0939,  9.5097,  7.1981,  9.6593,  8.4649, 11.4516,
         8.3176, 12.6737,  9.9500, 11.0796,  7.4423,  9.6538,  8.2361, 11.0146],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [15/80], Step [000/642], LR 1.0e-04, Loss: 18.1
Max Train Loss:  tensor([14.7956,  7.7395, 15.0754, 17.3023,  9.5400, 10.3296, 13.8621, 17.3594,
        17.6517, 12.1478,  8.9187, 13.2608, 19.7680, 19.1299, 12.5018,  9.3333,
         9.9370, 12.6355,  8.1609, 15.2923, 16.5415, 16.3123, 12.0400,  9.0564,
        13.9875, 10.1694, 11.6593, 11.5599, 10.0736,  9.0848, 12.2837, 13.9433,
        21.2686, 20.4069, 11.9808, 11.6322,  8.2557, 10.0835, 16.5086, 13.4236,
        17.5131, 20.5271, 10.3645, 11.0452, 10.9706, 17.2943, 12.9594,  8.7678,
        10.3662, 10.0535, 10.7499,  9.6328, 20.9390,  8.7406, 12.9286, 10.2844,
        17.1678, 13.7850,  7.8752, 15.7648, 18.2575, 10.2011, 10.1226, 10.1842,
         8.0540,  8.4453, 12.2678, 16.1526,  8.9462, 20.6094, 14.3406, 12.4825,
        11.0469, 19.0490, 19.2838,  8.3114,  6.3834,  9.5783, 13.3174, 10.0011],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [15/80], Step [100/642], LR 1.0e-04, Loss: 21.3
Max Train Loss:  tensor([15.8628,  6.4976, 11.2980,  7.8366, 14.8641,  9.6705,  9.0746, 11.9617,
         7.4031,  9.1985,  7.4205,  7.1332,  7.3916, 12.5554, 10.4220,  8.0995,
         8.8172, 11.4052,  7.3370, 13.0902,  9.5755,  8.3592, 11.7806, 10.2155,
        11.9021, 12.0348, 10.2935, 10.0314, 11.5014,  9.9899, 10.9993,  9.0352,
        10.5113, 11.6825,  4.8159,  8.5444, 17.3072, 10.4537, 10.0737,  8.9943,
        11.6562, 13.3650, 12.0944,  9.4903,  9.1167, 11.6570, 12.9195,  8.3168,
         9.2023,  9.2030,  7.5148,  8.0422, 13.6886, 10.9350,  9.3897,  8.2326,
        10.4473, 13.7520,  4.1859, 13.2266, 11.4088, 14.0871,  7.8972,  5.2286,
         7.5705,  8.2850, 10.2823,  7.5624,  7.7823,  8.5841, 14.4329, 10.1827,
         9.2316, 11.6904,  9.0260,  6.4164,  6.6041, 10.6634, 12.7721,  9.5157],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [15/80], Step [200/642], LR 1.0e-04, Loss: 17.3
Max Train Loss:  tensor([14.3057,  5.5501, 10.4006, 11.7260, 14.0121,  6.5515,  9.5890, 13.2243,
         8.9354,  8.6954,  7.3505,  7.0921,  8.3402, 10.8699,  8.1767,  8.9325,
         5.4222,  8.7686,  8.3440,  9.4607, 16.4609,  5.8325,  9.7053,  9.5552,
        10.1834,  8.6762, 10.1475, 13.9391,  9.3077,  8.5851, 11.0960,  9.6879,
        10.6104,  9.0677,  8.6715,  9.8061, 11.4891,  7.0367, 14.6689, 12.0929,
         9.0301, 11.4579,  9.1444,  9.3719,  7.8241, 11.8435, 12.4635,  8.0374,
         8.9061,  8.9912,  9.0354,  6.1542,  8.5305, 11.1040,  9.3128,  8.3476,
        15.0183, 10.4578,  8.4524, 14.1524, 10.9961, 11.9919, 10.4610,  6.9092,
         8.8781, 11.3351, 10.4823,  8.9781,  9.3074, 11.6918, 13.6986, 11.2631,
         9.1324, 11.0144, 13.6067, 12.1391,  5.4463,  7.1639, 12.6741, 10.5275],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [15/80], Step [300/642], LR 1.0e-04, Loss: 16.5
Max Train Loss:  tensor([12.2698,  8.2031,  9.4550, 11.6420, 10.9452, 10.1945,  8.3257,  9.6127,
         4.8319, 10.6013,  8.3335, 10.5500,  7.3258, 11.0021, 11.3089,  8.0009,
         8.1914, 13.5503,  8.8461, 10.0987,  9.4402,  7.1811, 11.6923,  9.7447,
        12.0270,  6.5863,  9.9250,  4.8247,  8.4889,  9.6752, 12.3390, 10.8793,
        14.3256, 11.3527,  7.0884, 10.9168, 10.0549, 12.1342,  9.3881, 11.8706,
         9.1218, 11.1232,  7.9319,  8.9801, 10.2908,  9.6756, 11.6892,  7.4514,
        10.0324,  7.3682,  9.0844,  7.5629, 11.0213, 10.6580, 10.3526, 11.4156,
        13.7283,  9.6337,  7.2292,  8.0184, 12.1154,  9.8418, 12.5440,  8.0412,
         9.7997,  5.2558, 11.2954, 11.1339,  7.5046, 11.5654,  4.5195,  9.3953,
        10.5542, 12.0707,  8.2621,  9.7257,  5.4524,  5.6865, 13.3820, 10.4476],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [15/80], Step [400/642], LR 1.0e-04, Loss: 14.3
Max Train Loss:  tensor([16.4668, 14.1442,  9.5113, 11.6914,  9.0115, 10.1932,  8.6385, 11.2474,
         8.3664,  9.3446,  8.4598,  8.8732,  8.7583, 10.5196, 10.4283, 13.5799,
         7.9336,  8.8495,  9.5705,  7.7113,  8.8754,  7.3456,  7.5069, 10.5445,
        15.4846, 10.0575, 11.1532, 10.9758,  8.3027, 10.6130,  9.2852, 10.1223,
         9.7690,  9.4488,  5.9337,  9.0805,  9.9820, 10.0267, 10.7737, 13.0023,
        10.8419, 14.2321,  7.4866,  8.5845, 10.3240, 11.0904, 11.8910,  8.2939,
        10.0267,  8.2769,  7.4884,  5.2260,  9.2573,  7.5034, 10.1410, 10.6059,
        21.2586,  8.3159,  6.6678, 12.4325, 10.7364, 11.9985, 11.5001,  6.4568,
        10.6064,  6.7229, 10.4296,  9.6758,  7.8569,  9.4132,  2.9124, 10.2111,
         9.3383, 11.9070, 10.0733,  8.5842,  7.6561,  6.9376, 12.7391, 10.4227],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [15/80], Step [500/642], LR 1.0e-04, Loss: 21.3
Max Train Loss:  tensor([18.6890,  4.7096, 14.7573,  9.1450,  7.6659,  9.1890, 13.0197, 14.6446,
         6.9345,  9.3121, 10.4623,  8.1989,  7.2861, 10.8276,  7.2590,  6.3498,
         9.0781,  9.4357,  7.0700,  6.1085,  9.0720,  5.8056,  6.2636,  5.6148,
        11.2139, 10.7608, 12.5551, 11.0803,  9.5051, 10.3742, 10.5778, 12.1300,
         9.5539, 13.6240,  5.8944,  9.8489, 12.3703,  6.8627,  8.8025, 13.7926,
        13.3490,  9.4398,  9.7542, 11.4484,  5.3943, 10.6318,  5.8381,  9.7899,
         8.3074,  9.1050, 13.2238,  6.0574,  7.6479,  7.7542, 10.3110,  9.7371,
        13.2040, 11.0592,  6.9108, 10.1661,  9.4512, 10.4200, 11.1506,  8.1355,
         8.9577,  9.8817, 10.5801, 13.6798,  8.4771,  9.5557,  4.0852,  8.7933,
        10.1266,  9.6128, 15.3711, 12.8278,  5.4059,  5.8203, 12.6453,  9.3912],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [15/80], Step [600/642], LR 1.0e-04, Loss: 18.7
Max_Val Meta Model:  tensor([ 22.2489,  23.5021,  34.8876,  21.4114,   5.5421,   7.9535,   8.4237,
         10.7845,   4.8693,   9.5991,   8.1418,   7.8176,   8.0137,  10.4464,
          7.4153,   5.0724, 123.6627,   4.0644,   6.2256,   4.9742,   5.9254,
          5.8551,   7.1354,   5.6858,  14.1946,   9.3469,  12.8603,   6.8503,
          9.1993,   9.5552,   4.8126,   9.0003,   7.5463,   4.8657,   4.7594,
          4.8972,   5.1371,   6.7099,   6.7980,  19.4532,  11.1078,  12.1536,
          7.5015,  11.0400,   8.7737,  14.2312,   2.0818,   8.4048,   7.2457,
          8.1677,   7.4635,   5.1928,   9.2234,   5.8289,   9.3505,   7.4582,
         14.5857,   8.4183,   7.9192,   5.4718,   9.9294,  15.7339,   5.5888,
          6.2032,   7.5250,   3.9499,   9.1124,   6.7383,   9.3978,  14.7476,
          2.8688,  11.0855,  13.0418,   7.1072,   9.4971,   8.4570,   5.4465,
          5.6723,  12.7611,   9.4771], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 19.2779,  22.5633,  29.4862,  22.0424,   5.7742,   8.4879,   9.0377,
         12.4495,   5.1223,  10.6934,   8.7346,   8.3029,   8.6355,  11.4059,
          7.8822,   5.4153, 118.1734,   4.5245,   6.8044,   5.3067,   6.4926,
          6.3743,   7.2010,   6.0086,  14.7564,   9.4122,  13.5197,   8.0109,
          9.8648,   9.9998,   5.2512,   9.7244,   8.7317,   5.3432,   5.2348,
          5.7017,   5.9149,   7.1393,   7.8870,  19.7413,  11.5815,  12.7460,
          8.0833,  11.4937,   9.2231,  13.5163,   2.3035,   9.0174,   7.8712,
          8.8664,   8.0829,   5.6994,   9.7388,   5.4930,  10.0243,   8.0877,
         14.4257,   8.7404,   8.0127,   5.8759,   9.1966,  16.2489,   6.1273,
          6.6855,   8.1610,   4.4076,   9.7728,   7.6191,   9.9768,  15.2795,
          3.2306,  11.3983,  13.5856,   7.6475,   9.8516,   8.9531,   5.9705,
          6.2169,  13.5642,  10.1517], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 49.0321,  61.3893,  70.7124,  47.1963,  12.2329,  17.9700,  18.7702,
         26.6215,  10.8841,  22.8085,  19.0517,  17.3382,  18.7519,  23.9611,
         16.6201,  14.9113, 308.3593,   9.7038,  15.2484,  10.6780,  14.1488,
         13.5078,  15.2852,  12.8594,  32.3389,  20.5688,  30.1005,  16.8048,
         21.0100,  21.7518,  11.3639,  20.5335,  18.0317,  10.6343,  12.9157,
         13.4469,  13.0654,  16.4331,  15.9690,  51.8290,  24.1577,  26.4107,
         18.5239,  25.1742,  18.6228,  26.8335,   4.9417,  19.8938,  17.0212,
         18.6001,  17.2600,  13.7758,  20.1252,  11.4974,  20.0080,  17.2492,
         27.0805,  18.5686,  24.6523,  12.0687,  18.3354,  35.2458,  15.2929,
         13.5779,  17.5730,   9.3316,  20.0257,  15.6346,  22.5232,  31.2682,
          6.6447,  25.0417,  28.4238,  15.9993,  19.3908,  20.2889,  13.8016,
         15.2590,  27.4115,  20.8233], device='cuda:0')
Outer loop valEpocw Maximum [15/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 123.7
model_train val_loss valEpocw [15/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 118.2
model_train val_loss  valEpocw [15/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 308.4
Max_Val Meta Model:  tensor([39.3810, 39.6496, 39.1269, 42.4237, 43.7815, 44.2122, 48.7744, 39.0392,
        38.7685, 43.7903, 39.7708, 45.6159, 40.6216, 40.0964, 45.8644, 37.7896,
        34.1247, 38.8446, 42.7193, 43.6538, 39.8739, 40.4955, 39.8633, 47.0447,
        39.5807, 39.5308, 38.3488, 40.3686, 40.5243, 40.1268, 39.8514, 41.7596,
        41.3398, 40.6385, 34.6778, 36.8800, 39.1614, 37.8801, 42.6035, 38.6002,
        40.3019, 40.5947, 37.4249, 39.1780, 40.6535, 42.4439, 39.2116, 38.9553,
        40.0583, 40.4623, 39.8353, 36.6065, 40.9106, 41.1454, 42.3699, 39.3708,
        42.9598, 40.8034, 37.8910, 39.9176, 45.6690, 42.4699, 35.0848, 42.6155,
        39.2292, 40.1005, 41.5476, 39.2800, 37.7177, 41.7827, 40.4637, 38.5538,
        40.5566, 41.2302, 44.0671, 38.7042, 36.5882, 35.5721, 42.5754, 42.3237],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([22.7201,  7.3868, 23.7738,  9.6888,  6.4244, 17.3272, 37.8470, 21.2050,
        16.7476, 18.6145, 10.5197, 84.9371,  8.9896,  8.2011,  9.7392,  4.9812,
         5.8515,  7.9183,  5.6913, 12.0204,  5.7067,  5.4373,  9.0426,  7.1901,
        12.6850,  9.9986, 14.8955,  8.6230,  9.6505,  7.1921,  4.7281,  8.4954,
         5.3133,  4.4206,  4.3630,  4.6199,  6.5807,  5.3199,  5.9968,  5.0860,
         7.5817,  3.2593,  5.4601,  4.7063,  3.3531,  4.0396,  1.7591,  7.0072,
         6.7802,  7.5346,  6.8121,  4.8817,  7.1460,  3.7079,  8.6465,  7.7062,
         6.3846,  5.7582,  4.5417,  4.7939,  3.7704,  6.2220,  4.4909,  4.5925,
         6.9888,  3.5200,  8.4176,  7.4512,  6.3027,  7.7293,  2.5759,  4.2480,
         7.9396,  4.7440,  7.6569,  5.2583,  5.0117,  4.6316, 11.9772,  8.7387],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 57.3460,  19.3723,  54.6578,  20.1252,  13.1604,  34.9062,  75.1597,
         44.9865,  35.6375,  38.2554,  22.6841, 174.8921,  18.9933,  17.0152,
         19.7632,  13.1425,  14.9660,  16.9179,  12.7290,  23.8034,  12.2245,
         11.4380,  18.9885,  14.5434,  27.5241,  21.6474,  33.0022,  17.9054,
         20.4298,  15.1530,  10.1040,  17.5175,  10.9654,   8.7865,  10.6522,
         10.5896,  14.2615,  11.9861,  11.9886,  12.9066,  15.7949,   6.6503,
         12.3040,  10.2830,   6.8040,   7.9102,   3.6963,  15.1665,  14.3752,
         15.6040,  14.5471,  11.4063,  14.7485,   7.6027,  17.0743,  16.4179,
         11.7206,  11.8943,  13.0335,   9.7436,   7.1319,  13.1965,  11.0744,
          9.1135,  14.8984,   7.3628,  17.1442,  15.7601,  14.1973,  15.8504,
          5.2742,   9.3604,  16.5801,   9.6467,  14.8989,  11.4944,  11.4556,
         11.0514,  24.2502,  17.7223], device='cuda:0')
Outer loop valEpocw Maximum [15/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 48.8
model_train val_loss valEpocw [15/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 84.9
model_train val_loss  valEpocw [15/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 174.9
Max_Val Meta Model:  tensor([34.7565, 35.3073, 39.3508, 41.6572, 42.0465, 42.2313, 41.9364, 39.3748,
        40.7525, 37.6370, 39.8303, 40.2450, 39.7574, 39.4572, 43.1947, 34.7904,
        34.2571, 41.5125, 37.2529, 42.9677, 39.4442, 42.3003, 42.4811, 42.6539,
        39.4400, 40.5959, 38.3600, 42.0188, 41.5105, 39.3465, 41.6643, 41.3875,
        43.2867, 40.1714, 34.8767, 36.7470, 39.1149, 37.9011, 41.6926, 36.0862,
        40.6732, 42.0596, 38.4154, 39.7273, 41.8559, 44.0856, 41.8460, 38.8960,
        43.2242, 41.4477, 43.2318, 36.5822, 44.0751, 41.3525, 43.0508, 40.2056,
        50.2322, 40.7801, 33.3914, 39.8010, 47.3645, 42.5954, 35.4357, 42.8347,
        39.3696, 40.2742, 41.7568, 39.5505, 37.9611, 41.8655, 40.8722, 38.8220,
        41.0955, 41.3395, 41.7715, 38.4316, 36.6321, 35.6317, 42.4594, 42.3831],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 6.7985,  4.8238,  3.2504,  7.5432,  4.2099,  5.6698,  3.9025,  6.8938,
         3.9858,  3.5554,  8.4777,  6.4012,  8.3083,  7.8155,  5.7377,  5.2113,
         5.2257,  4.1430,  7.1202,  4.4093,  6.3076,  6.5735,  4.6537,  4.6668,
         7.0058,  5.9643, 12.1686, 10.9193,  8.6253,  8.8341,  5.5348, 10.9494,
         7.8602,  5.1696,  5.5974,  5.2144,  8.0969,  7.8424,  7.0388, 13.4687,
        16.7709, 33.5018, 37.3335, 34.7284, 28.7196, 39.5481,  5.6902, 11.2779,
        20.5167, 10.5863, 18.7834, 19.4635, 26.5673, 16.5612, 30.5178, 43.6345,
        46.6700, 12.1905,  5.6254,  6.4299, 36.3937,  7.7102,  8.2337,  9.6130,
        11.1500,  4.8701, 11.7661, 12.8895,  8.5514,  8.6490,  3.3451, 10.4984,
         9.6297, 17.9532,  4.0245,  9.5660,  8.0485,  5.8991, 13.7274, 10.6971],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([17.0672, 12.6478,  7.5046, 15.7198,  8.7760, 11.6249,  7.9630, 14.3871,
         8.2502,  7.6136, 18.2950, 13.2382, 17.5983, 16.2020, 11.9117, 13.7519,
        13.3550,  8.6268, 15.9975,  8.7246, 13.5396, 13.5211,  9.5681,  9.7059,
        15.2657, 12.7874, 27.0382, 22.3308, 18.0158, 18.7095, 11.5166, 22.8718,
        15.9827, 10.3316, 13.6540, 12.0478, 17.6747, 17.7297, 14.0881, 34.2763,
        34.9744, 68.5626, 84.0476, 76.0820, 58.2095, 76.7207, 11.7020, 24.5768,
        43.3856, 21.5484, 38.0342, 45.9406, 55.0104, 34.0054, 60.5895, 94.0753,
        87.9686, 25.2006, 16.2838, 13.0519, 70.3900, 16.3584, 20.2067, 19.1138,
        23.8418, 10.1988, 23.9621, 27.1732, 19.2283, 17.7628,  6.8234, 23.1123,
        19.9976, 36.3630,  7.9644, 21.1905, 18.4906, 14.1192, 27.9597, 21.7448],
       device='cuda:0')
Outer loop valEpocw Maximum [15/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 50.2
model_train val_loss valEpocw [15/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 46.7
model_train val_loss  valEpocw [15/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 94.1
Max_Val Meta Model:  tensor([37.8305, 36.2044, 39.6618, 41.4088, 41.4920, 41.0082, 40.8910, 38.6483,
        39.8282, 36.3201, 38.2059, 39.9844, 38.6034, 38.6583, 39.7773, 35.4406,
        33.5975, 40.0963, 37.3597, 40.3753, 37.5201, 41.1007, 40.1854, 40.0223,
        37.6767, 39.9365, 36.6713, 40.9291, 40.0145, 38.1976, 37.0526, 36.7251,
        41.0367, 43.6603, 33.0251, 34.7241, 36.9829, 35.8669, 39.9955, 36.6264,
        38.6075, 38.4680, 35.7381, 37.4352, 38.7468, 41.2736, 40.3804, 37.1533,
        38.4047, 40.6348, 40.3530, 34.6126, 41.5536, 41.3235, 40.5455, 37.4474,
        38.3120, 39.3799, 34.6927, 39.9901, 40.8241, 42.1900, 34.7594, 40.6140,
        37.5021, 37.9100, 40.1248, 41.5286, 36.3316, 40.1145, 38.1529, 37.4460,
        40.1017, 40.0375, 47.3486, 36.6463, 34.8752, 33.8180, 35.2707, 39.8436],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 15.4365,   3.6229,  12.8273,   7.6320,   6.4917,   8.5806,   9.0627,
         12.7526,   3.3506,  12.7781,   7.4916,   6.6106,   6.8736,   7.5265,
          9.5867,   3.9590,   3.9206,   3.7428,   5.5176,   5.1451,   5.6233,
          5.5105,  10.4444,   6.3034,  10.4245,   4.8978,  10.2848,   4.9174,
          9.3954,   7.0005,   4.3260,   7.9457,   4.0314,   4.3999,   4.2396,
          3.8683,   4.7085,   4.8108,   4.8484,   5.7109,   7.4486,   4.4440,
          5.3250,   4.6013,   3.4446,   5.5555,   1.8112,   6.8303,   6.6466,
          6.8766,   6.9841,   4.7102,   7.1855,   4.5299,   8.5486,   6.6696,
          3.3026,   5.0680,   5.5787,   5.0138,   5.8253,   6.1266,   4.4082,
          4.6307,   6.8292,   3.3734,   8.3807,   5.2571,   6.2032,   6.8177,
          2.4665,   5.0478,   6.7680,   5.5896, 152.7661,   5.9496,   4.8763,
          5.3144,  10.5844,   8.6301], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 39.0282,   9.5187,  29.6140,  15.9747,  13.5304,  17.6323,  18.5002,
         26.7833,   6.9251,  27.4484,  16.1677,  13.6751,  14.5751,  15.6428,
         20.2602,  10.4710,  10.0499,   7.8253,  12.4448,  10.2707,  12.1454,
         11.3645,  21.7657,  13.2530,  22.7413,  10.5085,  22.7955,  10.0796,
         19.7229,  14.8956,   9.3791,  17.1365,   8.2166,   8.5676,  10.3805,
          8.9704,  10.2770,  10.8994,   9.7531,  14.5538,  15.6087,   9.1125,
         12.0379,  10.0772,   6.9632,  10.8305,   3.7082,  14.9151,  14.1252,
         13.9714,  14.3695,  11.1309,  14.4540,   9.1577,  16.9885,  14.3498,
          6.1769,  10.5226,  16.1214,  10.0906,  11.4257,  12.9953,  10.7984,
          9.1596,  14.6502,   7.0752,  17.0491,  10.8260,  13.9375,  13.9777,
          5.0399,  11.1245,  14.1116,  11.3509, 301.2541,  13.1693,  11.1870,
         12.7754,  24.0572,  17.6975], device='cuda:0')
Outer loop valEpocw Maximum [15/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 47.3
model_train val_loss valEpocw [15/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 152.8
model_train val_loss  valEpocw [15/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 301.3
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [86.59373058 97.2137279  91.17822639 97.02489005 97.26733349 96.5997003
 96.99808726 94.71254005 97.44398826 96.47786942 98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.07658289 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.30638028
 96.90915072 96.22689782 96.9627563  93.14823162 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.55468379 96.13796128 96.24273583 96.9067141
 91.65093018 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [84.65174645 97.2137279  90.84197317 97.02489005 97.26733349 96.5997003
 96.99808726 94.73934284 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.08099317
 96.90915072 96.22689782 96.9627563  93.87678025 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.17822639 96.13796128 96.24273583 96.9067141
 91.42554306 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [95.86942799  4.9311073  54.9564747   7.74187006  4.51157812  8.58646199
  8.18523503 31.40892095  3.12270651 26.64124253  1.71784611  1.32849894
  0.91240289 12.15907609  2.55693847  3.1769482   3.94125663  3.04945537
  0.91137919  1.69388877  1.54949402  0.48893554 41.01614266  1.89811191
 15.98841816  6.69376627 24.46058645  9.24209083  3.88964348  1.49718114
  2.55524136  0.99684143 14.17100081  1.52749946  1.90762126  3.75908195
  5.63236943  2.88852475  6.94611408 20.52242298  8.48982941 38.07351965
  6.11233971 15.6215797  12.5848799  30.14319183  2.75184692  1.68411382
  2.74276907  1.6886039   1.27017632  1.26410319  1.24117013 19.26174713
  1.71369501  4.61911893 62.19578183 18.27691616  8.53891791  4.15214138
 59.79102868  1.87607337  7.69112095  6.43237325  2.70027407  4.55307658
  3.06046842  9.20136855  1.91100209  4.11011821  0.27434604  9.76731765
  3.55468495 17.74678693  5.81772069  5.18039162  1.09657544  2.2055662
  0.16312156  0.77441952]
Accuracy th:0.5 is [45.74505671 97.2137279  72.12631425 97.02489005 97.26733349 77.03975341
 77.34311229 76.30511324 78.25440723 96.38405965 78.56751258 98.52097319
 99.41399349 80.01729998 78.16790731 96.56680596 96.29512311 77.85480196
 98.65376884 98.30776915 80.0197366  79.07067409 98.38695922 77.92059064
 80.66178531 96.65086926 94.0778012  77.54656985 98.01293844 78.46030141
 97.30875598 98.57457877 96.36213009 98.02024829 86.03452687 78.03998489
 77.87794983 89.3276154  97.11504489 75.14771994 78.99026571 92.05906361
 77.32483766 77.00076753 96.9627563  93.87434364 98.02877645 98.57336046
 88.39682752 86.65586433 86.44022368 98.55508583 98.99976852 77.51611213
 98.70615611 77.90353431 72.59658143 92.68161938 96.24273583 96.9067141
 89.79300934 97.17717864 91.01862794 77.9437385  98.42838172 78.45177325
 98.20786784 77.04097172 78.97686432 97.53536141 79.50804693 95.99054592
 78.64304772 95.45083515 77.23468281 81.9142067  85.50090764 78.52974501
 79.587237   99.14718388]
Accuracy th:0.7 is [45.7438384  97.2137279  72.12631425 97.02489005 97.26733349 77.03975341
 77.34311229 76.54268345 78.25440723 96.46934126 78.56751258 98.52097319
 99.41399349 80.46807422 78.16790731 96.56680596 96.29512311 77.85480196
 98.65376884 98.30776915 80.45223621 79.07067409 98.38695922 78.02414688
 81.20880594 96.65086926 94.0778012  77.54656985 98.01293844 78.46030141
 97.30875598 98.57457877 96.36213009 98.02024829 86.2343295  78.03998489
 77.87794983 89.61635458 97.11504489 75.14771994 79.7407439  92.05906361
 77.32483766 77.00076753 96.9627563  93.87434364 98.02877645 98.57336046
 90.5666354  87.54766633 86.61322352 98.55508583 98.99976852 77.51611213
 98.70615611 77.90353431 72.59658143 93.11290067 96.24273583 96.9067141
 89.79300934 97.17717864 91.20502918 77.9437385  98.42838172 78.45177325
 98.20786784 77.04097172 78.97686432 97.55972759 79.50804693 95.99054592
 78.68325191 95.45083515 77.23468281 82.00679816 85.63735822 78.52974501
 79.587237   99.14718388]
Avg Prec: is [55.6546321   3.07057666 11.21316058  3.41105548  2.22122557  3.85632773
  3.32203924  5.53065861  2.52520837  3.77703386  1.64614613  1.61360003
  0.59807545  5.0280582   2.56786456  3.15657225  3.52455027  2.71137187
  1.37720258  1.81395308  2.04658515  0.8172702   1.74793209  2.52257971
  5.00508545  3.58557196  6.49837875  3.35023996  2.07413424  1.86256654
  2.5578763   1.3183064   3.79158335  1.59019697  2.28523958  2.47224668
  3.03381466  2.550323    2.82649303  7.48385233  2.30742966  8.43652161
  3.331426    3.9237525   3.21612168  6.52799717  2.01679256  1.52422797
  2.18604224  1.56432179  1.84901699  1.62500069  1.09153187  2.95779273
  1.28061202  2.63205193 11.13926379  3.73542648  4.01420816  2.84760394
 10.89746683  2.19498222  3.98497406  3.05614605  1.63166352  2.51781982
  1.83856508  4.19085022  1.2590875   2.33844482  0.20895447  3.46364642
  1.94390809  4.48373439  4.02428602  3.21306399  0.89652577  1.8231818
  0.14927268  0.78977878]
mAP score regular 10.44, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [88.31751252 97.22450607 91.26990059 96.96290206 97.90716795 96.63651992
 96.80843112 94.65580387 97.38894287 96.43969405 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.34187906 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.57792062
 97.07750953 96.48703192 97.03764606 92.68505369 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.70682911 96.39235618 96.16314124 96.78102499
 91.73580487 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [87.19136956 97.22450607 91.6735182  96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.45085582
 97.07750953 96.48703192 97.03764606 94.03293719 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.90116351 96.39235618 96.16314124 96.78102499
 92.14440541 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [96.60616712  5.55420786 59.57217178  8.13988141  4.39084622  9.74811981
  9.29971321 32.91758494  3.97897012 29.67758661  1.98531052  1.34951009
  0.951594   13.63497509  2.80257029  3.37680533  4.02069562  3.09842578
  0.85394341  1.9641855   1.40468496  0.49966139 52.04166315  1.84886589
 16.91309783  7.39977525 24.6025091   9.51991106  4.64296537  1.58524848
  2.41382197  0.91601642 18.96170317  1.55528704  1.63703478  3.78110223
  5.38066487  3.74029026  8.28638283 20.72884667  7.95410627 39.90976102
  5.63321377 15.15310052 12.09057527 29.28610242  2.89874934  1.67325117
  2.89351925  1.75372246  1.33866901  1.47204706  1.49442738 19.18949494
  1.89773796  4.10711677 56.61077266 18.64977789  7.95424036  4.57710798
 60.79126482  1.84881163  7.27858691  6.10478086  2.60415517  4.155818
  2.80997861  9.76056318  1.56909541  3.67080761  0.25038133 10.02097598
  2.96256606 19.38900814  6.79593747  4.85320373  1.0573948   2.23097018
  0.15716762  0.7001666 ]
Accuracy th:0.5 is [45.31977975 97.22450607 70.63557316 96.96290206 97.90716795 76.15168049
 76.24386476 75.102773   77.69638986 96.41477938 77.87577547 98.5325261
 99.34972718 78.05516107 77.79355707 96.31262924 96.21047911 77.22051972
 98.78167277 98.34068316 78.78765229 78.51608242 98.31327703 77.30522959
 78.16478561 96.52938685 94.3393876  77.25789172 97.81747515 77.8433864
 97.52597354 98.67204823 96.39983058 98.18870369 86.66567008 77.45471759
 77.41236266 91.18768219 97.0276802  74.53222712 77.66649226 92.37362035
 76.5926701  76.2563221  97.03764606 94.02795426 98.18621222 98.77668984
 89.02758054 86.67812741 84.7995615  98.55993223 98.87385704 76.5777213
 98.6969629  77.25540025 71.22355931 93.83860279 96.16314124 96.78102499
 90.13379176 97.04761193 90.74918404 77.23297705 98.32075143 78.08256721
 98.13139996 76.40331863 78.53850562 97.52099061 78.9819867  96.07843137
 78.27939308 95.44559882 76.33604903 82.90853826 87.4205845  77.910656
 79.05174776 99.15040985]
Accuracy th:0.7 is [45.6611107  97.22450607 70.63557316 96.96290206 97.90716795 76.15168049
 76.24386476 75.19495727 77.69638986 96.41976231 77.87577547 98.5325261
 99.34972718 78.42638962 77.79355707 96.31262924 96.21047911 77.22051972
 98.78167277 98.34068316 79.16386377 78.51608242 98.31327703 77.32516132
 78.60328375 96.52938685 94.3393876  77.25789172 97.81747515 77.8433864
 97.52597354 98.67204823 96.39983058 98.18870369 86.93973142 77.45471759
 77.41236266 91.34962753 97.0276802  74.53222712 78.12741361 92.37362035
 76.5926701  76.2563221  97.03764606 94.02795426 98.18621222 98.77668984
 91.22256272 87.06679622 84.95652391 98.55993223 98.87385704 76.5777213
 98.6969629  77.25540025 71.22355931 94.14505319 96.16314124 96.78102499
 90.13379176 97.04761193 90.90365498 77.23297705 98.32075143 78.08256721
 98.13139996 76.40331863 78.53850562 97.53593941 78.9819867  96.07843137
 78.28935895 95.44559882 76.33604903 82.99075666 87.55014077 77.910656
 79.05174776 99.15040985]
Avg Prec: is [54.08235998  3.73277076 14.8255099   4.5437469   1.48749179  4.26285011
 13.36354615  8.63129763  8.02371804  5.24163714  2.33350715  4.81834193
  1.8594126   5.85339875  2.91293223  3.6549298  24.39135046  6.48972987
  1.56006158  2.71724372  3.53758748  1.48613016  1.16969388  5.31429116
  5.6805204   9.52175343  7.94715968  4.61990562  3.92745583  5.72754973
  2.24898889  0.86250107  3.00567001  1.15358826  1.71886923  2.29162756
  2.00111516  2.23261933  2.18946229  6.24376772  1.71659426  6.01825035
  2.1658114   2.69912072  2.33926963  4.86116661  1.70164637  1.02725852
  1.43173908  1.14810527  1.20224103  0.9925179   0.74448071  2.26567315
  0.88971444  1.86858175 10.16625975  3.01790514  3.94335003  2.88637678
  7.86311464  2.06217774  3.29023175  2.50629814  1.35288544  1.91787591
  1.55365778  3.51907531  1.07975487  2.21589065  0.19464926  3.237162
  1.57836794  4.02173201  3.19708381  2.30714247  0.57177117  1.45635647
  0.12346353  0.61008263]
mAP score regular 10.84, mAP score EMA 4.32
Train_data_mAP: current_mAP = 10.44, highest_mAP = 10.44
Val_data_mAP: current_mAP = 10.84, highest_mAP = 10.84
tensor([0.3956, 0.3757, 0.4273, 0.4731, 0.4809, 0.4814, 0.4880, 0.4686, 0.4803,
        0.4601, 0.4615, 0.4816, 0.4650, 0.4756, 0.4731, 0.3728, 0.3871, 0.4779,
        0.4339, 0.4978, 0.4584, 0.4833, 0.4770, 0.4730, 0.4572, 0.4636, 0.4511,
        0.4834, 0.4742, 0.4623, 0.4567, 0.4602, 0.4889, 0.4968, 0.4072, 0.4245,
        0.4542, 0.4351, 0.4919, 0.3878, 0.4773, 0.4819, 0.4383, 0.4571, 0.4954,
        0.5121, 0.4827, 0.4531, 0.4653, 0.4896, 0.4851, 0.4147, 0.4960, 0.4870,
        0.5009, 0.4694, 0.5271, 0.4751, 0.3424, 0.4914, 0.5037, 0.4699, 0.4039,
        0.4991, 0.4647, 0.4727, 0.4891, 0.4861, 0.4440, 0.4875, 0.4891, 0.4521,
        0.4794, 0.4871, 0.5062, 0.4411, 0.4345, 0.4090, 0.4430, 0.4827],
       device='cuda:0')
Max Train Loss:  tensor([14.1864,  5.9446, 13.2333, 12.5765,  6.6531,  6.7948,  9.7806, 10.1693,
         8.0916,  9.4110,  8.2792,  6.6814,  8.9551,  7.7129,  8.6452,  9.9553,
         6.4247,  9.9640,  8.8347,  7.5157,  7.9009, 10.1282,  8.7691,  7.8506,
        11.8800,  6.1872,  9.5692, 11.3095, 10.1369,  7.5586,  8.3047, 10.9551,
         9.7486,  7.9694,  8.6623,  7.4141,  7.3383,  8.6145, 10.9060, 16.3336,
        11.0946, 10.7913, 11.0404, 13.7851,  9.0297, 13.6994,  4.9508,  8.4125,
        11.4841,  7.5260, 10.9516,  6.1758,  8.9261,  9.0518, 11.7090, 12.5018,
        12.1784,  7.9826,  5.7024,  8.6567, 13.1490, 10.8872,  7.9180, 10.9056,
         9.1796,  9.6579, 10.7956, 13.9053,  8.7493,  9.5061,  2.8758, 11.9832,
         9.7945,  9.9179, 16.6744, 10.8500,  6.7674,  7.8175, 11.3299, 10.2320],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [16/80], Step [000/642], LR 1.0e-04, Loss: 16.7
Max Train Loss:  tensor([17.1151, 12.7445, 19.5174, 14.1586, 13.2504, 10.7030, 10.7383, 15.1806,
        17.2339, 16.2501, 13.9715, 13.5934, 20.5453, 14.7720, 10.7115,  8.8125,
        15.1541, 18.9369, 11.4548, 11.2876,  9.1734, 11.0957, 14.8486, 10.3576,
        14.0972, 13.7566, 19.8287, 12.8128, 16.3046, 11.7631,  8.8009, 13.2652,
        18.9666, 16.1033,  8.7011, 10.2038, 10.0050,  7.8602, 13.5136, 11.9025,
        12.6239, 18.6688,  8.0596,  9.7154, 12.4948, 14.5142, 15.4566,  9.4025,
         8.1106, 16.7021, 15.9737,  9.4740, 12.9842, 16.2903,  7.5879,  8.7378,
        15.6510, 13.5047,  6.8369, 12.6207, 15.9497,  8.9576, 10.3030, 14.5639,
         9.1659, 20.8465, 10.6776, 12.1758,  7.8980, 18.1567, 13.4209, 11.8219,
        20.3842, 16.2043, 14.4443,  9.3525,  7.0991,  7.7539,  5.0902,  8.6526],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [16/80], Step [100/642], LR 1.0e-04, Loss: 20.8
Max Train Loss:  tensor([17.0077,  7.5442, 12.5135, 10.3846, 12.4125, 10.9352,  8.5553, 10.8770,
        13.3758, 10.6927, 10.6714, 11.3992,  7.5301, 13.5459,  9.6212,  7.5569,
         9.8207,  9.9243,  7.7458,  8.1623,  9.3097,  8.6338, 10.8304, 10.4414,
         7.1280, 11.4711,  9.8497, 12.7314,  7.8377, 13.0062,  9.0093,  9.7889,
        12.4093, 10.0860,  9.7679,  7.2443, 14.0451, 12.1230, 11.9091, 12.4653,
        14.8280, 18.6614,  9.0492, 11.2660, 12.7538, 14.3755, 11.8289,  9.0816,
        11.2778,  8.4066,  6.8516,  6.2230,  4.9779,  9.5568,  8.2307, 11.5454,
        15.0974, 11.9302,  8.0945,  8.9914, 14.6778,  9.5501,  7.8610, 13.5115,
         9.7138, 10.9207, 12.0699, 11.2533,  8.8464, 13.1544,  8.2313, 10.5063,
         9.5178, 13.8022,  7.6721,  8.0346,  8.6755,  9.9499,  4.7781,  7.4022],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [16/80], Step [200/642], LR 1.0e-04, Loss: 18.7
Max Train Loss:  tensor([16.0797,  7.6668, 11.2109, 10.2987, 13.1639, 12.1255, 10.3081, 11.2645,
        14.7081,  8.0607,  8.8878,  9.3305,  8.6039, 14.9655,  9.8225, 10.5366,
         4.5966,  9.5333,  6.8182,  9.8526, 15.6921, 10.9957,  8.6580, 11.6249,
         9.7581, 11.6087, 10.7435,  9.6887, 11.0126,  8.7437, 10.2786, 11.7054,
         8.8819, 10.5912,  4.9059,  5.6699, 11.7576, 11.0765,  6.9156, 13.9649,
        11.3121, 17.6539,  8.0328,  7.9308, 13.1793, 14.8860,  9.9495,  8.1318,
         8.9864,  8.6498,  8.2994,  8.1860,  5.9671,  9.8313,  8.1946,  8.5088,
        15.6985, 11.7130,  7.1933,  8.4386, 13.4160, 10.5192, 12.1118, 16.3672,
        11.8737,  9.1379, 14.1977, 16.5428,  8.1921, 11.0102,  8.5493, 10.9592,
         9.8415, 12.7508, 11.5926, 11.8661,  6.9024,  6.6961,  5.0531,  8.7573],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [16/80], Step [300/642], LR 1.0e-04, Loss: 17.7
Max Train Loss:  tensor([18.9441,  7.3590, 12.6169,  7.5064, 11.4757, 15.0883, 10.8779, 10.5956,
        10.7047,  9.4097, 10.7909, 11.7208,  7.4778, 13.6073, 11.0400,  8.5255,
        12.2052,  8.9146,  6.3432,  5.9319,  8.3313, 12.9019,  9.8546,  5.0405,
        10.6963, 13.9048,  9.6287,  9.9267, 11.1864, 10.6853,  8.1723, 10.8626,
         9.8208,  5.6665,  6.7033,  8.7606, 10.5656, 12.4775,  7.6802, 12.9237,
        11.9391, 11.2728,  7.8187,  7.0344,  7.7381, 12.5839,  9.4045,  8.3625,
         6.5310, 10.4566,  6.2394,  6.2291,  5.6442,  5.6473,  8.7772,  9.8322,
        23.5448, 14.5443,  7.7245, 11.3291, 16.9213,  8.4746,  8.6545,  6.7925,
         8.2603, 11.8465,  9.3820, 10.4867,  7.2310, 10.6651,  8.9561, 10.9500,
         8.5965, 15.1202, 10.5722,  7.3022,  7.7364,  8.9583,  4.7324,  9.5790],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [16/80], Step [400/642], LR 1.0e-04, Loss: 23.5
Max Train Loss:  tensor([15.8981,  6.6443,  8.5018,  8.8629, 10.9636,  9.3334, 12.0928,  9.3663,
         6.3281,  9.6663,  9.6006, 11.7802,  8.3482, 11.0466,  9.1769,  8.1225,
         8.1608,  9.1296,  5.3602,  6.1026,  9.9452, 10.8461,  5.6951, 13.4694,
        11.0086,  6.2459, 12.0317,  7.6549,  8.4914,  8.7843,  9.9488, 10.5810,
         6.2102, 10.0814,  7.6156,  8.0490, 10.1742, 10.8784,  7.3333, 10.7002,
        11.6188, 12.8452,  8.1575,  7.7217,  7.6476, 11.6697, 11.6769,  8.2468,
         8.5063,  8.3716,  5.3599,  6.4935,  5.8168,  7.6465,  9.1340,  9.6056,
        13.9548, 12.7249,  5.8297, 13.3530, 11.5351, 11.1356,  9.8621,  9.3458,
         8.0642, 13.3913,  9.9367, 10.2301,  8.1972,  7.1823,  8.1065,  9.6028,
         7.7251, 12.4138, 10.5239,  7.2264,  6.1672,  7.5071,  4.6629,  8.1296],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [16/80], Step [500/642], LR 1.0e-04, Loss: 15.9
Max Train Loss:  tensor([13.9359,  8.0398, 11.3961,  7.8117,  7.2423, 11.2456, 11.8980, 10.2540,
         7.1966, 12.1476, 11.2638, 10.5737,  8.7991, 13.9985, 10.5312,  8.9264,
         8.8282,  8.5015,  7.1275,  8.0885, 11.0661, 10.1009,  8.7693, 14.4773,
        12.4715,  9.1091, 16.0554, 12.6431,  9.7698,  8.9822,  9.3227, 14.9581,
        10.0134,  5.9226,  9.1982,  7.6512,  8.0280,  6.4916, 10.0048,  7.0260,
        13.9810, 12.1823,  7.0333,  9.3822,  8.2291, 10.7978,  9.4749,  7.4982,
         6.0554, 11.0195,  9.7076,  6.3362,  5.8308,  7.9497, 10.5563,  6.1731,
        13.9084, 13.7120,  8.4124,  9.2651, 11.4001,  7.6676,  9.7242,  7.8162,
         9.9435,  8.9928, 10.5777,  8.6197,  8.0378, 10.5678,  9.0838,  9.1418,
         9.6202, 13.4646, 11.8433,  8.0874,  7.5842,  6.8050,  4.9700,  7.6307],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [16/80], Step [600/642], LR 1.0e-04, Loss: 16.1
Max_Val Meta Model:  tensor([ 25.6249,  24.0500,  38.2705,  22.3349,   6.2224,   7.9997,   9.1966,
         10.2693,   6.8543,   8.7208,   9.3354,  10.7324,   8.3362,   9.0729,
          7.3559,   4.7708, 120.8010,   6.2427,   5.5108,   4.9477,   7.1290,
          8.7249,   5.9464,   5.2466,  14.9664,  10.0783,  13.0381,   5.9422,
          9.6697,   9.9621,   6.1548,   9.8596,   8.7077,   5.7786,   4.6753,
          5.4388,   6.2957,   6.4119,   5.5502,  18.9343,  13.1761,  12.1058,
          7.4267,  10.5311,   9.8435,  12.8375,   5.2547,   8.1611,   5.8141,
          8.6223,   5.5369,   5.0531,   6.5208,   5.7006,   6.1266,   3.8232,
         14.6968,   9.5542,   7.4569,   5.7842,   8.6336,  14.0596,   5.4572,
          6.8979,   7.5816,   8.1075,   8.7306,   6.6399,   9.0838,  13.8270,
          8.3188,  10.9651,  12.4167,   8.3825,  10.3028,   9.1751,   5.6576,
          5.5468,   4.8057,   7.4653], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 23.2953,  23.3928,  31.1997,  22.5896,   6.8824,   8.6908,   9.9808,
         11.4475,   7.0670,   9.5229,   9.8102,  11.2557,   8.6993,   9.2946,
          7.6644,   5.0878, 116.8741,   6.7855,   5.8710,   5.1921,   7.5614,
          9.1682,   6.1932,   5.4933,  15.1746,   9.7139,  14.5919,   6.9000,
         10.2256,   9.8714,   6.8811,  10.3652,  10.0655,   6.0956,   4.9984,
          5.7810,   7.0056,   6.8043,   6.1505,  18.3659,  13.4940,  12.0744,
          7.8265,  10.7992,  10.1886,  12.3818,   5.3204,   8.6851,   5.7527,
          8.9308,   5.7369,   5.3939,   6.5812,   5.7562,   6.4897,   3.8238,
         13.7678,  10.2372,   7.1154,   5.7414,   7.7691,  13.4872,   5.7755,
          7.1697,   8.0127,   8.5125,   9.1543,   7.9595,   9.4459,  14.0206,
          8.7411,  10.9306,  13.0206,   8.3807,  10.7591,   9.1583,   6.0220,
          5.8679,   5.1480,   7.8517], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 58.8922,  62.2701,  73.0165,  47.7475,  14.3108,  18.0540,  20.4545,
         24.4289,  14.7131,  20.6955,  21.2562,  23.3707,  18.7080,  19.5419,
         16.2015,  13.6492, 301.9379,  14.1975,  13.5313,  10.4293,  16.4958,
         18.9718,  12.9844,  11.6140,  33.1926,  20.9535,  32.3470,  14.2733,
         21.5616,  21.3532,  15.0661,  22.5249,  20.5877,  12.2700,  12.2735,
         13.6185,  15.4241,  15.6376,  12.5042,  47.3606,  28.2740,  25.0553,
         17.8572,  23.6263,  20.5653,  24.1763,  11.0232,  19.1699,  12.3646,
         18.2428,  11.8253,  13.0064,  13.2679,  11.8196,  12.9570,   8.1462,
         26.1188,  21.5477,  20.7780,  11.6828,  15.4234,  28.7043,  14.3001,
         14.3643,  17.2413,  18.0101,  18.7153,  16.3750,  21.2735,  28.7621,
         17.8707,  24.1751,  27.1600,  17.2060,  21.2563,  20.7626,  13.8609,
         14.3469,  11.6200,  16.2676], device='cuda:0')
Outer loop valEpocw Maximum [16/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 120.8
model_train val_loss valEpocw [16/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 116.9
model_train val_loss  valEpocw [16/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 301.9
Max_Val Meta Model:  tensor([40.0760, 30.2163, 36.2285, 42.8636, 44.1122, 41.7337, 51.0503, 38.7154,
        39.7646, 39.5511, 39.2323, 46.7477, 39.1104, 41.5442, 42.5657, 30.8685,
        33.7489, 39.9434, 36.9886, 44.7325, 39.1324, 40.5133, 39.6394, 40.9079,
        38.5362, 38.6910, 37.3765, 39.7592, 40.9465, 37.6088, 37.1711, 38.1349,
        40.9162, 39.1807, 34.7987, 34.7413, 37.7754, 35.4705, 40.7772, 32.5861,
        40.9525, 40.4969, 36.7373, 38.5183, 40.6940, 41.9090, 39.5224, 37.4837,
        39.5066, 40.5275, 40.7021, 34.1269, 38.2037, 40.9014, 40.7310, 38.5063,
        41.4245, 40.0002, 33.0075, 40.2446, 42.8872, 38.4809, 33.7712, 40.4884,
        38.8133, 39.4462, 40.8529, 38.5096, 37.3721, 41.1171, 41.8499, 38.6076,
        40.4215, 40.4737, 37.9243, 35.4290, 35.5663, 33.7088, 38.0511, 40.3873],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([24.5465,  6.9144, 20.4511,  6.9054,  7.2073, 17.2711, 37.6094, 21.1151,
        16.8635, 15.9315, 11.6781, 70.4999,  8.5898,  5.3138,  9.5099,  4.4204,
         5.6878,  9.6085,  4.7809, 12.0372,  6.3618,  7.7283,  5.6562,  5.9189,
        12.9664,  9.9432, 15.0557,  8.2586,  9.8793,  6.5635,  5.6058,  8.6145,
         6.1429,  4.7964,  4.1350,  4.6494,  6.8547,  4.4748,  4.3270,  3.1824,
        10.2040,  2.6867,  5.2502,  4.3995,  4.7714,  3.0017,  4.0562,  6.4625,
         4.3104,  7.4643,  4.6511,  4.2743,  3.7361,  4.4494,  5.2099,  4.0990,
         4.5946,  7.0501,  2.8474,  3.8459,  2.4880,  3.2956,  4.1006,  4.6356,
         6.7933,  7.1075,  7.7118,  7.7505,  5.8694,  6.6603,  7.6179,  4.6090,
         7.4177,  5.3058,  9.5654,  5.5280,  4.8925,  4.1403,  4.2442,  6.4030],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 60.1430,  19.0812,  49.2603,  14.3232,  14.5012,  36.0234,  74.9204,
         45.3788,  35.8966,  34.2396,  25.3534, 147.3264,  18.6875,  11.1016,
         19.9279,  12.0672,  14.5958,  19.8840,  11.1043,  24.0595,  13.8173,
         16.2086,  11.8333,  12.4858,  28.4823,  21.7708,  33.8605,  17.1722,
         20.5064,  14.6301,  12.7773,  19.3384,  12.6703,  10.0906,   9.9644,
         11.2146,  15.3042,  10.6089,   8.8852,   8.3660,  20.8631,   5.5165,
         11.9474,   9.6806,   9.6304,   5.8621,   8.4918,  14.4102,   9.1502,
         15.3032,   9.5783,  10.6103,   7.8530,   9.0967,  10.5374,   8.7427,
          9.0657,  15.0126,   8.1413,   7.8189,   4.9411,   7.1022,  10.2727,
          9.6096,  14.5155,  15.1571,  15.8338,  16.4703,  13.2211,  13.7210,
         15.1964,  10.2008,  15.5799,  10.9884,  20.0924,  13.1193,  11.4076,
         10.3258,   9.3996,  13.4446], device='cuda:0')
Outer loop valEpocw Maximum [16/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 51.1
model_train val_loss valEpocw [16/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 70.5
model_train val_loss  valEpocw [16/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 147.3
Max_Val Meta Model:  tensor([35.0172, 30.6653, 37.2268, 41.3422, 41.5016, 40.8999, 37.7864, 41.8472,
        41.8896, 41.3794, 42.2238, 40.7458, 39.7296, 41.6599, 42.3855, 31.3083,
        33.9928, 39.3294, 37.9076, 41.7436, 39.6059, 40.2546, 42.0647, 42.0798,
        38.0249, 37.4742, 38.0278, 42.3142, 40.9786, 38.5288, 38.4052, 39.1811,
        42.1289, 40.4709, 35.2370, 35.4140, 38.4849, 36.7365, 41.8822, 33.1247,
        41.1747, 42.5073, 37.9918, 39.6717, 41.9974, 44.9441, 41.7072, 37.9858,
        41.8398, 41.5488, 43.5091, 35.1909, 39.9494, 41.5732, 41.4147, 40.2913,
        46.2661, 39.1675, 32.4643, 40.2902, 46.4752, 38.8310, 34.2388, 41.2952,
        39.3247, 40.0692, 41.3821, 38.8879, 37.8631, 40.5649, 41.8803, 39.0842,
        41.2526, 41.2544, 39.0341, 36.7487, 36.0825, 34.4959, 38.2637, 39.9307],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 6.9749,  5.4953,  3.4833,  6.0442,  6.4517,  5.8356,  4.4068,  7.8894,
         5.7927,  5.6600, 11.1257, 11.2540,  9.5683,  9.0615,  4.7871,  5.4881,
         6.9656,  7.3239,  7.5082,  5.9701,  9.3240, 10.8017,  6.7349,  5.9495,
         6.8144,  6.0813, 13.2567,  9.9085, 10.5377,  9.7431,  6.9341, 12.6712,
         8.8006,  7.0155,  6.5429,  7.1831, 10.3971,  8.1805,  7.5760, 18.7344,
        19.5993, 35.5260, 34.7023, 33.7470, 28.1200, 37.0431,  9.9202, 11.6355,
        21.3425, 11.8430, 17.7188, 18.2025, 27.4035, 14.6647, 30.3946, 50.9585,
        54.5202, 11.7164,  4.0743,  9.1311, 42.0429,  6.3697,  8.6798, 11.0003,
        12.1643, 10.4319, 12.5028, 12.9046,  9.4905,  8.5183, 10.6397, 10.3018,
        10.4651, 19.5571,  5.0478, 10.9352,  8.9946,  6.5787,  6.6489,  9.7200],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 17.3353,  15.0723,   8.1963,  12.6080,  13.3246,  12.2064,   9.3102,
         16.3054,  11.9619,  12.0037,  23.1327,  23.4520,  20.7280,  18.7531,
         10.0083,  14.9151,  17.8737,  15.4028,  17.1982,  12.1182,  20.2552,
         22.6225,  13.8303,  12.3585,  14.9716,  13.5540,  29.6633,  20.2950,
         21.7545,  21.4533,  15.5143,  27.9953,  17.8894,  14.4805,  15.7800,
         17.2181,  23.1275,  19.0140,  15.3713,  49.1341,  40.4987,  72.3784,
         78.9873,  73.9782,  56.8711,  70.8271,  20.1222,  25.9420,  44.4454,
         23.7817,  35.3205,  44.6400,  57.2309,  29.9811,  61.6441, 109.2535,
        107.1337,  25.1678,  11.4963,  18.5991,  82.5548,  13.6867,  21.7680,
         22.3535,  26.0235,  22.1585,  25.6655,  27.0488,  21.3723,  17.7048,
         21.4602,  22.7236,  21.8044,  40.1835,  10.3921,  25.3740,  20.9713,
         16.2465,  14.8442,  20.2985], device='cuda:0')
Outer loop valEpocw Maximum [16/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 46.5
model_train val_loss valEpocw [16/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 54.5
model_train val_loss  valEpocw [16/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 109.3
Max_Val Meta Model:  tensor([38.0313, 29.2341, 35.4280, 39.5276, 40.2300, 38.9507, 35.9008, 39.2186,
        38.0268, 40.2247, 37.5226, 39.1071, 37.5792, 38.8752, 39.3671, 29.9405,
        33.3069, 38.6422, 35.5632, 39.2205, 38.0613, 37.9991, 38.9073, 40.1471,
        36.3953, 35.8907, 36.6490, 40.0034, 39.5222, 35.5668, 35.0916, 35.9434,
        38.0830, 36.8823, 34.0595, 32.9813, 35.5952, 33.4674, 36.9607, 31.8761,
        37.5875, 39.4600, 35.6943, 37.6516, 38.0600, 37.0342, 39.6417, 36.2837,
        36.8911, 40.0687, 38.8894, 32.2084, 37.2250, 40.0200, 39.6754, 37.3944,
        37.7288, 36.9924, 31.3873, 39.1676, 39.8969, 38.4978, 33.2478, 39.3713,
        38.0188, 39.8791, 38.5204, 41.0451, 36.8397, 38.0467, 38.1700, 38.5002,
        39.7585, 38.6109, 43.2881, 32.9174, 34.4358, 31.9279, 37.4881, 36.0370],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 16.5783,   3.4191,  10.8153,   4.4123,   6.6080,   8.1111,  11.0960,
         10.9245,   5.4674,  11.3386,   8.5705,   9.1810,   6.6741,   7.0749,
          9.7255,   3.3897,   4.0519,   5.9515,   4.6851,   4.5358,   6.3540,
          7.6092,   5.9992,   5.2176,  10.5358,   4.8297,  11.2263,   4.4468,
          9.5411,   6.3653,   4.7943,   8.2361,   4.4668,   4.6507,   4.1799,
          4.5726,   5.0019,   4.1709,   3.7166,   4.0162,   9.9920,   3.3064,
          5.2515,   4.6846,   5.0590,   3.5163,   4.4432,   6.4172,   4.6230,
          7.0354,   4.8676,   4.1627,   3.8225,   4.6359,   5.2159,   3.3605,
          2.8933,   6.0879,   4.6316,   4.6738,   4.8522,   3.9417,   4.1557,
          4.8829,   6.8248,   7.2754,   7.6563,   5.1404,   5.9457,   5.7857,
          7.4361,   5.8337,   6.3067,   6.0782, 110.6494,   6.0498,   4.8802,
          4.7951,   4.3219,   6.2959], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 39.7790,   9.5473,  27.1336,   9.4383,  13.7193,  17.4521,  23.5950,
         23.6105,  11.8976,  24.3013,  18.8606,  19.4342,  14.8477,  15.2538,
         20.8647,   9.4133,  10.4911,  12.3018,  11.2184,   9.5082,  13.9394,
         16.4302,  12.6546,  11.1796,  23.6755,  10.9749,  25.3705,   9.3107,
         20.0328,  14.7521,  11.3627,  19.3534,   9.3691,  10.2135,  10.0705,
         11.3846,  11.6443,  10.2710,   7.9850,  10.6532,  21.0598,   6.9774,
         12.0636,  10.3328,  10.4373,   7.2473,   9.3168,  14.5328,  10.0363,
         14.4126,  10.0805,  10.7216,   8.1133,   9.5538,  10.7346,   7.1757,
          6.0228,  13.4120,  13.5048,   9.6025,   9.9721,   8.5195,  10.3817,
         10.3679,  14.6386,  15.4439,  16.0797,  10.8238,  13.3417,  12.2380,
         15.4237,  12.9937,  13.4126,  12.9350, 239.4722,  15.1866,  11.5209,
         12.3756,   9.4993,  13.7188], device='cuda:0')
Outer loop valEpocw Maximum [16/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 43.3
model_train val_loss valEpocw [16/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 110.6
model_train val_loss  valEpocw [16/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 239.5
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [86.61322352 97.2137279  91.3049305  97.02489005 97.26733349 96.5997003
 96.99808726 94.73690623 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.47085196
 96.90915072 96.22689782 96.9627563  93.89018165 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.61925415 96.13796128 96.24273583 96.9067141
 91.77032444 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [83.69902901 97.2137279  90.01839646 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.19916911
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 90.40825526 96.13796128 96.24273583 96.9067141
 91.14167712 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [95.92841225  3.92337114 57.0450666   6.56555639  2.70493494  9.50611118
  9.2649129  31.630176    3.40429519 18.66476053  1.55881302  1.25429201
  0.79528201  9.76670002  2.68151361  3.5196281   3.65257906  2.19427384
  0.80889182  1.18160119  1.18891552  0.45997654  0.87587436  1.37123944
 15.4805151   5.48071518 23.62953681  8.39524334  3.82910486  1.25082434
  2.62989102  0.88747413 18.99080701  1.36030822  1.53379878  1.49750824
  3.87914676  2.02678409  4.11926956 26.55857251 10.01255684 40.79734022
  8.64194612 18.08903393 12.39484872 30.62443149  3.28515067  2.15157859
  9.4410617   2.56357518  1.94501899  1.66068954  1.56709604 13.24603489
  2.15104112  6.6627852  60.58419768  9.90973583 10.32188339  7.26642331
 58.18977106  4.72943335 10.61335303  8.71776245  3.76117692  5.27551624
  4.1130878  10.01259211  3.20169362  6.85351866  0.57761262 17.78677689
  5.02472569 21.15478045 10.22802986  7.5966383   1.33844288  2.2865851
  0.22712858  0.96659416]
Accuracy th:0.5 is [45.63297231 97.2137279  72.12387763 97.02489005 97.26733349 77.09092238
 77.41621082 76.40623287 78.33481561 96.37918641 78.66497728 98.52097319
 99.41399349 80.2000463  78.20689319 96.56680596 96.29512311 78.03998489
 98.65376884 98.30776915 80.22441247 79.20712467 98.38695922 78.10821018
 80.6885881  96.65086926 94.0778012  77.69520352 98.01293844 78.54558302
 97.30875598 98.57457877 96.36213009 98.02024829 86.11128032 78.26171708
 77.98272438 89.14974233 97.11504489 75.17695935 79.24001901 92.05906361
 77.48078118 77.02269709 96.9627563  93.87434364 98.02877645 98.57336046
 88.84882007 86.71190653 86.58885735 98.55508583 98.99976852 77.61601345
 98.70615611 77.94252019 72.56977863 92.65481658 96.24273583 96.9067141
 89.79300934 97.17717864 91.21599396 77.98759762 98.42838172 78.52487177
 98.20786784 77.19447862 79.05727269 97.52561494 79.62744119 95.99054592
 78.75025889 95.45083515 77.36626016 81.9422278  85.67390748 78.61989986
 79.68713831 99.14718388]
Accuracy th:0.7 is [45.66952157 97.2137279  72.12387763 97.02489005 97.26733349 77.09092238
 77.41621082 76.63649322 78.33481561 96.46568633 78.66497728 98.52097319
 99.41399349 80.6459473  78.20689319 96.56680596 96.29512311 78.03998489
 98.65376884 98.30776915 80.68980641 79.20712467 98.38695922 78.22029459
 81.25510167 96.65086926 94.0778012  77.69520352 98.01293844 78.54558302
 97.30875598 98.57457877 96.36213009 98.02024829 86.33788575 78.26171708
 77.98272438 89.4531012  97.11504489 75.17695935 79.96613102 92.05906361
 77.48078118 77.02269709 96.9627563  93.87434364 98.02877645 98.57336046
 91.06248706 87.66340566 86.75211072 98.55508583 98.99976852 77.61601345
 98.70615611 77.94252019 72.56977863 93.0617317  96.24273583 96.9067141
 89.79300934 97.17717864 91.40970505 77.98759762 98.42838172 78.52487177
 98.20786784 77.19447862 79.05727269 97.55972759 79.62744119 95.99054592
 78.81361095 95.45083515 77.36626016 82.05187559 85.80548483 78.61989986
 79.68713831 99.14718388]
Avg Prec: is [55.8417346   3.15553589 11.26224847  3.39425883  2.32741836  3.76352763
  3.46063712  5.71538844  2.62331341  3.79823192  1.62043626  1.60262646
  0.59060752  5.09999266  2.63760087  3.14377418  3.57935859  2.62625772
  1.33650674  1.70712849  2.06960983  0.82191676  1.84129609  2.29924328
  5.07133012  3.67379138  6.59278759  3.36430496  2.09981365  1.86895592
  2.58009327  1.3400932   3.60512277  1.6516076   2.34003918  2.3412087
  2.96143519  2.54423549  2.76590986  7.38691492  2.22144803  8.23929935
  3.36179316  4.01940246  3.29703409  6.27998896  2.07413652  1.51618036
  2.15840045  1.6164543   1.85349425  1.57774777  1.03730864  3.01754709
  1.27632586  2.65138086 11.32603211  3.76440605  3.9491974   2.81735081
 10.78446455  2.14083604  3.81764413  2.99859732  1.67913041  2.44163037
  1.93697284  4.15977808  1.27072847  2.359666    0.19611674  3.47830338
  1.88498515  4.62179976  4.00083677  3.14872759  0.82599973  1.83764095
  0.13710784  0.74375138]
mAP score regular 10.34, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [88.26020878 97.22450607 92.06218701 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.68256222
 97.07750953 96.48703192 97.03764606 94.03044572 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.98089045 96.39235618 96.16314124 96.78102499
 92.03976381 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [87.27358796 97.22450607 90.69935471 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.57293769
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.23345043 96.39235618 96.16314124 96.78102499
 91.79560007 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [96.64968733  3.95192772 61.04628335  6.58355418  2.20393078 10.45127628
 10.57879997 32.95215767  4.04829435 21.12821459  1.5479759   1.22624979
  0.77068335 10.33772465  2.82394733  3.80809039  3.56835899  2.04830138
  0.72599991  1.14866926  1.0590431   0.46753108  0.90111691  1.30146344
 16.55943307  5.24668197 24.08233097  7.97997242  4.1555571   1.23368961
  2.34558959  0.79451191 24.24915314  1.30685422  1.27589577  1.28778149
  3.34883214  2.33133646  4.28029664 27.53711853 10.83543454 41.20937088
  8.86323938 19.62560282 12.66657108 30.87905398  3.35467108  2.34599877
 11.32843078  2.73767829  2.12038217  2.02117723  2.07803223 16.73470702
  2.38644438  6.65839055 55.61914231  9.87321963 10.2085028   8.09331053
 58.59517813  5.61331268 12.19601115 10.04654197  4.91156349  5.28443509
  4.92144202 11.03113242  3.6414902   8.23096949  0.83831125 21.50044713
  5.58335964 23.2497708  14.58341209  8.10203724  1.38854794  2.36200049
  0.23393049  0.9871558 ]
Accuracy th:0.5 is [45.40947256 97.22450607 70.52096569 96.96290206 97.90716795 76.03209009
 76.11929143 74.9831826  77.59174826 96.41477938 77.75120213 98.5325261
 99.34972718 77.97543414 77.67396666 96.31262924 96.21047911 77.10092932
 98.78167277 98.34068316 78.71539976 78.38652615 98.31327703 77.17816479
 78.09004161 96.52938685 94.3393876  77.15325012 97.81747515 77.71383013
 97.52597354 98.67204823 96.39983058 98.18870369 86.63826395 77.33014426
 77.29775519 91.13785285 97.0276802  74.41263672 77.56434213 92.37362035
 76.46311384 76.13174876 97.03764606 94.02795426 98.18621222 98.77668984
 89.37140294 86.59590901 84.7920871  98.55993223 98.87385704 76.45314797
 98.6969629  77.12584398 71.11891771 93.79873932 96.16314124 96.78102499
 90.13379176 97.04761193 90.86628298 77.10840372 98.32075143 77.95799387
 98.13139996 76.28871116 78.41891522 97.52099061 78.85243043 96.07843137
 78.16478561 95.44559882 76.21645863 82.81884545 87.4430077  77.78109973
 78.92219149 99.15040985]
Accuracy th:0.7 is [45.7134315  97.22450607 70.52096569 96.96290206 97.90716795 76.03209009
 76.11929143 75.09031567 77.59174826 96.41976231 77.75120213 98.5325261
 99.34972718 78.38652615 77.67396666 96.31262924 96.21047911 77.10092932
 98.78167277 98.34068316 79.10406857 78.38652615 98.31327703 77.19809652
 78.52604828 96.52938685 94.3393876  77.15325012 97.81747515 77.71383013
 97.52597354 98.67204823 96.39983058 98.18870369 86.89737649 77.33014426
 77.29775519 91.33218726 97.0276802  74.41263672 78.04270374 92.37362035
 76.46311384 76.13174876 97.03764606 94.02795426 98.18621222 98.77668984
 91.54894486 87.00201809 84.94406657 98.55993223 98.87385704 76.45314797
 98.6969629  77.12584398 71.11891771 94.12262999 96.16314124 96.78102499
 90.13379176 97.04761193 91.05314299 77.10840372 98.32075143 77.95799387
 98.13139996 76.28871116 78.41891522 97.53593941 78.85243043 96.07843137
 78.17475148 95.44559882 76.21645863 82.91102972 87.5625981  77.78109973
 78.92219149 99.15040985]
Avg Prec: is [53.83875453  3.72340817 14.76930664  4.54446736  1.49219364  4.26167693
 13.26461197  8.62347131  7.9250635   5.26229192  2.3226311   4.82980127
  1.78830374  5.85523269  2.91253962  3.65456581 24.3422366   6.45259668
  1.5581401   2.69997313  3.55103111  1.48552872  1.16788914  5.34344615
  5.69832517  9.62692062  7.96871516  4.59627449  3.9566531   5.81404642
  2.23104226  0.85633669  3.05324875  1.1184748   1.69709339  2.27868425
  1.98427271  2.21894131  2.19302924  6.36263837  1.71603428  6.03818361
  2.15132524  2.68343394  2.34377432  4.85496612  1.72802741  1.02748247
  1.41355005  1.18016661  1.20451618  1.05159212  0.76696445  2.2081349
  0.90145909  1.87513286 10.17314441  3.04564271  3.98845168  2.84651286
  7.89944995  2.04908773  3.31533588  2.5091137   1.36899481  1.92477999
  1.58089643  3.53789958  1.08461186  2.20870979  0.19470683  3.23657694
  1.57353885  4.05373036  3.21944021  2.32840405  0.56042478  1.44884684
  0.12350352  0.60095838]
mAP score regular 10.90, mAP score EMA 4.32
Train_data_mAP: current_mAP = 10.34, highest_mAP = 10.44
Val_data_mAP: current_mAP = 10.90, highest_mAP = 10.90
tensor([0.4120, 0.3570, 0.4020, 0.4650, 0.4827, 0.4658, 0.4688, 0.4633, 0.4616,
        0.4631, 0.4549, 0.4749, 0.4517, 0.4671, 0.4674, 0.3585, 0.3864, 0.4817,
        0.4166, 0.4786, 0.4533, 0.4665, 0.4731, 0.4683, 0.4452, 0.4401, 0.4461,
        0.4772, 0.4773, 0.4338, 0.4271, 0.4319, 0.4770, 0.4600, 0.4136, 0.4030,
        0.4319, 0.4082, 0.4673, 0.3753, 0.4669, 0.4743, 0.4337, 0.4556, 0.4821,
        0.4894, 0.4786, 0.4421, 0.4585, 0.4897, 0.4839, 0.3893, 0.4756, 0.4842,
        0.4884, 0.4710, 0.4839, 0.4540, 0.3452, 0.4850, 0.4886, 0.4617, 0.3982,
        0.4751, 0.4646, 0.4710, 0.4761, 0.4769, 0.4463, 0.4734, 0.4790, 0.4509,
        0.4735, 0.4716, 0.4675, 0.4015, 0.4264, 0.3878, 0.4540, 0.4575],
       device='cuda:0')
Max Train Loss:  tensor([18.2358,  7.1282, 11.1949,  7.3492,  9.4551, 10.0590, 10.6000, 11.2458,
         7.4555,  8.6161, 10.5011, 10.5503,  7.4879, 12.7056, 11.3269,  8.7212,
        11.2304,  8.5131,  7.0302,  7.7019,  8.4181,  9.6194,  6.9456, 15.0118,
        10.0394,  6.2489,  8.5426, 11.7039,  8.0504, 10.1542,  8.7099, 10.2466,
         9.5957,  8.1671,  9.4188,  9.5044, 12.6335,  9.1482,  8.4975, 12.2283,
        13.1352, 14.3488,  7.3513,  8.0185,  5.9289, 10.7128,  7.0897,  8.6828,
         6.8862, 10.7356,  9.2129,  6.1213,  4.6165,  9.5448,  8.1037,  7.7956,
        12.1513,  9.2265,  6.7583,  6.6929, 12.4841,  7.4726, 10.1057, 10.1777,
        11.8759, 10.9438, 10.7817, 12.9612,  7.5406,  8.1510,  8.2410,  9.9777,
         7.9805,  9.9846, 15.1350,  6.7841,  7.2281,  5.5186,  5.0447,  9.2753],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [17/80], Step [000/642], LR 1.0e-04, Loss: 18.2
Max Train Loss:  tensor([18.2741,  5.0891, 17.6985, 16.8004, 11.8414, 14.5130, 14.1041, 17.3612,
         6.9603, 14.7157, 12.5384,  6.2400, 18.9605, 12.0591, 13.1995,  8.8436,
         7.8819,  9.3317,  5.6793, 13.5422, 11.5747,  9.2098,  9.3724,  9.9903,
        14.5212, 12.3289, 15.1774, 11.7730, 10.8457,  7.6489,  8.0068,  8.3531,
        13.6520,  8.8177,  8.5052,  6.4310,  9.1722,  9.6546, 16.9126, 10.0559,
         6.4797, 15.5211, 10.5271,  9.2185,  9.6437, 16.3007, 19.1118, 10.0202,
         9.5202, 17.7111, 11.3176,  6.9956, 16.3125, 15.9757, 13.9264, 19.2639,
        21.1693,  9.9911,  6.3993, 15.5730, 16.2506, 14.0480,  8.5214, 12.8636,
        18.4659, 17.1003,  9.6412, 18.4255,  7.2928, 10.2293, 12.6893, 10.4210,
        17.8436, 12.9796, 11.5234,  7.2567,  6.9574,  6.0824, 17.8485,  9.2497],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [17/80], Step [100/642], LR 1.0e-04, Loss: 21.2
Max Train Loss:  tensor([16.9673,  5.9467, 14.2086,  8.9898,  8.4221, 12.9956, 10.5391, 13.5338,
        10.8559, 12.2471, 10.7393, 12.0974,  6.7257, 10.2432,  9.4786,  5.9198,
        10.9831, 12.8553,  7.0734, 11.5893, 11.6827,  7.5542,  6.7469, 14.8966,
        12.8122,  8.8316, 13.0225, 10.2947, 13.4494,  8.4407,  6.7710,  8.0458,
         8.5934, 13.2495,  6.6164,  6.0416,  7.7223,  6.2535, 11.9677,  9.4543,
         5.7271, 11.3891,  6.7420,  8.2944,  8.3738,  9.4556,  9.6247,  8.0998,
         8.9706,  7.7747, 13.7338,  8.0809,  9.1146, 12.0536,  8.5560,  9.9764,
        12.4111, 10.6436,  3.8472, 15.6561, 11.7411, 10.5312,  8.2269, 12.2569,
         6.1739,  7.8084,  8.8453, 12.7220,  8.1619,  9.6512, 12.8294,  8.8775,
         8.9062,  8.9069,  8.4658,  5.4165,  6.3561,  5.6632,  8.1788, 10.2205],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [17/80], Step [200/642], LR 1.0e-04, Loss: 17.0
Max Train Loss:  tensor([16.3513,  2.4342, 13.5054, 10.1669,  8.3446,  9.8865, 11.4774, 13.0291,
         9.1697, 10.3581,  9.9705,  4.4938,  8.6181,  8.3519,  9.0411,  9.2520,
         6.8022, 11.8167,  5.9172,  6.4212,  9.7779, 10.0345, 10.3444, 10.2174,
         7.8736,  7.2807, 11.1880, 10.1955, 13.3532,  6.5749,  7.6629,  5.7217,
        10.3797,  5.8170, 13.2648, 11.7120,  7.4829, 15.8886, 11.0570, 11.6002,
         7.2640, 11.7748,  7.2903,  8.6180,  9.9252, 12.5877,  9.3491,  8.9283,
         8.9054,  7.3359, 11.7373,  4.2896,  9.1628, 10.9599,  8.6604,  8.7748,
        15.1095,  7.8693,  8.0289,  9.0917, 12.6378, 10.1728,  6.9797, 10.5310,
         7.0173,  7.5013,  8.6486,  9.4640,  8.3412, 11.4062, 12.1994,  9.8478,
        10.1873,  9.3698, 12.7903,  5.8718,  5.5208,  7.1922,  8.2082,  7.9505],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [17/80], Step [300/642], LR 1.0e-04, Loss: 16.4
Max Train Loss:  tensor([14.5587,  2.3986, 10.2826, 11.0470,  7.4170, 10.2367,  6.0977, 11.6047,
        11.7942, 10.1187, 12.2182,  9.9148,  7.3534, 12.2729,  8.4658,  7.4079,
        10.5249,  8.0572,  6.3113,  9.4554, 11.0556, 10.9321,  5.1529, 10.3525,
        12.7354,  9.5468, 12.7606,  7.5611, 13.6185,  7.4960,  8.1036,  7.9871,
        13.0457,  8.5716,  8.7311, 11.8029,  9.2017,  8.7158, 12.1385, 11.3165,
         5.6161, 10.1099,  7.6477,  7.3742,  9.3793, 10.4589,  8.8536,  6.8994,
         9.6102,  5.1787,  7.9575,  7.8194, 10.2266,  7.7175,  9.1764,  9.9034,
        14.6518,  6.6376,  7.1914, 10.5536, 10.4837, 10.3176,  6.2454,  7.0350,
         7.0441,  7.4530,  7.8357,  9.8078,  9.4480,  9.6728, 12.2220,  8.5037,
         8.4051,  6.5793, 12.5006,  7.0284,  5.5262,  6.1249,  7.9949,  8.9853],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [17/80], Step [400/642], LR 1.0e-04, Loss: 14.7
Max Train Loss:  tensor([22.1806,  3.7367, 14.5438,  7.3770,  8.4792,  7.0259, 10.3443, 13.0367,
         7.3866,  6.0307,  9.3380,  7.6537,  7.5337, 10.6209,  9.2913,  7.2738,
         8.0185, 11.0670,  5.6549,  5.6482, 12.5303,  8.8286,  7.9485,  5.9241,
         7.9470,  9.8183,  7.8495, 10.8796,  5.4520,  6.8073,  7.2473,  8.0191,
        10.5124,  9.7752,  7.9837,  8.4905, 14.4935,  9.6236,  9.8191, 10.1642,
         7.2177, 12.4310,  8.9506,  9.0174,  9.6578, 10.1287,  9.1701,  7.8435,
         7.2052,  6.3320,  8.7377,  8.4318, 10.2044,  9.4895,  9.9148,  8.7180,
        12.5864,  9.9626,  7.4918,  9.2119, 10.2132, 12.8445,  7.2769,  5.6685,
         6.1538,  7.4223,  8.4597,  6.2320,  8.9129, 10.4350, 12.2292,  9.7947,
         7.4300,  9.1990, 10.6595,  8.9212,  5.5675,  6.7820,  7.3252,  8.2240],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [17/80], Step [500/642], LR 1.0e-04, Loss: 22.2
Max Train Loss:  tensor([18.9262,  6.1998, 13.2751,  9.8367, 10.0420,  9.3536, 12.8586, 14.5301,
        12.9942, 10.9266, 10.8889,  5.8210,  6.9592, 11.3158, 10.7952,  7.2156,
         9.8440,  9.0060,  8.0021,  7.8801,  7.5915,  8.1449,  8.3714, 10.9567,
        13.1810,  7.5250, 13.5835, 10.8661,  6.3667,  7.0659, 13.1054,  8.7544,
        10.0857,  8.1465,  9.6706,  8.6800,  8.7366,  7.3797, 10.7150,  8.6111,
         5.3376,  9.1552, 12.6008, 12.4939,  9.7227, 11.1623,  9.5735,  8.4934,
         8.6244,  7.6016,  7.1804,  5.4523,  8.6630,  7.4368, 11.5947, 13.9340,
        12.9422,  8.0034,  6.3993, 10.8716, 14.1314,  8.1762,  7.8421,  7.1753,
         7.3522, 10.0326,  8.8395, 12.5658,  9.0283, 11.2902, 12.4821,  8.3369,
         7.8701, 11.5716, 13.6756,  6.2295,  6.6493,  6.0234,  7.5830,  9.5600],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [17/80], Step [600/642], LR 1.0e-04, Loss: 18.9
Max_Val Meta Model:  tensor([ 24.0998,  29.2366,  33.1992,  21.4446,   6.1732,   7.6927,   7.9345,
          9.6045,   6.7765,   8.3107,  10.2903,   7.0208,   7.5837,  10.1479,
          6.8286,   4.5039, 109.0601,   6.0785,   5.1191,   5.8813,   5.4061,
          6.6732,   5.4429,   4.7202,  13.6536,   9.1144,  12.3452,   5.1788,
          5.9406,   7.7456,   5.6831,   5.9471,   7.4925,   4.6411,   5.3072,
          4.6866,   5.7823,   7.5065,   7.3486,  20.3608,   7.1230,  12.4727,
          7.6538,  10.2367,   8.9536,  12.4244,   7.3619,   7.9331,   6.7034,
          6.3935,   5.8734,   4.4735,  10.0188,   6.4931,   7.8910,   7.3852,
         13.8371,   7.9046,   8.0370,   5.6792,   8.8558,  14.9162,   5.6852,
          5.6795,   6.3494,   5.5358,   8.0949,   6.8070,   9.4610,  14.4552,
         12.5043,  10.8137,  11.9391,   7.4480,   8.3897,   6.8841,   5.7368,
          4.9815,   7.5415,   7.2473], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 17.1674,  26.9113,  27.5163,  22.3955,   5.9648,   8.7523,   8.2916,
         11.3473,   6.9783,   8.9095,  10.7212,   7.3270,   8.0433,  10.9060,
          6.8259,   4.8268, 106.2715,   6.4447,   5.4778,   6.1448,   5.5645,
          7.0747,   5.5485,   4.8634,  13.7018,   9.2771,  13.9356,   5.8929,
          6.4141,   7.8131,   6.3164,   6.3640,   9.3934,   5.0195,   5.6730,
          5.0277,   6.4249,   7.8161,   8.5492,  19.6426,   7.1610,  12.3243,
          7.9600,  10.2481,   8.9157,  11.2871,   7.8496,   8.4297,   7.1064,
          6.4585,   5.8508,   4.8012,  10.1391,   6.6423,   8.3405,   7.8766,
         13.7814,   8.0050,   8.0347,   5.7796,   8.2306,  15.2771,   5.7184,
          5.9455,   6.7391,   5.7844,   8.5220,   8.3287,   9.9295,  14.8060,
         12.9872,  10.6347,  12.4057,   7.4120,   8.2087,   7.1111,   6.1191,
          5.2715,   7.9681,   7.6769], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 41.6688,  75.3893,  68.4550,  48.1595,  12.3569,  18.7907,  17.6869,
         24.4902,  15.1185,  19.2391,  23.5705,  15.4279,  17.8078,  23.3466,
         14.6053,  13.4632, 275.0050,  13.3787,  13.1485,  12.8391,  12.2754,
         15.1649,  11.7279,  10.3860,  30.7799,  21.0800,  31.2371,  12.3479,
         13.4393,  18.0111,  14.7874,  14.7362,  19.6911,  10.9121,  13.7155,
         12.4753,  14.8774,  19.1498,  18.2966,  52.3368,  15.3387,  25.9815,
         18.3544,  22.4922,  18.4938,  23.0614,  16.4022,  19.0694,  15.4983,
         13.1882,  12.0899,  12.3325,  21.3187,  13.7175,  17.0771,  16.7227,
         28.4823,  17.6338,  23.2761,  11.9172,  16.8462,  33.0880,  14.3623,
         12.5136,  14.5038,  12.2809,  17.8985,  17.4646,  22.2498,  31.2737,
         27.1133,  23.5850,  26.1996,  15.7165,  17.5587,  17.7114,  14.3523,
         13.5924,  17.5518,  16.7816], device='cuda:0')
Outer loop valEpocw Maximum [17/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 109.1
model_train val_loss valEpocw [17/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 106.3
model_train val_loss  valEpocw [17/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 275.0
Max_Val Meta Model:  tensor([38.9332, 39.6054, 35.3147, 39.1855, 41.6284, 44.0301, 42.1734, 38.2709,
        38.0694, 46.3189, 39.3278, 50.2386, 39.0523, 42.6305, 45.5950, 36.5148,
        33.9403, 40.1306, 37.4978, 41.8624, 38.9728, 39.8644, 39.9363, 45.2518,
        37.9487, 38.0420, 37.6159, 38.3747, 40.7591, 36.9771, 36.5252, 37.3748,
        40.6434, 38.6382, 35.4952, 34.3860, 37.1756, 35.1910, 40.2784, 32.8803,
        39.0426, 40.0883, 37.0195, 38.8829, 40.0215, 42.2438, 39.2904, 37.4885,
        39.3481, 41.0918, 41.1185, 33.6381, 40.3960, 41.3868, 41.0877, 39.2828,
        41.0004, 39.3335, 39.2313, 40.6945, 42.5330, 42.9047, 34.5129, 40.7456,
        38.9978, 40.2234, 42.0796, 39.5076, 37.8578, 40.7088, 40.7068, 41.6305,
        40.2482, 40.5687, 39.4981, 34.5153, 35.6182, 33.2205, 38.9818, 39.4773],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([23.3626,  6.5718, 21.4191,  7.6728,  7.6079, 15.9468, 38.2886, 19.9844,
        16.5012, 16.2581, 11.9501, 94.6961,  8.1261,  7.2109,  8.1844,  4.0878,
         6.8872,  9.4565,  4.3655, 11.9339,  4.8098,  5.5910,  4.7697,  5.2461,
        12.2127,  8.9095, 15.3375,  7.4448,  7.2082,  4.5772,  4.7807,  4.9949,
         4.6987,  3.8834,  4.4564,  3.8993,  6.6951,  5.4749,  5.7571,  2.2107,
         2.7924,  3.0544,  5.1766,  3.6034,  3.4120,  2.8459,  6.3492,  6.0708,
         5.6081,  5.1975,  4.4132,  3.7184,  7.3097,  5.0617,  6.7084,  7.2955,
         5.7217,  4.5344,  4.6728,  4.1131,  3.2295,  4.9619,  3.4867,  3.5749,
         5.3620,  4.4156,  6.8805,  7.9560,  5.9976,  7.9554, 11.0862,  3.6886,
         6.7777,  3.6898,  6.3471,  3.8013,  4.7723,  3.5438,  6.4273,  6.1237],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 56.3893,  17.6125,  52.9057,  16.4148,  15.5173,  35.5498,  79.8934,
         43.0544,  35.8892,  33.7566,  26.1646, 200.7133,  17.8630,  14.9399,
         16.8057,  11.1173,  17.6695,  19.6423,  10.1830,  24.6740,  10.5364,
         11.9961,  10.0064,  10.7438,  27.3959,  20.0408,  34.5215,  15.6930,
         15.0156,  10.4405,  11.1874,  11.4740,   9.8194,   8.3618,  10.6629,
          9.5933,  15.3231,  13.2596,  12.2022,   5.8616,   5.9458,   6.3870,
         11.8204,   7.9362,   7.0769,   5.6316,  13.2943,  13.6695,  12.1140,
         10.5564,   9.0989,   9.4602,  15.3250,  10.3562,  13.6494,  15.5862,
         11.7220,   9.8833,  12.9464,   8.4569,   6.5221,  10.4303,   8.7207,
          7.4358,  11.4830,   9.2734,  14.3358,  16.8233,  13.4866,  16.7903,
         23.1065,   8.0524,  14.3429,   7.7478,  13.5964,   9.3205,  11.2361,
          9.0585,  14.1381,  13.3002], device='cuda:0')
Outer loop valEpocw Maximum [17/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 50.2
model_train val_loss valEpocw [17/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 94.7
model_train val_loss  valEpocw [17/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 200.7
Max_Val Meta Model:  tensor([35.7277, 35.2060, 35.6947, 39.5229, 41.8230, 35.8073, 41.1227, 40.2104,
        40.5788, 40.5768, 38.9461, 38.8515, 38.9643, 41.2314, 41.7638, 34.0080,
        33.9119, 40.2862, 37.7205, 40.6479, 42.2192, 39.7316, 39.7543, 41.8342,
        38.3539, 41.5353, 37.9375, 39.2149, 38.0899, 38.1085, 38.2647, 37.7973,
        41.6853, 39.3410, 35.3257, 34.5099, 37.3822, 35.7859, 40.5921, 32.9951,
        38.8683, 41.3130, 37.5361, 39.7328, 40.8438, 45.1883, 41.5661, 37.4674,
        40.8821, 42.7449, 43.2898, 34.0699, 41.2323, 41.5107, 41.7655, 40.5250,
        43.6300, 39.1786, 35.8710, 39.9639, 45.3331, 42.0307, 34.4669, 41.4171,
        39.1215, 40.1525, 40.8804, 39.3608, 38.0032, 40.9382, 40.2107, 40.9641,
        40.6797, 41.0100, 40.1440, 34.9734, 35.8633, 33.3128, 38.8842, 39.5488],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 7.2221,  2.1009,  4.8533,  8.0532,  3.6089,  3.8917,  4.4626,  7.6680,
         6.0970,  3.8965, 10.9273,  5.7920,  8.1350,  8.5772,  5.4790,  4.9477,
         5.7248,  5.4646,  6.6118,  6.5236,  5.2176,  7.9936,  5.5720,  5.1704,
         5.4876,  5.7662, 11.4540,  8.7326,  3.8083,  7.0267,  6.3708,  8.1864,
         8.1510,  4.5663,  6.6190,  5.9480,  9.2750,  8.4833,  8.4437, 13.5568,
        14.1347, 36.7965, 35.7507, 33.8416, 25.4435, 37.0477, 10.5267, 11.2737,
        20.2677,  8.5791, 17.9571, 18.1760, 26.6580, 13.8028, 29.9038, 44.4590,
        40.4885, 11.2768,  6.6219,  6.8548, 38.3132,  6.7680,  9.6115, 10.0285,
        10.1648,  7.0473, 11.2776, 13.4825,  9.3817, 10.2994, 13.8675,  9.2807,
         9.4731, 17.9682,  3.9690,  8.5220,  8.7553,  5.4547,  8.9340,  8.8499],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([17.7182,  5.6061, 11.7816, 17.1715,  7.3274,  8.7096,  9.3612, 16.0634,
        12.9147,  8.2925, 23.9945, 12.2695, 17.9780, 17.8258, 11.6069, 13.3594,
        14.7423, 11.3377, 15.3514, 13.4948, 11.0476, 17.1136, 11.6346, 10.7833,
        12.2601, 12.6704, 25.7273, 18.2340,  8.0881, 15.6737, 14.4006, 18.6705,
        16.8917,  9.7054, 15.9727, 14.6374, 21.2342, 20.3301, 17.8469, 35.9443,
        30.5926, 76.9659, 82.3952, 74.0897, 52.9869, 72.2756, 21.4189, 25.5173,
        43.2703, 17.4098, 35.7704, 46.0640, 55.5198, 28.3420, 61.1230, 95.1397,
        83.1578, 24.7627, 18.1691, 14.0645, 78.0225, 14.2722, 24.1652, 20.6230,
        21.8298, 14.8801, 23.4488, 28.1436, 21.0967, 21.6798, 29.3391, 20.1776,
        19.9482, 37.6012,  8.3920, 20.7370, 20.5765, 13.9592, 19.7681, 19.2523],
       device='cuda:0')
Outer loop valEpocw Maximum [17/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 45.3
model_train val_loss valEpocw [17/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 44.5
model_train val_loss  valEpocw [17/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 95.1
Max_Val Meta Model:  tensor([37.1083, 34.2258, 35.8463, 39.2409, 39.7032, 34.7638, 40.6519, 39.4002,
        39.0212, 40.0852, 38.4496, 37.7451, 38.0134, 39.3412, 40.7784, 33.3729,
        33.6545, 40.0783, 36.7548, 39.5732, 39.6127, 38.4722, 37.5273, 40.7139,
        37.5189, 37.8470, 37.1858, 38.5742, 37.1993, 36.9701, 37.0441, 35.6704,
        40.0621, 41.4931, 34.8321, 33.3941, 36.1329, 34.4042, 39.9961, 32.9553,
        38.1910, 40.4911, 36.1401, 38.3274, 38.9790, 38.8358, 39.0733, 36.5843,
        38.9041, 39.1672, 38.1055, 32.6675, 39.5501, 40.8584, 39.8853, 37.0855,
        37.7448, 40.6416, 34.4281, 38.5434, 40.7797, 40.6232, 34.3568, 40.4757,
        36.7987, 37.5201, 37.1903, 42.1845, 37.4261, 40.1992, 37.0693, 40.5277,
        40.5174, 39.5433, 43.0210, 33.2561, 34.9564, 32.3743, 38.4945, 39.2980],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 14.9178,   1.2328,  15.9122,   5.9604,   8.3566,   8.5988,   7.7044,
         12.9848,   5.5930,  11.7508,   9.5446,   5.3913,   6.2332,   8.1750,
          8.9592,   3.3858,   5.6126,   6.1067,   4.6186,   5.5109,   5.4392,
          5.9170,   5.3885,   4.6833,   9.5191,   4.1922,  10.6920,   3.9340,
          6.7870,   4.8926,   4.6376,   5.1928,   3.5764,   3.9369,   4.7285,
          4.1017,   5.4453,   5.5508,   5.2012,   3.1136,   2.9894,   4.0052,
          5.4510,   4.2700,   4.2123,   3.9399,   6.6897,   6.3664,   5.9862,
          5.0468,   5.0878,   3.9160,   7.7922,   5.6998,   7.0419,   6.5899,
          4.0359,   4.4712,   6.1222,   4.8716,   5.6985,   5.7616,   4.2905,
          4.0645,   5.6122,   4.8234,   7.1186,   5.2988,   6.3757,   7.7182,
         11.1434,   4.8976,   5.9211,   4.9794, 131.5586,   4.8876,   5.0572,
          4.5007,   6.8337,   6.5056], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 35.9880,   3.3191,  39.5991,  12.7664,  17.2039,  19.4427,  16.1999,
         27.5441,  12.0001,  25.0581,  21.0671,  11.4921,  13.8822,  17.4750,
         19.1068,   9.1977,  14.4733,  12.5652,  10.8546,  11.5560,  11.7837,
         12.8346,  11.4778,   9.8949,  21.4189,   9.4461,  24.1004,   8.2409,
         14.5374,  11.1264,  10.7532,  12.1411,   7.4886,   8.3769,  11.3601,
         10.2306,  12.6375,  13.5905,  11.0574,   8.2296,   6.3849,   8.3624,
         12.5734,   9.3918,   8.7232,   8.0437,  14.0046,  14.5030,  12.9766,
         10.3534,  10.6699,  10.0986,  16.4089,  11.6335,  14.5080,  14.3554,
          8.4929,   9.6566,  17.0003,  10.0938,  11.6710,  12.2755,  10.7357,
          8.4324,  12.2739,  10.3303,  15.2955,  10.9198,  14.3162,  16.2954,
         24.3384,  10.7171,  12.5420,  10.4957, 284.7979,  12.2560,  11.9601,
         11.6286,  15.0306,  14.1727], device='cuda:0')
Outer loop valEpocw Maximum [17/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 43.0
model_train val_loss valEpocw [17/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 131.6
model_train val_loss  valEpocw [17/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 284.8
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.50258891 97.2137279  91.23670521 97.02489005 97.26733349 96.5997003
 96.99808726 94.74787101 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.08145612 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.27104933
 96.90915072 96.22689782 96.9627563  93.94500554 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.28787417 96.13796128 96.24273583 96.9067141
 91.9884017  97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.42646898 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [84.80890827 97.2137279  90.95893081 97.02489005 97.26733349 96.5997003
 96.99808726 94.74056115 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.2807958
 96.90915072 96.22689782 96.9627563  93.87312533 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.33904314 96.13796128 96.24273583 96.9067141
 91.47427541 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [96.50394676  3.97011407 55.58749194  4.19952439  6.22508978 27.42205621
  6.16222174 32.51386469  2.78244369 22.21115142  1.62838623  1.24228064
  0.79133072 11.42747418  2.26615403  3.50247784  3.82611232  2.44659413
  0.81484946  1.16812353  1.47972644  0.45855887  0.94654235  1.3521389
 16.32991604  6.13034667 24.40578658  5.83547936  3.96514667  1.32723632
  2.39264842  0.86118157 20.65455701  1.6285257   1.68628214  1.63628739
  3.19492918  2.09540694  7.31658154 26.50551719  7.93909025 39.72598615
  6.24524921 19.1411373  16.16118725 33.0757898   2.88553298  2.59532495
  2.94583246  3.27541884  2.43052115  1.49379135  1.30087413  6.83005372
  1.96085035  4.01411192 61.34092034 21.21084048 10.39247837  6.04815452
 61.17814222  2.96491839 21.20487321  8.84666686  4.40567519  5.48622391
  4.47502953  9.31930397  3.49772167  7.30522089  0.60687634 13.60259482
  5.45162758 24.02587894  8.76127045  7.69259963  1.44167164  2.5222989
  0.19657184  0.87171262]
Accuracy th:0.5 is [45.52941606 97.2137279  72.14580719 97.02489005 97.26733349 76.87162681
 77.19447862 76.24541611 78.13257636 96.3767498  78.43593523 98.52097319
 99.41399349 79.8942508  77.98272438 96.56680596 96.29512311 77.71591477
 98.65376884 98.30776915 80.0051169  78.9610263  98.38695922 77.80485131
 80.660567   96.65086926 94.0778012  77.57824588 98.01293844 78.31166774
 97.30875598 98.57457877 96.36213009 98.02024829 86.05645643 77.95470328
 77.74637249 89.14974233 97.11504489 74.96010039 79.04508961 92.05906361
 77.24199267 76.86919019 96.9627563  93.87434364 98.02877645 98.57336046
 88.87318624 86.56936441 86.35494207 98.55508583 98.99976852 77.40402773
 98.70615611 77.76464712 72.44307452 92.73522496 96.24273583 96.9067141
 89.79300934 97.17717864 91.15629683 77.76586543 98.42838172 78.17643547
 98.20786784 76.96543658 78.80873771 97.53048818 79.39596252 95.99054592
 78.48223097 95.45083515 77.12259841 81.80821384 85.67878072 78.31775929
 79.41911039 99.14718388]
Accuracy th:0.7 is [45.56718364 97.2137279  72.14580719 97.02489005 97.26733349 76.87162681
 77.19447862 76.44643706 78.13257636 96.47055957 78.43593523 98.52097319
 99.41399349 80.35477151 77.98272438 96.56680596 96.29512311 77.71591477
 98.65376884 98.30776915 80.46198268 78.9610263  98.38695922 77.9437385
 81.25022843 96.65086926 94.0778012  77.57824588 98.01293844 78.31166774
 97.30875598 98.57457877 96.36213009 98.02024829 86.30620972 77.95470328
 77.74637249 89.47503076 97.11504489 74.96010039 79.69079324 92.05906361
 77.24199267 76.86919019 96.9627563  93.87434364 98.02877645 98.57336046
 91.12340249 87.4745678  86.53159684 98.55508583 98.99976852 77.40402773
 98.70615611 77.76464712 72.44307452 93.15310486 96.24273583 96.9067141
 89.79300934 97.17717864 91.36462762 77.76586543 98.42838172 78.17643547
 98.20786784 76.96543658 78.80873771 97.55972759 79.39596252 95.99054592
 78.5419281  95.45083515 77.12259841 81.91055177 85.80670313 78.31775929
 79.41911039 99.14718388]
Avg Prec: is [55.78924317  3.19469688 11.23680391  3.41994251  2.23519388  3.84394529
  3.33889586  5.59637603  2.38745329  3.72644257  1.56332996  1.66204006
  0.64185144  5.11908171  2.71492943  3.14490195  3.63522048  2.71049777
  1.36616146  1.81948339  1.99182647  0.79519213  1.80089788  2.38498024
  5.09092469  3.70910135  6.57264088  3.1152158   2.05427106  1.90249251
  2.66681148  1.32486358  3.7529701   1.66098456  2.36913103  2.38561546
  2.96996041  2.551977    2.81476335  7.43494393  2.26258307  8.19109659
  3.38286483  4.02580963  3.2093769   6.30898137  2.17660634  1.59131542
  2.14706883  1.67065605  1.80742072  1.6765427   1.04909113  2.98356023
  1.35122367  2.7230346  11.35283433  3.72945357  3.95148259  2.8442076
 10.76600484  2.17675812  3.73158645  2.93722201  1.57129918  2.49691433
  1.73837649  4.22626483  1.32313657  2.44551082  0.19010528  3.40903641
  1.94985012  4.51399639  3.82711908  3.10798123  0.85451598  1.9023737
  0.15019401  0.70450695]
mAP score regular 10.77, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [88.7609936  97.22450607 91.24000299 96.96290206 97.90716795 96.63651992
 96.80843112 94.82522361 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.30949    96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.51563395
 97.07750953 96.48703192 97.03764606 93.89341505 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.0789795  96.39235618 96.16314124 96.78102499
 92.27894461 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [87.95126691 97.22450607 91.91269901 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.70996836
 97.07750953 96.48703192 97.03764606 94.07529212 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.03819418 96.39235618 96.16314124 96.78102499
 92.35867155 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [96.89468184  4.10916014 60.24884213  4.21016156  5.64967756 27.15663598
  7.21765465 34.01355558  3.29864094 24.05410841  1.56561815  1.17216285
  0.6850537  12.42968626  2.48335097  3.87567778  3.96697631  2.43918471
  0.73800588  1.17995533  1.33283093  0.47353533  0.98118999  1.34154815
 16.71166331  6.42992346 23.98128957  5.24954371  4.61756603  1.33078377
  2.22389932  0.79540446 25.30590237  1.72280593  1.41843626  1.41949845
  2.90277363  2.49054613  8.08808056 27.6516898   7.93324266 40.76934751
  6.36516389 20.45918937 17.78133213 33.58114745  3.35880035  2.8110976
  3.28989613  3.92105299  3.05415886  1.97351944  1.55179894  8.28454597
  2.32138474  3.9536922  56.74589546 21.61489974  9.95660803  6.59548028
 62.82753552  3.00442458 23.81258576  9.60934959  5.46354956  5.21488493
  5.18930648  9.90516206  3.27069436  7.55830743  0.77161667 13.91875837
  5.1903957  26.2156596   9.84567436  7.74789867  1.31442687  2.72796675
  0.19209654  0.80920263]
Accuracy th:0.5 is [45.43937016 97.22450607 70.38642649 96.96290206 97.90716795 75.87263622
 75.95983756 74.8486434  77.44226026 96.41477938 77.59174826 98.5325261
 99.34972718 77.89321574 77.52946159 96.31262924 96.21047911 76.94645838
 98.78167277 98.34068316 78.62570695 78.23205521 98.31327703 77.02120238
 78.00782321 96.52938685 94.3393876  76.99379625 97.81747515 77.55935919
 97.52597354 98.67204823 96.39983058 98.18870369 86.63078955 77.17069039
 77.14328425 91.06310885 97.0276802  74.28806338 77.46717493 92.37362035
 76.30365996 75.97229489 97.03764606 94.02795426 98.18621222 98.77668984
 89.65792162 86.50870768 84.69990283 98.55993223 98.87385704 76.2936941
 98.6969629  76.97635598 71.00431024 93.74890998 96.16314124 96.78102499
 90.13379176 97.04761193 90.92109525 76.97386451 98.32075143 77.8134888
 98.13139996 76.14420609 78.26942721 97.51849914 78.69297656 96.07843137
 78.02526347 95.44559882 76.06697063 82.75157585 87.4355333  77.63161173
 78.76273762 99.15040985]
Accuracy th:0.7 is [45.71094003 97.22450607 70.38642649 96.96290206 97.90716795 75.87263622
 75.95983756 74.96325087 77.44226026 96.41976231 77.59174826 98.5325261
 99.34972718 78.29932481 77.52946159 96.31262924 96.21047911 76.94645838
 98.78167277 98.34068316 79.0044099  78.23205521 98.31327703 77.04860852
 78.47372748 96.52938685 94.3393876  76.99379625 97.81747515 77.55935919
 97.52597354 98.67204823 96.39983058 98.18870369 86.90983382 77.17069039
 77.14328425 91.25495179 97.0276802  74.28806338 77.9405536  92.37362035
 76.30365996 75.97229489 97.03764606 94.02795426 98.18621222 98.77668984
 91.81054887 86.93474849 84.84939084 98.55993223 98.87385704 76.2936941
 98.6969629  76.97635598 71.00431024 94.06532626 96.16314124 96.78102499
 90.13379176 97.04761193 91.09051499 76.97386451 98.32075143 77.8134888
 98.13139996 76.14420609 78.26942721 97.53593941 78.69297656 96.07843137
 78.03273787 95.44559882 76.06697063 82.84625159 87.56758103 77.63161173
 78.76273762 99.15040985]
Avg Prec: is [54.21938323  3.71653439 14.88364002  4.54233754  1.48851807  4.26152669
 12.97651979  8.64312777  7.86364558  5.23113597  2.32751748  4.91031516
  1.65250943  5.86766579  2.98401874  3.70487973 24.40232102  6.44813458
  1.56867605  2.71191094  3.5491767   1.47412978  1.1476963   5.42041035
  5.69734251  9.8236718   7.95016995  4.55981319  3.94836798  6.10513972
  2.25222842  0.86232906  3.08477347  1.10730446  1.6966126   2.34133576
  2.01311095  2.1924209   2.24577563  6.2127112   1.73043155  6.00534091
  2.18441977  2.71406224  2.36691937  4.85893614  1.71897015  1.03805035
  1.37850254  1.16750464  1.20718172  0.97939963  0.72397898  2.32990128
  0.84911466  1.83724122 10.04080355  2.87980488  3.84774495  2.77942303
  7.82472132  2.04349087  3.1902818   2.50216062  1.34579275  1.8257646
  1.53506573  3.44041323  1.0835444   2.23641671  0.19243703  3.21008457
  1.57569006  3.93956082  3.53932827  2.31699729  0.60192822  1.49352624
  0.12969927  0.58983375]
mAP score regular 11.18, mAP score EMA 4.32
Train_data_mAP: current_mAP = 10.77, highest_mAP = 10.77
Val_data_mAP: current_mAP = 11.18, highest_mAP = 11.18
tensor([0.4182, 0.3640, 0.3912, 0.4588, 0.4855, 0.4340, 0.4720, 0.4619, 0.4585,
        0.4632, 0.4504, 0.4663, 0.4436, 0.4586, 0.4685, 0.3618, 0.3858, 0.4874,
        0.4144, 0.4689, 0.4564, 0.4572, 0.4661, 0.4672, 0.4391, 0.4393, 0.4432,
        0.4736, 0.4613, 0.4290, 0.4227, 0.4194, 0.4736, 0.4566, 0.4158, 0.3937,
        0.4236, 0.3980, 0.4621, 0.3739, 0.4708, 0.4721, 0.4312, 0.4548, 0.4824,
        0.4807, 0.4703, 0.4356, 0.4587, 0.4820, 0.4749, 0.3796, 0.4658, 0.4853,
        0.4805, 0.4603, 0.4613, 0.4568, 0.3541, 0.4756, 0.4786, 0.4663, 0.3979,
        0.4707, 0.4538, 0.4596, 0.4610, 0.4745, 0.4465, 0.4722, 0.4577, 0.4550,
        0.4716, 0.4660, 0.4538, 0.3847, 0.4208, 0.3799, 0.4575, 0.4520],
       device='cuda:0')
Max Train Loss:  tensor([14.6308,  5.0655, 12.7974, 13.9217,  7.9937,  7.5293,  6.7647, 10.8644,
         7.9198,  9.1795,  9.5733,  4.7742,  6.9074,  8.8103, 10.1558,  7.6658,
        10.9805,  8.4307,  6.4901,  8.5575,  7.6025,  7.7081,  7.6349,  8.6243,
         8.1167, 11.1160, 13.2486,  8.7650,  6.0799,  9.3889,  8.0060,  9.3633,
         7.8037, 10.9259,  6.4896,  4.7095, 11.0093,  9.8142,  9.1988, 14.0228,
         5.9471, 12.7698,  8.0251,  8.8344,  6.2829,  8.7460, 11.4067, 10.9666,
         8.9987,  9.5256,  9.0140,  8.3791, 11.6849,  9.6544,  9.5520, 10.7727,
         9.0783,  8.1844,  8.7003,  7.7906, 10.5827,  9.4885,  7.7996,  8.2137,
         7.8216,  8.5780, 10.1219,  8.4914,  9.3341, 10.4833, 11.9937, 11.3356,
         7.5587, 10.2971,  9.9126, 10.9717,  8.1547,  8.7772,  7.7162,  7.2850],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [18/80], Step [000/642], LR 1.0e-04, Loss: 14.6
Max Train Loss:  tensor([15.3966,  8.0941, 15.0384, 14.9883, 14.6878,  9.6698,  8.5894, 20.4244,
         8.1410, 13.8385,  8.7525, 11.1905,  8.1639, 15.4001, 11.0765,  8.8404,
         6.9285, 12.3870,  7.7259, 11.6097, 10.0015, 18.7844,  8.4214, 13.0258,
        17.4707,  8.6742, 15.0037,  9.9506, 13.9539, 10.4008,  8.7098,  8.5749,
        20.3058,  9.8218,  7.9174,  9.9123,  8.2411,  8.7448, 10.5675,  9.2227,
        12.0554, 16.0231, 11.1441, 10.6088, 20.7164, 18.6898, 13.1451,  7.2099,
        11.8722, 15.2222, 15.1647,  6.5143,  8.6970, 17.0163,  8.6487, 17.2314,
        17.0351, 10.8252,  4.0407, 10.6472, 16.1994, 10.9036,  7.6884,  8.9361,
         7.8137, 11.8117,  9.1651, 15.8209, 17.8083,  7.1307,  5.0594,  9.8834,
        17.9015, 17.5486, 13.1670,  6.8251,  6.5785,  6.6674, 10.4772, 15.1263],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [18/80], Step [100/642], LR 1.0e-04, Loss: 20.7
Max Train Loss:  tensor([11.3581,  6.0161, 11.6460,  9.3083, 12.5377,  9.0717, 11.5383, 11.1070,
         7.8080,  8.8847, 10.2663, 10.6802,  6.9641, 11.7613, 10.3187,  8.0600,
         9.5485, 12.6203,  7.5906,  8.5615,  8.8944,  7.3470,  9.7191, 16.1068,
        11.9695,  9.4990, 13.8438,  9.3716,  7.8380,  6.8760,  5.4067,  5.8750,
        13.0687, 13.5068,  7.5252,  4.5855,  7.6114,  9.4732, 13.2460, 11.7075,
        10.2379, 14.4789,  8.0898, 10.9886,  9.8966, 11.6613, 12.5626,  7.5792,
        12.2624,  4.9530, 12.4940,  6.0298,  6.6305, 11.8290,  8.9608, 13.9303,
        15.4342,  9.2705,  6.2363, 10.3573, 14.0622,  8.9528,  9.2239,  8.0448,
         9.4675, 10.2356,  9.5744, 12.5949,  7.3538,  7.6818,  5.8298,  9.3893,
         8.9494,  9.7932,  8.2166,  5.3050,  7.1310,  9.4405, 11.7177, 10.8102],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [18/80], Step [200/642], LR 1.0e-04, Loss: 16.1
Max Train Loss:  tensor([21.8915,  8.3549, 10.6259,  7.1588, 12.8047, 13.5546,  9.8997, 11.3774,
         8.4676,  7.3128,  7.6866, 10.3848,  7.5882,  9.8974, 10.0433,  6.0851,
         9.4326, 12.6334,  6.1963,  8.2422, 14.9316,  7.0923, 11.8585, 12.7758,
        12.2180,  7.1723,  9.9341, 13.7219,  9.2949,  7.4897, 10.1180,  9.9081,
         5.8787,  5.7844,  6.9001,  6.1601, 10.3855,  8.0329,  6.3678,  7.6446,
         8.2884, 10.7663,  6.1844,  6.9692,  8.2999,  9.7636,  5.4897,  7.6496,
         8.6666,  9.1407,  9.0370,  3.9868,  6.3782, 10.0516,  9.3507,  6.2817,
        10.7179, 11.4833, 10.6874, 12.1638, 10.3599,  6.6264,  9.5194,  9.6277,
        10.4612, 12.4046, 11.2875,  9.4062,  6.3256,  6.6985,  4.6482,  5.9153,
         7.7986, 14.3977, 12.6180,  8.2820,  4.7983,  5.4400,  9.8858,  4.2360],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [18/80], Step [300/642], LR 1.0e-04, Loss: 21.9
Max Train Loss:  tensor([12.6516,  6.5496, 11.1685,  7.1558, 14.4895,  9.0534, 10.6246,  9.2244,
         7.7912,  8.9616,  7.3072,  8.6703,  8.8034,  9.6915,  7.9655,  7.4527,
         9.0904,  6.9828,  6.1799,  7.7130,  7.4740,  8.4644,  8.0251, 11.2474,
         6.4766,  7.4790, 10.3013,  8.3548, 11.1675,  8.7917, 11.0242,  7.9498,
        10.4323,  7.0354,  6.8637,  7.1164, 10.3323, 11.6195,  9.1282, 15.4756,
        11.1098, 14.2582,  7.9781,  9.6681,  6.1638, 10.9580,  5.7888,  7.7867,
        10.3789, 11.1877, 12.5794,  6.3822,  7.4915, 11.6033,  9.0254,  7.3573,
        12.2765, 10.6991,  4.3738, 12.0393, 13.0000,  8.7412,  7.1723,  7.3943,
         7.4488, 10.1278,  9.4069, 12.3145,  8.0557,  9.1543,  5.8180, 12.2970,
         8.6037, 14.8766, 10.1743,  8.6459,  5.9294,  7.4657, 10.0358,  6.6394],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [18/80], Step [400/642], LR 1.0e-04, Loss: 15.5
Max Train Loss:  tensor([16.3795,  5.1546, 10.7348,  9.4751, 11.1099,  9.5955,  8.4843,  8.2376,
         9.9829,  8.3506,  8.3346,  9.5590,  6.9586,  7.8909, 13.4628,  7.1073,
         7.9029,  7.4944,  6.2973, 10.1608,  6.2161,  8.9635,  7.9908, 14.3865,
        10.3137,  8.2114, 12.9048,  7.2731,  9.3251,  7.7717, 13.4364, 12.7417,
         8.8320, 10.2948,  7.8311,  7.0580,  8.9970,  7.0923,  7.5495, 13.3038,
        12.9279, 15.1426, 10.3527,  9.2624, 12.9703, 12.6369,  5.6389,  6.8675,
        11.7644,  3.7880,  9.9829,  7.2994,  7.5151,  8.8201,  7.4098,  9.1806,
        16.9047, 10.4762,  5.3330,  8.9269, 12.3356, 11.1938,  7.0447,  9.3589,
         9.1502,  7.7800, 10.1611, 15.6271,  6.5661,  8.4060,  4.8771, 12.2562,
         9.8610, 10.1798,  8.0909,  7.2835,  6.1419,  4.9708, 10.1372,  4.4588],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [18/80], Step [500/642], LR 1.0e-04, Loss: 16.9
Max Train Loss:  tensor([17.5875,  8.8987, 11.5398,  8.6949, 10.1094,  7.8328,  8.0831, 10.4508,
         8.0709, 11.0505,  9.9133, 12.7774,  7.0639, 13.3936, 14.7685,  8.7680,
        12.8098,  8.6130,  6.9204, 10.7525,  8.1774,  5.0667,  6.9836,  8.0014,
        13.8966, 11.7770, 13.7152,  9.3588,  9.3937, 11.0821,  8.1681, 11.0753,
         9.8765,  8.2338,  7.5025,  6.8728, 10.8068,  6.7886,  8.8092, 11.5092,
        12.3975, 12.5918, 10.6989,  9.2818, 10.1256, 13.3110,  8.0045,  8.9805,
        10.0682,  4.8799,  4.5761,  4.2676,  6.7386,  6.7015,  7.5191,  8.3179,
        15.9522, 15.0987,  7.6594, 10.1861, 15.3429, 11.3328,  9.9030,  8.7547,
         8.2602,  9.0119, 10.1550, 10.9427,  7.7414,  6.8682,  4.9751, 14.4414,
         8.9833, 13.3337,  6.6328,  6.2896,  6.3330,  8.0602, 10.2480,  5.4280],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [18/80], Step [600/642], LR 1.0e-04, Loss: 17.6
Max_Val Meta Model:  tensor([ 26.1248,  23.2877,  33.1778,  21.5618,   4.9253,   7.2572,   7.7806,
          9.7245,   7.5116,   8.9544,   6.7452,  10.3703,   7.4383,   8.7496,
          7.4045,   4.4524, 126.0118,   4.9811,   5.0606,   7.7431,   5.0811,
          4.4804,   5.5300,   4.3678,  13.5894,   9.8426,  12.9012,   4.4464,
          9.3921,   8.0182,   5.9825,   5.7605,   5.4233,   5.9194,   5.2131,
          4.4729,   5.5752,   6.6649,   5.7459,  18.6110,  10.8755,  11.6153,
          7.7737,   9.7806,   9.5346,  12.7580,   4.3340,   7.6184,   8.2532,
          4.6600,   4.3213,   4.0748,   7.9527,   5.0962,   7.3040,   5.3261,
         11.7998,   8.1642,   8.0974,   6.1074,   7.8740,  13.7505,   5.5230,
          5.4246,   6.6998,   4.8132,   8.7754,   6.0303,   8.9490,  12.6205,
          4.7572,  11.8744,  11.5741,   7.8615,   8.5728,   6.7546,   4.9041,
          4.6954,  10.0759,   4.3389], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 18.8334,  22.6396,  27.1845,  22.0117,   5.1799,   8.5234,   8.5975,
         11.9524,   7.7042,  10.7736,   6.9864,  10.6335,   7.7407,   9.3353,
          7.4307,   4.5821, 123.2761,   5.3155,   5.2914,   8.0073,   5.2477,
          4.5712,   5.5864,   4.4025,  13.8702,   9.7556,  14.3838,   4.9193,
          9.8435,   8.1482,   6.4227,   6.0107,   6.1304,   6.1877,   5.4489,
          4.6838,   6.1234,   6.7445,   6.1615,  18.7715,  11.0599,  12.2578,
          8.0595,  10.0513,   9.6229,  11.8193,   4.5346,   7.9565,   8.4909,
          4.9707,   4.3904,   4.2730,   8.0329,   4.8123,   7.5842,   5.5489,
         10.3807,   8.1719,   8.1641,   5.8726,   7.1358,  13.6165,   5.7869,
          5.6255,   6.9735,   4.9523,   9.0572,   6.6581,   9.2336,  12.9779,
          4.9754,  11.7198,  12.0604,   8.2220,   8.3369,   6.8305,   5.1320,
          4.9166,  10.3986,   4.5340], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 45.0364,  62.1959,  69.4827,  47.9754,  10.6685,  19.6380,  18.2131,
         25.8740,  16.8016,  23.2607,  15.5130,  22.8023,  17.4490,  20.3563,
         15.8606,  12.6650, 319.5474,  10.9050,  12.7682,  17.0775,  11.4991,
          9.9985,  11.9846,   9.4239,  31.5907,  22.2082,  32.4537,  10.3861,
         21.3399,  18.9917,  15.1940,  14.3334,  12.9441,  13.5514,  13.1043,
         11.8959,  14.4540,  16.9454,  13.3325,  50.2107,  23.4921,  25.9670,
         18.6919,  22.0994,  19.9492,  24.5872,   9.6427,  18.2669,  18.5093,
         10.3118,   9.2441,  11.2559,  17.2454,   9.9163,  15.7845,  12.0558,
         22.5054,  17.8881,  23.0581,  12.3470,  14.9085,  29.2039,  14.5440,
         11.9513,  15.3676,  10.7743,  19.6454,  14.0317,  20.6796,  27.4844,
         10.8699,  25.7573,  25.5735,  17.6440,  18.3719,  17.7555,  12.1967,
         12.9429,  22.7271,  10.0302], device='cuda:0')
Outer loop valEpocw Maximum [18/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 126.0
model_train val_loss valEpocw [18/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 123.3
model_train val_loss  valEpocw [18/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 319.5
Max_Val Meta Model:  tensor([43.5535, 38.9116, 36.7031, 40.8777, 40.7952, 40.9183, 42.3691, 38.9275,
        37.9234, 41.2689, 39.1347, 49.5225, 38.9016, 42.9169, 44.1396, 38.5620,
        32.5316, 40.8231, 36.0006, 42.2534, 39.5655, 39.5605, 39.7331, 43.1120,
        38.2091, 37.0543, 38.2587, 40.2785, 40.3753, 37.4995, 37.2997, 37.1846,
        40.7480, 38.2527, 35.4998, 34.2770, 37.0114, 35.0230, 40.3030, 38.2603,
        39.6259, 39.0360, 36.9772, 39.4887, 40.5488, 41.2382, 38.8188, 37.2725,
        39.5003, 40.9265, 40.8377, 33.4157, 40.0695, 41.6146, 41.0468, 38.6966,
        40.7218, 40.2206, 38.7372, 40.6392, 41.4007, 41.4141, 32.4566, 41.4282,
        38.4889, 39.6166, 37.6892, 39.7357, 38.1118, 40.8372, 38.3032, 39.3167,
        39.0840, 40.8185, 37.1356, 33.9647, 35.9326, 33.1371, 39.4682, 39.5485],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([24.6851,  7.2757, 24.9432,  7.5893,  5.1669, 16.2293, 36.8384, 18.8934,
        16.2631, 20.2861,  9.3871, 72.1964,  7.9762,  6.0426,  9.4127,  4.0404,
         5.3919,  8.7738,  4.2479, 12.3333,  4.7170,  3.9751,  5.2799,  5.5789,
        12.3353,  9.5242, 14.9388,  7.0685,  9.4516,  5.0396,  5.0273,  4.8890,
         2.3539,  4.9028,  4.2769,  3.7255,  6.2711,  4.5461,  4.2425,  3.2152,
         7.0378,  2.3832,  5.3027,  3.1620,  4.3549,  3.3810,  3.4533,  5.7161,
         6.9405,  3.6993,  3.3050,  3.3731,  5.4118,  3.2759,  6.1918,  5.1679,
         5.2256,  5.0016,  3.0888,  3.9135,  2.5295,  3.0205,  3.8374,  3.6438,
         5.6467,  3.7184,  7.3565,  7.1526,  5.3991,  4.5908,  3.8230,  5.3440,
         6.4561,  4.2509,  7.0485,  3.4804,  4.0615,  3.2888,  8.7158,  3.5312],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 60.3284,  19.2122,  60.7643,  16.0867,  10.6836,  35.8167,  76.4376,
         40.8864,  35.4092,  43.1897,  20.7539, 155.0365,  17.7866,  12.6171,
         19.3612,  10.7810,  14.1895,  18.0786,  10.0566,  25.5446,  10.2927,
          8.6410,  11.2563,  11.5954,  27.9102,  21.7519,  33.5002,  14.8127,
         20.3029,  11.4698,  11.6541,  11.4112,   4.9388,  10.8239,  10.3423,
          9.2986,  14.5777,  11.1785,   9.0471,   8.4044,  15.0735,   5.0297,
         12.2533,   6.9211,   9.0268,   6.8988,   7.3291,  13.0676,  15.1013,
          7.6138,   6.9227,   8.7339,  11.5194,   6.7162,  12.7302,  11.2916,
         10.8886,  10.7892,   8.1539,   8.1492,   5.2984,   6.3906,  10.0207,
          7.5369,  12.3939,   8.0070,  16.1975,  15.1420,  12.1637,   9.6946,
          8.4633,  11.6931,  13.7539,   8.9751,  15.4157,   8.7691,   9.5682,
          8.5234,  19.1693,   7.6961], device='cuda:0')
Outer loop valEpocw Maximum [18/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 49.5
model_train val_loss valEpocw [18/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 72.2
model_train val_loss  valEpocw [18/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 155.0
Max_Val Meta Model:  tensor([34.8202, 34.4865, 35.6185, 40.4338, 39.3753, 40.4035, 41.6957, 38.1735,
        37.9559, 39.2940, 39.8180, 41.8951, 37.9828, 38.4707, 39.7083, 34.5454,
        31.8902, 39.8901, 34.8453, 39.6980, 40.0825, 38.3988, 39.0487, 41.8792,
        37.2963, 39.7311, 37.4879, 36.0292, 39.5016, 35.3632, 36.7351, 35.6339,
        40.6152, 36.5008, 35.2305, 33.4328, 36.0299, 33.9197, 39.1594, 35.5493,
        40.1401, 39.8272, 37.2750, 39.4811, 41.1719, 44.8501, 39.6163, 36.6772,
        40.5536, 45.6985, 40.1781, 32.5597, 39.5099, 40.9684, 40.8787, 39.0849,
        41.0887, 39.3751, 34.5671, 39.9408, 42.3461, 40.4021, 32.0009, 39.6180,
        38.0292, 38.6081, 36.9457, 37.1464, 37.8178, 40.1951, 38.7715, 38.7075,
        38.5845, 39.7913, 35.6025, 32.4484, 35.3000, 32.2383, 39.4218, 38.7103],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 6.8472,  5.0132,  2.9849,  5.6380,  3.9604,  3.7155,  3.4789,  7.0916,
         7.3611,  3.2146,  6.9916, 10.0651,  8.0583,  7.8168,  4.4136,  4.6134,
         5.2941,  4.1971,  6.1420,  8.6892,  4.7307,  4.3182,  4.9133,  3.0585,
         5.4053,  7.5259, 11.8930,  7.8409,  8.9283,  7.0033,  5.2454,  7.6819,
         5.1168,  6.6679,  6.3142,  5.4993,  8.0701,  6.7994,  6.4125, 17.4342,
        16.6793, 31.5751, 34.8064, 35.4788, 26.0120, 33.6565,  7.5247, 10.6741,
        20.6607,  7.1249, 16.4866, 17.5884, 26.3968, 14.0001, 29.0845, 44.9157,
        41.6195, 11.0101,  4.0915,  8.7327, 44.1288,  4.8992,  7.8588,  8.6783,
        10.3028,  6.2221, 11.2471,  9.5793,  8.3129,  5.4099,  5.8174, 11.7588,
         8.0967, 18.7944,  4.2362,  7.7456,  7.8280,  4.9182, 11.4531,  5.2684],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([17.1839, 13.4558,  7.5497, 12.0841,  8.2714,  8.3341,  7.2647, 15.4862,
        16.7916,  6.9312, 15.3941, 21.1296, 18.2130, 17.2358,  9.4504, 12.4723,
        13.9405,  8.6154, 14.8493, 18.5209, 10.1739,  9.5456, 10.5972,  6.4648,
        12.3376, 16.7750, 26.8940, 17.0161, 19.4021, 16.5726, 12.3271, 18.5192,
        10.7396, 15.2602, 15.2163, 13.9100, 19.0934, 17.1244, 13.9389, 45.7733,
        35.1835, 67.8947, 80.7145, 77.9929, 53.5716, 70.5405, 15.9227, 24.5891,
        44.3818, 14.8797, 34.8619, 46.4775, 57.1273, 28.9362, 60.5842, 99.0129,
        89.8961, 24.0312, 11.0558, 18.3022, 92.8237, 10.4035, 20.4146, 18.5734,
        22.7132, 13.5841, 24.9950, 20.7425, 18.6889, 11.4555, 12.5699, 25.9883,
        17.4823, 40.4820,  9.5692, 20.2263, 18.5902, 12.9447, 25.0168, 11.5837],
       device='cuda:0')
Outer loop valEpocw Maximum [18/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 45.7
model_train val_loss valEpocw [18/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 44.9
model_train val_loss  valEpocw [18/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 99.0
Max_Val Meta Model:  tensor([34.9007, 33.6472, 35.2176, 39.3950, 38.7647, 37.9778, 39.6021, 37.6209,
        32.2384, 39.0107, 39.2083, 38.8282, 37.3361, 36.9594, 38.8542, 33.6330,
        31.2702, 38.7360, 34.0867, 39.3318, 39.5497, 37.7013, 36.6343, 39.0473,
        35.9299, 39.3700, 37.2350, 35.0623, 38.9454, 34.7769, 36.3880, 34.8309,
        38.6351, 38.0902, 34.7419, 32.6614, 35.4366, 33.2298, 38.2079, 34.8256,
        39.2186, 39.0617, 35.9703, 36.6654, 38.1902, 39.0699, 39.5347, 36.0810,
        38.6963, 37.6715, 39.2683, 31.6626, 37.7360, 39.9050, 36.7291, 36.7696,
        37.7727, 39.5789, 33.6715, 38.3238, 37.3188, 39.9083, 33.9339, 38.4446,
        39.8952, 37.7058, 36.2561, 40.2080, 37.4310, 39.5188, 38.3166, 38.4803,
        38.7922, 39.1026, 42.2092, 31.6141, 34.6313, 31.6257, 39.1678, 37.6726],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 12.5092,   3.8054,  15.2212,   5.5986,   5.7613,   9.2348,   9.1084,
         11.6651,   6.0117,  14.4708,   6.5726,   9.3207,   6.4618,   7.0502,
         10.3579,   3.4556,   3.9590,   5.3494,   4.6889,   7.5151,   5.6470,
          4.6778,   6.3056,   5.2456,  10.1261,   5.9747,  10.6899,   3.2937,
          9.8097,   5.4309,   4.9489,   5.3122,   1.9658,   5.3603,   4.8715,
          4.1502,   5.3542,   4.4620,   4.2361,   4.7570,   7.9939,   2.9630,
          5.9543,   4.1485,   4.9964,   4.2664,   3.9727,   6.3647,   7.8139,
          3.1628,   3.9365,   3.7481,   6.0087,   4.0222,   6.7058,   4.7161,
          3.2613,   4.9722,   5.0836,   5.3255,   3.7357,   4.3156,   4.5158,
          4.0432,   6.5691,   4.2959,   8.1290,   5.1246,   6.1809,   3.9650,
          4.5068,   8.6777,   5.7286,   5.5781, 113.8043,   4.6297,   4.5682,
          4.5469,   9.7589,   4.0342], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 31.3784,  10.2123,  38.7009,  12.1366,  12.0361,  21.1808,  19.3101,
         25.6329,  14.0156,  31.3622,  14.5392,  20.0869,  14.6908,  15.7874,
         22.2906,   9.3473,  10.4176,  11.0363,  11.4377,  16.0641,  12.2252,
         10.3819,  13.8736,  11.3577,  23.5427,  13.3461,  24.1304,   7.1751,
         21.4244,  12.9852,  11.7335,  12.9396,   4.1737,  12.2395,  11.7553,
         10.6041,  12.7644,  11.3444,   9.2748,  12.4822,  16.9094,   6.2969,
         13.8393,   9.1124,  10.5236,   8.9767,   8.4283,  14.7181,  17.0959,
          6.7253,   8.3400,   9.9832,  13.1589,   8.3872,  14.3700,  10.4072,
          7.1586,  10.8386,  13.7556,  11.2808,   8.2090,   9.1492,  11.3447,
          8.7453,  13.9983,   9.4702,  18.1597,  10.8959,  13.8700,   8.4146,
          9.7190,  19.1765,  12.4088,  12.0313, 256.8306,  12.2260,  10.8965,
         12.0433,  21.2666,   8.9510], device='cuda:0')
Outer loop valEpocw Maximum [18/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 42.2
model_train val_loss valEpocw [18/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 113.8
model_train val_loss  valEpocw [18/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 256.8
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [88.08006725 97.2137279  91.15507852 97.02489005 97.26733349 96.5997003
 96.99808726 94.67233586 97.44398826 96.4608131  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.10704061 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.55613358
 96.90915072 96.22689782 96.9627563  93.80490004 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.55224717 96.13796128 96.24273583 96.9067141
 92.01885942 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [85.69583704 97.2137279  91.26107138 97.02489005 97.26733349 96.5997003
 96.99808726 94.73934284 97.44398826 96.47665111 98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.17358463
 96.90915072 96.22689782 96.9627563  93.8828718  98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.03081103 96.13796128 96.24273583 96.9067141
 91.28909248 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [96.70414729  3.95026204 56.87265176  6.20150154  3.27959398 24.50146154
  8.98364162 30.68497942  2.61347699 27.02566973  1.63460258  1.196873
  0.77405329 13.79793143  3.76533599  3.64390621  3.82064601  2.94067767
  0.87856041  1.26946683  1.61817274  0.56369358  1.46264906  3.02978667
 16.65601977  5.52408759 25.7888615   7.92863986  3.79145017  1.33489812
  8.58136608  0.86555399 24.34140836  1.31298417  1.6787049   1.61802752
  4.55773012 29.940653    5.95798172 26.14221884  7.2914412  42.47392648
  5.8616702  19.91497351 16.13890953 32.29459472  2.71483993  2.11234718
  3.0317736   2.13956592  1.74190593  1.29223274  1.21576958 14.00441628
  1.75438676  4.7688954  61.2828645  19.51764289 11.09289761  8.24077203
 61.29381788  5.10976517 12.07864056  7.59771198  4.07908976  5.59244163
  4.33755957  9.35686916  3.86728714  7.34750918  0.72865502 28.23760695
  5.63021405 23.57718898 13.41651856  7.9822007   1.27058413  2.18389978
  0.19877447  0.81828121]
Accuracy th:0.5 is [45.71216238 97.2137279  72.06052558 97.02489005 97.26733349 76.92766901
 77.29681656 76.37212022 78.21542135 96.36213009 78.45055494 98.52097319
 99.41399349 79.99780704 78.11917496 96.56680596 96.29512311 77.8182527
 98.65376884 98.30776915 80.15009564 79.03656145 98.38695922 77.93033711
 80.64351068 96.65086926 94.0778012  77.49296427 98.01293844 78.42131553
 97.30875598 98.57457877 96.36213009 98.02024829 86.07838598 78.07653416
 77.74149925 89.12172123 97.11504489 75.04781862 79.21443452 92.05906361
 77.27854193 76.87650004 96.9627563  93.87434364 98.02877645 98.57336046
 89.09613674 86.67170234 86.45971662 98.55508583 98.99976852 77.43326714
 98.70615611 77.87185829 72.54541246 92.70842217 96.24273583 96.9067141
 89.79300934 97.17717864 91.30249388 77.86576674 98.42838172 78.36649164
 98.20786784 77.06046466 78.92082211 97.52561494 79.46418781 95.99054592
 78.58091398 95.45083515 77.1079787  81.77288288 85.68365395 78.36892825
 79.49220892 99.14718388]
Accuracy th:0.7 is [45.7292187  97.2137279  72.06052558 97.02489005 97.26733349 76.92766901
 77.29681656 76.55730315 78.21542135 96.46812295 78.45055494 98.52097319
 99.41399349 80.49000378 78.11917496 96.56680596 96.29512311 77.8182527
 98.65376884 98.30776915 80.59356002 79.03656145 98.38695922 78.09237217
 81.21611579 96.65086926 94.0778012  77.49296427 98.01293844 78.42131553
 97.30875598 98.57457877 96.36213009 98.02024829 86.32935759 78.07653416
 77.74149925 89.41898856 97.11504489 75.04781862 79.87232124 92.05906361
 77.27854193 76.87650004 96.9627563  93.87434364 98.02877645 98.57336046
 91.3609727  87.59639868 86.62053338 98.55508583 98.99976852 77.43326714
 98.70615611 77.87185829 72.54541246 93.10071758 96.24273583 96.9067141
 89.79300934 97.17717864 91.49011342 77.86576674 98.42838172 78.36649164
 98.20786784 77.06046466 78.92082211 97.55972759 79.46418781 95.99054592
 78.65157588 95.45083515 77.1079787  81.86669266 85.81888622 78.36892825
 79.49220892 99.14718388]
Avg Prec: is [55.89734338  3.05045928 11.33807092  3.17390837  2.2942029   3.86995783
  3.23989855  5.60019067  2.50014816  3.74388355  1.61249198  1.72453243
  0.62984536  5.16622406  2.61173047  3.17511926  3.62369229  2.66892955
  1.38210009  1.69455701  1.94768497  0.84377537  1.7974693   2.38930911
  5.16502942  3.71219171  6.48685505  3.3663264   2.09802752  1.93177811
  2.73074822  1.35886788  3.6445082   1.63410808  2.30248774  2.33019942
  3.15560403  2.60901708  2.77573759  7.40441222  2.23167355  8.29205413
  3.39433554  4.02861804  3.2703634   6.39622705  2.06269275  1.46869642
  2.11794493  1.53069066  1.76716391  1.52363412  1.05744839  3.08894229
  1.34139979  2.6283496  11.07288985  3.59870952  3.98376534  2.82799582
 10.95026865  2.21645992  3.83833476  2.92085256  1.63584398  2.45943974
  1.87575751  4.10291861  1.29905861  2.38986599  0.18466697  3.55047291
  1.99641404  4.74523665  3.86048326  3.07080303  0.82592817  1.86664056
  0.15787247  0.73599067]
mAP score regular 11.64, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [89.09235867 97.22450607 91.15280165 96.96290206 97.90716795 96.63651992
 96.80843112 94.64085507 97.38894287 96.30266338 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.36430226 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.94167476
 97.07750953 96.48703192 97.03764606 93.72150385 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.97839898 96.39235618 96.16314124 96.78102499
 92.49321075 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [88.15307572 97.22450607 91.89525874 96.96290206 97.90716795 96.63651992
 96.80843112 94.90245908 97.38894287 96.41727085 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.58539502
 97.07750953 96.48703192 97.03764606 94.06283479 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.76413285 96.39235618 96.16314124 96.78102499
 91.83546354 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [97.04534453  4.41343332 60.79917881  6.3279478   2.85047255 25.24042425
  9.48689681 32.60233949  3.13628731 29.10743698  1.54617154  1.18543432
  0.69917486 15.37667916  4.30447695  3.89324298  3.7889355   3.12274609
  0.81238396  1.27002268  1.48915873  0.56423791  1.51957459  3.28465445
 17.08811294  5.92222227 25.86281818  7.65610802  4.35093925  1.37956757
 10.577947    0.79803425 30.69485656  1.26860452  1.40820763  1.41245827
  4.32637661 40.6497004   8.38442327 26.78697205  7.39454965 43.48688095
  5.92380698 20.17202863 16.09693981 32.27001724  3.02209199  2.23118406
  3.29036603  2.13929454  1.91860775  1.52390426  1.47737634 17.02519001
  1.97624207  4.59323069 56.70639452 20.4515598  10.45909946  9.16182009
 62.133368    5.23610466 12.78139876  7.82932411  4.53181794  5.41818629
  4.68025273  9.97121221  3.42653396  7.46121874  0.78308247 32.0117213
  5.41083703 24.55802465 19.76482439  7.62042932  1.19583163  2.18197579
  0.17872833  0.77274497]
Accuracy th:0.5 is [45.44186162 97.22450607 70.30669955 96.96290206 97.90716795 75.77796049
 75.86516182 74.77639086 77.35256746 96.41477938 77.51202133 98.5325261
 99.34972718 77.8658096  77.43478586 96.31262924 96.21047911 76.85178264
 98.78167277 98.34068316 78.60577522 78.14236241 98.31327703 76.93400105
 77.99785734 96.52938685 94.3393876  76.90410345 97.81747515 77.46468346
 97.52597354 98.67204823 96.39983058 98.18870369 86.54358821 77.08099758
 77.04860852 90.99583925 97.0276802  74.21331938 77.41983706 92.37362035
 76.20898423 75.88260209 97.03764606 94.02795426 98.18621222 98.77668984
 89.82983282 86.46884421 84.68246257 98.55993223 98.87385704 76.19901836
 98.6969629  76.88666318 70.9395321  93.72399532 96.16314124 96.78102499
 90.13379176 97.04761193 91.09549792 76.88417171 98.32075143 77.73376187
 98.13139996 76.06946209 78.17475148 97.51849914 78.59830082 96.07843137
 77.93806214 95.44559882 75.98724369 82.67434038 87.45297357 77.53693599
 78.66806189 99.15040985]
Accuracy th:0.7 is [45.74083763 97.22450607 70.30669955 96.96290206 97.90716795 75.77796049
 75.86516182 74.91342153 77.35256746 96.41976231 77.51202133 98.5325261
 99.34972718 78.25946134 77.43478586 96.31262924 96.21047911 76.85178264
 98.78167277 98.34068316 78.98447816 78.14236241 98.31327703 76.97635598
 78.43386402 96.52938685 94.3393876  76.90410345 97.81747515 77.46468346
 97.52597354 98.67204823 96.39983058 98.18870369 86.85253008 77.08099758
 77.04860852 91.19515659 97.0276802  74.21331938 77.88574134 92.37362035
 76.20898423 75.88260209 97.03764606 94.02795426 98.18621222 98.77668984
 91.98993447 86.90485089 84.8219847  98.55993223 98.87385704 76.19901836
 98.6969629  76.88666318 70.9395321  94.03542866 96.16314124 96.78102499
 90.13379176 97.04761193 91.29979819 76.88417171 98.32075143 77.73376187
 98.13139996 76.06946209 78.17475148 97.53593941 78.59830082 96.07843137
 77.94304507 95.44559882 75.98724369 82.75655879 87.58751277 77.53693599
 78.66806189 99.15040985]
Avg Prec: is [54.28985973  3.72301609 14.85415617  4.54535033  1.48506304  4.26356526
 12.89585541  8.63049426  7.83079315  5.21004552  2.30548166  4.95037854
  1.58592375  5.8712516   3.02468078  3.69065514 24.43481062  6.52945394
  1.57019683  2.71579743  3.55566807  1.46765484  1.14907122  5.48854539
  5.68856877  9.99173365  7.94823701  4.55784076  3.92468426  6.13084043
  2.27573065  0.86276033  3.06723525  1.10805864  1.69137672  2.35799953
  2.01223592  2.19769988  2.24510902  6.20789355  1.7383787   5.99625901
  2.18801884  2.71801357  2.36726176  4.84871607  1.72381016  1.04205142
  1.37868422  1.16817684  1.19428883  0.97554751  0.73037968  2.36159514
  0.85095211  1.83092491 10.04843671  2.8820478   3.86733275  2.75930377
  7.84813266  2.04116661  3.20092442  2.5047811   1.34645686  1.82794923
  1.53423116  3.43694557  1.08437142  2.23347244  0.19261503  3.20896025
  1.57048541  3.9301027   3.53840807  2.31841486  0.60008913  1.50192075
  0.12976852  0.59203533]
mAP score regular 12.27, mAP score EMA 4.32
Train_data_mAP: current_mAP = 11.64, highest_mAP = 11.64
Val_data_mAP: current_mAP = 12.27, highest_mAP = 12.27
tensor([0.3975, 0.3709, 0.3933, 0.4588, 0.4790, 0.4343, 0.4717, 0.4553, 0.4281,
        0.4604, 0.4516, 0.4632, 0.4386, 0.4465, 0.4658, 0.3677, 0.3785, 0.4843,
        0.4084, 0.4665, 0.4605, 0.4521, 0.4549, 0.4630, 0.4292, 0.4471, 0.4430,
        0.4581, 0.4576, 0.4168, 0.4216, 0.4117, 0.4700, 0.4381, 0.4150, 0.3886,
        0.4184, 0.3921, 0.4562, 0.3788, 0.4712, 0.4708, 0.4295, 0.4547, 0.4692,
        0.4771, 0.4717, 0.4314, 0.4572, 0.4710, 0.4727, 0.3746, 0.4576, 0.4802,
        0.4637, 0.4549, 0.4552, 0.4575, 0.3687, 0.4696, 0.4582, 0.4712, 0.3969,
        0.4633, 0.4712, 0.4538, 0.4476, 0.4693, 0.4463, 0.4729, 0.4613, 0.4521,
        0.4638, 0.4629, 0.4526, 0.3772, 0.4186, 0.3767, 0.4595, 0.4478],
       device='cuda:0')
Max Train Loss:  tensor([13.2822,  5.6642, 12.1714,  9.4460,  6.1931,  7.9622,  9.9934,  9.4494,
         7.2001, 10.1600,  9.2911,  9.4373,  7.8078, 13.5664, 14.8426, 12.0161,
         9.0345,  7.7888,  5.9987,  8.6012,  8.8385,  4.4114,  7.5347,  7.7618,
        11.8061, 10.9888,  9.8486,  6.4687,  9.9310,  9.3008,  6.0046,  6.6103,
         7.6250,  7.0059,  8.3020,  5.5147, 12.6417,  8.3871,  8.3628, 12.4348,
         9.2390, 13.7957,  8.0625, 10.2161,  9.6114, 13.1997,  6.0677,  7.5432,
        10.5813,  4.8831,  8.4972,  5.8275,  6.4328,  8.2753,  7.0756, 10.4833,
         9.7600,  9.8764,  9.5025, 11.3472, 10.7155,  9.4698,  9.9942, 11.1946,
        10.2726,  7.5871, 11.1063,  8.4330,  8.1904,  6.4280,  4.8361, 10.4000,
        11.5604, 11.5666,  9.7418,  7.1791,  6.8021,  4.7732, 10.1215,  6.7122],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [19/80], Step [000/642], LR 1.0e-04, Loss: 14.8
Max Train Loss:  tensor([23.5559,  8.7703, 18.3111, 14.5534, 13.6351,  7.8584, 15.6392, 18.7228,
         7.5846, 15.2174, 20.6581,  9.0307, 16.2097, 11.5041, 11.7432,  5.5983,
        10.8689, 16.8822,  6.2726, 14.5635, 12.7808,  8.6617,  7.3397, 12.4361,
         7.2404, 11.5868, 14.1104,  9.3168, 12.3080, 10.8905, 11.2160, 10.0895,
        14.3663,  9.3198,  7.8312,  5.9638,  7.6235,  8.9151,  8.1893, 10.8015,
         8.3880, 16.3190,  8.9249, 16.6738,  8.5956, 13.3709, 14.4074,  7.6677,
        10.8758, 12.2104, 10.8513,  4.9302, 16.7156, 14.7014, 14.7067,  7.6608,
        15.6599,  9.5406,  8.0199,  7.5415, 14.6033, 11.4384,  7.1293, 11.8097,
        14.3255,  8.6240,  9.0532, 14.6572, 11.9507, 10.3397, 18.4172,  8.1951,
        15.1386, 11.2745, 10.3798,  7.8874,  6.0114,  6.3734,  5.3602, 12.5869],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [19/80], Step [100/642], LR 1.0e-04, Loss: 23.6
Max Train Loss:  tensor([11.3712,  7.2306, 16.1145,  8.7033, 11.3717, 12.6003,  6.7841, 12.9887,
         8.4471, 11.8204,  8.0927, 10.6896,  5.2138, 15.8748,  6.9100,  6.2051,
         9.7169, 10.5149, 10.3811, 11.1364,  7.0049,  6.3064,  6.1960,  6.8809,
        10.6350,  8.2024, 11.0214, 11.8119,  8.8833,  7.7226,  7.1429,  6.0503,
         9.7817,  6.5460,  7.2732,  4.4868, 12.1807, 14.1377,  6.1333, 12.9011,
        10.5175, 13.5359,  9.2309, 11.2349,  9.8192, 10.9942, 12.1168,  7.3425,
        10.0732,  9.6877, 10.0399,  4.1238,  6.5033, 10.7964,  6.8214,  9.4827,
        12.6192, 10.2222, 11.8389, 10.7304, 12.5203, 13.7992,  6.5648, 11.3323,
         5.4535, 11.6446,  8.1546, 12.8127, 12.0651, 14.1844,  7.1163, 14.7022,
         9.2811, 14.9390, 12.8837, 10.6142,  5.7024,  5.7236,  5.3584,  7.6618],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [19/80], Step [200/642], LR 1.0e-04, Loss: 16.1
Max Train Loss:  tensor([13.8696,  6.6190, 11.4959, 10.4701,  8.5084,  8.2660,  9.6115, 11.1194,
         6.4657, 10.0486,  9.7228,  5.6737,  9.5732, 10.6964,  4.9483, 10.2606,
         7.7863,  9.7615,  5.0200,  7.3666, 11.6593,  7.3416,  7.8937,  8.3017,
        10.8613,  7.8356, 12.8123,  8.2930,  6.5677,  8.3964,  9.8334,  6.0245,
         7.8879,  9.8295, 12.7779,  8.6720,  8.8242, 11.9299,  5.5194,  9.7054,
         8.3368, 14.0207, 11.7880,  7.8028, 12.5761, 14.8290, 13.9515,  7.4634,
        10.8629, 10.6745, 11.2275,  6.9683,  8.2998, 11.4181,  9.7376,  7.3488,
        14.2623,  8.2720,  7.6686,  7.3355, 12.8634, 10.6072,  6.5514,  7.6788,
         6.3323,  8.1900,  8.8996,  8.6221,  9.0916,  9.9645,  7.0887, 11.7562,
         8.4271, 11.8237, 10.9601,  5.2076,  5.6758,  6.7260,  6.6137,  9.2643],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [19/80], Step [300/642], LR 1.0e-04, Loss: 14.8
Max Train Loss:  tensor([15.2851,  4.5911, 11.4758, 10.2594,  9.3126,  8.6511,  9.9971, 11.7282,
         9.8279,  6.9654,  9.7017,  9.3813,  6.7286,  9.7347,  8.0465,  5.9264,
         8.9785,  7.5076,  8.1103,  7.7275,  7.9528,  7.4258,  7.7222,  6.6785,
         9.8334, 13.0943, 13.3026,  9.3674, 10.2539,  8.5311,  7.4205,  5.8484,
         9.4386,  9.2919,  7.6173,  6.6989,  6.1428, 10.0182,  7.8518, 11.5271,
        10.8761, 13.7195,  7.2099,  7.9676, 11.5491, 13.2328, 11.9401,  6.5744,
         3.6178,  7.9080,  8.6687,  4.7567,  8.1841, 11.5430,  6.2148,  9.4704,
        12.6972, 10.1292,  7.6403, 11.8836, 12.3266, 12.0324,  8.5760,  7.2272,
         5.9503,  7.8193,  9.7869,  6.9616,  8.4174,  7.8562,  6.8970, 10.2597,
         9.9753,  9.3625, 10.5340,  5.6567,  6.5649,  4.6395,  5.1414,  7.4451],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [19/80], Step [400/642], LR 1.0e-04, Loss: 15.3
Max Train Loss:  tensor([ 9.8320,  7.2965, 11.2819, 10.8041,  8.1669,  9.5790, 11.4142, 10.1293,
         8.7737, 10.4516,  8.6937,  8.8255,  6.0459, 14.5477,  4.9195,  5.6561,
         8.7327,  6.9658,  7.6139,  5.3032,  8.4672,  6.1618,  6.0564,  8.1068,
        10.9139,  7.6883, 12.5769, 10.0897,  7.5614,  6.4716,  7.7191,  8.7489,
         8.4741,  6.4081,  7.8559,  9.8220,  8.6547,  7.2823,  9.4870, 12.6377,
         9.2520,  9.6765, 12.1093,  9.0568,  8.1788, 15.3285, 13.2734, 10.4813,
         5.4810, 10.0037,  8.5140,  6.2313,  8.0330,  8.6298,  7.5110, 10.1771,
        11.1843, 10.1497,  9.2797,  8.2282, 10.6329, 10.2666, 11.0024,  8.5322,
        11.4217, 10.1456, 11.1731, 12.1102, 10.1332,  7.5526,  7.7836, 10.5925,
         7.7019,  7.6885,  9.3176,  8.1283,  7.1572,  3.9174,  5.2089,  8.6237],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [19/80], Step [500/642], LR 1.0e-04, Loss: 15.3
Max Train Loss:  tensor([14.7232,  6.1496, 11.0863,  8.0706,  9.5515,  7.8451,  7.0683,  9.4927,
         9.2725,  7.9963,  9.3189,  7.5128,  5.1128, 14.0215,  8.8283,  3.9725,
         8.3369, 15.1871,  6.9907,  7.3806,  9.6954,  8.9232,  7.7871, 11.0786,
         8.3235,  4.9905,  8.9056,  7.9450,  7.5139,  7.2509, 12.5284,  7.2841,
        11.0457, 12.3670,  9.5467,  8.0989,  7.0996,  7.8163,  7.5844,  8.9125,
         8.3509, 14.4096,  8.2643,  9.4925,  9.5110,  9.3313, 12.5550,  6.7102,
         6.8307,  9.8568,  8.1165,  5.8953,  6.4246,  7.2170,  6.3447,  8.2480,
        11.0525,  8.4868,  8.4162,  5.6819,  9.0865,  9.9351,  7.6139,  7.4278,
         5.2893,  8.0005,  8.0935,  8.1488,  9.6308,  7.1518,  7.0418,  8.7434,
         7.8862,  6.3326,  8.2272,  6.2959,  6.4699,  6.0213,  6.0031,  9.2569],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [19/80], Step [600/642], LR 1.0e-04, Loss: 15.2
Max_Val Meta Model:  tensor([ 21.0490,  25.1291,  34.2533,  19.5453,   5.1018,   8.1296,   7.4949,
          9.5018,   7.1945,   8.3441,   8.0184,   7.7513,   5.9079,   8.8818,
          3.9568,   2.5919, 111.8453,   5.8560,   5.0484,   5.4786,   7.0339,
          6.3380,   6.3584,   5.2678,  13.0237,   9.4327,  13.2486,   5.4570,
          6.9455,   7.4618,   4.1755,   6.0666,   7.5670,   6.5681,   5.8476,
          5.3136,   5.6062,   6.8175,   2.9268,  18.9720,   9.5198,  10.9333,
          8.1974,  10.6573,   8.9836,  12.0937,   4.7006,   7.6035,   3.7582,
          8.8497,   5.0914,   4.1138,   8.0248,   5.5370,   5.5524,   4.7298,
         12.3895,   8.8271,   9.2532,   4.9742,   7.8010,  14.0673,   5.6273,
          6.8311,   5.3585,   6.5861,   8.2060,   7.3908,  10.2004,  13.0398,
          7.1460,  10.7541,  12.0777,   7.5741,   7.5947,   6.4072,   5.7127,
          4.7360,   5.3570,   7.6978], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 17.1879,  23.4429,  28.9338,  18.9911,   5.2975,   9.7966,   8.3820,
         10.6250,   7.8357,   9.4168,   8.6872,   8.1294,   6.5200,   9.4862,
          4.4820,   2.9192, 106.8301,   6.6894,   5.6574,   6.1361,   7.7912,
          7.0366,   6.7139,   5.6464,  13.1295,  10.1455,  13.5133,   6.1882,
          7.5945,   7.7441,   4.8534,   6.7450,   8.7501,   7.2845,   6.5122,
          6.1825,   6.8568,   7.2324,   3.4419,  18.8709,  10.1248,  11.7458,
          8.8513,  11.1394,   9.2487,  11.5203,   5.2162,   8.3600,   3.9700,
          9.7486,   5.4890,   4.6415,   8.5772,   5.8716,   6.1944,   5.1636,
         12.7152,   9.1419,   9.4141,   5.4268,   7.4633,  14.1511,   6.2028,
          7.4712,   5.9741,   7.2972,   8.9851,   8.9863,  10.9203,  13.6261,
          7.9175,  11.1321,  12.8549,   8.2106,   7.9718,   6.8711,   6.3675,
          5.2346,   6.0167,   8.4791], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 43.2406,  63.1980,  73.5736,  41.3884,  11.0596,  22.5592,  17.7707,
         23.3367,  18.3045,  20.4551,  19.2354,  17.5497,  14.8650,  21.2467,
          9.6226,   7.9399, 282.2602,  13.8124,  13.8538,  13.1525,  16.9176,
         15.5632,  14.7594,  12.1952,  30.5936,  22.6935,  30.5060,  13.5081,
         16.5981,  18.5789,  11.5123,  16.3828,  18.6152,  16.6266,  15.6915,
         15.9097,  16.3897,  18.4451,   7.5447,  49.8180,  21.4862,  24.9485,
         20.6061,  24.4959,  19.7104,  24.1460,  11.0589,  19.3765,   8.6834,
         20.6957,  11.6122,  12.3915,  18.7437,  12.2276,  13.3599,  11.3508,
         27.9360,  19.9820,  25.5338,  11.5568,  16.2886,  30.0345,  15.6295,
         16.1250,  12.6792,  16.0818,  20.0757,  19.1499,  24.4697,  28.8125,
         17.1618,  24.6210,  27.7139,  17.7373,  17.6141,  18.2149,  15.2126,
         13.8971,  13.0949,  18.9331], device='cuda:0')
Outer loop valEpocw Maximum [19/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 111.8
model_train val_loss valEpocw [19/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 106.8
model_train val_loss  valEpocw [19/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 282.3
Max_Val Meta Model:  tensor([45.1985, 30.9167, 38.8962, 38.1864, 40.4268, 39.7602, 42.2679, 38.1491,
        35.3442, 40.8103, 38.3812, 55.8192, 37.2910, 40.2191, 45.9799, 30.8484,
        33.0174, 40.1458, 34.3450, 41.1208, 39.2993, 38.1601, 37.8317, 43.4192,
        36.3880, 37.8546, 37.0923, 38.1427, 38.7893, 34.8450, 35.6543, 35.1175,
        39.7130, 34.9747, 34.8735, 33.0661, 35.2949, 33.1744, 37.9822, 37.1803,
        39.4617, 42.6862, 36.0836, 38.6235, 38.4142, 38.8178, 38.9572, 36.1930,
        38.1552, 39.2050, 39.6150, 31.8166, 38.2093, 40.2460, 38.1977, 37.1689,
        43.3547, 37.4334, 30.9226, 38.9258, 37.5089, 42.0450, 33.6318, 38.9674,
        38.8065, 37.9238, 37.6927, 38.2933, 37.7604, 40.3077, 38.8586, 36.8847,
        39.6776, 37.0983, 43.0140, 31.7192, 34.9867, 31.7577, 38.7927, 38.1666],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([22.5216,  6.8336, 22.1046,  8.0863,  5.0023, 17.6367, 38.3465, 20.2599,
        15.3307, 16.4003,  9.6374, 98.8453,  6.4466,  4.6510,  6.6513,  2.6009,
         5.8873,  9.5792,  3.9577, 10.6342,  5.6141,  4.9916,  5.7860,  5.7168,
        11.1080,  8.8831, 14.7386,  7.3793,  7.0843,  4.0276,  3.2190,  4.7772,
         4.4668,  5.1337,  4.5768,  3.9906,  6.1231,  4.4879,  1.9606,  2.8647,
         5.3709,  3.1358,  5.3105,  4.2606,  3.5667,  3.3621,  3.5954,  5.4065,
         2.4989,  7.1229,  3.7027,  3.1855,  5.1702,  3.8892,  4.2557,  4.2003,
         6.0309,  4.8260,  4.5374,  3.2623,  3.3288,  3.8931,  3.8174,  4.4551,
         3.9773,  5.0950,  6.5550,  7.5051,  6.3243,  5.6120,  5.7362,  3.8276,
         6.8842,  4.1622,  5.2630,  3.0569,  4.5053,  3.1173,  4.1549,  6.1327],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 58.2579,  18.4674,  54.8911,  17.6162,  10.4052,  39.5219,  79.4535,
         44.5884,  35.8581,  35.0285,  21.3746, 216.5782,  14.6708,  10.1624,
         13.8003,   7.0565,  15.3974,  19.7339,   9.6347,  22.3965,  12.1531,
         11.0942,  12.7494,  11.9657,  25.8516,  19.9267,  33.2516,  16.0258,
         15.4631,   9.6668,   7.6259,  11.5966,   9.5094,  11.8560,  11.0683,
         10.1436,  14.6348,  11.4407,   4.3031,   7.4131,  11.3210,   6.4601,
         12.3558,   9.3738,   7.6481,   7.1279,   7.6480,  12.5025,   5.4681,
         15.1279,   7.8433,   8.4919,  11.3269,   8.0894,   9.2012,   9.3416,
         13.0086,  10.6941,  12.3602,   6.9299,   7.3183,   8.0761,   9.6697,
          9.6266,   8.4662,  11.2870,  14.6224,  16.2152,  14.1503,  11.8020,
         12.3323,   8.6092,  14.7559,   9.2228,  11.6455,   8.0794,  10.7114,
          8.2623,   9.0689,  13.6744], device='cuda:0')
Outer loop valEpocw Maximum [19/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 55.8
model_train val_loss valEpocw [19/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 98.8
model_train val_loss  valEpocw [19/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 216.6
Max_Val Meta Model:  tensor([33.1039, 31.0362, 36.5170, 38.4399, 40.2363, 40.1739, 39.9048, 38.7570,
        39.0405, 39.5263, 40.8773, 40.4150, 37.2492, 39.5309, 40.0877, 30.8302,
        33.0482, 37.2762, 34.4405, 41.7142, 38.5842, 41.4051, 38.5859, 39.7864,
        36.1411, 38.4252, 36.8156, 38.3873, 39.9615, 35.0450, 33.3789, 35.4155,
        39.9651, 35.4145, 34.4392, 32.9991, 35.3083, 33.2198, 37.7879, 35.8840,
        39.2418, 41.4920, 36.8929, 39.7452, 39.2147, 41.9162, 40.4247, 36.0732,
        39.6900, 41.0106, 41.1990, 32.4366, 41.2182, 40.2299, 39.1729, 38.7710,
        41.5149, 37.4386, 31.3963, 37.8337, 41.9876, 39.5720, 33.4767, 38.7579,
        37.4601, 37.7972, 37.6675, 38.4109, 37.4851, 40.1116, 38.0214, 37.2570,
        39.6439, 37.4257, 39.3275, 32.3624, 34.8615, 31.7268, 38.2925, 38.0500],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 6.5863,  4.3746,  3.3082,  4.2893,  3.3739,  3.8098,  2.9957,  6.8229,
         7.5763,  3.4926,  8.0747,  6.3758,  5.6874,  7.9273,  0.7057,  1.6791,
         5.6116,  3.0623,  5.8257,  5.7752,  7.9155,  7.1380,  4.6137,  3.9421,
         5.0232,  4.3936, 11.2092,  7.8330,  5.2219,  5.9418,  4.8639,  7.7682,
         7.0654,  7.4068,  6.5421,  5.4694,  7.1746,  6.6117,  3.3342, 16.7794,
        15.4612, 31.6532, 34.7599, 34.2316, 24.8299, 36.4963,  7.3180, 10.0272,
        18.7432, 11.0671, 16.4828, 16.8854, 25.8032, 12.9503, 29.4179, 45.2237,
        40.9694, 14.0301,  6.5561,  6.0820, 38.6434,  5.8119,  7.9024,  9.7804,
         9.3428,  7.4971, 10.6484, 10.7406,  9.4501,  7.4310,  7.8109,  9.7766,
         8.8378, 17.5513,  3.6727,  7.3789,  8.1670,  4.6409,  5.9828,  8.6076],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 17.4118,  11.6734,   8.1054,   9.2930,   7.0025,   8.4538,   6.2923,
         14.8519,  17.0483,   7.5131,  17.2782,  13.5873,  12.8833,  17.1856,
          1.5039,   4.5210,  14.6621,   6.4905,  14.0097,  11.9849,  17.1839,
         15.3328,  10.1029,   8.4111,  11.6032,   9.7685,  25.1384,  16.9994,
         11.1891,  14.0520,  11.7568,  18.5607,  14.8538,  16.7704,  15.8911,
         13.7972,  16.9878,  16.6614,   7.2487,  43.1724,  32.9168,  66.0070,
         80.8803,  74.9629,  53.0372,  75.1738,  15.1128,  23.1338,  40.4433,
         22.8112,  34.0509,  44.6098,  55.8367,  26.8466,  63.2103, 100.6520,
         87.1138,  30.9348,  17.5416,  12.9029,  84.6261,  12.2517,  20.0885,
         20.8566,  20.1143,  16.5334,  23.6593,  22.9538,  21.1709,  15.5587,
         17.0348,  21.8404,  18.8138,  38.4838,   7.9544,  19.1258,  19.3583,
         12.1876,  13.1016,  19.1242], device='cuda:0')
Outer loop valEpocw Maximum [19/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 42.0
model_train val_loss valEpocw [19/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 45.2
model_train val_loss  valEpocw [19/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 100.7
Max_Val Meta Model:  tensor([37.5444, 30.3810, 35.1605, 38.1944, 38.6545, 39.3071, 39.5640, 36.9272,
        37.8795, 39.2051, 37.5255, 35.7052, 36.5456, 38.0956, 39.4588, 30.3287,
        32.9628, 37.1206, 33.5596, 37.5608, 37.7863, 38.2908, 36.6462, 38.9409,
        35.8213, 37.9021, 36.5558, 38.0352, 37.9202, 33.9394, 31.9592, 34.2122,
        36.0309, 39.3674, 34.3847, 32.3534, 34.4334, 32.3183, 37.5463, 35.8900,
        37.8711, 37.7494, 35.6916, 37.8937, 37.8072, 36.8737, 37.3951, 35.7903,
        36.2747, 37.8579, 37.9538, 31.1200, 39.1242, 37.8418, 38.3729, 36.4446,
        37.4102, 35.2415, 30.5802, 36.0590, 36.0780, 39.0428, 33.5491, 37.5226,
        36.5674, 36.6603, 36.6297, 41.9513, 37.3140, 37.2562, 38.7437, 37.2213,
        39.5230, 36.0123, 43.1336, 30.8729, 34.4217, 31.1035, 38.3302, 38.5123],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 13.1711,   3.2041,  11.3822,   6.1168,   5.7956,  10.1391,   7.6294,
         11.7232,   5.7651,  11.7422,   6.9750,   5.9960,   4.4696,   6.6004,
          7.7460,   1.1342,   4.9083,   5.6329,   4.3379,   4.8187,   6.1753,
          5.5727,   7.6388,   6.0010,   8.9274,   3.7497,  10.7731,   4.0935,
          7.3319,   4.3993,   3.0313,   5.2175,   3.9714,   5.7407,   5.0601,
          3.7664,   4.8395,   4.4853,   1.7287,   4.4754,   5.8869,   3.1774,
          5.8850,   4.6114,   4.1574,   4.8187,   4.0578,   5.9894,   3.0206,
          7.0929,   4.3756,   3.4934,   5.7589,   4.4974,   4.7284,   3.6135,
          3.9119,   4.9295,   6.1149,   4.0517,   4.1360,   5.0595,   4.2344,
          5.0860,   4.3955,   5.5977,   7.2346,   5.7089,   7.0653,   5.2574,
          6.3856,   5.6447,   6.2113,   5.1790, 141.0462,   4.2334,   4.9704,
          4.1890,   4.6136,   6.8122], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 33.9962,   8.6903,  28.9668,  13.3472,  12.1866,  22.8562,  16.0584,
         26.0347,  13.3135,  25.3469,  15.5448,  13.2025,  10.2620,  14.7141,
         16.6060,   3.0855,  12.8379,  11.8387,  10.6402,  10.4715,  13.5485,
         12.3800,  16.9964,  13.0370,  20.8651,   8.4104,  24.3049,   8.9203,
         16.0967,  10.6765,   7.5748,  12.8271,   8.6730,  13.1079,  12.2404,
          9.6310,  11.6580,  11.5405,   3.8634,  11.5858,  12.5401,   6.8633,
         13.6698,  10.1973,   8.9272,  10.4085,   8.7217,  13.8331,   6.6946,
         15.2692,   9.3565,   9.3621,  12.4731,   9.5596,  10.1892,   8.0427,
          8.6723,  11.1719,  16.7401,   8.7207,   9.2309,  10.7938,  10.6878,
         11.1772,   9.5985,  12.5824,  16.2398,  12.1722,  15.8179,  11.4340,
         13.5944,  12.7169,  13.3544,  11.6290, 318.8044,  11.3091,  11.8430,
         11.1557,  10.0279,  15.1271], device='cuda:0')
Outer loop valEpocw Maximum [19/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 43.1
model_train val_loss valEpocw [19/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 141.0
model_train val_loss  valEpocw [19/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 318.8
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.794983   97.2137279  91.4036135  97.02489005 97.26733349 96.5997003
 96.99808726 94.75883578 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.54273218
 96.90915072 96.22689782 96.9627563  93.56611152 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.69357098 96.13796128 96.24273583 96.9067141
 92.14800015 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [84.69926049 97.2137279  90.0927133  97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.1272889
 96.90915072 96.22689782 96.9627563  93.89139996 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.10269124 96.13796128 96.24273583 96.9067141
 91.14655036 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [96.66188483  5.04713603 57.27749776 16.79665606  3.97629549 24.54384837
  8.02058318 33.29638741  2.42288997 27.52550808  1.6471813   1.18550029
  1.0009547  11.14436639  5.94969774  3.97326722  4.27275147  5.25602674
  0.8443798   1.30587378  1.4268938   0.47829406  1.77431168  3.09421321
 16.87232785 10.87277417 24.1287293   7.30641256  4.74939371  1.37992229
  2.03307572  0.87769733  9.00670914  1.31327698  1.63851824  3.67693125
  6.78713212  5.16075211  4.41556575 25.06288575  5.80747866 43.1480168
  5.23119319 15.99021959 16.60452604 31.88623776  2.91231876  1.99761124
  6.9396929   1.99340013  1.86349792  1.38550309  1.26623792  7.43235564
  1.78978438  5.99937873 62.35300804 21.0525866   8.88568192  6.61826805
 62.3541192   3.83717676  9.20028184  8.23256644  4.01713651  4.60125966
  3.43133686 10.21641567  2.47952541  7.04572748  0.3583359  18.78630129
  3.73965006 20.12393663  6.57617323  6.15348421  1.17525441  2.45660002
  0.16623751  0.82489424]
Accuracy th:0.5 is [45.65246525 97.2137279  71.72061744 97.02489005 97.26733349 76.59019749
 76.97640136 76.05535995 77.88282307 96.36578502 78.21785797 98.52097319
 99.41399349 79.76511007 77.79632314 96.56680596 96.29512311 77.50514735
 98.65376884 98.30776915 79.82602551 78.70883639 98.38695922 77.61966838
 80.53020797 96.65086926 94.0778012  77.25783068 98.01293844 78.04242151
 97.30875598 98.57457877 96.36213009 98.02024829 86.03574518 77.7756119
 77.53560507 89.05958748 97.11504489 74.85898076 79.01706851 92.05906361
 77.05071819 76.60238058 96.9627563  93.87434364 98.02877645 98.57336046
 89.67361509 86.42438567 86.24894921 98.55508583 98.99976852 77.16889414
 98.70615611 77.57337264 72.28834931 92.65359827 96.24273583 96.9067141
 89.79300934 97.17717864 91.33173329 77.54291493 98.42838172 78.03389335
 98.20786784 76.64258476 78.60528015 97.5195234  79.13646276 95.99054592
 78.35309024 95.45083515 76.92401408 81.69247451 85.68730888 78.16303408
 79.22296268 99.14718388]
Accuracy th:0.7 is [45.66708495 97.2137279  71.72061744 97.02489005 97.26733349 76.59019749
 76.97640136 76.27465552 77.88282307 96.46690464 78.21785797 98.52097319
 99.41399349 80.22563078 77.79632314 96.56680596 96.29512311 77.50514735
 98.65376884 98.30776915 80.32065886 78.70883639 98.38695922 77.78535837
 81.10037646 96.65086926 94.0778012  77.25783068 98.01293844 78.04242151
 97.30875598 98.57457877 96.36213009 98.02024829 86.2891534  77.7756119
 77.53560507 89.40071393 97.11504489 74.85898076 79.68957493 92.05906361
 77.05071819 76.60238058 96.9627563  93.87434364 98.02877645 98.57336046
 91.87022575 87.3064412  86.43656876 98.55508583 98.99976852 77.16889414
 98.70615611 77.57337264 72.28834931 93.06782325 96.24273583 96.9067141
 89.79300934 97.17717864 91.53397254 77.54291493 98.42838172 78.03389335
 98.20786784 76.64258476 78.60528015 97.55972759 79.13646276 95.99054592
 78.41278737 95.45083515 76.92401408 81.78506597 85.8164496  78.16303408
 79.22296268 99.14718388]
Avg Prec: is [55.94714545  3.12132517 11.23654159  3.37374812  2.22079968  3.69148994
  3.30679518  5.53672548  2.55676986  3.85548434  1.5661456   1.61070257
  0.65889652  5.22092352  2.77103953  3.16479227  3.74995056  2.66839703
  1.39227187  1.73627883  2.055082    0.84484125  1.80319761  2.40407372
  5.04279434  3.55493864  6.45980256  3.39786457  2.10468736  1.98962442
  2.61715318  1.37766633  3.65836787  1.65899793  2.27584384  2.32504314
  3.11514978  2.56735647  2.80268993  7.58771077  2.35851626  8.22738696
  3.36148596  4.23738428  3.21728638  6.43999149  2.07147341  1.5541393
  2.09808203  1.5543588   1.83279483  1.53890505  0.9759902   2.94687608
  1.34082707  2.67760127 11.25783251  3.66203245  3.97185676  2.77478591
 10.82563584  2.25477933  3.74664768  2.98749033  1.53098683  2.44345916
  1.76901849  4.20102324  1.26439467  2.41641591  0.19511823  3.44895147
  1.93011413  4.51948974  3.93926808  2.97178959  0.81012121  1.83637211
  0.13024664  0.72000495]
mAP score regular 10.89, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [89.18454294 97.22450607 91.97000274 96.96290206 97.90716795 96.61409672
 96.80843112 94.86508708 97.38894287 96.41727085 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 93.01143583
 97.07750953 96.48703192 97.03764606 93.29297157 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.75665844 96.39235618 96.16314124 96.78102499
 92.59286942 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [87.93631811 97.22450607 90.81645365 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.56297182
 97.07750953 96.48703192 97.03764606 94.08027506 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.88621471 96.39235618 96.16314124 96.78102499
 92.01734061 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [97.00920344  6.47321844 61.46936343 19.1290605   3.53102785 23.76433461
  8.55016077 33.76951214  2.90685464 29.82784868  1.70971739  1.15914381
  0.90554785 12.65843685  6.87210146  4.29120923  4.54849964  5.65438975
  0.77050698  1.39531025  1.28276421  0.4931085   2.01182821  3.47413929
 17.13515659 12.3746518  23.33036225  6.84109656  5.81789981  1.38344657
  1.86150582  0.81615561 10.19447682  1.27793428  1.3683144   3.38130233
  6.67592158  6.64310494  5.05980981 24.53755993  5.50903969 44.29228837
  4.7149426  15.84183166 17.18138955 32.25687372  3.05059263  2.0927345
  7.82261848  2.07830455  2.00787203  1.54516428  1.4080929   7.76663167
  1.77499424  5.90222856 57.43143188 23.00690431  8.64381151  7.41762296
 63.33540056  4.0329169   9.94910905  8.66978888  4.4670311   4.27993199
  3.79796341 10.52167757  2.22307471  7.38698082  0.3699764  21.29699033
  3.55242872 22.66304235  7.80069021  5.78807703  1.05777225  2.39697162
  0.18663424  0.77149123]
Accuracy th:0.5 is [45.49667389 97.22450607 70.16966888 96.96290206 97.90716795 75.62598102
 75.70819942 74.64185166 77.21055385 96.41477938 77.35505892 98.5325261
 99.34972718 77.7761168  77.28280639 96.31262924 96.21047911 76.70478611
 98.78167277 98.34068316 78.49365922 77.99038294 98.31327703 76.78451304
 77.9330792  96.52938685 94.3393876  76.76707278 97.81747515 77.30772106
 97.52597354 98.67204823 96.39983058 98.18870369 86.55106261 76.92901811
 76.91157785 90.93853552 97.0276802  74.08625458 77.31519546 92.37362035
 76.06198769 75.73062262 97.03764606 94.02795426 98.18621222 98.77668984
 90.55235817 86.36669407 84.61021003 98.55993223 98.87385704 76.04703889
 98.6969629  76.73468371 70.8124673  93.67665745 96.16314124 96.78102499
 90.13379176 97.04761193 91.18519072 76.75710691 98.32075143 77.61168
 98.13139996 75.94239729 78.02277201 97.51849914 78.44133842 96.07843137
 77.7910656  95.44559882 75.84523009 82.61952812 87.4504821  77.38495652
 78.51109948 99.15040985]
Accuracy th:0.7 is [45.8105987  97.22450607 70.16966888 96.96290206 97.90716795 75.62598102
 75.70819942 74.79881406 77.21055385 96.41976231 77.35505892 98.5325261
 99.34972718 78.18471734 77.28280639 96.31262924 96.21047911 76.70478611
 98.78167277 98.34068316 78.86239629 77.99038294 98.31327703 76.82935944
 78.33669681 96.52938685 94.3393876  76.76707278 97.81747515 77.30772106
 97.52597354 98.67204823 96.39983058 98.18870369 86.82263248 76.92901811
 76.91157785 91.13037845 97.0276802  74.08625458 77.81598027 92.37362035
 76.06198769 75.73062262 97.03764606 94.02795426 98.18621222 98.77668984
 92.62276702 86.82761542 84.78212123 98.55993223 98.87385704 76.04703889
 98.6969629  76.73468371 70.8124673  93.99556519 96.16314124 96.78102499
 90.13379176 97.04761193 91.35211899 76.75710691 98.32075143 77.61168
 98.13139996 75.94239729 78.02277201 97.53593941 78.44133842 96.07843137
 77.79854    95.44559882 75.84523009 82.67434038 87.58003837 77.38495652
 78.51109948 99.15040985]
Avg Prec: is [54.36750576  3.72747084 14.82848501  4.5449517   1.48645432  4.26444433
 12.7618155   8.64446553  7.81659076  5.20307131  2.29345156  4.98083103
  1.54900956  5.87988117  3.08024089  3.68409295 24.25681553  6.40922547
  1.56826416  2.6960092   3.57126155  1.46190068  1.16307398  5.55977288
  5.70838769 10.14239085  7.9886971   4.53553158  3.97305656  6.20618071
  2.25910378  0.85800793  3.0870849   1.10675436  1.68088313  2.36325297
  2.0008072   2.21275307  2.24505961  6.20611447  1.72938646  6.01940411
  2.20067759  2.73141042  2.37536796  4.86636963  1.72831531  1.04474632
  1.37195075  1.16711753  1.19331832  0.98811602  0.73307583  2.34454954
  0.85183873  1.83125072 10.03579044  2.86484296  3.87545633  2.71917679
  7.82959765  2.0410579   3.19693714  2.49916135  1.35087149  1.82490061
  1.53199673  3.428809    1.08209473  2.23385507  0.19383708  3.18995634
  1.57380938  3.910884    3.54390764  2.32112868  0.59792788  1.50421769
  0.12948737  0.59388403]
mAP score regular 11.23, mAP score EMA 4.32
Train_data_mAP: current_mAP = 10.89, highest_mAP = 11.64
Val_data_mAP: current_mAP = 11.23, highest_mAP = 12.27
tensor([0.3851, 0.3675, 0.3943, 0.4569, 0.4760, 0.4422, 0.4743, 0.4500, 0.4320,
        0.4608, 0.4481, 0.4532, 0.4343, 0.4498, 0.4678, 0.3668, 0.3812, 0.4740,
        0.4049, 0.4599, 0.4533, 0.4529, 0.4497, 0.4614, 0.4273, 0.4451, 0.4444,
        0.4579, 0.4545, 0.4124, 0.4015, 0.4090, 0.4567, 0.4394, 0.4131, 0.3889,
        0.4149, 0.3884, 0.4483, 0.3845, 0.4645, 0.4624, 0.4298, 0.4541, 0.4671,
        0.4648, 0.4659, 0.4317, 0.4503, 0.4650, 0.4674, 0.3728, 0.4633, 0.4703,
        0.4636, 0.4527, 0.4493, 0.4400, 0.3650, 0.4607, 0.4480, 0.4679, 0.3946,
        0.4571, 0.4587, 0.4414, 0.4438, 0.4604, 0.4471, 0.4604, 0.4659, 0.4442,
        0.4654, 0.4447, 0.4507, 0.3749, 0.4192, 0.3746, 0.4601, 0.4487],
       device='cuda:0')
Max Train Loss:  tensor([15.9989,  3.8139, 12.1009,  8.4083,  7.2579,  8.4918, 10.5806,  9.7258,
         7.7504,  8.5914,  8.0363,  7.5067,  6.1681,  7.7350, 10.2546,  8.0293,
         9.7417,  7.6212,  5.0605,  7.6045,  6.9803,  8.5384,  9.7904,  7.3652,
         6.7799,  7.2980,  8.3062,  8.4848,  6.9101,  6.2967,  5.3601,  7.0705,
        11.0038,  9.1143, 11.6163,  7.6235,  5.4189,  7.1985,  4.5516, 13.9800,
         8.1589, 12.7439, 10.7723,  9.0350,  8.0556, 16.1842,  8.2494,  9.6641,
         6.8937, 10.9309,  8.6786,  7.1797,  6.6475, 13.2273,  9.6678,  7.2941,
        12.7925,  9.1045,  8.3662, 11.1362, 14.9298,  8.4283, 12.2075, 12.0379,
        11.6149, 11.7298, 12.4926, 11.0428,  9.6151,  8.4106,  7.2576,  8.1553,
         9.8623,  8.7703, 10.3402,  6.1046,  6.6453,  5.6428,  5.4257,  8.5033],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [20/80], Step [000/642], LR 1.0e-04, Loss: 16.2
Max Train Loss:  tensor([14.4600,  8.1110, 15.8599,  7.9843, 15.0267,  9.0831, 15.4563, 18.5227,
        10.8712, 16.4304, 16.3834, 10.3996,  7.5409, 16.5223, 14.1782, 10.0139,
        13.1218, 10.0262,  7.6112,  9.0327, 15.8274, 18.1511,  8.8560, 16.6423,
        12.6718, 10.5029, 17.6578,  7.5113, 12.5344, 10.0435,  5.6072,  5.6881,
        18.1503, 10.8083,  7.6839, 10.5411, 10.9436,  8.2573, 16.3700, 14.1605,
        16.7875, 20.0101, 11.6177, 11.6351, 18.8934, 14.0961, 13.1622,  9.6227,
        15.2205,  7.9693,  9.3201,  7.2430, 16.9853, 14.1619, 19.4843,  9.3220,
        17.2706,  8.9403,  4.7646,  9.4557, 17.2445, 11.7468,  9.3324,  9.5449,
        12.8526, 12.6004,  9.3998, 12.0643,  5.3387,  8.8134, 12.3310, 13.3098,
        11.4013, 11.3985, 10.7700,  9.9510,  6.8140,  7.2943, 17.9740, 11.6211],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [20/80], Step [100/642], LR 1.0e-04, Loss: 20.0
Max Train Loss:  tensor([12.8443,  5.9037, 13.9761, 11.2059,  7.2986,  7.3199, 10.3052, 10.6015,
         8.2951, 12.2937,  6.6875,  9.1477,  9.0509, 10.2318, 12.4859,  6.7867,
        10.6422,  9.4029,  5.5334,  4.9741,  7.7576,  7.2851, 10.4333, 12.9804,
         9.7394,  8.4090, 12.8369,  9.8419, 10.0731,  7.2981,  7.9791,  8.7543,
         8.5177,  9.1695, 10.2660,  8.6026,  8.5002,  6.3192, 10.4199, 11.9912,
        11.0537, 13.6235,  9.7244, 13.5085, 12.7523, 11.6715, 11.2072, 10.0384,
        10.7880,  9.1402,  7.8366,  4.7497,  7.7464, 13.6885,  7.3772,  8.7834,
        11.6506,  8.5078,  4.5135,  5.9898, 14.5274,  8.7659,  7.1597,  9.1154,
         8.5798,  9.8055,  8.3217, 13.1345,  7.1161,  7.3573, 11.1486,  7.6509,
         9.4841,  8.9337, 12.3614,  7.2584,  5.3699,  4.5059,  6.6123,  8.5630],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [20/80], Step [200/642], LR 1.0e-04, Loss: 14.5
Max Train Loss:  tensor([16.9916,  9.4038, 15.9742, 10.0548,  5.1090,  7.3700, 10.5920, 10.8156,
         9.7362, 15.4310,  6.0513,  8.0895,  6.0720,  8.8300,  8.0572, 12.0241,
        11.5266,  9.5776,  6.0030,  6.5809,  9.8719,  6.3229,  7.3389,  4.6731,
         9.2962,  7.2928, 14.3237, 11.4574,  9.1540, 10.8764,  7.9770,  7.0705,
         7.7520, 11.4881,  8.0448,  8.0645,  8.6078,  6.8734,  7.8028,  8.8810,
        10.2283, 11.1625,  8.7887, 11.5297,  9.9346, 10.7117, 10.9017,  8.1300,
        10.6091,  8.0997,  6.3189,  4.3246,  5.9454,  9.2062,  8.1276,  9.4599,
        15.5065, 11.8577,  5.4675,  9.0646, 10.3840,  8.6560,  9.4119, 14.0520,
         9.2837,  7.0658,  9.6279, 13.4242,  6.6294,  7.9461, 10.9843,  9.1301,
        11.3200,  3.6149, 10.8078,  7.0824,  5.2073,  6.6123,  6.4341,  8.3885],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [20/80], Step [300/642], LR 1.0e-04, Loss: 17.0
Max Train Loss:  tensor([12.3253,  4.3072, 13.1531,  8.9251,  8.1034,  8.1962, 13.7109, 14.3235,
        11.1578, 12.0374,  6.0205,  8.1993,  6.2493,  9.8837, 11.1797,  5.6470,
        10.8503,  6.2600,  6.6360,  4.0203, 10.0130,  8.8156,  7.2553, 10.7695,
         8.6918,  6.6317, 14.9302,  6.6862,  8.5314,  9.6935, 11.8329,  8.2576,
         8.4112,  8.5906,  7.3550,  5.4397,  4.8910,  8.7097, 10.2784, 10.6160,
        10.7774, 12.9745, 12.1149, 13.6521, 11.9128, 10.2720,  7.5808,  7.5456,
        12.1247,  7.4114,  6.9508,  6.9295,  7.2207,  9.1484, 11.1817,  6.0041,
        11.6915,  7.7013,  7.7794,  6.1471, 12.1861,  6.4218,  7.1909,  6.4187,
         9.4052,  9.8378,  8.3125, 10.2187,  5.8868,  9.2553, 11.1720, 10.2546,
         9.5005,  7.5262,  9.1333,  5.4146,  6.3773,  4.4605,  6.6200,  9.3810],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [20/80], Step [400/642], LR 1.0e-04, Loss: 14.9
Max Train Loss:  tensor([14.3922,  9.7703, 10.9975, 11.9681, 11.2074,  9.7439,  9.9800, 11.2285,
         5.9703,  8.4887,  8.0789,  9.4524,  7.4735, 12.8266,  6.2643, 12.7254,
         5.8515,  8.9607,  7.0691,  4.0816,  7.1107,  7.8747,  7.2391,  6.3719,
        12.4878, 11.4172,  9.7049,  8.5529, 10.4336,  9.1120, 12.8639,  4.9860,
         7.8026,  7.6480,  6.1759,  7.5272,  4.7149,  8.9207,  6.4668, 13.9382,
         6.0557, 16.8618,  9.4872,  8.1307,  8.3010, 13.7289,  6.2675,  6.5645,
        11.2897,  8.5820,  7.0406,  6.5885,  8.1835,  7.3420,  8.4146,  9.5939,
        15.1942,  9.2335,  8.6740,  9.8475, 12.5948,  9.4414,  8.2794, 10.0796,
         9.2556,  8.6665,  8.9688,  9.3212,  6.9159,  8.3985, 11.2531, 11.2497,
         2.7357, 11.0565,  9.3726,  8.0871,  5.4585,  4.5897,  6.7105, 10.5529],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [20/80], Step [500/642], LR 1.0e-04, Loss: 16.9
Max Train Loss:  tensor([17.3776,  7.5081, 12.5743,  7.6429,  9.1165, 10.6342,  7.6608, 11.1795,
         4.7687, 10.2105,  4.7117,  7.4664,  7.7458, 11.8436,  7.6458,  8.2591,
         7.2773, 13.9131,  7.8456,  5.3367, 10.8722, 10.6271,  7.2589,  9.7207,
        12.6286,  7.1270, 12.5796, 14.1744,  7.2023,  7.7710, 10.0979,  8.6577,
         6.8411,  4.2403,  5.7616,  6.0871,  8.4916,  8.2513, 11.3588, 11.3596,
         7.8539, 12.8584,  7.9119,  7.2774,  8.6272,  9.6525,  9.8945,  6.6176,
         8.0199,  7.3858,  8.0376,  7.1373,  8.1647,  7.2854,  8.6456,  7.9870,
        15.5255,  8.8328,  8.8540,  7.5360, 11.0015,  6.6555,  7.7078, 10.1658,
         8.5895, 10.3786,  9.8471, 10.9435,  8.1296,  5.1381, 12.0148, 11.9846,
         3.5236,  6.4630, 14.1257,  5.8667,  7.2806,  4.7038,  6.7685, 10.5079],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [20/80], Step [600/642], LR 1.0e-04, Loss: 17.4
Max_Val Meta Model:  tensor([ 19.8728,  24.6913,  30.2639,  22.3952,   5.3487,   6.9808,   7.1348,
          9.2064,   4.8501,   9.3669,   5.6590,   8.4008,   7.0274,  10.4790,
          5.8129,   4.9816, 115.5687,   5.9238,   7.2436,   4.2153,   5.9297,
          6.6764,   6.6512,   4.8897,  12.7046,   9.5406,  12.6978,   6.4971,
          6.2520,   7.6032,   5.9120,   5.0440,   7.7770,   3.3067,   4.2047,
          4.4410,   4.8485,   8.0342,   5.8430,  18.4147,   8.8055,  10.4817,
          6.2278,   9.3210,   8.9238,  11.6907,   4.9459,   7.4518,   5.5256,
          7.1293,   4.8960,   3.9194,   7.8077,   5.6649,   7.5798,   4.2758,
         14.6541,   6.8169,   8.4818,   5.8303,  11.0908,  14.3099,   5.4019,
          5.5844,   7.8814,   5.5183,   7.6774,   4.9249,   7.6268,  12.1500,
         11.3846,   9.7342,   9.9202,   7.3299,   8.1233,   5.9689,   5.5227,
          4.5759,   6.7980,   8.7704], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 16.3742,  23.3272,  25.7017,  21.8071,   5.8143,   8.0099,   8.1858,
         10.8309,   5.3361,  11.8526,   6.3324,   9.0843,   7.7832,  12.0513,
          6.2489,   5.4632, 110.4700,   6.8560,   7.4047,   4.7992,   6.6366,
          7.4290,   6.8568,   5.4486,  13.2142,  10.6788,  13.8678,   7.7059,
          6.8308,   8.0732,   6.5288,   5.6896,   8.7260,   3.8463,   5.0365,
          5.0213,   5.9062,   8.1393,   6.8240,  18.2064,   9.3558,  10.9780,
          6.4001,   9.5572,   8.8997,  10.4818,   5.5262,   8.1399,   5.7803,
          7.9708,   5.4070,   4.4583,   8.3768,   5.4054,   8.3354,   4.6106,
         14.4056,   6.8081,   8.2010,   6.0434,   9.0021,  14.4394,   5.9675,
          5.9262,   8.6771,   6.1233,   8.4637,   5.7656,   8.3191,  12.2971,
         12.3489,   9.7650,  10.0876,   7.2267,   8.7562,   6.4764,   6.1980,
          5.0623,   7.5700,   9.6321], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 42.5183,  63.4761,  65.1825,  47.7314,  12.2139,  18.1144,  17.2593,
         24.0666,  12.3507,  25.7242,  14.1313,  20.0435,  17.9213,  26.7922,
         13.3583,  14.8955, 289.7582,  14.4628,  18.2866,  10.4363,  14.6394,
         16.4046,  15.2461,  11.8095,  30.9272,  23.9919,  31.2050,  16.8293,
         15.0283,  19.5770,  16.2591,  13.9111,  19.1062,   8.7530,  12.1910,
         12.9123,  14.2366,  20.9572,  15.2224,  47.3504,  20.1431,  23.7436,
         14.8910,  21.0462,  19.0548,  22.5520,  11.8614,  18.8544,  12.8364,
         17.1408,  11.5687,  11.9587,  18.0822,  11.4935,  17.9815,  10.1837,
         32.0627,  15.4747,  22.4702,  13.1190,  20.0953,  30.8577,  15.1243,
         12.9637,  18.9174,  13.8721,  19.0713,  12.5222,  18.6080,  26.7100,
         26.5077,  21.9824,  21.6770,  16.2507,  19.4285,  17.2772,  14.7864,
         13.5141,  16.4524,  21.4647], device='cuda:0')
Outer loop valEpocw Maximum [20/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 115.6
model_train val_loss valEpocw [20/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 110.5
model_train val_loss  valEpocw [20/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 289.8
Max_Val Meta Model:  tensor([46.1753, 45.4533, 34.0318, 44.8786, 41.1210, 39.9810, 41.9972, 36.7409,
        36.4119, 40.7484, 38.1337, 52.0847, 36.8644, 38.8809, 43.7066, 41.4902,
        40.2807, 39.3846, 34.0219, 40.1090, 38.6579, 38.6089, 37.6412, 41.9810,
        36.2870, 36.7020, 37.6462, 38.1225, 38.7289, 34.5545, 34.0451, 34.4633,
        38.5113, 34.0684, 35.0138, 32.7270, 35.0398, 32.9654, 37.8995, 43.9542,
        38.9835, 38.3115, 36.0016, 38.8894, 38.4391, 38.6898, 37.5806, 36.1612,
        37.9308, 38.6696, 39.2570, 31.6848, 38.6626, 39.4318, 38.5430, 37.3608,
        38.0152, 37.4987, 41.7473, 37.7408, 37.8991, 44.2606, 47.5599, 38.3321,
        38.2156, 37.0390, 37.4317, 37.5609, 37.7500, 39.3433, 39.6329, 45.0706,
        40.5454, 37.7680, 38.4424, 31.2569, 34.8790, 31.4730, 39.3265, 38.4442],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([21.5163,  6.7557, 26.7931,  4.9862,  6.2554, 15.2868, 39.0715, 18.7446,
        15.4018, 20.0702,  8.5001, 87.1983,  7.0078,  8.5974,  7.4593,  4.2809,
         5.4592,  8.7558,  6.2141, 10.0392,  4.7298,  5.3487,  5.8356,  5.1353,
        11.3311,  9.4178, 14.0484,  7.4384,  6.4670,  4.2533,  4.5092,  3.8806,
         4.2274,  2.6308,  3.4142,  3.4469,  5.6058,  5.7489,  4.4184,  2.4592,
         4.8742,  2.2003,  2.9997,  2.8886,  3.0930,  2.3806,  3.5613,  5.2737,
         3.5422,  5.8721,  3.6020,  3.0136,  4.9933,  3.0423,  6.0168,  3.6487,
         6.9235,  3.1708,  2.9002,  3.1759,  2.6660,  3.1624,  3.7522,  2.8409,
         6.3688,  3.8765,  6.1395,  6.4122,  3.9340,  3.8406,  9.6407,  2.4651,
         3.1004,  2.2894,  8.0788,  2.8763,  4.3254,  2.9440,  5.4095,  7.1329],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 55.5558,  17.8767,  68.4217,  11.2901,  12.9237,  33.9070,  80.9934,
         42.6448,  35.7737,  42.9322,  19.0479, 192.5389,  16.3166,  19.0841,
         15.4909,  11.4385,  14.1155,  18.5390,  15.4390,  21.6568,  10.4776,
         11.8530,  12.9935,  10.9282,  26.6639,  21.4561,  31.7391,  16.3230,
         14.2523,  10.3935,  11.3503,   9.6630,   9.3303,   6.2352,   8.2692,
          8.9152,  13.6053,  14.8649,   9.9291,   6.2972,  10.4799,   4.7831,
          7.0231,   6.3573,   6.6744,   5.1199,   7.7696,  12.3009,   7.9167,
         12.7152,   7.7697,   8.1416,  10.8973,   6.5111,  13.0470,   8.1221,
         15.4437,   7.2373,   7.6709,   6.8936,   6.0396,   6.5231,   9.2783,
          6.2843,  13.9498,   8.8454,  13.8879,  14.1661,   8.8372,   8.3173,
         20.5762,   5.6346,   6.7136,   5.1712,  17.8470,   7.7876,  10.4010,
          7.9472,  11.7790,  15.9334], device='cuda:0')
Outer loop valEpocw Maximum [20/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 52.1
model_train val_loss valEpocw [20/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 87.2
model_train val_loss  valEpocw [20/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 192.5
Max_Val Meta Model:  tensor([32.9115, 41.6056, 35.8651, 42.3284, 41.5561, 41.1061, 42.5769, 39.0051,
        37.7761, 40.0925, 39.7093, 40.0904, 37.8914, 37.6194, 39.8290, 40.3824,
        38.4105, 42.3279, 35.1045, 40.8648, 42.7581, 41.8057, 41.7440, 42.2796,
        34.9695, 39.9577, 38.2027, 41.4592, 39.5748, 32.0740, 35.3923, 34.1975,
        38.7752, 36.0408, 35.0178, 33.5592, 36.2872, 34.1342, 37.6153, 42.4871,
        38.8040, 40.7318, 37.0765, 40.0710, 39.8718, 51.6002, 40.3385, 36.8121,
        39.2030, 45.8358, 41.3284, 32.9195, 39.8739, 40.4693, 40.0745, 39.3234,
        41.6047, 38.5752, 40.6261, 38.1206, 48.4551, 41.0552, 42.3473, 38.7450,
        39.2080, 38.0234, 38.1184, 37.1784, 37.8931, 39.9466, 38.7364, 42.4647,
        41.5385, 39.4354, 40.3891, 32.9961, 35.4486, 32.2958, 39.3198, 38.2480],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 7.2284,  4.9135,  5.0896,  3.7188,  3.5735,  5.2045,  3.5122,  6.7958,
         4.4491,  4.8375,  5.6319,  8.3109,  7.9440,  7.6590,  2.5744,  5.4150,
         5.3513,  4.6944,  5.8281,  4.4305,  7.2986,  8.1853,  5.4508,  4.4756,
         5.6371,  6.3399, 12.8498, 10.7150,  4.8649,  6.5680,  5.8665,  7.1475,
         6.3468,  3.4249,  4.8239,  5.7129,  7.9546,  6.9208,  6.9728, 26.5204,
        14.9155, 30.6939, 34.0581, 33.4218, 24.9773, 38.4895,  8.9290, 10.2950,
        19.5871,  9.5282, 17.3022, 17.4989, 26.1743, 14.2946, 27.9448, 43.4384,
        37.1827, 10.9865,  4.2202,  9.2499, 30.6449,  6.0415,  8.2164, 10.4244,
        11.7329,  7.6798, 10.6855,  8.9382,  7.1481,  4.7793, 12.8031,  7.9523,
         4.5404, 17.6849,  5.3908,  7.8853,  8.6033,  4.9687,  8.1159, 10.5682],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([19.2536, 12.6810, 12.3635,  8.3821,  7.3494, 11.3128,  7.2452, 14.7341,
        10.0334, 10.4025, 12.3648, 17.9311, 18.1155, 16.9126,  5.5250, 14.2273,
        13.7935,  9.7923, 14.1303,  9.3469, 15.5015, 17.4075, 11.7765,  9.3772,
        13.4806, 13.8792, 28.7319, 22.8916, 10.5525, 16.4656, 14.1729, 17.5918,
        13.8254,  7.7133, 11.7598, 14.5125, 18.7893, 17.4064, 15.4448, 67.1367,
        32.6351, 65.0894, 79.4870, 72.6664, 53.4234, 78.8342, 18.6168, 23.7856,
        43.2693, 20.6476, 36.3784, 46.0169, 56.0726, 30.1697, 59.6241, 95.5942,
        79.8968, 24.5708, 10.7829, 19.9360, 67.4308, 12.7307, 20.5983, 22.3046,
        25.2845, 17.1921, 23.9311, 19.4759, 16.1169, 10.2618, 28.1644, 18.3630,
         9.6637, 38.6780, 11.4102, 20.3978, 20.5282, 13.1640, 17.8021, 23.3894],
       device='cuda:0')
Outer loop valEpocw Maximum [20/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 51.6
model_train val_loss valEpocw [20/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 43.4
model_train val_loss  valEpocw [20/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 95.6
Max_Val Meta Model:  tensor([41.8909, 39.4596, 34.1216, 41.6876, 41.0481, 40.1297, 40.9633, 37.7750,
        35.9638, 39.6490, 38.9220, 39.5755, 36.9356, 35.5618, 39.1853, 40.0031,
        38.6770, 39.6886, 34.1536, 39.4004, 40.3358, 40.6110, 40.2249, 39.7070,
        34.2600, 38.1778, 37.5078, 39.9710, 38.7127, 30.8372, 30.5511, 32.5594,
        41.3915, 30.3416, 34.7760, 32.8453, 33.1819, 35.0569, 32.2295, 38.5547,
        36.4143, 36.5318, 36.0047, 38.7406, 38.4869, 38.4529, 39.0766, 36.2415,
        38.9542, 36.0718, 40.6542, 31.8018, 37.4030, 40.6485, 38.5849, 37.0955,
        37.4251, 35.3582, 39.4816, 41.7618, 37.5138, 39.6168, 41.4023, 36.5779,
        37.2783, 37.8286, 35.9124, 37.4453, 37.5100, 39.0952, 39.6264, 40.9703,
        40.1524, 37.7764, 49.4978, 31.4110, 34.8614, 31.5391, 39.0853, 38.4782],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 12.4219,   3.1009,  18.3408,   2.8598,   6.9779,   7.5716,   7.7801,
         10.8952,   3.2638,  13.7118,   5.2022,   6.8252,   5.5230,   7.8777,
          8.3175,   3.5606,   4.3631,   5.6587,   7.6415,   3.9365,   5.3439,
          5.9785,   8.4065,   5.0053,   8.7177,   4.8281,   9.8180,   3.7578,
          6.6504,   4.3652,   4.0868,   4.1321,   3.2799,   2.6790,   3.2906,
          3.8007,   4.3851,   5.7986,   4.0810,   3.8294,   5.0986,   2.5304,
          3.4891,   3.2749,   4.2071,   3.2900,   3.9815,   5.7473,   4.2865,
          5.2910,   4.1539,   3.3136,   5.3991,   4.1644,   6.6355,   3.0744,
          3.5315,   3.0094,   4.8334,   4.1627,   4.9057,   3.9601,   3.9834,
          3.4373,   6.8494,   4.2717,   6.6023,   3.1832,   4.3177,   3.1291,
         10.3083,   3.3086,   1.7566,   3.4018, 112.4164,   3.9659,   4.7169,
          3.9796,   5.8788,   7.7596], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 32.0378,   8.3008,  46.5255,   6.5488,  14.4390,  16.6604,  16.2231,
         24.1180,   7.5736,  29.5564,  11.5042,  14.8847,  12.7771,  18.0291,
         17.9700,   9.4957,  11.2488,  11.9219,  18.8335,   8.4991,  11.6183,
         12.9631,  18.3627,  10.8410,  21.1161,  10.7990,  22.1609,   8.1134,
         14.5637,  11.2882,  10.7546,  10.5066,   7.0380,   6.5753,   7.9749,
          9.7413,  10.9371,  14.4609,   9.6846,  10.0196,  11.3578,   5.5515,
          8.1283,   7.2084,   9.0422,   7.0406,   8.4842,  13.3275,   9.4428,
         11.6744,   8.7978,   8.8656,  11.8836,   8.7879,  14.2974,   6.8407,
          7.8726,   6.9258,  12.7708,   8.7470,  11.0824,   8.4625,  10.0908,
          7.6641,  15.1543,   9.6126,  15.0757,   6.9751,   9.7063,   6.7623,
         21.9869,   7.8219,   3.7956,   7.6370, 245.8646,  10.6261,  11.2966,
         10.6616,  12.8281,  17.1810], device='cuda:0')
Outer loop valEpocw Maximum [20/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 49.5
model_train val_loss valEpocw [20/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 112.4
model_train val_loss  valEpocw [20/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 245.9
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [88.32129238 97.2137279  89.98062889 97.02489005 97.26733349 96.5997003
 96.99808726 94.71132174 97.44398826 96.48517927 98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.10460399 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.3633484  98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.34049293 97.84237521 92.67674614
 96.90915072 96.22689782 96.9627563  93.31392161 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 90.62145929 96.13796128 96.24273583 96.9067141
 91.42554306 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.18402554 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [86.97749784 97.2137279  91.58026827 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.28445073
 96.90915072 96.22689782 96.9627563  93.9523154  98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.59732459 96.13796128 96.24273583 96.9067141
 91.82027509 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.43987037 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [96.77296651  4.61064658 57.89008782  9.95305781  7.14903261 12.03677351
  7.39009667 31.74626645  2.86779445 28.93583259  1.68009475  1.15001881
  0.8182006  11.23483441  4.21176575  3.63942446  4.16243191  3.83940501
  1.99208668  1.35400156  1.31752164  0.46793867 12.53097346  1.66505138
 18.39723782  9.35858712 26.93935253 12.92293967  4.43894277  1.32114415
  7.84500019  0.93894961 30.31712105  1.67687079  2.45881156  1.54470807
  6.05102231 38.95066556  3.55974015 24.45102497  5.84870083 44.46663407
 12.47244336 23.17721624 23.18100839 33.32598652  2.40333321  1.94911231
  5.41385428  1.94635165  1.37836784  1.29891277  1.17531734 11.81312603
  1.70626361  7.43409875 61.30954861 26.84993302 10.56152004  8.1929182
 60.51943449  3.41123794 12.77665796 11.63608637  4.42856687  6.59596479
  4.73934214  9.67463226  4.13326684 10.97380178  0.73835202 19.30092999
  8.84870056 22.74536868 12.36639174  7.34222066  1.27442812  2.2133351
  0.24758295  0.96456813]
Accuracy th:0.5 is [45.63053569 97.2137279  71.7571667  97.02489005 97.26733349 76.50247926
 76.87162681 75.97373326 77.71469646 96.38405965 78.02292857 98.52097319
 99.41399349 79.85039169 77.60383036 96.56680596 96.29512311 77.43448545
 98.65376884 98.30776915 79.77851147 78.54558302 98.38695922 77.44301361
 80.6459473  96.65086926 94.0778012  77.11163363 98.01293844 77.93277372
 97.30875598 98.57457877 96.36213009 98.02024829 85.93949879 77.60017544
 77.34311229 88.84394683 97.11504489 74.61044578 78.76366029 92.05906361
 76.75832409 76.40501456 96.9627563  93.87434364 98.02877645 98.57336046
 89.70772773 86.36834347 86.275752   98.55508583 98.99976852 76.97883798
 98.70615611 77.31752781 72.07636359 92.52811247 96.24273583 96.9067141
 89.79300934 97.17717864 91.30980373 77.33823906 98.42838172 77.83652733
 98.20786784 76.564613   78.48101266 97.50368538 78.99026571 95.99054592
 78.16181577 95.45083515 76.69253542 81.55724224 85.66537932 77.985161
 79.06214593 99.14718388]
Accuracy th:0.7 is [45.67195819 97.2137279  71.7571667  97.02489005 97.26733349 76.50247926
 76.87162681 76.20521192 77.71469646 96.46934126 78.02292857 98.52097319
 99.41399349 80.34867996 77.60383036 96.56680596 96.29512311 77.43448545
 98.65376884 98.30776915 80.25974342 78.54558302 98.38695922 77.61723176
 81.25144674 96.65086926 94.0778012  77.11163363 98.01293844 77.93277372
 97.30875598 98.57457877 96.36213009 98.02024829 86.11006201 77.60017544
 77.34311229 89.17532681 97.11504489 74.61044578 79.49464553 92.05906361
 76.75832409 76.40501456 96.9627563  93.87434364 98.02877645 98.57336046
 92.00058479 87.31984259 86.45484339 98.55508583 98.99976852 76.97883798
 98.70615611 77.31752781 72.07636359 92.93746421 96.24273583 96.9067141
 89.79300934 97.17717864 91.50960636 77.33823906 98.42838172 77.83652733
 98.20786784 76.564613   78.48101266 97.55972759 78.99026571 95.99054592
 78.23491429 95.45083515 76.69253542 81.64130554 85.8152313  77.985161
 79.06214593 99.14718388]
Avg Prec: is [56.02848858  3.10430913 11.15006018  3.38922056  2.19136419  3.80844338
  3.25532419  5.52807408  2.49539333  3.79249702  1.61068208  1.58710583
  0.59916402  5.06292885  2.67547644  3.17956971  3.7132538   2.64232442
  1.32285493  1.77142845  1.98226674  0.86294824  1.79569263  2.38215871
  4.9812722   3.64075703  6.43658774  3.31902094  2.03891697  1.95083097
  2.59508462  1.36795925  3.74549104  1.65527288  2.52838909  2.52037748
  3.03146297  2.56994062  2.7389803   7.30467082  2.30684735  8.21763307
  3.40220425  4.05897919  3.2403724   6.36082771  2.10909959  1.58780901
  2.0550302   1.63726243  1.83737278  1.58296558  1.02923809  3.01913092
  1.32561347  2.67782067 11.31001834  3.77100354  4.00174507  2.84057666
 10.75470463  2.14303551  3.84979867  3.1068455   1.54817121  2.53377081
  1.76203782  4.23901907  1.29148667  2.46585331  0.19320693  3.40298783
  1.90956056  4.61890866  3.89801604  3.04111753  0.87761493  1.9709287
  0.15417398  0.76056443]
mAP score regular 12.39, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [88.9628024  97.22450607 89.71024242 96.96290206 97.90716795 96.63651992
 96.80843112 94.88252734 97.38894287 96.44218551 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.36430226 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39484765 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  91.93013927 97.82744101 93.0438249
 97.07750953 96.48703192 97.03764606 93.02389317 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 89.77751202 96.39235618 96.16314124 96.78102499
 91.42935446 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.09928495 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [88.71863866 97.22450607 92.19423475 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.68754516
 97.07750953 96.48703192 97.03764606 94.16498493 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.01078805 96.39235618 96.16314124 96.78102499
 92.28641901 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.46054762 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [97.02568248  5.35008959 63.00512622 11.79685125  7.29269574 13.37103761
  8.1711024  32.63811733  3.40346461 32.66475999  1.64055053  1.07726294
  0.75320414 12.1601541   4.83823007  3.91240222  4.3723318   4.10497171
  2.12938646  1.34693294  1.20387831  0.47573111 16.87893716  1.53106178
 18.75195911 10.13673495 26.64509455 13.03035377  5.46599298  1.3594071
  8.88148455  0.87694596 34.72499132  1.73400978  2.130449    1.35783674
  5.74505406 44.23292347  3.74955066 24.78947915  6.22015874 44.92856149
 14.17988307 22.92551784 22.83816123 33.95868738  2.54309014  1.97738938
  6.37994389  1.86033019  1.46439764  1.49467959  1.40651242 12.30061966
  1.89904301  8.03445798 56.67933239 27.13507562  9.59492652  8.85770902
 60.19133425  3.30745888 12.76799338 11.95677127  5.15855158  6.25129595
  5.54589402 10.73500704  3.84618448 12.18181727  0.98750706 21.45660363
  9.17491754 24.46857256 16.87502497  6.58859568  1.37306072  2.25106498
  0.24537425  0.90318645]
Accuracy th:0.5 is [45.48421656 97.22450607 70.04758701 96.96290206 97.90716795 75.50389915
 75.58611755 74.53970152 77.08348905 96.41477938 77.22301119 98.5325261
 99.34972718 77.68393253 77.15075865 96.31262924 96.21047911 76.58270424
 98.78167277 98.34068316 78.44133842 77.86830107 98.31327703 76.65246531
 77.8882328  96.52938685 94.3393876  76.64997384 97.81747515 77.17567332
 97.52597354 98.67204823 96.39983058 98.18870369 86.51369061 76.80693624
 76.77953011 90.86130005 97.0276802  73.96417271 77.22799412 92.37362035
 75.93990582 75.60355781 97.03764606 94.02795426 98.18621222 98.77668984
 90.4825971  86.35672821 84.59526123 98.55993223 98.87385704 75.91997409
 98.6969629  76.61260184 70.72028303 93.64925131 96.16314124 96.78102499
 90.13379176 97.04761193 91.24747739 76.64997384 98.32075143 77.48959813
 98.13139996 75.83028129 77.90069014 97.51849914 78.30929068 96.07843137
 77.65901786 95.44559882 75.72813115 82.50492065 87.47788823 77.25290879
 78.38403468 99.15040985]
Accuracy th:0.7 is [45.77571817 97.22450607 70.04758701 96.96290206 97.90716795 75.50389915
 75.58611755 74.68171513 77.08348905 96.41976231 77.22301119 98.5325261
 99.34972718 78.09502454 77.15075865 96.31262924 96.21047911 76.58270424
 98.78167277 98.34068316 78.77270349 77.86830107 98.31327703 76.70727757
 78.27191868 96.52938685 94.3393876  76.64997384 97.81747515 77.17567332
 97.52597354 98.67204823 96.39983058 98.18870369 86.78775195 76.80693624
 76.77953011 91.08553205 97.0276802  73.96417271 77.72877893 92.37362035
 75.93990582 75.60355781 97.03764606 94.02795426 98.18621222 98.77668984
 92.63273289 86.80519222 84.75222363 98.55993223 98.87385704 75.91997409
 98.6969629  76.61260184 70.72028303 93.95819319 96.16314124 96.78102499
 90.13379176 97.04761193 91.45925206 76.64997384 98.32075143 77.48959813
 98.13139996 75.83028129 77.90069014 97.53593941 78.30929068 96.07843137
 77.66898373 95.44559882 75.72813115 82.58963052 87.6448165  77.25290879
 78.38403468 99.15040985]
Avg Prec: is [54.18807485  3.7214956  14.77516252  4.55569346  1.50966239  4.27010938
 12.57449449  8.64708821  7.78030375  5.20325713  2.29946063  5.00483897
  1.54455499  5.89489959  3.07166955  3.63006055 24.37702077  6.41260101
  1.55099037  2.66973422  3.58066242  1.454982    1.16040631  5.59607598
  5.72701641 10.31104199  8.04218003  4.56944948  3.93711152  6.27840571
  2.27080797  0.86352213  2.97733116  1.15343857  1.72486187  2.40265615
  2.02731389  2.1401817   2.19687332  6.20851036  1.73056683  6.03086949
  2.18716037  2.7191025   2.34743018  4.85004257  1.72268457  1.05032138
  1.39452323  1.15790338  1.21074099  0.99348398  0.74591305  2.28291234
  0.90516996  1.87013167 10.178948    2.99498204  3.98220171  2.80032932
  7.90589698  2.05743789  3.29348953  2.50493446  1.34970493  1.89909777
  1.54049367  3.51840532  1.0657309   2.20961586  0.1964275   3.20070554
  1.5509482   3.99803885  3.1920428   2.3000236   0.57309072  1.45252972
  0.12093914  0.61007717]
mAP score regular 12.97, mAP score EMA 4.32
Train_data_mAP: current_mAP = 12.39, highest_mAP = 12.39
Val_data_mAP: current_mAP = 12.97, highest_mAP = 12.97
tensor([0.3798, 0.3711, 0.3924, 0.4328, 0.4841, 0.4505, 0.4785, 0.4504, 0.4288,
        0.4622, 0.4521, 0.4575, 0.4323, 0.4361, 0.4638, 0.3734, 0.3867, 0.4755,
        0.4022, 0.4626, 0.4591, 0.4632, 0.4579, 0.4621, 0.4118, 0.4463, 0.4450,
        0.4618, 0.4546, 0.3853, 0.3792, 0.3956, 0.4657, 0.4045, 0.4132, 0.3879,
        0.4002, 0.3992, 0.4199, 0.3799, 0.4385, 0.4533, 0.4290, 0.4552, 0.4670,
        0.4638, 0.4678, 0.4316, 0.4534, 0.4530, 0.4727, 0.3730, 0.4549, 0.4720,
        0.4633, 0.4540, 0.4437, 0.4313, 0.3773, 0.4719, 0.4451, 0.4658, 0.3931,
        0.4473, 0.4461, 0.4432, 0.4359, 0.4605, 0.4339, 0.4624, 0.4671, 0.4211,
        0.4599, 0.4441, 0.4610, 0.3732, 0.4190, 0.3734, 0.4605, 0.4487],
       device='cuda:0')
Max Train Loss:  tensor([ 9.7385,  7.7650, 11.8701,  3.9227,  9.1811,  7.4418,  9.9048,  8.1245,
         7.6252,  7.4747,  4.7031,  8.5955,  6.3252, 12.1727, 10.7797,  8.0218,
         7.3337,  9.1721, 10.9995,  8.6560,  7.0729,  8.1497,  7.4009,  6.9929,
         7.7187,  7.9175, 10.8327,  9.2616,  7.8765,  6.0293,  8.7654,  8.7325,
        13.7643,  4.2536, 10.5863, 11.1721,  6.5560,  8.5188, 13.4683, 10.4890,
         8.8729, 10.0032,  7.8323,  9.1771,  9.0464, 11.0388,  5.7078,  7.1837,
         8.2725, 11.5604,  6.7623,  4.7050,  6.1051,  7.3488, 10.0666,  6.2325,
        12.1223,  9.7012, 10.9130,  9.3148, 11.0098,  9.6947,  6.7745,  7.3148,
         8.5266, 10.3003,  8.4001,  7.7410,  4.8802,  6.2232, 11.8314,  7.6891,
         3.1911, 11.0990, 11.0965, 11.1569,  6.6065,  6.3010,  7.5207,  9.2585],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [21/80], Step [000/642], LR 1.0e-04, Loss: 13.8
Max Train Loss:  tensor([15.2098,  4.3018, 17.5054, 17.2292, 14.5275,  8.3836, 18.3524, 17.5073,
         9.8056, 12.6979, 19.5371, 11.7126, 17.1878, 11.1898, 13.6476,  8.4966,
         7.9484, 12.3221, 11.1392, 12.9215, 16.9559, 10.7510,  9.3982,  8.2071,
        13.9408, 11.0599, 11.6147, 12.6073, 11.1529,  4.8634,  7.5659,  8.9523,
         4.4345,  8.5707,  7.8188,  6.8314,  6.5278,  8.2398, 10.3434, 12.9708,
         7.8677, 15.9813,  9.1642, 13.7056,  9.6584, 15.1906, 16.0625, 12.3381,
         7.6771, 18.1676,  9.8231,  6.3558, 13.5342, 15.9210,  7.9629, 12.0322,
        12.3325, 10.4275, 16.4704,  9.3650, 14.2897, 15.1089,  9.1557,  7.9542,
         7.0521, 11.7339,  8.1130, 14.2763,  9.4267, 15.9845,  5.6449,  9.7325,
        15.0936, 15.4632, 17.6555, 10.1343,  9.0734,  6.6424, 11.2987,  7.0274],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [21/80], Step [100/642], LR 1.0e-04, Loss: 19.5
Max Train Loss:  tensor([12.2086,  3.5492, 11.6368, 10.7808,  7.0597,  9.7300, 10.0214, 11.9500,
         8.3827,  8.8775,  5.5112, 10.8475,  7.5741,  8.0297, 11.8543, 11.2227,
         7.2667,  8.1059,  6.6595,  8.7313,  7.0192,  9.7248, 10.4529,  6.5150,
        13.5355,  8.2430, 14.3875,  9.6748, 11.0316,  8.2924, 11.6002,  8.4023,
         3.9188, 10.9880,  8.4449,  6.5083,  7.5370, 12.8488,  7.4449, 11.7648,
         9.1773, 11.9040,  8.2562, 14.1137,  9.2173,  9.6576,  8.9327,  5.7019,
         7.9977,  6.8947, 11.1090,  5.4210,  6.3130,  8.9772,  4.5612,  9.7251,
        12.2702,  5.3390,  8.2233, 10.4452,  8.9544,  9.3842,  7.9242,  9.8582,
         8.3010,  6.0136,  8.3206,  9.0652,  9.3821, 12.4653,  5.5306,  7.3796,
        10.7222,  8.6637, 13.7559,  7.8347,  7.2222,  7.1508, 10.5447,  4.2104],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [21/80], Step [200/642], LR 1.0e-04, Loss: 14.4
Max Train Loss:  tensor([14.4598,  5.7497, 10.5969,  8.5709, 13.7402,  7.4130,  7.7798,  9.4561,
         8.3491,  6.6471,  3.2260,  9.3516,  5.9937, 10.7186, 11.0386,  8.0606,
         7.4981,  7.3276,  5.9048,  8.4754,  4.5142, 11.4783,  7.5009,  9.6402,
        11.1213,  6.8606, 12.2331, 12.4373,  6.6540,  7.8891, 10.3892,  9.0684,
         4.7037,  8.0759,  8.5864,  8.5880,  6.8800,  8.4085,  9.3004, 11.0889,
         8.3025,  9.0400,  7.7260,  7.2582,  8.5341,  9.2146, 13.0799,  7.9097,
         7.1030, 10.3291,  8.5235,  6.0045,  6.6667,  7.1875,  7.6558,  8.8754,
        13.3783, 12.5434,  9.1495, 10.4586, 11.6058, 11.3258,  8.0031, 12.3215,
         7.4469,  7.6117,  8.2790,  9.1601,  7.6581,  9.7391,  4.3081,  9.7391,
        10.6280, 14.3984,  6.6923,  7.5888,  7.5718,  5.2245, 10.5119,  6.2899],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [21/80], Step [300/642], LR 1.0e-04, Loss: 14.5
Max Train Loss:  tensor([10.0498,  6.4367, 12.6828,  8.9003,  7.2886,  8.8302,  5.3736, 11.3922,
        10.5401,  7.7749,  4.5176, 10.1259,  5.9827, 10.7635,  9.3106,  5.4452,
         6.2344,  6.9290,  4.4757,  9.9746,  4.7822, 13.4725,  7.2723, 11.7274,
         7.8056,  5.7308, 15.1367,  8.2914,  6.5164,  5.8733, 10.4795,  8.7484,
         4.7644,  7.7204,  6.3557,  7.4300, 10.9465,  7.5171,  8.3545, 10.9511,
         6.6892,  9.9602,  9.2071, 10.3356, 11.2862, 10.5599,  9.9376,  9.9767,
         8.0705,  9.8810, 10.5815,  8.9966,  4.6076,  6.9499,  5.4126,  9.3952,
        10.3635,  9.2485,  8.5843,  7.7213, 11.7427, 11.1277,  9.6234, 11.1061,
         8.3825,  8.4419,  9.3186, 10.9749,  8.0926,  8.8701,  5.0826,  9.3018,
         8.7249, 11.2183,  9.7101,  6.6508,  6.4845,  6.2325, 10.5136,  4.2014],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [21/80], Step [400/642], LR 1.0e-04, Loss: 15.1
Max Train Loss:  tensor([15.1853,  6.0536, 12.3013,  6.8867,  7.6795,  8.8651,  5.6125,  8.7163,
         6.6585,  7.4839,  5.8157,  8.1298,  7.9905, 11.8337,  9.6131,  6.5140,
         8.4153, 11.6183,  8.0562,  8.0918,  5.8004,  2.9922,  6.5640,  5.7572,
         7.4725,  6.9764, 10.8913, 12.9220,  6.3904,  7.1984,  7.4579,  5.3974,
        10.7407,  8.0072,  6.9763,  7.6051,  7.7352,  9.0074,  9.8282,  9.3087,
         6.1126,  9.1087,  8.0146, 11.6978, 10.9451, 13.0849,  8.2544,  5.1892,
         8.0206,  6.7312, 10.6996,  9.0354,  8.0726,  5.6110,  8.2508, 11.0464,
        12.9541,  7.5364, 11.4163,  8.1713, 12.4137, 10.6789,  7.9464,  8.7745,
         7.2570,  9.8078,  8.3453,  7.3446,  5.9650,  9.8860,  4.3911, 13.7303,
        10.4919,  8.9011, 12.4811,  8.5204,  6.5406,  5.3802, 10.6516,  4.3016],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [21/80], Step [500/642], LR 1.0e-04, Loss: 15.2
Max Train Loss:  tensor([14.8622,  3.4768, 11.5719,  8.3917, 10.6992,  7.3893,  6.8641,  9.8488,
        13.2847,  7.3490,  8.9665, 12.9178,  7.0325,  7.2646, 10.9731,  9.4112,
         5.2687, 10.5254,  7.4582,  8.0798,  7.4423,  2.9390,  6.5772,  8.0022,
         9.9801,  8.4088,  7.3061,  9.7788,  6.6701,  5.0831,  9.0161,  8.0392,
         6.0365,  9.3399,  9.3155,  8.9452,  6.6344,  7.1076,  8.7058, 10.0827,
         8.5404, 11.4039,  6.5703, 10.1098, 10.2733,  8.8775,  6.9074,  7.4057,
         8.6980,  7.4953,  6.1830,  4.8232,  4.8208,  7.8558,  4.7734,  9.6250,
        14.0123,  7.4170,  7.9994,  7.3682,  7.8698,  6.2639,  7.2111,  7.4658,
         6.7814,  7.0531,  7.8154, 10.4238,  7.7230, 11.5699,  4.5152,  9.8867,
         9.3059,  6.5799, 12.0229,  8.6263,  5.9917,  5.3685, 10.7743,  4.4222],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [21/80], Step [600/642], LR 1.0e-04, Loss: 14.9
Max_Val Meta Model:  tensor([ 20.0067,  27.8648,  35.6614,  19.9947,   6.7302,   8.0685,   7.6230,
          9.8864,   6.6083,   7.9008,   4.5648,   9.5654,   6.9635,   8.4957,
          6.5796,   4.5697, 115.6025,   2.9116,   4.7260,   4.6512,   3.6460,
          3.0237,   5.3882,   4.7914,  13.5281,   9.2101,  12.6813,   5.6685,
          7.8397,   6.4405,   5.6014,   5.5665,   4.5369,   4.9141,   5.3804,
          4.3298,   4.6009,   6.3442,   6.2492,  18.2255,   8.8366,  11.4321,
          5.4805,   9.6801,   8.0096,  10.4630,   4.1988,   5.5497,   6.3831,
          6.9039,   5.5121,   3.9539,   6.4244,   5.8280,   4.8268,   8.1995,
         11.6691,   7.3183,  10.0393,   5.4946,   8.7749,  14.3084,   5.8483,
          6.8049,   5.9815,   6.3034,   7.1276,   5.9569,   8.6215,  12.0700,
          4.5536,   9.9049,  12.7557,   7.3708,   7.4814,   6.1573,   6.0511,
          4.4660,  10.8919,   4.4595], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 16.4293,  26.3421,  26.5602,  20.4289,   5.9640,   9.5983,   8.1255,
         12.6989,   6.4777,   8.1279,   4.4351,   9.3909,   6.9205,   8.5952,
          6.2055,   4.5524, 119.4516,   3.3047,   4.6360,   4.4179,   3.4700,
          2.7661,   5.1485,   4.4367,  13.6214,   8.7632,  14.8330,   6.6549,
          7.6594,   6.0310,   5.8679,   5.4612,   4.9314,   4.8245,   5.2881,
          4.2316,   4.9104,   6.4403,   6.5832,  17.6872,   8.7429,  11.3094,
          4.8629,   9.2211,   7.6927,  10.1231,   3.9376,   5.3159,   6.1795,
          6.8533,   4.6923,   3.8592,   6.3811,   4.8748,   4.6740,   8.0565,
         10.8432,   6.7376,  10.0764,   5.0439,   7.6079,  14.0279,   5.7729,
          6.5986,   5.8514,   6.1654,   6.9910,   6.3063,   8.6352,  11.8323,
          4.4484,   9.3098,  12.6211,   6.9372,   7.3424,   6.0895,   5.9423,
          4.4032,  10.7519,   4.3422], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 43.2610,  70.9805,  67.6813,  47.2024,  12.3190,  21.3036,  16.9825,
         28.1920,  15.1058,  17.5870,   9.8100,  20.5275,  16.0089,  19.7094,
         13.3812,  12.1909, 308.8607,   6.9493,  11.5277,   9.5505,   7.5577,
          5.9721,  11.2429,   9.6016,  33.0803,  19.6336,  33.3348,  14.4123,
         16.8502,  15.6546,  15.4761,  13.8038,  10.5888,  11.9279,  12.7971,
         10.9097,  12.2691,  16.1312,  15.6773,  46.5615,  19.9381,  24.9471,
         11.3351,  20.2581,  16.4729,  21.8257,   8.4174,  12.3177,  13.6298,
         15.1278,   9.9273,  10.3464,  14.0271,  10.3274,  10.0876,  17.7442,
         24.4398,  15.6229,  26.7040,  10.6893,  17.0937,  30.1151,  14.6862,
         14.7532,  13.1175,  13.9100,  16.0369,  13.6960,  19.9008,  25.5902,
          9.5236,  22.1066,  27.4460,  15.6213,  15.9288,  16.3187,  14.1811,
         11.7929,  23.3485,   9.6767], device='cuda:0')
Outer loop valEpocw Maximum [21/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 115.6
model_train val_loss valEpocw [21/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 119.5
model_train val_loss  valEpocw [21/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 308.9
Max_Val Meta Model:  tensor([39.0397, 40.5107, 42.5683, 39.1875, 43.3324, 41.9667, 42.7691, 38.1764,
        37.0671, 40.3038, 38.7255, 48.5179, 37.2694, 39.3152, 44.6179, 31.8224,
        33.9101, 39.6193, 34.4949, 41.3099, 39.7131, 39.6654, 38.7657, 44.3509,
        35.7699, 37.2314, 38.4258, 38.8499, 37.9716, 32.6829, 32.7018, 34.2075,
        39.6819, 34.3522, 35.3238, 33.2360, 34.3403, 34.5002, 36.0618, 33.0514,
        36.6053, 41.2190, 36.2583, 39.2174, 38.7591, 38.8281, 37.7928, 36.4389,
        38.5490, 38.2593, 40.2331, 32.1185, 38.6996, 40.1611, 38.8119, 38.0408,
        38.4799, 37.1861, 40.9495, 38.9466, 38.2691, 43.6239, 33.9214, 38.3826,
        37.0517, 37.7423, 35.0120, 39.0670, 35.7162, 39.9672, 39.3278, 38.6777,
        44.0875, 38.3799, 37.6466, 31.8945, 35.6592, 31.9435, 39.8476, 38.7252],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([21.2788,  6.5161, 18.8515,  7.3634, 10.8176, 18.3506, 38.0206, 23.9738,
        15.0178, 15.4010,  8.2084, 75.3126,  7.4259,  5.6677,  8.1251,  4.0177,
         5.2786,  8.3917,  3.8629, 10.8049,  3.2023,  2.2032,  4.7315,  5.2903,
        12.7488,  8.4870, 13.4687,  8.0107,  7.6815,  3.1798,  5.0629,  4.5713,
         1.7454,  4.0813,  4.4325,  3.5003,  5.2404,  4.3451,  5.0667,  2.7542,
         5.1474,  2.6024,  2.2515,  2.6913,  2.4536,  1.9549,  2.8994,  3.5215,
         5.0652,  5.9629,  3.5092,  3.1568,  3.6394,  3.4733,  3.8335,  7.6403,
         6.3504,  4.0045,  6.4977,  3.4259,  2.3489,  4.7948,  4.3087,  4.7960,
         4.8693,  5.0542,  5.7694,  6.5889,  4.8726,  4.2786,  3.6319,  3.1075,
         7.1597,  3.8086,  5.3406,  3.0134,  5.0353,  3.0729,  9.4671,  3.5538],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 58.0182,  16.9163,  47.0006,  16.6281,  21.7725,  39.3768,  78.0953,
         53.5936,  34.6524,  32.9923,  18.2355, 165.0353,  17.2539,  12.7351,
         16.9099,  10.7866,  13.5955,  17.7065,   9.5421,  22.8886,   6.9331,
          4.7577,  10.3239,  10.9660,  30.8228,  19.2328,  30.4309,  17.3851,
         17.1305,   8.2760,  13.3786,  11.5896,   3.7408,  10.0157,  10.7682,
          9.0005,  13.1040,  10.8305,  12.0862,   7.2718,  11.9080,   5.5316,
          5.2759,   5.9187,   5.2873,   4.1973,   6.3099,   8.1964,  11.2605,
         13.1688,   7.4531,   8.4949,   7.9914,   7.3748,   8.3026,  17.0207,
         14.1002,   9.3287,  16.7136,   7.2553,   5.3114,   9.8787,  11.0118,
         10.7296,  11.0813,  11.4512,  13.6358,  14.1716,  11.7050,   9.2264,
          7.8234,   7.3065,  16.1347,   8.5414,  11.8795,   8.0728,  11.9639,
          8.2521,  20.6188,   7.9105], device='cuda:0')
Outer loop valEpocw Maximum [21/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 48.5
model_train val_loss valEpocw [21/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 75.3
model_train val_loss  valEpocw [21/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 165.0
Max_Val Meta Model:  tensor([31.1977, 41.4259, 36.5619, 39.6581, 40.1297, 40.2854, 42.2264, 40.4325,
        37.7728, 35.2994, 37.9885, 40.8788, 37.8403, 39.0553, 39.8159, 32.0980,
        33.9852, 41.0487, 35.1758, 42.6614, 41.4662, 42.2980, 42.9763, 42.6906,
        35.8984, 38.9331, 38.6466, 42.3168, 37.8750, 33.7978, 33.9930, 35.3102,
        40.4471, 35.8806, 35.1142, 33.7242, 35.0282, 35.3334, 36.9259, 33.2931,
        36.1697, 42.8585, 37.0921, 39.9636, 39.8234, 51.1141, 39.3676, 36.9441,
        39.8252, 44.6028, 41.9992, 32.9020, 39.3067, 40.7390, 40.0399, 39.3148,
        41.5406, 37.8180, 41.8056, 37.7337, 48.5129, 42.0794, 33.7939, 40.0864,
        37.5796, 38.3311, 35.0859, 38.3074, 35.3370, 40.1647, 38.3755, 38.5497,
        41.6984, 39.4985, 37.7504, 33.1131, 35.8941, 32.4009, 39.5940, 39.2604],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 6.4422,  2.7238,  3.4144,  6.0447,  3.0692,  4.3438,  3.4723,  7.2532,
         7.2327,  3.5469,  4.1133,  9.6895,  7.6441,  7.9877,  4.5660,  4.7803,
         7.4381,  1.8558,  6.1705,  4.3865,  3.7486,  3.4612,  6.1355,  5.2718,
         4.9290,  4.6823, 12.8433,  9.2678,  6.7354,  5.2833,  6.1157,  8.0528,
         4.0666,  6.4023,  6.5749,  5.6258,  7.7206,  6.7881,  7.0747, 17.1152,
        14.0700, 34.6143, 32.5742, 34.9904, 25.5249, 33.1181,  8.0315,  8.2209,
        19.9069,  8.9166, 16.7820, 17.3148, 26.4767, 15.9692, 29.9537, 40.2773,
        49.2298, 10.6280,  8.9165,  7.5049, 37.0724,  8.5236,  8.7697, 10.5714,
         9.7979,  8.0878,  9.8402, 11.2724,  7.9772,  6.5765,  5.6319, 11.2637,
        10.1503, 17.4544,  3.8814,  7.7847,  9.2299,  5.0194, 12.5050,  5.8322],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 18.0791,   6.9118,   8.2801,  13.4860,   6.3522,   9.3609,   7.1563,
         15.4903,  16.2438,   8.0282,   9.1757,  20.5620,  17.4645,  17.6524,
          9.8914,  12.7050,  19.0986,   3.9097,  14.9292,   9.0499,   7.9379,
          7.2362,  12.9461,  10.9272,  11.7935,  10.3553,  28.8085,  19.4372,
         14.8748,  13.2803,  15.5334,  19.7617,   8.5813,  15.0243,  16.0497,
         14.2390,  18.9244,  16.5182,  16.4632,  44.9340,  33.0918,  71.7546,
         75.9746,  76.2601,  54.5815,  68.1236,  16.9139,  18.9122,  43.6078,
         19.9307,  34.7894,  45.6108,  57.3372,  33.5149,  63.8757,  88.9734,
        105.8875,  24.3275,  22.3956,  15.9091,  82.5802,  17.7375,  22.4905,
         22.6467,  22.0059,  18.0235,  23.1385,  24.0186,  19.3549,  14.0882,
         12.4166,  26.1814,  22.4715,  38.1479,   8.3301,  20.0909,  21.7905,
         13.2722,  27.3870,  12.7885], device='cuda:0')
Outer loop valEpocw Maximum [21/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 51.1
model_train val_loss valEpocw [21/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 49.2
model_train val_loss  valEpocw [21/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 105.9
Max_Val Meta Model:  tensor([43.0413, 39.7736, 33.8964, 36.1465, 35.5551, 38.8740, 36.2247, 35.8740,
        31.2361, 30.8886, 32.3825, 36.5339, 33.3671, 31.7956, 32.0701, 28.6729,
        32.2644, 35.7237, 30.4110, 35.5518, 35.5621, 35.1595, 34.9006, 36.6789,
        29.7906, 33.3664, 28.8529, 36.2074, 33.8157, 28.8034, 29.2839, 28.3746,
        30.1175, 30.2203, 31.0961, 29.3354, 28.1267, 26.5311, 30.1584, 29.9687,
        32.5825, 35.5600, 31.5847, 34.0869, 33.5841, 34.0280, 33.2344, 32.1563,
        35.0303, 31.3125, 35.4484, 28.2600, 33.6192, 38.0327, 36.2612, 35.6104,
        39.9060, 34.5408, 33.3866, 36.9934, 37.6817, 38.1846, 30.5053, 40.7303,
        37.3637, 33.0386, 31.0927, 47.0220, 31.6227, 32.3738, 34.1104, 37.5051,
        34.2241, 31.9172, 44.5413, 31.0716, 31.9533, 28.1239, 36.3594, 28.9894],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 12.8178,   2.0475,  11.5247,   5.9980,  12.9367,  10.4605,   9.0578,
         17.5517,   5.2618,   9.8042,   4.3101,   8.2141,   5.8824,   7.3263,
          9.3814,   3.2753,   4.5086,   3.4438,   4.3213,   5.1037,   3.9806,
          2.8427,   5.9203,   5.2147,   9.7674,   3.3012,   8.9825,   4.7733,
          8.2806,   3.6091,   5.2068,   4.9267,   1.2336,   4.5161,   4.9099,
          3.9505,   4.1697,   3.9948,   4.9693,   3.2678,   5.7215,   3.0663,
          2.9096,   3.2681,   3.0854,   2.2017,   3.5240,   4.1574,   5.8124,
          5.5174,   4.5285,   3.5742,   4.2605,   4.2696,   4.4363,   7.4028,
          3.5597,   3.8677,   7.8704,   4.2958,   4.0266,   6.0756,   4.7971,
          5.4719,   5.4065,   5.6360,   6.4092,   3.9041,   5.4123,   3.7762,
          4.0706,   4.8474,   6.9313,   5.1542, 142.3650,   4.3383,   5.5038,
          4.2511,  10.3069,   3.8983], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 34.9663,   5.3003,  29.1476,  13.5601,  26.9305,  22.8898,  18.8768,
         38.0976,  12.2244,  22.2203,   9.6269,  17.5843,  13.5242,  16.7935,
         20.4704,   8.7962,  11.6039,   7.1737,  10.6057,  10.8322,   8.4741,
          6.0537,  12.8699,  11.2607,  24.2722,   7.3797,  21.4654,  10.1072,
         18.4615,   9.2665,  13.5571,  12.8556,   2.6976,  11.1462,  11.9355,
         10.0493,  10.7709,  10.5755,  11.9856,   8.6127,  13.1872,   6.7924,
          6.7817,   7.1846,   6.6151,   4.6951,   7.5562,   9.5532,  12.6893,
         12.5409,   9.5782,   9.5110,   9.3592,   8.9785,   9.4165,  16.6906,
          7.9189,   8.8210,  20.8016,   8.9671,   8.9584,  12.8926,  12.2326,
         12.1033,  12.3725,  12.7380,  15.0818,   8.1609,  13.0044,   8.4266,
          8.7580,  11.4397,  15.7875,  11.8341, 314.2419,  11.2238,  13.2404,
         11.3084,  22.4597,   8.8707], device='cuda:0')
Outer loop valEpocw Maximum [21/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 47.0
model_train val_loss valEpocw [21/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 142.4
model_train val_loss  valEpocw [21/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 314.2
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [88.54058796 97.2137279  91.7654512  97.02489005 97.26733349 96.6009186
 96.99808726 94.28247707 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.15089972 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.4903449
 96.90915072 96.22689782 96.9627563  93.87556194 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.16726161 96.13796128 96.24273583 96.9067141
 91.97621861 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [87.90097587 97.2137279  90.53008613 97.02489005 97.26733349 96.5997003
 96.99808726 94.77101887 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.08632936 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.25521132
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 89.94651625 96.13796128 96.24273583 96.9067141
 91.04664904 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [96.98775224  6.07537493 59.14883255  6.38647733 22.57560454 25.28044968
  9.60351729 33.16587516  2.40831625 26.71089606  1.6694278   1.26067645
  0.8001155  11.95164915  3.19121764  3.47578193  4.15573497  3.48361264
  0.80255128  1.41774139  1.32018589  0.49050601  0.94998296  1.533868
 13.77672877  8.58140338 29.43832052  7.86230876  3.84219192  1.21615339
  2.06264611  0.8272005  35.00671315  1.25465716  1.38650159  1.37088348
  5.31090167  5.54362558  3.68338546 28.43657038  8.0455108  43.63508738
 19.88650269 27.04168749 17.93094056 34.91419166  3.017314    2.19616167
  3.45531511  2.17737415  4.56379068  1.5715911   1.58226464 15.60569693
  1.89700104  4.40528517 60.45750598 24.79804421 11.3641445   7.74774247
 61.08371577  3.59505303 13.23360967  9.00852172  4.57353113  5.8538825
  4.82966508 10.42195236  3.55194468  9.56104815  0.68800413 23.45947238
  6.40378328 23.87983956  8.62735784  8.73095148  1.4523508   2.3764815
  0.21997302  0.93013072]
Accuracy th:0.5 is [45.42829644 97.2137279  71.66213862 97.02489005 97.26733349 76.41232441
 76.76685226 75.93352907 77.63916132 96.36091178 77.96201313 98.52097319
 99.41399349 79.84917338 77.57946419 96.56680596 96.29512311 77.21762649
 98.65376884 98.30776915 79.84430014 78.45786479 98.38695922 77.33580244
 80.72026413 96.65086926 94.0778012  77.01904217 98.01293844 77.80119638
 97.30875598 98.57457877 96.36213009 98.02024829 85.98092129 77.456415
 77.19935186 88.80983419 97.11504489 74.66405136 78.88183623 92.05906361
 76.74614101 76.37090191 96.9627563  93.87434364 98.02877645 98.57336046
 90.3583046  86.1636676  86.1222451  98.55508583 98.99976852 76.91548592
 98.70615611 77.25661237 71.90092713 92.44526748 96.24273583 96.9067141
 89.79300934 97.17717864 91.31345866 77.35042214 98.42838172 77.72444293
 98.20786784 76.46958492 78.35430855 97.50490369 78.90742072 95.99054592
 78.14110452 95.45083515 76.67791572 81.64496047 85.83472424 77.85358366
 78.95006152 99.14718388]
Accuracy th:0.7 is [45.48433864 97.2137279  71.66213862 97.02489005 97.26733349 76.41232441
 76.76685226 76.17231759 77.63916132 96.46690464 77.96201313 98.52097319
 99.41399349 80.31944055 77.57946419 96.56680596 96.29512311 77.21762649
 98.65376884 98.30776915 80.30360254 78.45786479 98.38695922 77.5538797
 81.36231284 96.65086926 94.0778012  77.01904217 98.01293844 77.80119638
 97.30875598 98.57457877 96.36213009 98.02024829 86.24651259 77.456415
 77.19935186 89.13268601 97.11504489 74.66405136 79.57139898 92.05906361
 76.74614101 76.37090191 96.9627563  93.87434364 98.02877645 98.57336046
 92.57075328 87.12734981 86.30133648 98.55508583 98.99976852 76.91548592
 98.70615611 77.25661237 71.90092713 92.87654877 96.24273583 96.9067141
 89.79300934 97.17717864 91.55468379 77.35042214 98.42838172 77.72444293
 98.20786784 76.46958492 78.35430855 97.55729097 78.90742072 95.99054592
 78.21663966 95.45083515 76.67791572 81.73633362 85.97604805 77.85358366
 78.95006152 99.14718388]
Avg Prec: is [55.93684355  3.211804   11.31723221  3.38453742  2.31230107  3.89931383
  3.15810265  5.65371681  2.53300337  3.87684571  1.56354063  1.60143966
  0.62769275  4.99065329  2.61616664  3.18896455  3.51613765  2.75306837
  1.3937209   1.74875568  1.93446793  0.84427905  1.85284172  2.46481621
  5.17802279  3.74241836  6.53540862  3.3861383   2.12576348  1.92647701
  2.49089657  1.32406043  3.71373919  1.60001615  2.34552614  2.47193672
  3.01841991  2.56273765  2.77750337  7.32905648  2.27797781  8.26452505
  3.37626987  4.01608813  3.19479092  6.25343157  2.05837924  1.46152789
  2.07141567  1.54134512  1.75186492  1.5133134   1.02787695  2.97206457
  1.33034123  2.60416284 11.27553446  3.65749859  3.80604245  2.88409678
 10.85185628  2.1714626   3.86063769  3.03157398  1.6415283   2.54905038
  1.88270282  4.26963638  1.27129972  2.3472049   0.18801274  3.33119496
  1.88628638  4.53284257  3.86293456  3.17058712  0.88278651  1.87327428
  0.15183665  0.75822134]
mAP score regular 12.17, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [88.98024267 97.22450607 92.28392755 96.96290206 97.90716795 96.63901139
 96.80843112 93.74392705 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.14007026 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.56546329
 97.07750953 96.48703192 97.03764606 94.04539452 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.73921818 96.39235618 96.16314124 96.78102499
 92.21665795 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [89.12723921 97.22450607 91.5165558  96.96290206 97.90716795 96.63651992
 96.80843112 94.77539427 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.37426813 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.74235743
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.00423549 96.39235618 96.16314124 96.78102499
 91.725839   97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [97.08250541  7.36482619 63.22078691  6.84575512 22.5375591  25.03255114
 10.72032147 32.65767835  2.73363818 29.17124708  1.69642968  1.24049904
  0.80026685 13.34213049  3.54485942  3.64589867  4.13960193  3.68831512
  0.72839293  1.42077986  1.19060889  0.49279748  0.96799159  1.45902019
 13.47288345 10.43488569 28.94918254  7.39084999  4.45168988  1.25499077
  1.87484939  0.76000379 39.71154167  1.20349745  1.20854349  1.22592908
  5.26363879  6.96342092  4.02868592 29.13472762  8.46959572 44.66240008
 22.38352205 27.60329152 18.12919725 35.24462059  3.19157978  2.36335485
  3.84369668  2.12767706  5.19057635  1.80660348  1.99277169 17.33155093
  2.22534807  4.39236991 54.38946206 26.64006048 10.18819282  8.61412593
 61.23693014  3.32218662 13.67755908  8.86629951  4.77985273  5.68676489
  4.7025944  11.00387786  3.06181574  9.37704651  0.87336715 25.78915405
  6.09084802 26.21832419 10.60652375  7.72319966  1.41616958  2.39736268
  0.18815905  0.83439513]
Accuracy th:0.5 is [45.53155443 97.22450607 69.94543688 96.96290206 97.90716795 75.37683434
 75.46901861 74.44004285 76.96140718 96.41228791 77.09594638 98.5325261
 99.34972718 77.649052   77.03365972 96.31262924 96.21047911 76.46062237
 98.78167277 98.34068316 78.37406881 77.75120213 98.31327703 76.5403493
 77.8583352  96.52938685 94.3393876  76.54284077 97.81747515 77.06355732
 97.52597354 98.67204823 96.39983058 98.18870369 86.39908314 76.68485437
 76.65744824 90.76911578 97.0276802  73.86202257 77.16072452 92.37362035
 75.81284102 75.47649301 97.03764606 94.02795426 98.18621222 98.77668984
 91.13536139 86.2670354  84.52550016 98.55993223 98.87385704 75.79789222
 98.6969629  76.48553704 70.61813289 93.61686225 96.16314124 96.78102499
 90.13379176 97.04761193 91.40194833 76.52789197 98.32075143 77.37249919
 98.13139996 75.71318235 77.77362533 97.50604181 78.18222588 96.07843137
 77.53195306 95.44559882 75.61601515 82.43765105 87.49283703 77.13082692
 78.25696988 99.15040985]
Accuracy th:0.7 is [45.8105987  97.22450607 69.94543688 96.96290206 97.90716795 75.37683434
 75.46901861 74.59451379 76.96140718 96.41976231 77.09594638 98.5325261
 99.34972718 78.09004161 77.03365972 96.31262924 96.21047911 76.46062237
 98.78167277 98.34068316 78.69546802 77.75120213 98.31327703 76.61509331
 78.22208934 96.52938685 94.3393876  76.54284077 97.81747515 77.06355732
 97.52597354 98.67204823 96.39983058 98.18870369 86.67812741 76.68485437
 76.65744824 90.93604405 97.0276802  73.86202257 77.66898373 92.37362035
 75.81284102 75.47649301 97.03764606 94.02795426 98.18621222 98.77668984
 93.28051424 86.73543115 84.6799711  98.55993223 98.87385704 75.79789222
 98.6969629  76.48553704 70.61813289 93.92580412 96.16314124 96.78102499
 90.13379176 97.04761193 91.58880833 76.52789197 98.32075143 77.37249919
 98.13139996 75.71318235 77.77362533 97.53593941 78.18222588 96.07843137
 77.54939333 95.44559882 75.61601515 82.53730971 87.66225677 77.13082692
 78.25696988 99.15040985]
Avg Prec: is [54.24095994  3.7316729  14.71425139  4.55087584  1.51628392  4.26511381
 12.4838869   8.65981193  7.73839189  5.19741796  2.30016358  5.03292758
  1.54348842  5.89366595  3.06856473  3.63748939 24.50522427  6.36998982
  1.54557026  2.66238418  3.57942567  1.43795768  1.26570181  5.63538873
  5.72059681 10.49087217  8.07767491  4.55341887  3.92815738  6.28070866
  2.29020528  0.86517565  3.06503258  1.11981875  1.75530922  2.346238
  1.98989599  2.23843044  2.19771218  6.35300224  1.72376198  6.03947854
  2.17024286  2.70534948  2.34529335  4.85964029  1.74103393  1.03560224
  1.4004822   1.2067627   1.2097477   1.04916171  0.76927772  2.27890707
  0.90901569  1.85274201 10.24293702  3.00478571  4.05898295  2.84469608
  7.95311431  2.05719017  3.34029965  2.50969286  1.34718075  1.89978638
  1.57308138  3.52442306  1.08733288  2.20822958  0.19573655  3.20883112
  1.61402857  4.04386752  3.20850856  2.31241867  0.56376446  1.4527302
  0.12058852  0.60928472]
mAP score regular 12.52, mAP score EMA 4.34
Train_data_mAP: current_mAP = 12.17, highest_mAP = 12.39
Val_data_mAP: current_mAP = 12.52, highest_mAP = 12.97
tensor([0.3549, 0.3811, 0.3899, 0.4377, 0.4813, 0.4507, 0.4785, 0.4547, 0.4246,
        0.4363, 0.4450, 0.4638, 0.4308, 0.4315, 0.4572, 0.3702, 0.3871, 0.4806,
        0.4004, 0.4670, 0.4656, 0.4697, 0.4590, 0.4599, 0.3988, 0.4457, 0.4150,
        0.4688, 0.4442, 0.3843, 0.3796, 0.3794, 0.4543, 0.3894, 0.4121, 0.3878,
        0.3842, 0.3735, 0.4105, 0.3767, 0.4325, 0.4467, 0.4275, 0.4562, 0.4705,
        0.4608, 0.4614, 0.4324, 0.4542, 0.4367, 0.4705, 0.3727, 0.4523, 0.4716,
        0.4675, 0.4457, 0.4383, 0.4324, 0.3747, 0.4729, 0.4425, 0.4706, 0.3907,
        0.4322, 0.4336, 0.4383, 0.4212, 0.4553, 0.4175, 0.4477, 0.4644, 0.4220,
        0.4394, 0.4300, 0.4463, 0.3823, 0.4114, 0.3725, 0.4617, 0.4331],
       device='cuda:0')
Max Train Loss:  tensor([12.3379,  4.0005, 12.7898,  9.1678,  8.3219,  8.8395, 10.4457, 14.3534,
         6.7279,  5.6987,  6.1237, 10.0760,  6.3837, 12.7329,  8.3105,  8.7413,
         9.3130,  7.0867,  4.8357, 10.1968,  6.7732,  3.1388,  9.9142,  7.5502,
        13.1082,  7.8807, 16.1146,  7.7067,  6.5618,  7.5903,  8.0577,  8.0169,
         7.8622,  7.2475,  7.7804,  8.1790,  8.5450,  5.7421,  9.7295, 11.3903,
         8.0011, 12.4272,  6.9849,  8.6141,  8.8241, 20.8322,  9.9686,  6.3236,
         9.8530,  6.7483,  9.4537,  6.3255,  7.8652,  9.2402,  8.0839,  9.5347,
        14.2292, 11.6252,  8.8473,  9.2275, 15.4082,  9.9917,  8.0004,  9.2072,
         6.6862, 10.2592,  7.0027,  8.7873,  6.7031,  7.0432,  4.6745, 11.4186,
        11.0367, 12.7690, 10.7245, 10.0979,  6.0632,  5.0141, 11.6795,  5.7050],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [22/80], Step [000/642], LR 1.0e-04, Loss: 20.8
Max Train Loss:  tensor([16.5754, 10.5828, 17.0671,  9.6354, 14.1680, 12.5325, 14.5895, 12.0241,
        10.7385, 13.3863, 11.3006,  6.8177, 11.1070, 14.9212, 14.3050,  4.2040,
         9.1202, 17.5436,  8.1391, 10.8418, 17.9481, 11.0851,  8.9740, 11.1719,
         8.3945,  9.0276, 12.1011, 13.8178, 15.1596,  5.8634,  9.5097,  4.6944,
        12.1342,  8.7009,  7.1774,  7.0534, 12.9355,  5.7296,  7.4626,  6.1596,
         9.4062, 13.2185,  7.4609, 13.2144, 13.4953, 12.8452, 11.5902,  6.8613,
         9.3213, 15.8275, 15.0003,  4.6849, 12.7536, 12.3936, 17.5994,  8.7088,
        13.3056, 11.8495,  6.0545, 10.4522, 16.1285, 16.4006,  8.9677, 14.0902,
         7.4443, 15.5863, 10.9908, 12.3570,  7.2154,  7.9296, 14.6223,  9.8113,
         7.7474,  9.1926, 13.3973,  6.5960,  6.7099,  6.2895,  4.4817, 11.2595],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [22/80], Step [100/642], LR 1.0e-04, Loss: 17.9
Max Train Loss:  tensor([12.9336,  7.5834, 12.1641,  5.8613,  9.3228, 11.2836,  7.9304,  9.7578,
         7.4405, 10.0268,  7.5810,  6.1574,  9.8630,  8.6862,  9.7138,  4.0750,
        17.8109,  8.3829,  7.2907,  7.5865,  7.9278,  8.8180,  7.3467,  7.1880,
        11.8118,  7.2343, 11.2159, 10.6081,  7.0435,  8.2103,  4.0393,  6.4555,
        16.6173,  9.9331, 11.0814, 10.6071,  8.6477,  7.5260, 10.7215, 14.2102,
        11.8220, 13.4160,  8.8685, 11.8304, 11.5462, 10.5932, 10.1026,  7.1821,
         8.5164,  8.2990,  7.4325,  5.7763,  8.1382,  7.8708,  7.7236,  8.0534,
        10.9104, 12.1108,  5.2990, 10.1353, 13.8899,  6.8639,  6.8274,  8.2403,
         6.6801,  3.8126,  7.6467, 12.3661,  9.9320, 10.7430,  6.6760,  8.1844,
        11.9167,  9.7154,  7.4900,  8.8175,  6.2827,  8.3555,  4.1828,  9.8933],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [22/80], Step [200/642], LR 1.0e-04, Loss: 17.8
Max Train Loss:  tensor([14.8642,  9.2951, 13.8812, 11.9328,  7.9903,  9.0680, 10.7851,  8.9624,
         8.2228,  7.4006,  8.3397, 12.7601, 10.2429, 12.4618,  6.1886,  4.9235,
         9.2618, 10.3489,  4.9509,  5.8650,  6.7603,  7.2679,  7.1670,  8.4925,
        12.1980, 10.0599, 10.2952,  9.5859,  7.0537,  7.9577, 11.1672,  5.1810,
        11.8062,  4.6620, 10.6804, 10.4593,  6.0063,  6.4890,  8.7428,  9.7596,
        12.1282, 10.3293,  8.7403, 11.8569,  5.9508, 10.4149, 10.5041,  8.9197,
        10.0411, 12.5276,  9.4909,  3.4595,  7.5315,  8.6094,  8.7276,  7.8455,
        10.7330,  6.9913,  8.0709,  5.9053, 10.9455,  8.5000,  6.0117,  6.7363,
         5.7533,  3.6371,  6.1533,  7.8814,  7.4824,  9.4479,  6.4583, 10.7027,
         6.6176,  6.9591,  5.2025,  6.3094,  5.2880,  4.1199,  4.9670,  7.6543],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [22/80], Step [300/642], LR 1.0e-04, Loss: 14.9
Max Train Loss:  tensor([14.0274,  6.0502, 10.8806,  8.4991,  4.7318,  8.4037,  6.2039,  5.6408,
         8.2019,  7.8806,  7.6764,  6.8790,  9.9589, 11.1588,  7.0226,  6.8309,
         4.3156,  9.9789,  6.9080,  7.1369,  6.8785,  9.9311,  6.7301, 12.2401,
         9.7145,  8.3081,  8.4882,  8.3599,  9.6515,  4.4173,  8.5376,  5.4584,
         8.3900,  6.5710, 10.1195,  8.1838,  5.8388,  9.4336,  8.2372,  9.3394,
         9.1699, 12.0095,  9.6306,  9.1587,  9.0941, 10.6156,  8.0830,  5.7713,
         5.7354,  5.8733, 14.3561,  6.9758,  7.9349,  8.2949,  8.3748, 11.0287,
        14.5430, 11.1891,  7.6208, 11.1119, 12.3845, 10.0066,  9.4911,  9.5837,
         8.9647,  6.8652,  8.4238, 12.4996,  7.5229,  7.3999,  6.7649,  9.2736,
         6.3948, 11.1641, 11.7529, 13.0023,  7.2226,  6.7298,  5.3381,  8.0735],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [22/80], Step [400/642], LR 1.0e-04, Loss: 14.5
Max Train Loss:  tensor([13.3663,  7.9356, 12.6339,  7.5223,  5.3955,  6.2159,  6.3684, 11.4832,
         7.1865,  6.6343,  9.8455,  7.9692, 10.4229, 10.3332, 10.0052,  5.7945,
        11.0560,  9.6474,  5.0841,  7.7570,  6.7736,  9.2405,  6.4855,  8.7340,
         9.6804,  9.9884,  8.6000,  8.8066,  6.8442,  7.2070,  8.8123,  3.6497,
        10.0620,  7.7440,  6.6278,  8.7773,  9.5925,  7.3393,  8.4914, 13.9483,
         2.5160, 12.4646,  8.5144,  8.3610,  8.5562,  9.5278,  6.6515,  6.4434,
         4.6122,  6.5537,  7.8827,  7.8419,  7.8523, 11.2546,  5.3370,  6.8289,
        11.1560, 10.1900,  5.8195,  7.1670,  9.5353, 11.6166, 10.6405,  7.5019,
         8.2971,  9.4340,  7.7730, 11.0141,  6.5846,  4.3450,  6.6013,  7.8406,
         6.3870,  9.4654,  8.6952,  6.5887,  6.3803,  7.2275,  4.9422,  7.7811],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [22/80], Step [500/642], LR 1.0e-04, Loss: 13.9
Max Train Loss:  tensor([11.4658,  9.0867, 13.2324, 10.7211,  7.3628,  9.9231,  8.1016, 12.6433,
         5.0627, 11.0580,  8.5807, 10.5398, 10.5869,  5.8929,  9.0551,  9.7601,
         6.4780,  8.9092,  5.1925,  9.2668,  5.2071,  6.2688,  9.0973,  7.5179,
        12.0029, 10.4475, 10.8003,  6.3079,  6.6063,  5.2328,  9.1651,  4.7008,
        12.5203,  7.8936,  9.9014,  8.8430,  8.4416,  7.9859,  6.4361,  8.0713,
         4.6413,  8.6659,  6.4449,  7.5520,  9.6778, 11.1758,  7.7619,  6.5468,
         5.6202,  6.6702,  5.9066,  3.6628,  8.0834,  6.9514,  4.2044,  9.2880,
        10.8954,  8.3338,  5.9705,  9.6304,  9.1746,  8.4217,  8.1162, 12.3183,
         8.2264,  7.4051,  7.0178, 11.2637,  6.7765,  7.9086,  6.7332,  8.2433,
         7.4949, 11.3522, 10.8075,  6.7535,  6.2738,  6.2315,  4.2054,  7.1627],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [22/80], Step [600/642], LR 1.0e-04, Loss: 13.2
Max_Val Meta Model:  tensor([ 19.1705,  24.8840,  34.7209,  21.0555,   4.4706,   7.0017,   6.2244,
          8.9054,   4.9612,   7.5794,   8.4552,   6.5466,  10.5914,   9.6633,
          5.8877,   3.6971, 146.1828,   4.5275,   5.3205,   4.3525,   4.2002,
          4.8734,   5.6540,   5.4939,  12.6979,   9.3817,  11.3613,   3.2230,
          7.1943,   6.3869,   8.0261,   3.8524,   8.2689,   4.3498,   5.8952,
          4.5272,   4.3068,   6.8741,   5.2110,  16.4044,   5.9389,   9.9112,
          6.6585,   9.9572,   9.5777,  10.2984,   6.0547,   6.8685,   3.9194,
          6.7820,   4.8780,   3.7729,   8.6929,   4.4016,   4.3284,   5.0885,
         12.0172,   8.1531,   8.6160,   7.0536,   8.4524,  13.5622,   5.3506,
          5.8708,   6.1582,   4.0430,   6.5596,   6.3885,   8.3389,  11.5955,
          6.8922,  10.0881,  11.1015,   8.2051,   8.2680,   6.8874,   5.6709,
          4.2853,   4.3398,   7.3164], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 15.4204,  23.3355,  27.9298,  21.2930,   4.9006,   8.0149,   6.7266,
         10.4564,   5.5164,   8.3703,   9.3743,   7.1583,  11.6414,  10.3982,
          5.9224,   4.2336, 129.8309,   5.0533,   6.1090,   4.7784,   4.9451,
          5.2926,   6.1442,   5.2872,  12.3194,   9.9070,  12.7397,   4.1276,
          7.9623,   6.8350,   8.3696,   4.5000,   9.4651,   4.8126,   6.7401,
          5.3821,   5.2269,   7.3551,   5.9471,  16.6945,   6.2122,  10.6103,
          7.2154,  10.5976,  10.0721,  10.7083,   6.5870,   7.6397,   4.3887,
          7.5715,   5.3386,   4.4072,   9.4075,   4.7192,   4.9570,   5.8757,
         15.1238,   8.8693,   8.9445,   8.0697,   9.0199,  12.9743,   6.2165,
          6.7891,   7.0191,   4.8343,   7.4396,   7.8541,   9.1188,  11.9907,
          7.7965,  10.5352,  11.7881,  10.2214,   8.4651,   7.3890,   6.4932,
          4.9036,   5.0676,   8.2295], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 43.4491,  61.2324,  71.6262,  48.6472,  10.1824,  17.7818,  14.0580,
         22.9987,  12.9916,  19.1847,  21.0642,  15.4328,  27.0247,  24.0986,
         12.9527,  11.4364, 335.3895,  10.5153,  15.2574,  10.2325,  10.6200,
         11.2676,  13.3853,  11.4968,  30.8874,  22.2290,  30.7017,   8.8048,
         17.9259,  17.7849,  22.0457,  11.8604,  20.8358,  12.3590,  16.3545,
         13.8792,  13.6040,  19.6922,  14.4886,  44.3144,  14.3619,  23.7508,
         16.8783,  23.2300,  21.4085,  23.2363,  14.2763,  17.6664,   9.6623,
         17.3381,  11.3470,  11.8253,  20.8013,  10.0064,  10.6035,  13.1823,
         34.5093,  20.5094,  23.8737,  17.0625,  20.3830,  27.5726,  15.9105,
         15.7086,  16.1866,  11.0302,  17.6616,  17.2490,  21.8426,  26.7822,
         16.7888,  24.9668,  26.8298,  23.7734,  18.9655,  19.3261,  15.7815,
         13.1651,  10.9761,  19.0031], device='cuda:0')
Outer loop valEpocw Maximum [22/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 146.2
model_train val_loss valEpocw [22/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 129.8
model_train val_loss  valEpocw [22/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 335.4
Max_Val Meta Model:  tensor([35.0670, 40.5508, 42.9280, 43.1564, 41.9009, 49.3572, 43.5895, 37.8947,
        36.5508, 38.8104, 37.2942, 49.1502, 37.7965, 39.9999, 44.5612, 31.7027,
        41.9804, 39.1987, 35.5940, 41.7200, 40.1127, 40.7833, 38.9179, 43.5035,
        34.9079, 37.6790, 35.3701, 39.2793, 35.7819, 33.3966, 33.4129, 33.4505,
        39.3214, 33.4834, 35.5101, 33.4110, 33.2357, 32.3027, 35.7576, 41.6901,
        36.1608, 36.7866, 36.4646, 39.3860, 39.9456, 39.0773, 39.7262, 37.0353,
        38.9080, 37.4393, 40.5202, 32.5827, 39.0420, 40.6199, 39.6789, 37.8841,
        37.8615, 36.4641, 41.9365, 39.9603, 38.9226, 42.9503, 43.4927, 35.9021,
        37.0009, 37.9621, 36.1556, 36.6349, 35.7044, 38.6612, 39.1797, 43.7082,
        38.1479, 37.5614, 38.7034, 33.3019, 34.7814, 32.1953, 39.6470, 37.7895],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([20.7776,  7.5930, 18.9414,  7.3224,  7.4558, 17.2422, 41.1816, 19.2961,
        15.0621, 16.2091, 10.7787, 94.0211, 10.9897,  8.7771,  9.1047,  3.3417,
         4.8667,  9.2708,  5.0082, 10.7429,  3.8609,  4.5894,  5.6993,  7.4343,
        10.8392,  9.5647, 12.3067,  6.7042,  7.1180,  4.1929,  7.1782,  3.5645,
         5.2742,  4.0683,  5.4596,  4.0937,  5.3665,  5.1819,  4.1347,  3.4644,
         2.0073,  2.8329,  3.9197,  3.9387,  4.0784,  2.4525,  5.1867,  5.4384,
         2.8921,  6.1926,  4.0230,  3.4655,  6.6809,  3.2989,  3.6668,  5.6659,
         5.2799,  5.3009,  4.9870,  3.8672,  3.4284,  4.4009,  3.7928,  3.7563,
         5.7494,  2.8629,  6.0799,  6.9862,  5.6205,  4.1786,  6.3742,  4.3592,
         6.3416,  4.0141,  8.6460,  4.2895,  5.2124,  3.3103,  3.9223,  6.8684],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 63.0828,  19.4701,  46.8949,  16.9102,  15.2315,  36.9277,  83.3575,
         42.9188,  35.1341,  36.6473,  24.5132, 203.2983,  25.2578,  19.5716,
         19.1616,   9.0041,  12.3014,  19.4270,  12.2013,  22.5454,   8.2917,
          9.6699,  12.3892,  15.6061,  26.8218,  21.5455,  29.8652,  14.2990,
         16.4211,  10.6784,  18.6040,   9.2067,  11.5698,  10.2151,  13.1866,
         10.4558,  13.8446,  13.7953,   9.9127,   8.9880,   4.6612,   6.3725,
          9.1481,   8.6288,   8.5584,   5.2052,  11.0317,  12.4730,   6.3303,
         13.9640,   8.4724,   9.1784,  14.5755,   6.9017,   7.7467,  12.6171,
         11.9701,  12.3068,  12.7342,   8.1565,   7.6219,   9.1257,   9.4455,
          8.7762,  13.0979,   6.4152,  14.3488,  15.6132,  13.4977,   9.3000,
         13.8109,  10.2520,  14.3544,   9.2139,  19.1072,  11.0094,  12.6794,
          8.8056,   8.5143,  15.7133], device='cuda:0')
Outer loop valEpocw Maximum [22/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 49.4
model_train val_loss valEpocw [22/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 94.0
model_train val_loss  valEpocw [22/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 203.3
Max_Val Meta Model:  tensor([28.9723, 40.5560, 41.7341, 42.2990, 41.8793, 37.2941, 41.3683, 39.2181,
        39.1916, 38.5963, 36.1216, 40.8589, 37.8268, 35.5255, 41.3153, 31.6161,
        41.9425, 41.6907, 35.9316, 42.2529, 40.0403, 42.8726, 42.9481, 42.4606,
        34.7918, 41.9781, 35.3228, 41.1543, 36.6746, 33.4154, 33.4780, 33.4761,
        35.9156, 33.5438, 35.5252, 33.3740, 33.3448, 32.3747, 35.7503, 41.5889,
        36.4109, 38.2814, 37.2836, 39.6977, 40.6715, 40.6843, 39.3235, 37.2414,
        39.8010, 36.6465, 42.2820, 32.7513, 39.4814, 39.7842, 39.0254, 39.0734,
        39.1927, 36.4294, 41.9638, 40.0274, 40.1223, 42.6102, 41.4162, 35.7925,
        37.1113, 38.0604, 36.1666, 36.3848, 35.7013, 38.6418, 39.2677, 41.8076,
        38.1250, 37.7624, 38.6838, 33.3964, 34.8315, 32.1965, 39.6998, 37.8405],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 5.9695,  5.2378,  3.3827,  6.6003,  2.3405,  2.9924,  2.0480,  6.1153,
         3.5063,  2.7799,  8.2395,  4.8093, 11.0763,  6.9311,  2.1603,  3.4179,
         2.6609,  2.7564,  6.3479,  3.5245,  4.5757,  4.4631,  5.3291,  3.1193,
         4.8476,  6.7466, 10.9226,  6.0972,  5.9976,  5.3558,  5.1953,  5.4458,
         5.7367,  3.8440,  6.8145,  4.4611,  7.1128,  6.8537,  4.5828, 20.7426,
        12.9814, 30.8154, 31.7779, 32.4404, 25.5893, 32.9524,  9.3544,  9.4166,
        19.6183,  8.5422, 16.4100, 17.5193, 24.9258, 12.4281, 30.3634, 46.1643,
        32.6274, 10.1616,  6.1874,  8.9586, 32.1473,  8.3305,  9.2253, 10.6045,
         9.5819,  6.3921,  8.9666, 11.2765,  7.7573,  5.7213,  7.5511,  9.9641,
         7.5828, 20.3961,  3.3277,  8.0263,  8.2895,  4.3021,  4.9958,  8.2151],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 18.1272,  13.4167,   8.4802,  15.3589,   4.7840,   6.6601,   4.2669,
         13.3094,   8.0034,   6.2865,  19.0150,  10.1184,  25.4320,  16.2625,
          4.6766,   9.2319,   6.7304,   5.6651,  15.3953,   7.3651,   9.8082,
          9.2272,  11.2461,   6.6070,  12.0139,  14.6895,  26.4953,  12.8375,
         13.6683,  13.6375,  13.4456,  14.0644,  12.8213,   9.6383,  16.4578,
         11.4116,  18.3249,  18.2381,  10.9941,  53.8193,  30.1502,  69.2223,
         73.9410,  71.2640,  53.7267,  69.5330,  19.9564,  21.5088,  42.7210,
         19.4590,  33.9546,  46.3751,  54.3294,  26.2382,  65.2012, 102.5852,
         73.7366,  23.5700,  15.7835,  18.8707,  71.5815,  17.3460,  23.4007,
         24.8357,  21.8234,  14.2929,  21.1821,  25.3067,  18.6443,  12.7410,
         16.3296,  23.8949,  17.1805,  46.7663,   7.3572,  20.5772,  20.1675,
         11.4480,  10.8347,  18.7748], device='cuda:0')
Outer loop valEpocw Maximum [22/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 42.9
model_train val_loss valEpocw [22/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 46.2
model_train val_loss  valEpocw [22/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 102.6
Max_Val Meta Model:  tensor([35.9002, 39.6173, 40.1530, 40.9435, 40.1354, 36.8378, 40.8811, 37.3149,
        37.1117, 38.3070, 35.7965, 38.9623, 37.3026, 33.7481, 40.8921, 31.3625,
        41.5182, 39.8385, 35.3944, 40.0948, 39.6093, 39.4525, 40.7888, 40.5743,
        32.2818, 36.2866, 35.0695, 39.0828, 34.9134, 32.5460, 32.5762, 32.6124,
        36.7646, 32.5101, 30.3598, 32.9589, 32.7111, 31.7942, 31.6043, 41.1619,
        36.3187, 35.8706, 36.2789, 34.7761, 35.2392, 35.9490, 38.5339, 33.8019,
        39.1871, 33.7305, 37.8610, 32.0102, 35.2030, 37.7199, 38.9883, 37.2787,
        40.3303, 33.9668, 40.0186, 40.5936, 35.0419, 40.4355, 41.5656, 34.5738,
        37.6035, 33.2217, 35.8257, 36.7671, 35.6450, 38.2607, 39.4879, 40.6099,
        37.9916, 35.7745, 48.7741, 32.3724, 34.4597, 31.7471, 39.5288, 39.1399],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 11.0107,   4.3930,  12.4095,   5.9827,   7.6008,   8.9511,   7.3625,
         12.2350,   4.3194,  11.6030,   8.5281,   5.2979,  10.3223,   7.9274,
         10.2623,   2.8248,   2.6328,   5.4231,   5.4546,   5.1487,   4.1954,
          5.3288,   6.9764,   8.2313,   8.4801,   4.7115,   8.9610,   2.4352,
          7.6545,   4.5119,   6.3031,   3.8527,   4.1384,   4.1813,   5.4075,
          3.9428,   4.2823,   5.0846,   3.5957,   4.2619,   2.4559,   4.0536,
          4.7219,   5.2478,   4.9938,   3.8750,   6.1910,   5.6942,   3.5262,
          5.9128,   4.9121,   3.7741,   7.0093,   4.0850,   4.2568,   5.0496,
          4.4770,   4.9894,   6.4127,   6.0514,   4.8405,   6.7001,   4.3379,
          4.4208,   6.2979,   3.0755,   6.6164,   4.5217,   6.1603,   3.8717,
          7.0674,   6.6808,   5.8997,   5.6451, 107.0130,   5.3070,   5.6759,
          4.4908,   4.3546,   7.6082], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 32.6726,  11.4987,  32.2171,  14.1824,  15.7755,  20.1686,  15.3811,
         27.3475,  10.1032,  26.2890,  19.7811,  11.3649,  23.9276,  19.1393,
         22.2743,   7.6646,   6.7124,  11.2585,  13.3544,  11.0071,   9.0267,
         11.4404,  14.9880,  17.8287,  21.8923,  10.7268,  21.7791,   5.2173,
         17.8673,  11.7218,  16.6757,  10.1433,   9.1863,  10.7513,  14.3970,
         10.1491,  11.1530,  13.6811,   9.0762,  11.1751,   5.6365,   9.2506,
         11.0131,  12.3199,  11.2326,   8.5859,  13.3985,  13.7485,   7.7108,
         14.0315,  10.6877,  10.1114,  16.1714,   8.8030,   9.0648,  11.3520,
         10.3750,  11.9430,  16.9525,  12.6683,  11.2153,  14.1942,  11.0158,
         10.6177,  14.3212,   7.3324,  15.6770,  10.1228,  14.7395,   8.6466,
         15.1126,  16.3157,  13.4385,  13.3417, 241.4240,  13.9374,  13.8596,
         12.0435,   9.4203,  17.2086], device='cuda:0')
Outer loop valEpocw Maximum [22/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 48.8
model_train val_loss valEpocw [22/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 107.0
model_train val_loss  valEpocw [22/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 241.4
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [88.43703171 97.2137279  91.55102886 97.02489005 97.26733349 96.5997003
 96.99808726 94.59801903 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.11435046 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.73157003 97.84237521 92.59877438
 96.90915072 96.22689782 96.9627563  93.95962525 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 90.78714928 96.13796128 96.24273583 96.9067141
 91.92992288 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.13529319 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [85.42415419 97.2137279  90.64095223 97.02489005 97.26733349 96.5997003
 96.99808726 94.73203299 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.43308439
 96.90915072 96.22689782 96.9627563  93.87312533 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.66676819 96.13796128 96.24273583 96.9067141
 91.76666951 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [96.92589526  4.85694062 57.98255294  5.12652962  9.19267948 17.98497448
  9.18526759 32.6628641   4.14460522 24.32904279  1.74684007  1.30492617
  0.98946371 11.67437835  5.36420991  4.44074397  4.73450195  3.80722399
  0.86715045  1.92457507  1.419025    0.52015973  1.15656184 13.77457941
 16.11210613  8.23031981 26.41446836 10.17756617  4.29946302  1.3747571
 30.69418879  0.94840421 28.9859798   1.94220511  1.68455414  3.92298317
  4.92944902  2.66777422 14.37772673 29.58853872 10.37846922 45.48374223
 19.65055504 26.40075087 24.54168936 36.27149503  2.96910138  1.74889095
  5.74953126  1.77853261  2.33038286  1.16768029  1.08164213  8.4959515
  1.78607148  3.38064018 62.2782258  13.14775598  9.47238128 13.03318184
 62.48661991 11.93478413 20.31142041 11.06838029  3.67633782  8.23844077
  3.94593893 10.2820192   3.16584959 11.87423203  0.48079855 36.83451421
  5.59696675 24.91753671 15.31555848  6.33332341  1.24601228  2.18523464
  0.22750063  0.89928889]
Accuracy th:0.5 is [45.60251459 97.2137279  71.77178641 97.02489005 97.26733349 76.5780144
 76.97396474 76.13333171 77.7488091  96.37431318 78.15450591 98.52097319
 99.41399349 80.16715196 77.72566124 96.56680596 96.29512311 77.54413323
 98.65376884 98.30776915 80.12451115 78.68447022 98.38695922 77.61723176
 81.04798918 96.65086926 94.0778012  77.22859127 98.01293844 78.04242151
 97.30875598 98.57457877 96.36213009 98.02024829 86.11493525 77.71956969
 77.46494317 88.77328493 97.11504489 74.75907945 79.18763173 92.05906361
 76.94594364 76.52440882 96.9627563  93.87434364 98.02877645 98.57336046
 90.66897333 86.3171745  86.32570266 98.55508583 98.99976852 77.11285194
 98.70615611 77.49052765 72.2591099  92.4062816  96.24273583 96.9067141
 89.79300934 97.17717864 91.49133173 77.54535154 98.42838172 77.99490747
 98.20786784 76.70837344 78.59553368 97.49028399 79.11940644 95.99054592
 78.31654098 95.45083515 76.90208453 81.79846737 85.94437202 78.19714672
 79.17423033 99.14718388]
Accuracy th:0.7 is [45.6731765  97.2137279  71.77178641 97.02489005 97.26733349 76.5780144
 76.97396474 76.41963426 77.7488091  96.46203141 78.15450591 98.52097319
 99.41399349 80.66422193 77.72566124 96.56680596 96.29512311 77.54413323
 98.65376884 98.30776915 80.59599664 78.68447022 98.38695922 77.8596752
 81.68760127 96.65086926 94.0778012  77.22859127 98.01293844 78.04242151
 97.30875598 98.57457877 96.36213009 98.02024829 86.36468854 77.71956969
 77.46494317 89.08882689 97.11504489 74.75907945 79.87110293 92.05906361
 76.94594364 76.52440882 96.9627563  93.87434364 98.02877645 98.57336046
 92.79004885 87.26867363 86.5121039  98.55508583 98.99976852 77.11285194
 98.70615611 77.49052765 72.2591099  92.81319672 96.24273583 96.9067141
 89.79300934 97.17717864 91.66554988 77.54535154 98.42838172 77.99490747
 98.20786784 76.70837344 78.59553368 97.55850928 79.11940644 95.99054592
 78.38720288 95.45083515 76.90208453 81.88984052 86.11493525 78.19714672
 79.17423033 99.14718388]
Avg Prec: is [56.09656826  3.10272667 11.15335994  3.28449711  2.28808676  3.71135549
  3.25389685  5.57671583  2.56583302  3.75616489  1.59028257  1.64155207
  0.63259119  5.05031943  2.67342394  3.10790101  3.57333145  2.69249339
  1.37171602  1.73119785  1.9607339   0.88247512  1.87613562  2.42817263
  5.13568246  3.69940193  6.64163067  3.28533161  2.02970942  1.8251049
  2.7400128   1.35197258  3.69023837  1.64631273  2.41388857  2.45667443
  3.11241996  2.59894087  2.80915343  7.31975785  2.31892921  8.270442
  3.31814573  4.04843497  3.27031101  6.46141471  2.02076804  1.50860121
  2.16005753  1.54183707  1.83224582  1.61643245  1.13923677  3.08031899
  1.33991286  2.78521626 11.29180874  3.6297545   3.84473717  2.90127856
 10.84880115  2.14467635  3.77808516  3.0712728   1.60100284  2.45676852
  1.80364991  4.12501348  1.23966099  2.35609163  0.19900938  3.31426885
  1.8802716   4.49393438  3.86303529  3.06587826  0.8056064   1.82642952
  0.13419372  0.7293433 ]
mAP score regular 12.93, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [89.58566908 97.22450607 92.26399581 96.96290206 97.90716795 96.63651992
 96.80843112 94.53870494 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.31447293 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.81211849 97.82744101 92.57044622
 97.07750953 96.48952338 97.03764606 93.98559932 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 89.28171014 96.39235618 96.16314124 96.78102499
 91.29730672 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 94.99713481 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [88.50188106 97.22450607 91.64112913 96.96290206 97.90716795 96.63651992
 96.80843112 94.90245908 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.8868625
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.07805765 96.39235618 96.16314124 96.78102499
 92.45833022 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [97.1448565   5.2390702  63.67817268  5.27069762  8.29816773 17.48474941
  9.61633985 33.79408787  5.1713063  27.41772125  1.66961274  1.19647064
  0.89284412 12.85222604  6.18480539  5.07450727  5.32340646  4.00844754
  0.78779435  2.16970439  1.31778457  0.54151179  1.21212599 14.91087576
 16.04991972  9.27150391 25.98763665 10.29775667  4.99851933  1.44625012
 39.44429116  0.77309771 33.92699577  2.02886831  1.4164761   3.55600651
  4.64606266  3.62627064 20.3951083  31.94969941 12.14216594 45.88523962
 20.13397395 27.02716859 24.83877206 36.82379824  3.17841314  1.81704677
  6.89786419  1.78941297  2.75775348  1.29726558  1.28271951 10.11141412
  1.92051476  3.29290249 57.97868318 11.1479616   8.87677738 15.22106883
 62.87283818 12.73362418 22.02745452 11.13066734  4.04847275  7.78610793
  4.23208402 10.79078747  3.07590117 14.53003698  0.51035004 39.20029551
  5.64970787 27.0726656  21.2797202   5.85283327  1.26402028  2.28994977
  0.20777134  0.86166917]
Accuracy th:0.5 is [45.54401176 97.22450607 69.83830381 96.96290206 97.90716795 75.24478661
 75.35191968 74.33540125 76.83932531 96.41228791 76.96888158 98.5325261
 99.34972718 77.6042056  76.90161198 96.31262924 96.21047911 76.33355757
 98.78167277 98.34068316 78.30181628 77.6191544  98.31327703 76.4207589
 77.82594613 96.52938685 94.3393876  76.4207589  97.81747515 76.93649251
 97.52597354 98.67204823 96.39983058 98.18870369 86.32184767 76.55778957
 76.5403493  90.71928644 97.0276802  73.7399407  77.11089518 92.37362035
 75.68079328 75.34444527 97.03764606 94.02795426 98.18621222 98.77668984
 91.41191419 86.14993647 84.50058549 98.55993223 98.87385704 75.67082742
 98.6969629  76.36345517 70.53591449 93.54460971 96.16314124 96.78102499
 90.13379176 97.04761193 91.48914966 76.4058101  98.32075143 77.26038319
 98.13139996 75.58113461 77.6415776  97.50604181 78.05017814 96.07843137
 77.39990532 95.44559882 75.50389915 82.39529611 87.50529437 77.00376211
 78.12492214 99.15040985]
Accuracy th:0.7 is [45.82803897 97.22450607 69.83830381 96.96290206 97.90716795 75.24478661
 75.35191968 74.49485512 76.83932531 96.41976231 76.96888158 98.5325261
 99.34972718 78.02526347 76.90161198 96.31262924 96.21047911 76.33355757
 98.78167277 98.34068316 78.62819842 77.6191544  98.31327703 76.5029773
 78.23952961 96.52938685 94.3393876  76.4207589  97.81747515 76.93649251
 97.52597354 98.67204823 96.39983058 98.18870369 86.63078955 76.55778957
 76.5403493  90.92607818 97.0276802  73.7399407  77.60918853 92.37362035
 75.68079328 75.34444527 97.03764606 94.02795426 98.18621222 98.77668984
 93.50972918 86.64324688 84.6500735  98.55993223 98.87385704 75.67082742
 98.6969629  76.36345517 70.53591449 93.85604305 96.16314124 96.78102499
 90.13379176 97.04761193 91.67102673 76.4058101  98.32075143 77.26038319
 98.13139996 75.58113461 77.6415776  97.53344794 78.05017814 96.07843137
 77.42232852 95.44559882 75.50389915 82.47004011 87.67222264 77.00376211
 78.12492214 99.15040985]
Avg Prec: is [54.5097351   3.72495037 14.85046802  4.54379503  1.49037008  4.27115412
 12.32643992  8.66377248  7.62437311  5.16194314  2.29858827  5.08396673
  1.5479509   5.88966986  3.09358146  3.72773348 24.71327561  6.34759988
  1.5586982   2.65835279  3.58205305  1.42827053  1.15767913  5.71167154
  5.7122334  10.66589243  8.11707371  4.49663483  3.88417977  6.68288936
  2.32589848  0.86308295  3.08799677  1.10719947  1.69559784  2.38479753
  2.01882684  2.19150306  2.2457081   6.22424603  1.73461193  6.02307489
  2.19879438  2.73432121  2.36939323  4.87574414  1.75376103  1.05164588
  1.37641088  1.17030348  1.20280291  0.97296885  0.73923848  2.33770141
  0.85517565  1.84542358 10.08498051  2.89675768  3.89441233  2.73226236
  7.86776442  2.0375953   3.22668232  2.52406127  1.35169392  1.83079177
  1.55247402  3.42954605  1.08583877  2.2388974   0.19406295  3.19511668
  1.58046049  3.92348624  3.53078301  2.33203246  0.59523286  1.51930668
  0.12809715  0.59068057]
mAP score regular 13.66, mAP score EMA 4.34
Train_data_mAP: current_mAP = 12.93, highest_mAP = 12.93
Val_data_mAP: current_mAP = 13.66, highest_mAP = 13.66
tensor([0.3311, 0.3876, 0.3950, 0.4254, 0.4813, 0.4507, 0.4799, 0.4560, 0.4344,
        0.4425, 0.4343, 0.4692, 0.4379, 0.4220, 0.4611, 0.3719, 0.3934, 0.4796,
        0.4135, 0.4725, 0.4676, 0.4703, 0.4671, 0.4683, 0.3898, 0.4435, 0.4140,
        0.4692, 0.4324, 0.3940, 0.3870, 0.3888, 0.4533, 0.3996, 0.3730, 0.3929,
        0.3887, 0.3766, 0.4026, 0.3840, 0.4308, 0.4462, 0.4305, 0.4259, 0.4479,
        0.4622, 0.4703, 0.4181, 0.4591, 0.4280, 0.4591, 0.3803, 0.4406, 0.4700,
        0.4742, 0.4484, 0.4417, 0.4236, 0.3863, 0.4794, 0.4400, 0.4704, 0.3932,
        0.4283, 0.4471, 0.4237, 0.4255, 0.4552, 0.4174, 0.4404, 0.4635, 0.4134,
        0.4462, 0.4307, 0.4634, 0.3928, 0.4123, 0.3781, 0.4611, 0.4445],
       device='cuda:0')
Max Train Loss:  tensor([13.5414,  6.8621,  9.9387,  6.5002,  6.2152,  6.9868,  6.8815, 11.9565,
         9.3167,  9.2600,  9.1747,  6.1732, 10.1553, 10.2686,  7.1881,  6.0278,
         6.4332,  4.5075,  8.8115,  8.2203, 12.7908,  4.7657,  9.5134,  6.5979,
        12.2721, 11.6468, 10.8803,  7.4027,  7.5628,  6.0622, 10.5757,  9.7645,
         9.8619,  7.1777,  6.0691,  5.3520, 13.8346,  9.3812,  7.6896,  9.7404,
         3.3907,  7.4102,  8.1473,  7.8348,  7.6424,  9.1541,  9.6428,  6.6830,
         5.1001,  6.8583,  7.0196,  3.7767,  7.7349,  6.5337,  5.6638,  8.1086,
        11.0754,  7.1041,  7.3708,  9.4532, 10.8740, 11.3309,  6.8333,  8.1773,
         7.9692,  6.7627,  7.2765,  6.1704,  6.8117,  6.6556,  6.7700,  6.4811,
         9.2236,  8.9572,  8.1084,  6.0408,  5.5909,  5.5576,  4.2488,  7.3948],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [23/80], Step [000/642], LR 1.0e-04, Loss: 13.8
Max Train Loss:  tensor([13.2246,  7.8908, 19.1865, 13.1772, 17.2587, 13.4107, 15.5698, 18.2356,
         8.9879, 12.8164, 11.2045,  9.9183,  5.2263, 14.0021, 14.9550,  5.3185,
         6.8396, 10.1657,  9.5753, 12.5294, 12.2557, 16.3874,  8.0831, 15.5260,
        12.8473, 10.5866,  9.4871, 16.1565,  9.7684,  8.1598,  8.4835,  6.3236,
        11.8154,  8.7768,  8.0343, 10.2532,  8.8613, 10.4978,  7.2221, 13.0742,
        13.1666, 10.3680,  9.2033,  8.1801,  7.7086, 10.7687, 13.0512,  8.0019,
        10.9786, 15.0137, 12.1346,  6.5750,  9.2578,  6.9410, 11.5153,  9.5063,
        14.1607,  7.8839,  6.5336, 10.7999, 14.6690,  8.9609,  9.4848,  6.8163,
        11.1251,  8.2466, 10.0446, 11.7567, 16.0697, 17.7224,  8.1518, 10.4601,
        13.6083, 10.6005, 14.7910,  7.1642,  6.1159,  5.5147, 17.4835,  7.7597],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [23/80], Step [100/642], LR 1.0e-04, Loss: 19.2
Max Train Loss:  tensor([10.9439,  5.4191, 13.2068,  7.9062, 10.0098,  8.6412, 10.5610,  8.6556,
         9.3319,  7.7990,  7.3715,  7.8901,  5.8874, 13.7407,  8.7963,  8.5672,
         6.9228, 10.5382,  8.4734,  7.1266, 11.4532,  6.3010,  9.7890, 14.3276,
        11.7080,  9.8213,  9.1561, 12.6804,  9.1296,  6.8082, 10.1482,  8.3903,
        11.5230,  6.2605,  8.7852,  8.7322,  9.0097,  6.1967,  8.8455,  8.7624,
         9.0211, 11.0421,  9.5449,  7.7000,  6.8953, 10.3128,  9.2199,  8.3830,
         8.4294,  6.5012,  9.3438,  4.6287,  6.3159,  6.5962,  7.5369,  8.8071,
        10.6655,  7.2579,  8.7780,  7.1072, 11.0615, 10.2639,  8.1715, 11.1329,
        13.0761,  5.7301,  9.9798,  9.8111,  5.3294,  6.1377,  7.5755, 10.7651,
         8.5334,  9.4364,  6.9162,  8.2879,  5.5422,  3.4037,  7.5556,  5.5199],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [23/80], Step [200/642], LR 1.0e-04, Loss: 14.3
Max Train Loss:  tensor([10.4318,  8.5196, 12.0342,  7.0981,  6.9062,  7.1737,  7.8614, 10.1159,
         7.9903,  8.2860,  9.8294,  7.6733,  3.6210, 14.4708,  7.3900,  7.2758,
         8.3524,  5.4463,  6.6580,  8.0033,  6.6186,  9.2258,  8.3311,  7.7359,
        10.1164,  5.8654, 10.0039,  8.0600,  9.5216,  8.5241,  6.5037,  6.8751,
         9.7128, 10.1240,  4.1624,  7.3431,  9.3017,  8.9557,  7.1555, 11.0272,
         8.4734,  9.0729, 10.6975, 10.4955,  7.5693,  9.8843,  9.5028,  6.7076,
         8.7963,  7.5369,  8.7894,  6.5159,  6.1691,  6.7117,  9.3306, 12.2416,
        15.5436,  9.6007,  9.0884,  6.1009, 14.3328,  6.5309,  7.2549,  8.4890,
        11.6081,  6.5567,  9.0654, 10.2574,  5.1814,  6.8538,  7.4319,  8.4205,
         7.9104, 10.4331, 11.8303,  6.4769,  6.3201,  5.9923,  6.1422,  8.4918],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [23/80], Step [300/642], LR 1.0e-04, Loss: 15.5
Max Train Loss:  tensor([12.6370,  5.2946, 10.7544,  5.6881, 10.4566,  8.9839,  6.2900,  9.3548,
         7.1058,  9.1400,  7.6252,  8.7397,  3.7477, 11.3719, 13.0201,  8.3803,
         8.0823,  8.0280,  9.6471,  8.1136,  7.2324,  8.4691,  8.4512,  5.9006,
         7.9000,  9.7053, 10.9536,  5.7903,  6.5337,  6.7642,  8.5757,  7.2035,
         8.9267,  6.8067,  1.8503,  5.3860,  6.1866,  8.6865,  8.6915,  9.5036,
         6.9779,  8.5652,  9.4165,  8.2013,  9.5674, 10.2224,  8.0922,  8.0168,
         7.2325,  5.5807,  5.2407,  4.6776,  6.2996,  4.9540, 11.4585,  7.8644,
         8.5394, 10.0127, 13.4449,  5.7648, 10.7424,  8.1563,  6.7366,  6.0731,
        10.4537,  5.8024,  9.2267, 12.6290,  8.7854,  6.8755,  7.5615, 10.0648,
         8.1556,  9.3390,  8.3686,  9.5540,  6.3105,  5.2373,  6.2762,  6.4848],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [23/80], Step [400/642], LR 1.0e-04, Loss: 13.4
Max Train Loss:  tensor([13.2964,  4.1938,  6.2268,  8.4791,  8.4103,  6.8778, 13.9050,  5.9430,
         9.8278,  7.8930,  4.5720,  6.9812,  3.8279,  8.1350, 11.8038, 10.9642,
         5.3785,  9.6631,  6.8531,  8.2097,  5.8878,  7.7787,  8.6520,  6.6564,
        10.5174,  7.5219, 13.5630,  8.3664,  8.9614,  8.7262,  6.5660,  6.9968,
         7.6213,  4.9953,  4.5395,  7.2693,  7.0513,  7.6184,  6.7585, 11.7567,
         9.5104, 13.4334,  7.6087, 10.1599,  9.2048,  9.8166,  8.4585, 10.5916,
         6.9726,  8.9819,  4.4090,  7.7469,  7.1986,  9.1493,  9.5526, 11.7880,
        11.4186, 12.1784,  6.3366,  8.3683, 16.7486,  7.5865,  5.9653,  9.9688,
        10.7062,  5.7741,  9.5057, 10.7664,  8.9363,  9.8282,  7.6439,  9.3053,
         7.2715, 10.1268, 15.3689, 10.2159,  7.3817,  4.8990,  6.3640,  7.6092],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [23/80], Step [500/642], LR 1.0e-04, Loss: 16.7
Max Train Loss:  tensor([10.0726,  6.3414,  8.7853,  7.5008,  6.7868,  8.6194,  4.7430,  7.9823,
         7.2221,  5.9450,  4.5103,  9.8237,  3.7723,  8.3439,  5.7002,  6.2775,
         8.5439,  6.5078,  7.8058,  6.1965,  7.0640,  6.2648,  8.8911, 10.5447,
         8.0904, 10.1729, 14.5828, 10.2733,  7.3244,  5.8255,  6.5956,  7.6074,
        12.1799,  8.8816,  7.8277,  7.1312,  8.9531,  7.4935, 11.2817, 10.4150,
        10.8697, 11.7250,  7.3592,  9.6838,  9.0422, 12.6353,  7.3452,  6.5811,
         6.2851,  8.0147,  6.7014,  5.3919,  6.3107,  8.2656,  8.3849,  8.2232,
        10.0025,  9.4287,  9.1529,  6.5640, 11.6882,  6.9905,  6.8283,  6.8243,
         3.4511,  8.2674,  9.4185,  9.2459,  9.6106,  9.9491,  7.5722, 10.7631,
         8.0228,  7.4371,  6.9455,  9.0855,  7.1336,  6.9926,  6.2940,  5.5230],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [23/80], Step [600/642], LR 1.0e-04, Loss: 14.6
Max_Val Meta Model:  tensor([ 17.9417,  25.4233,  33.3783,  19.1355,   5.2515,   8.0756,   7.3252,
          8.4925,   6.0736,   7.7823,   5.6031,   8.1246,   4.6655,   9.1505,
          4.8957,   4.9847, 130.4090,   4.2709,   5.7920,   4.0271,   5.0758,
          6.5753,   5.5661,   6.0694,  11.7878,  10.6051,  11.2345,   4.8443,
          7.5415,   6.7461,   6.1520,   5.0752,   7.2487,   4.1029,   2.0044,
          5.2913,   5.8401,   6.1530,   5.1139,  17.3629,   9.6196,  11.5698,
          6.8153,   8.7445,   8.9614,  11.3831,   5.0733,   6.7646,   5.4255,
          6.6512,   4.5151,   3.8331,   8.2752,   3.6203,   7.8058,   4.6833,
         10.0595,   8.5906,  10.3960,   5.4777,   8.1078,  12.3452,   5.3923,
          6.3919,   1.8942,   5.1850,   8.8970,   6.0525,   8.0938,  12.7272,
          7.8611,  10.9374,  10.6662,   6.8227,   8.6211,   7.0757,   5.7724,
          4.3980,   6.5607,   5.7632], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 15.1647,  23.9534,  26.2112,  18.6669,   5.9614,  10.6895,   8.6374,
          9.3338,   6.9558,   9.4218,   6.2834,   9.0106,   5.4071,   9.9256,
          5.4016,   5.5800, 121.7737,   5.0874,   6.5752,   4.5720,   5.8225,
          7.4082,   6.3894,   6.9962,  11.8629,  12.1953,  12.2228,   5.2577,
          8.2609,   7.2811,   6.5740,   5.7925,   7.5574,   4.6426,   2.3922,
          5.8297,   6.5621,   6.5759,   5.7468,  17.2559,  10.3100,  11.5791,
          6.5689,   8.9132,   8.7009,  10.4315,   5.6635,   7.4863,   6.0065,
          7.3538,   4.9724,   4.4415,   8.7228,   3.5157,   8.6852,   4.7019,
          9.9540,   8.7693,  10.8564,   6.1083,   6.8187,  11.9370,   5.9230,
          7.1148,   2.2595,   5.8974,   9.8348,   6.6860,   8.7352,  12.9557,
          8.8098,  11.2202,  10.9696,   7.3176,   8.8813,   7.5536,   6.5490,
          4.8728,   7.4267,   6.5721], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 45.7950,  61.8037,  66.3629,  43.8758,  12.3862,  23.7187,  17.9983,
         20.4670,  16.0108,  21.2936,  14.4668,  19.2058,  12.3467,  23.5182,
         11.7149,  15.0025, 309.5471,  10.6080,  15.9003,   9.6760,  12.4527,
         15.7526,  13.6784,  14.9400,  30.4360,  27.4965,  29.5208,  11.2064,
         19.1038,  18.4784,  16.9881,  14.8976,  16.6714,  11.6169,   6.4131,
         14.8358,  16.8824,  17.4622,  14.2761,  44.9373,  23.9340,  25.9521,
         15.2580,  20.9303,  19.4260,  22.5689,  12.0411,  17.9042,  13.0820,
         17.1811,  10.8318,  11.6802,  19.7999,   7.4798,  18.3166,  10.4865,
         22.5369,  20.7026,  28.1014,  12.7414,  15.4960,  25.3765,  15.0652,
         16.6104,   5.0542,  13.9172,  23.1160,  14.6870,  20.9300,  29.4192,
         19.0065,  27.1438,  24.5853,  16.9898,  19.1653,  19.2319,  15.8831,
         12.8875,  16.1080,  14.7866], device='cuda:0')
Outer loop valEpocw Maximum [23/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 130.4
model_train val_loss valEpocw [23/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 121.8
model_train val_loss  valEpocw [23/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 309.5
Max_Val Meta Model:  tensor([35.2765, 35.6057, 41.8852, 40.1582, 41.9467, 40.5954, 43.1438, 37.4648,
        37.1337, 40.2259, 35.4302, 48.9097, 37.8872, 37.2067, 44.2199, 40.2237,
        33.0030, 40.5342, 36.8203, 42.1781, 40.5881, 40.5531, 39.1467, 43.3340,
        34.0093, 38.6210, 35.4585, 39.6242, 34.0351, 33.7121, 33.2566, 33.4595,
        38.8249, 33.7096, 31.7191, 33.5886, 33.4157, 32.1494, 34.4413, 36.8906,
        36.6960, 39.8405, 36.3367, 36.7786, 37.5309, 38.9527, 39.6973, 35.3460,
        39.0212, 36.3204, 39.1125, 32.8552, 37.5315, 39.9135, 40.2549, 36.4801,
        39.9831, 36.5914, 30.7303, 39.6115, 38.0849, 40.9172, 33.1991, 36.9381,
        37.2586, 36.4480, 36.3524, 38.4726, 35.6860, 37.5908, 39.5296, 35.6429,
        36.1549, 37.0783, 36.9797, 33.4356, 35.1281, 32.2738, 39.7011, 38.5469],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([20.0236,  6.9619, 21.3446,  7.8637,  6.8640, 19.8434, 38.6124, 18.3222,
        15.5244, 18.6968,  8.7467, 77.0409,  6.0211,  6.4337,  7.2921,  4.3985,
         5.3051,  8.1213,  5.1901, 10.0863,  4.9974,  5.8110,  5.4019,  7.6840,
        10.0409, 11.4968, 12.1671,  7.1396,  7.1462,  4.3297,  4.6322,  4.3911,
         3.5079,  3.3814,  1.6247,  4.2544,  5.0887,  4.1812,  4.0096,  3.1847,
         6.3617,  2.3351,  3.0759,  2.8264,  3.1711,  2.4781,  4.0037,  5.0971,
         4.2588,  5.6305,  3.4840,  3.3003,  5.7309,  2.2020,  6.7727,  3.4402,
         5.7831,  4.9068,  5.3353,  3.0977,  3.3193,  3.7236,  3.6389,  4.7565,
         1.4513,  4.3990,  7.9333,  6.5015,  4.7891,  5.4578,  7.0250,  4.4586,
         4.6791,  3.8206,  9.6458,  3.8186,  5.0848,  3.0616,  5.7305,  5.0536],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 60.4357,  17.6911,  53.8460,  18.1718,  14.0545,  43.1653,  78.5869,
         41.0144,  35.7117,  41.1794,  20.6030, 167.4646,  13.7176,  14.9726,
         15.2516,  11.5587,  13.8166,  16.8080,  12.2582,  20.8481,  10.6116,
         12.3621,  11.6804,  15.9987,  25.5552,  25.8481,  29.4993,  15.1880,
         17.1747,  10.9392,  12.0466,  11.3714,   7.7210,   8.4459,   4.3605,
         10.8428,  13.1095,  11.1802,   9.9929,   8.1829,  14.6949,   5.0414,
          7.2099,   6.6379,   7.0845,   5.3300,   8.5212,  12.2623,   9.3369,
         13.0955,   7.6084,   8.6764,  13.0123,   4.6702,  14.1840,   7.9472,
         12.6903,  11.6338,  14.5464,   6.4474,   7.5459,   7.8944,   9.4774,
         11.0520,   3.2537,  10.3080,  18.6806,  14.1718,  11.5036,  12.5312,
         15.1164,  10.7708,  10.7111,   8.8622,  21.2653,   9.7658,  12.2579,
          8.1304,  12.4722,  11.3219], device='cuda:0')
Outer loop valEpocw Maximum [23/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 48.9
model_train val_loss valEpocw [23/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 77.0
model_train val_loss  valEpocw [23/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 167.5
Max_Val Meta Model:  tensor([29.1184, 35.1106, 40.2285, 39.4601, 41.7132, 40.2205, 41.8174, 38.4559,
        38.6584, 37.9650, 35.6400, 37.6759, 34.4795, 36.5271, 41.6378, 38.9634,
        32.5324, 39.2035, 36.7413, 39.9858, 41.2571, 41.9875, 41.0301, 40.0739,
        33.5321, 38.7591, 35.2365, 39.4894, 33.9834, 33.2429, 32.7792, 33.1071,
        37.3704, 33.2398, 31.3524, 33.0687, 33.0286, 31.7393, 34.0383, 36.4972,
        36.6184, 40.0443, 36.8474, 37.0071, 38.0924, 48.0646, 40.7557, 35.0999,
        39.4842, 42.7528, 40.2814, 32.5034, 37.6135, 39.6851, 40.6282, 37.6530,
        39.5403, 36.1294, 30.1673, 36.9076, 46.3715, 40.7143, 32.8794, 36.4076,
        36.9491, 36.0017, 35.9818, 36.6319, 35.5266, 37.3982, 39.2708, 35.5777,
        35.8212, 36.9079, 36.1890, 32.9014, 34.7418, 31.7803, 39.6051, 38.0574],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 6.2703,  3.9384,  2.4465,  3.3419,  2.4417,  3.1496,  2.1558,  5.8575,
         4.0924,  2.5961,  4.7615,  5.5945,  3.9498,  7.1136,  2.4442,  4.5638,
         3.0266,  3.0829,  6.3277,  2.9752,  3.6573,  6.2311,  4.6376,  3.9434,
         4.3782,  4.2949, 12.0194,  8.6485,  6.0561,  5.2950,  4.3676,  6.1242,
         5.1284,  3.5929,  2.0768,  4.0992,  5.9790,  5.9379,  5.0679, 18.5060,
        14.7253, 29.4295, 28.7833, 29.7320, 24.5523, 33.1489,  8.1941,  8.4453,
        19.6923,  8.0844, 15.9820, 17.2992, 24.1242, 12.5631, 29.0574, 38.6151,
        44.2874, 12.3479, 10.1469,  7.4688, 32.1646,  7.1352,  7.6192,  8.5320,
         6.1326,  5.6691, 10.5933, 11.0191,  6.6803,  5.6802,  8.1019, 15.5145,
         4.9615, 17.3102,  2.9887,  8.2847,  8.0071,  3.8084,  6.7765,  6.0237],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([18.9298, 10.1073,  6.2954,  7.8193,  4.9869,  6.8941,  4.4324, 12.7899,
         9.2315,  5.8561, 11.1980, 12.1471,  9.4721, 16.7442,  5.2099, 12.1318,
         7.9105,  6.4768, 15.0099,  6.2829,  7.7423, 13.0619,  9.8536,  8.4665,
        11.1971,  9.5558, 29.0950, 18.4901, 14.5750, 13.4962, 11.4500, 15.9508,
        11.3808,  9.0421,  5.5941, 10.5444, 15.4845, 16.0042, 12.7020, 47.9852,
        34.1383, 64.2305, 67.6444, 69.9585, 54.9571, 71.9911, 17.3449, 20.3867,
        43.0934, 19.1535, 34.4280, 45.9447, 55.0725, 26.8050, 61.0606, 88.8647,
        98.6382, 29.5601, 27.9378, 15.9111, 74.5564, 15.1618, 19.9583, 20.0396,
        13.7993, 13.3783, 25.1270, 24.3590, 16.0445, 13.0354, 17.4686, 37.5981,
        11.4515, 40.3789,  6.6555, 21.4768, 19.4478, 10.2096, 14.7106, 13.5975],
       device='cuda:0')
Outer loop valEpocw Maximum [23/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 48.1
model_train val_loss valEpocw [23/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 44.3
model_train val_loss  valEpocw [23/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 98.6
Max_Val Meta Model:  tensor([35.9711, 34.3680, 30.6069, 37.9538, 39.3285, 37.9312, 39.1200, 34.8767,
        35.6264, 37.8705, 35.4299, 37.2609, 34.1213, 35.9626, 38.8740, 35.2488,
        32.1387, 38.7859, 36.4083, 39.0767, 39.5254, 38.5035, 38.9230, 39.3901,
        33.2454, 36.3450, 32.7963, 37.1093, 33.7761, 32.5045, 32.1019, 32.5915,
        35.6844, 31.4924, 31.2395, 32.8265, 32.5549, 30.7846, 30.2333, 37.2277,
        36.6458, 38.1189, 35.9609, 36.2110, 34.6519, 36.4337, 38.3357, 34.8318,
        39.0640, 34.1775, 38.2170, 31.9319, 36.6027, 28.2845, 38.7979, 35.7583,
        37.3995, 39.1711, 30.2255, 38.9711, 36.6076, 39.6664, 34.4038, 34.3168,
        35.5015, 35.0723, 35.7925, 40.4295, 35.4742, 36.9249, 36.1981, 35.2809,
        35.6978, 36.1622, 43.8741, 32.2241, 34.4532, 31.5337, 39.4214, 38.4338],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([12.1220,  3.4148, 13.7273,  5.7013,  8.1701, 10.7922,  9.4951,  9.4975,
         4.9627, 13.3487,  5.4726,  7.2427,  3.5765,  6.8739,  7.3583,  4.1116,
         3.6901,  3.9915,  5.5800,  4.3406,  5.9539,  6.3711,  6.6346,  8.1728,
         8.0768,  5.7390,  8.7622,  2.7866,  7.7696,  4.5967,  3.8536,  4.7004,
         2.4940,  3.2957,  1.6506,  4.0272,  3.6125,  4.0429,  3.6361,  4.0538,
         6.9575,  1.9275,  4.1560,  3.2650,  3.7738,  2.9068,  4.6687,  5.5192,
         5.0142,  5.5189,  4.1445,  3.5635,  6.2275,  2.7587,  7.2805,  3.2719,
         2.4759,  5.0055,  7.2130,  4.2969,  4.1088,  6.5043,  4.1881,  5.0252,
         1.6855,  4.7484,  8.4403,  4.3448,  5.3256,  5.0181,  7.3428,  7.3387,
         3.6820,  4.6871, 84.5951,  5.2552,  5.4667,  4.1867,  6.2862,  5.5256],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 35.8327,   8.8350,  37.0656,  13.3693,  17.0717,  24.2079,  19.9461,
         21.6179,  11.5529,  29.9906,  12.8567,  15.8085,   8.5932,  16.3246,
         16.0478,  11.1144,   9.6563,   8.3672,  13.2865,   9.2669,  12.7879,
         13.8963,  14.3285,  17.7163,  20.7578,  13.0263,  22.0410,   6.0667,
         18.7539,  11.8980,  10.2418,  12.3612,   5.5937,   8.5297,   4.4166,
         10.3649,   9.4044,  10.9943,   9.5176,  10.5219,  15.9271,   4.3345,
          9.7462,   7.7021,   8.7033,   6.4616,  10.1415,  13.3183,  10.9541,
         13.2194,   9.1021,   9.5083,  14.3372,   6.5389,  15.4865,   7.6143,
          5.6620,  11.6133,  19.8339,   8.9598,   9.5292,  13.9495,  10.6946,
         12.1326,   3.8580,  11.3151,  19.9998,   9.4516,  12.7163,  11.5525,
         16.3439,  17.9455,   8.5374,  11.0516, 191.4008,  13.8036,  13.2817,
         11.2273,  13.6189,  12.4096], device='cuda:0')
Outer loop valEpocw Maximum [23/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 43.9
model_train val_loss valEpocw [23/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 84.6
model_train val_loss  valEpocw [23/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 191.4
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.35761017 97.2137279  91.74839488 97.02489005 97.26733349 96.5022356
 96.99808726 94.75030762 97.44398826 96.50345391 98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.09485752 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.3633484  98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.7266968  97.84237521 92.54760541
 96.91646057 96.23177106 96.96519292 93.53321719 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.41457828 96.13796128 96.24273583 96.9067141
 92.09804949 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.16232746 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [83.9768034  97.2137279  91.42919799 97.02489005 97.26733349 96.5997003
 96.99808726 94.73690623 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.15409169
 96.90915072 96.22689782 96.9627563  93.89870981 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 90.03667109 96.13796128 96.24273583 96.9067141
 91.80443708 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [96.5965527   5.77745859 60.21978856 13.85557414  8.77505354 22.26856559
  8.56985859 32.66997633  3.22259427 28.32171364  1.60935456  1.69372055
  0.88856998 12.79898503  2.89677751  5.51563597  4.8319192   4.13359442
  0.86895787  1.90729758  2.21241753  0.49188398  1.23804441  2.89780641
 17.12405059 10.21645189 27.9423421  10.34986455  4.57620873  1.46330572
 13.45624522  0.86491598 30.49644837  2.09118166  2.71135299  7.09636835
  8.64184365  2.99340486  6.21159915 30.51735793  5.29367479 45.00793685
 33.29570583 28.67392491 26.00280504 34.23739014  2.74137724  1.69591661
  2.93078857  1.80474848  1.63574715  1.14215083  1.06845708 17.23571986
  1.56735536 11.71615472 62.02940257 22.37619469 16.6217109  11.25502869
 62.76878082 30.54686385 12.82485309  6.84024523  4.71931956  4.83904474
  3.46219006 10.39888901  2.4286807   4.97655971  0.3510576  27.39169029
  3.80588669 22.74077054 21.17340812 12.14723787  1.1049673   2.26654197
  0.18489015  0.8187559 ]
Accuracy th:0.5 is [45.54647239 97.2137279  71.35390651 97.02489005 97.26733349 76.01150083
 76.36115544 75.64966314 77.31387288 96.35603855 77.57337264 98.52097319
 99.41399349 79.63962427 77.21031664 96.56680596 96.29512311 76.92401408
 98.65376884 98.30776915 79.5738356  78.07897077 98.38695922 77.03244356
 80.7019895  96.65086926 94.0778012  76.60116227 98.01293844 77.49052765
 97.30875598 98.57457877 96.36213009 98.02024829 85.73116799 77.1213801
 76.94472533 88.34200363 97.11504489 74.28028411 78.68812514 92.05906361
 76.39404978 76.0066276  96.9627563  93.87434364 98.02877645 98.57336046
 90.4691707  86.04427334 86.14661127 98.55508583 98.99976852 76.57070455
 98.70615611 76.88746482 71.59269502 92.31125352 96.24273583 96.9067141
 89.79300934 97.17717864 91.57661335 76.9958943  98.42838172 77.46737978
 98.20786784 76.11262046 77.97785115 97.49759384 78.5431464  95.99054592
 77.78048513 95.45083515 76.26247244 81.30505233 85.65319623 77.51854875
 78.58578721 99.14718388]
Accuracy th:0.7 is [45.66830326 97.2137279  71.35390651 97.02489005 97.26733349 76.01150083
 76.36115544 75.91403614 77.31387288 96.46568633 77.57337264 98.52097319
 99.41399349 80.12694777 77.21031664 96.56680596 96.29512311 76.92401408
 98.65376884 98.30776915 80.05141263 78.07897077 98.38695922 77.2882884
 81.32820019 96.65086926 94.0778012  76.60116227 98.01293844 77.49052765
 97.30875598 98.57457877 96.36213009 98.02024829 85.96264665 77.1213801
 76.94472533 88.69531317 97.11504489 74.28028411 79.36550481 92.05906361
 76.39404978 76.0066276  96.9627563  93.87434364 98.02877645 98.57336046
 92.71085879 87.03110342 86.33788575 98.55508583 98.99976852 76.57070455
 98.70615611 76.88746482 71.59269502 92.71816864 96.24273583 96.9067141
 89.79300934 97.17717864 91.76910613 76.9958943  98.42838172 77.46737978
 98.20786784 76.11262046 77.97785115 97.55972759 78.5431464  95.99054592
 77.85480196 95.45083515 76.26247244 81.40617195 85.78964681 77.51854875
 78.58578721 99.14718388]
Avg Prec: is [56.08994388  3.06294203 11.29469132  3.24578641  2.2264909   3.78828199
  3.17037074  5.55829057  2.51859118  3.74124737  1.57639344  1.57990264
  0.63054749  5.32571338  2.65120863  3.15630949  3.77650253  2.85105281
  1.37092453  1.76537322  2.00073946  0.88982568  1.82958582  2.3750062
  5.08639805  3.55956449  6.34258488  3.46683629  2.11045274  1.85872138
  2.74078031  1.36346945  3.71116233  1.66145136  2.37657627  2.3682679
  2.95724153  2.51504319  2.76186951  7.44771056  2.19942485  8.26274213
  3.29054284  3.9855844   3.26887968  6.36779074  2.15153932  1.52757796
  2.06704115  1.59587806  1.78969995  1.53307327  1.08261894  2.97957568
  1.38385427  2.78998334 11.36331329  3.78532806  4.09092886  2.78595872
 10.87177142  2.13024562  3.87936419  3.01257656  1.61119573  2.53475704
  1.81625923  4.22866123  1.23651365  2.35437998  0.18088744  3.33762316
  1.89250852  4.65989657  3.99749926  3.19754823  0.85829675  1.8587927
  0.14988304  0.71228808]
mAP score regular 13.31, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [88.71116426 97.22450607 92.03478088 96.96290206 97.90716795 96.25034258
 96.80843112 94.83269801 97.38894287 96.45215138 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.38174253 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.77972943 97.82744101 92.9391833
 97.1024242  96.48703192 96.94795326 93.11358597 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.06310885 96.39235618 96.16314124 96.78102499
 92.15187981 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07593991
 98.03174129 95.44559882 95.80437003 97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [86.59840048 97.22450607 92.20170915 96.96290206 97.90716795 96.63651992
 96.80843112 94.88751028 97.38894287 96.42225378 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.58788649
 97.07750953 96.48703192 97.03764606 94.12013853 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.22348457 96.39235618 96.16314124 96.78102499
 92.39106062 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [96.98262292  6.82472832 65.12370583 15.31163553  9.97771287 21.56369309
  9.03277337 34.3568183   3.71124713 32.36335336  1.52214545  1.59758023
  0.75797663 14.54239794  3.11201942  6.50906393  5.18469263  4.58447978
  0.78614233  2.01542451  2.21423248  0.49509878  1.22934144  2.82209104
 17.41645478 11.07298716 27.80550761 11.25122803  5.91684632  1.57337377
 17.81371613  0.82627287 36.01388524  2.22137053  2.39403821  7.59934931
  8.73367846  3.86400152  7.99252081 32.86363765  5.02855086 45.17660674
 34.29799406 28.57262617 24.74557604 34.46816542  2.85653742  1.66844114
  3.02138478  1.82271279  1.67044255  1.16288251  1.25165928 17.60159141
  1.5100463  11.99295645 57.28195896 24.47129444 15.80243665 12.58968894
 63.04416788 31.53448864 12.84127425  6.52383337  4.70040167  4.48200796
  3.31702772 11.96315882  2.11165312  4.93840792  0.37549003 31.53691561
  3.53639626 25.75582445 29.2801668  12.22471086  1.14775887  2.28302478
  0.18585977  0.76282776]
Accuracy th:0.5 is [45.57640083 97.22450607 69.69379874 96.96290206 97.90716795 75.08533274
 75.19744874 74.22079378 76.67987144 96.41228791 76.81441064 98.5325261
 99.34972718 77.51451279 76.76707278 96.31262924 96.21047911 76.17410369
 98.78167277 98.34068316 78.19966614 77.45970053 98.31327703 76.26877943
 77.72130453 96.52938685 94.3393876  76.2712709  97.81747515 76.77703864
 97.52597354 98.67204823 96.39983058 98.18870369 86.31686474 76.40331863
 76.38587837 90.64454244 97.0276802  73.61536737 77.00127065 92.37362035
 75.53130528 75.19495727 97.03764606 94.02795426 98.18621222 98.77668984
 91.5614022  86.02536313 84.41836709 98.55993223 98.87385704 75.51137355
 98.6969629  76.20898423 70.42130702 93.47484864 96.16314124 96.78102499
 90.13379176 97.04761193 91.57136806 76.2563221  98.32075143 77.11089518
 98.13139996 75.43662954 77.49208959 97.50355034 77.89072427 96.07843137
 77.26287465 95.44559882 75.36935994 82.32553504 87.5476493  76.84430824
 77.96546827 99.15040985]
Accuracy th:0.7 is [45.89780003 97.22450607 69.69379874 96.96290206 97.90716795 75.08533274
 75.19744874 74.37028178 76.67987144 96.41976231 76.81441064 98.5325261
 99.34972718 77.910656   76.76707278 96.31262924 96.21047911 76.17410369
 98.78167277 98.34068316 78.52853975 77.45970053 98.31327703 76.36345517
 78.13488801 96.52938685 94.3393876  76.2712709  97.81747515 76.77703864
 97.52597354 98.67204823 96.39983058 98.18870369 86.62829808 76.40331863
 76.38587837 90.82891098 97.0276802  73.61536737 77.51202133 92.37362035
 75.53130528 75.19495727 97.03764606 94.02795426 98.18621222 98.77668984
 93.61935371 86.54607968 84.58778683 98.55993223 98.87385704 75.51137355
 98.6969629  76.20898423 70.42130702 93.79624785 96.16314124 96.78102499
 90.13379176 97.04761193 91.77317687 76.2563221  98.32075143 77.11089518
 98.13139996 75.43662954 77.49208959 97.53344794 77.89072427 96.07843137
 77.29028079 95.44559882 75.36935994 82.41522784 87.74198371 76.84430824
 77.96546827 99.15040985]
Avg Prec: is [54.17991915  3.7470255  14.7848558   4.56218507  1.53395505  4.27764363
 12.03325664  8.67174491  7.53850128  5.15104334  2.30364985  5.13639557
  1.55314995  5.89026666  3.04069903  3.7696514  25.35761491  6.34752219
  1.54559753  2.63855398  3.58585915  1.4268931   1.15705879  5.8028503
  5.7315745  10.98041064  8.13530545  4.55507886  3.90033932  6.85436729
  2.30695051  0.87194284  2.97115873  1.15359808  1.71867056  2.41624816
  2.0267319   2.19340804  2.19898499  6.2060341   1.72856036  6.04209826
  2.1935709   2.72836089  2.34940925  4.8533856   1.72906415  1.06743642
  1.45455466  1.18905486  1.20747119  0.98872176  0.78144067  2.29048613
  0.91320525  1.8722985  10.17275474  2.98479291  3.98673505  2.97626543
  7.92827217  2.04993446  3.37191667  2.54107014  1.34731188  1.89512048
  1.56392045  3.51810279  1.06585382  2.20845887  0.19707315  3.18680142
  1.54900503  4.02645254  3.21050105  2.30764079  0.58174483  1.48989527
  0.12089821  0.60453695]
mAP score regular 13.97, mAP score EMA 4.36
Train_data_mAP: current_mAP = 13.31, highest_mAP = 13.31
Val_data_mAP: current_mAP = 13.97, highest_mAP = 13.97
tensor([0.3304, 0.3887, 0.3777, 0.4282, 0.4815, 0.4472, 0.4773, 0.4421, 0.4325,
        0.4400, 0.4251, 0.4606, 0.4174, 0.4254, 0.4608, 0.3703, 0.3819, 0.4752,
        0.4200, 0.4726, 0.4637, 0.4639, 0.4624, 0.4671, 0.3912, 0.4420, 0.3942,
        0.4573, 0.4145, 0.3903, 0.3805, 0.3855, 0.4469, 0.3937, 0.3718, 0.3881,
        0.3863, 0.3691, 0.3854, 0.3839, 0.4307, 0.4475, 0.4252, 0.4270, 0.4361,
        0.4556, 0.4636, 0.4120, 0.4562, 0.4199, 0.4583, 0.3764, 0.4397, 0.4189,
        0.4723, 0.4352, 0.4471, 0.4300, 0.3662, 0.4784, 0.4322, 0.4689, 0.3882,
        0.4209, 0.4393, 0.4200, 0.4206, 0.4557, 0.4205, 0.4381, 0.4403, 0.4123,
        0.4323, 0.4276, 0.4571, 0.3849, 0.4123, 0.3732, 0.4624, 0.4424],
       device='cuda:0')
Max Train Loss:  tensor([11.3457,  6.5072,  9.8796,  5.1057,  7.0264,  8.3857, 11.2906,  7.5226,
         9.4256,  7.0643,  5.6555,  9.0358,  3.8411, 10.5915,  5.9455,  6.7263,
         6.7845, 10.9356,  7.1818,  6.2942,  8.1345,  7.4824, 10.0262,  6.9398,
         8.1763,  9.1798,  7.7539,  6.8235,  7.8342,  5.8443,  7.1383,  9.0126,
         8.0749,  9.8127,  5.3616,  6.4849, 11.0258,  7.6026, 10.4108, 15.6016,
         9.4003, 14.6273,  7.7312,  7.2711,  6.6750, 11.4629,  6.4212, 10.9284,
         5.4878, 10.5039,  6.7290,  8.8946,  6.5994,  7.4194,  8.6516,  7.1268,
        16.8498, 10.6479,  7.9469,  8.4961, 12.7226,  9.4757,  6.8250,  9.7663,
         3.3974,  7.7181,  8.8186,  8.9463,  7.3200,  7.1582,  7.4980, 10.8526,
        10.7427,  8.7987,  9.2826,  6.0166,  5.8224,  6.4364,  6.6255,  5.7917],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [24/80], Step [000/642], LR 1.0e-04, Loss: 16.8
Max Train Loss:  tensor([11.4440,  7.4072, 15.6538,  9.9413,  9.6313, 16.9364, 11.9621, 14.4326,
        11.4127, 10.6464, 12.7814, 13.2046, 17.5631, 15.4827, 11.0758,  7.7147,
         8.1747,  6.2035, 13.9577, 13.0640, 17.1997,  9.0232,  7.6250,  9.6389,
         8.7740,  9.1601,  9.5526, 11.1163,  7.5842,  6.7297, 11.0255,  8.2524,
        16.9114,  6.8510,  5.9211,  6.0797, 10.5291,  6.8824,  8.7621, 12.6367,
        10.0183, 13.8339,  8.7768, 10.1784,  9.1894, 14.6465, 15.9526,  8.7385,
        15.4173,  6.8858, 14.0168,  8.0163, 11.3656, 15.9600,  8.3806,  9.2733,
         9.8068, 11.5411, 11.1123, 16.0726, 19.0294, 11.0920,  7.3610,  7.6378,
        16.6437,  8.5281,  4.4356, 15.4952,  8.3805, 12.8436,  7.2000,  9.6398,
        14.3099, 11.6275, 16.3178,  7.6577,  8.3620,  6.8878,  9.3765, 13.5005],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [24/80], Step [100/642], LR 1.0e-04, Loss: 19.0
Max Train Loss:  tensor([ 8.1508,  7.0785, 10.0089,  8.0240,  7.1643,  8.5484, 10.2595,  9.9178,
        11.6384,  7.2608,  8.2970, 12.7691,  6.3110, 11.8881, 12.6565,  7.5361,
         7.5106,  8.8335,  7.4913,  9.2717, 12.1099,  9.2186,  7.8103,  9.8170,
         9.6283,  9.2406, 11.8468,  8.3735,  8.4872,  6.8039,  7.3728,  6.6696,
        12.1229,  9.1244,  7.0433,  5.1565,  7.0766, 10.9093,  9.6553, 12.3897,
         9.4713, 14.4107,  7.2970, 10.4249,  5.8363, 13.6935,  8.5900,  7.7261,
         6.3842,  8.5977, 10.5061,  6.6257, 11.1328,  9.3032,  6.8080,  9.3401,
        17.2509, 10.4784,  6.4048,  9.1853, 14.9915, 10.0539,  7.1281,  8.9685,
         8.1267,  8.4204,  6.1885,  8.5017,  7.4861, 11.2240,  7.0654,  9.0942,
         8.5894, 11.9808, 11.4598,  8.0934,  7.3216,  6.9552,  9.1850,  6.8322],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [24/80], Step [200/642], LR 1.0e-04, Loss: 17.3
Max Train Loss:  tensor([15.1378,  9.3008, 11.1927,  9.1594,  9.5108,  9.4777, 10.1914,  9.4075,
        10.5604, 10.2200, 11.2703,  6.4588,  6.2031,  9.3688, 10.5698,  8.2806,
        10.5187,  9.5884,  6.2655,  6.1456,  8.9421,  9.6977, 11.5824,  8.9303,
        12.3518, 11.3008, 10.5028,  7.4090, 11.1950,  6.1591,  7.4730,  9.2664,
         7.8977,  4.3128,  4.9439,  5.2957,  5.8498,  7.1053,  7.3414,  9.2130,
         7.3197, 10.9910,  8.5307, 11.6500,  5.9821, 12.2880,  7.5964,  7.3082,
         7.1007,  5.7215,  8.6012,  3.5254,  2.6848, 10.8626,  8.5444,  6.8788,
        12.8760,  5.6374,  3.5409,  6.8061, 10.4107,  9.6883,  7.8433,  7.8236,
         7.7694,  6.4702,  6.3823,  6.8259,  7.0448,  8.2036,  6.9551,  9.4296,
         8.5050,  7.4871,  8.1672,  6.1580,  6.5386,  6.2317,  9.0705,  5.5977],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [24/80], Step [300/642], LR 1.0e-04, Loss: 15.1
Max Train Loss:  tensor([15.5197,  7.2757, 10.3948,  8.0282,  7.0929,  9.0609,  8.1936,  9.4848,
         7.0039, 11.4170, 10.5877,  5.3199,  6.1471, 10.4814,  7.7043,  6.4412,
         9.6360,  6.1243,  8.7476,  8.4375,  9.0035,  9.2804,  7.2877,  8.8238,
         8.8110,  8.0268,  9.6813,  6.4768,  7.3376,  6.7415,  7.4772,  6.9709,
         8.7158,  5.6181,  7.0858,  6.4766, 10.2213,  8.3958,  7.0744, 12.0589,
        11.3874, 12.7137,  7.9391,  8.5059,  9.6581, 15.7055,  9.5349,  9.6581,
         8.1458,  9.3510,  8.4348, 10.5269,  4.0041,  7.3037,  8.5332,  7.0690,
        11.4627,  6.4250,  5.5654,  5.8266, 12.4200,  8.7712, 10.3493, 11.9323,
         9.8260,  7.2112,  9.9753, 12.4151,  5.6571,  7.3446,  6.8975,  9.6469,
         8.5070, 12.8855, 10.1648,  5.9440,  7.2190,  9.4073, 10.5509,  4.6317],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [24/80], Step [400/642], LR 1.0e-04, Loss: 15.7
Max Train Loss:  tensor([11.8102,  7.8182, 13.0672,  7.6110,  9.7493,  6.6341, 10.0158,  9.5287,
         6.2858,  5.7130,  7.2786,  4.1826,  6.0888, 12.1736, 11.4020,  5.0962,
         6.6189,  4.5365,  7.3749,  6.6468,  8.9878,  8.4194,  5.8320,  7.6429,
        10.8545,  8.7953, 10.4842, 10.0178,  6.3818,  8.9185, 10.0122,  5.9691,
         8.7089,  8.0192,  5.4759,  7.6827,  8.8929,  8.3158,  4.1482, 14.1317,
         9.8544, 12.1230,  6.5061,  6.8387, 12.6152, 14.9363,  8.1299,  6.6762,
         7.9887,  6.4362,  5.3431,  6.0558,  6.9382,  8.7857,  4.6995,  6.6221,
        13.3117,  9.5817,  6.6565,  8.2538,  9.2582,  5.6763,  8.6632,  7.3873,
         8.0814,  8.2852,  8.7699,  8.5538,  9.6806, 10.7697,  6.8405,  7.8450,
         9.8534, 10.6438,  7.8799,  6.9214,  6.2870,  4.9131,  8.9418,  4.5740],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [24/80], Step [500/642], LR 1.0e-04, Loss: 14.9
Max Train Loss:  tensor([16.0790,  6.7937, 10.7544,  9.2437,  8.3994, 10.4804,  5.6718,  8.5058,
         6.6999,  8.0275,  7.5936,  9.4225,  6.3868, 11.6492,  9.9480,  9.9496,
         9.2444,  9.3615,  8.2770,  6.5958,  6.8484,  9.8032,  4.8560,  6.5069,
        11.7783,  7.3414, 10.7015,  5.9025,  8.8467,  4.5525,  9.2379,  8.7228,
         8.7398,  6.4573,  7.7619,  8.1515,  6.2871,  7.9027,  7.1622, 10.2223,
         9.9688,  9.0340,  9.2395, 10.1187,  7.7967, 13.3519,  8.5853,  6.8655,
        11.8631,  8.5367,  9.2480,  5.6977,  6.5786,  7.0546,  6.0387,  5.0750,
        15.2883, 12.1850,  6.8405,  7.6306, 11.4133,  5.6782,  7.9404,  7.7044,
         7.6543,  9.1574,  6.1832,  7.1739,  6.5895,  9.5354,  7.8419, 11.0348,
        10.2915,  9.4878, 11.6374,  9.9649,  6.5893,  5.2959,  9.2840,  5.8923],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [24/80], Step [600/642], LR 1.0e-04, Loss: 16.1
Max_Val Meta Model:  tensor([ 17.6556,  24.4869,  26.1552,  19.8226,   5.1116,   6.8466,   6.2995,
          8.7826,   5.2044,   8.4367,   4.5040,   6.3665,   6.9200,   8.5190,
          5.8763,   5.2451, 124.7924,   6.8197,   6.3531,   4.3917,   4.5809,
          7.5512,   3.5011,   4.7147,  11.5134,   9.1605,  10.5990,   4.3939,
          7.5489,   6.6314,   4.8836,   4.5371,   6.9318,   4.3103,   3.7571,
          5.2463,   5.4751,   5.3183,   4.7462,  17.9504,   8.2251,  10.0673,
          7.2077,   8.8696,   8.4375,  10.9341,   7.6048,   6.5571,   6.2801,
          6.5796,   4.4818,   3.5101,   4.9456,   5.0411,   4.8109,   3.8492,
         11.2410,   6.2962,   8.3563,   5.3419,   6.9449,  12.7655,   5.0814,
          4.2325,   6.8095,   5.3647,   4.2765,   5.4124,   8.2152,  11.8023,
          6.9877,   9.8823,  11.9999,   6.4000,   7.0888,   6.3306,   5.6401,
          4.1061,   9.1243,   4.6821], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 14.4007,  23.3406,  21.5656,  19.0810,   5.3437,   8.3470,   7.0523,
         11.3853,   6.0851,   9.8151,   5.1655,   7.0279,   7.6931,   8.9384,
          5.9113,   5.6520, 117.4447,   7.8851,   6.9964,   4.7428,   4.9179,
          8.2441,   3.7452,   4.6268,  12.0361,  10.2088,  11.1675,   5.8753,
          8.1909,   7.1347,   5.3650,   5.1712,   8.2404,   4.9185,   4.3110,
          6.1240,   7.0513,   5.7300,   5.5199,  17.6089,   8.6695,  10.3177,
          7.2516,   8.8292,   8.5449,  10.2180,   8.1331,   7.2899,   6.7820,
          7.2900,   4.8216,   4.0546,   5.0686,   5.2871,   5.4637,   4.2173,
         13.1966,   6.7413,   8.4611,   5.7094,   6.9298,  11.8407,   5.4924,
          4.6598,   7.4437,   6.0528,   4.9001,   6.5116,   8.7618,  12.0176,
          7.7874,  10.0008,  12.5510,   7.0333,   7.2863,   6.6998,   6.3605,
          4.5887,  10.0276,   5.3201], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 43.5835,  60.0407,  57.0990,  44.5588,  11.0976,  18.6647,  14.7768,
         25.7554,  14.0704,  22.3077,  12.1525,  15.2572,  18.4299,  21.0114,
         12.8285,  15.2635, 307.5503,  16.5923,  16.6580,  10.0351,  10.6053,
         17.7698,   8.0996,   9.9064,  30.7653,  23.0981,  28.3273,  12.8488,
         19.7628,  18.2810,  14.0984,  13.4143,  18.4384,  12.4939,  11.5962,
         15.7796,  18.2521,  15.5252,  14.3223,  45.8679,  20.1306,  23.0564,
         17.0545,  20.6775,  19.5951,  22.4285,  17.5432,  17.6953,  14.8671,
         17.3616,  10.5201,  10.7717,  11.5285,  12.6205,  11.5675,   9.6904,
         29.5152,  15.6785,  23.1072,  11.9338,  16.0321,  25.2518,  14.1478,
         11.0698,  16.9427,  14.4103,  11.6509,  14.2898,  20.8386,  27.4286,
         17.6864,  24.2557,  29.0323,  16.4467,  15.9415,  17.4069,  15.4288,
         12.2955,  21.6863,  12.0263], device='cuda:0')
Outer loop valEpocw Maximum [24/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 124.8
model_train val_loss valEpocw [24/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 117.4
model_train val_loss  valEpocw [24/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 307.6
Max_Val Meta Model:  tensor([33.7777, 33.8006, 42.3325, 36.7777, 40.5778, 40.1649, 42.4991, 37.3573,
        36.8152, 37.6425, 36.1111, 49.3087, 35.5086, 35.8683, 42.7047, 31.1058,
        32.4223, 39.4705, 38.0256, 41.6869, 39.7549, 39.8299, 38.5364, 42.4711,
        33.7120, 37.7768, 33.5152, 38.3220, 35.5589, 32.7678, 32.5199, 33.0914,
        38.0635, 32.6645, 31.6595, 32.8004, 32.8937, 31.2399, 32.6031, 36.3930,
        36.4181, 37.2318, 35.8904, 36.4444, 36.3070, 37.8934, 37.6994, 34.6480,
        39.0585, 35.2934, 38.8159, 32.2240, 36.7807, 35.5779, 39.6406, 36.2713,
        40.4254, 36.7531, 32.5038, 38.9994, 36.6527, 42.4452, 33.0237, 35.6976,
        36.5292, 35.5750, 35.3451, 37.1085, 36.2386, 37.4778, 37.5295, 33.7247,
        37.2270, 36.5417, 38.4543, 32.2197, 34.4663, 31.5330, 39.7836, 37.8794],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([19.1456,  7.2937, 32.1834,  8.3996,  7.3384, 16.1518, 37.8166, 20.7031,
        15.5540, 19.9486,  7.4929, 86.8078,  6.9604,  5.9881,  7.4750,  4.0851,
         5.0464,  9.7922,  5.7667, 10.1947,  4.0424,  6.2735,  3.1078,  5.6021,
         9.2665,  9.4455, 12.1728,  7.1082,  7.2773,  3.5186,  3.8818,  3.6598,
         3.4721,  3.4624,  3.0197,  3.8438,  6.2262,  3.2939,  3.1754,  2.3162,
         4.4930,  1.9500,  3.2108,  2.5744,  3.2239,  2.0639,  5.7503,  4.6664,
         4.7264,  5.4042,  3.1693,  2.7823,  2.0127,  3.2432,  3.7635,  3.2840,
         3.6675,  3.6533,  3.8993,  2.6051,  1.9708,  2.9277,  2.6368,  2.0241,
         5.3353,  3.3155,  3.3870,  6.5378,  4.7132,  3.5630,  5.8856,  3.4283,
         6.8354,  3.1804,  4.8074,  2.8611,  4.5972,  2.6318,  7.7057,  3.6731],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 57.5464,  18.6049,  82.3796,  19.5566,  16.4014,  35.3925,  77.7349,
         47.3506,  35.8728,  45.2125,  17.7350, 188.3947,  16.8889,  14.1244,
         15.7323,  11.1486,  13.3716,  20.7174,  13.2401,  21.2747,   8.6964,
         13.5357,   6.7498,  11.8314,  23.6466,  21.5131,  31.0032,  15.4611,
         17.6007,   9.0847,  10.2374,   9.5199,   7.7876,   8.8702,   8.1135,
          9.9647,  16.1700,   8.9904,   8.3001,   5.9512,  10.3802,   4.3743,
          7.5864,   6.0728,   7.4075,   4.5245,  12.5637,  11.3898,  10.3044,
         12.8685,   6.9279,   7.4066,   4.5987,   7.7287,   7.9283,   7.5798,
          8.0761,   8.5375,  10.5586,   5.4879,   4.6070,   6.1004,   6.8521,
          4.8146,  12.2589,   7.9176,   8.1104,  14.7071,  11.0911,   8.1259,
         13.2673,   8.5087,  15.8103,   7.4474,  10.6234,   7.5341,  11.2320,
          7.1047,  16.7030,   8.3101], device='cuda:0')
Outer loop valEpocw Maximum [24/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 49.3
model_train val_loss valEpocw [24/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 86.8
model_train val_loss  valEpocw [24/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 188.4
Max_Val Meta Model:  tensor([29.0709, 33.7288, 36.9527, 36.3394, 38.2893, 39.3014, 40.5276, 38.6824,
        37.3739, 37.6526, 36.3559, 39.5496, 35.1599, 33.7549, 39.8515, 30.8566,
        31.9474, 39.2820, 37.8289, 40.2000, 39.2151, 39.5565, 39.1192, 40.1294,
        33.1565, 39.0766, 33.0043, 40.4397, 34.0684, 32.4150, 32.1967, 32.7183,
        36.8317, 32.3326, 31.2980, 32.4274, 32.5737, 30.9173, 32.1725, 35.4045,
        36.3333, 38.2118, 36.4763, 36.8630, 36.7912, 44.8393, 41.8950, 34.4905,
        40.4534, 35.0663, 42.0714, 32.1459, 36.9479, 35.3445, 40.0637, 42.3904,
        39.9744, 36.3514, 32.2022, 38.0329, 43.3116, 40.4070, 32.6679, 35.1937,
        36.2374, 35.2107, 35.0346, 36.6183, 36.0138, 37.0796, 37.2031, 33.2077,
        36.7934, 36.3961, 38.2718, 36.2113, 34.2133, 31.1538, 39.5064, 37.3908],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 6.3179,  5.3306,  3.2590,  3.0680,  2.2550,  3.3825,  1.3824,  5.7231,
         3.5467,  3.2052,  3.2515,  3.8326,  6.7428,  7.1117,  2.9471,  4.9361,
         4.1772,  1.5645,  6.3159,  2.5728,  2.8229,  7.4085,  2.7892,  2.6524,
         5.3747,  4.5892, 10.0158,  7.9495,  6.4012,  4.9525,  5.1412,  5.9126,
         4.8671,  4.8433,  4.2800,  4.9894,  6.8619,  6.1417,  4.3312, 19.7078,
        14.0363, 29.6178, 29.1390, 29.7024, 24.8578, 31.6915, 10.6280,  9.1265,
        20.2671,  8.2542, 16.9314, 17.3063, 27.7629, 12.1855, 30.8433, 42.5287,
        40.2133,  9.2255,  6.2979,  8.4296, 37.4259,  4.4140,  8.8312,  7.6848,
         9.7773,  8.4459,  6.5736, 10.3171,  7.3156,  4.9926,  7.6069, 11.6991,
         8.5913, 16.7169,  3.6783,  6.9028,  8.0329,  3.8211,  9.7469,  5.0524],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([18.9803, 13.6303,  8.6217,  7.1618,  5.0763,  7.4622,  2.8845, 12.8046,
         8.0844,  7.2379,  7.6304,  8.2463, 16.3797, 17.1270,  6.3458, 13.4954,
        11.0751,  3.2751, 14.5353,  5.4161,  6.0776, 16.0058,  6.0277,  5.7028,
        13.7489, 10.3060, 25.5067, 16.8547, 15.6896, 12.8051, 13.5716, 15.4184,
        10.9561, 12.4168, 11.5180, 12.9554, 17.8496, 16.7946, 11.3554, 50.7417,
        32.4679, 66.5935, 68.8291, 70.0680, 57.0892, 69.7105, 22.6068, 22.2257,
        43.5117, 19.6975, 35.4570, 46.2676, 63.4265, 29.1046, 65.1467, 98.1661,
        88.9854, 21.6309, 17.0605, 17.7739, 88.4746,  9.3362, 22.9931, 18.3830,
        22.5232, 20.2033, 15.7472, 23.2649, 17.1841, 11.3848, 17.1621, 29.0695,
        19.9548, 39.1753,  8.1614, 18.2276, 19.6349, 10.3323, 21.1283, 11.4660],
       device='cuda:0')
Outer loop valEpocw Maximum [24/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 44.8
model_train val_loss valEpocw [24/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 42.5
model_train val_loss  valEpocw [24/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 98.2
Max_Val Meta Model:  tensor([34.2667, 32.6698, 32.8008, 34.8926, 32.8455, 36.3625, 36.9716, 35.2201,
        35.0132, 36.2278, 34.7934, 37.5725, 33.5260, 30.9923, 35.8806, 29.8247,
        30.5116, 35.6605, 36.1085, 36.1105, 37.2068, 36.0343, 35.5329, 36.4267,
        31.7071, 35.8921, 32.0453, 36.4772, 32.7155, 29.9854, 29.8002, 30.3228,
        34.8567, 29.4757, 30.7160, 30.9495, 30.7233, 29.0214, 30.1950, 35.7864,
        35.1842, 35.1603, 37.1874, 35.0760, 34.5451, 34.5219, 34.2252, 33.2264,
        35.3512, 32.9609, 35.4807, 29.7549, 36.0895, 37.2880, 32.5666, 34.0401,
        33.0045, 34.8070, 31.2082, 35.3297, 34.8723, 35.0548, 34.8170, 33.8078,
        35.6731, 33.5014, 33.5613, 39.9508, 35.0571, 35.2823, 37.5077, 31.1428,
        35.6126, 34.0874, 41.1968, 29.1747, 32.6656, 29.5250, 34.9494, 35.1704],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 11.2966,   4.3256,  20.4424,   6.4129,   9.1384,   8.6120,  10.4966,
         12.7385,   4.6803,  14.2919,   4.7577,   5.8944,   5.7530,   6.8060,
          9.3079,   4.3400,   3.6744,   4.2813,   6.5992,   5.0085,   5.0861,
          7.1839,   4.5585,   6.7513,   7.9570,   4.3823,   8.6844,   2.4711,
          7.6763,   3.9395,   4.2532,   4.0943,   2.5790,   3.8426,   3.6273,
          3.7976,   5.0790,   3.6154,   2.9819,   2.9717,   5.3548,   2.3169,
          4.4346,   3.2051,   4.0094,   1.7768,   6.7043,   5.3876,   5.6868,
          5.3129,   3.9443,   3.1906,   2.4562,   4.2345,   4.2114,   2.8461,
          2.0202,   3.5855,   5.7133,   3.9898,   3.8226,   5.0741,   3.4848,
          2.6433,   6.3559,   3.8081,   3.9759,   3.3342,   5.5314,   3.3081,
          7.0266,   5.3913,   6.8048,   4.2376, 127.7413,   3.9657,   5.2472,
          3.8355,   8.3224,   4.2661], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 32.2603,  11.4287,  57.6921,  15.2541,  21.0271,  19.7003,  22.3897,
         29.7691,  11.0682,  32.5317,  11.2545,  12.9973,  14.2642,  17.0830,
         20.8398,  12.0182,   9.7412,   9.0156,  15.6706,  11.0427,  11.1144,
         16.0813,  10.1091,  15.1377,  20.7571,  10.1304,  22.0917,   5.3765,
         19.1189,  10.6758,  11.7658,  11.1703,   5.9269,  10.4731,   9.6275,
         10.0294,  13.5552,  10.1852,   8.0786,   7.7316,  12.1856,   5.4264,
         10.1824,   7.6441,   9.3489,   4.1265,  15.4951,  13.2236,  12.8664,
         13.0529,   8.9563,   8.8018,   5.6620,  10.0347,   9.4996,   6.7160,
          4.8883,   8.6299,  16.0020,   8.6397,   9.2771,  11.1829,   8.7765,
          6.6292,  14.7528,   9.3126,   9.6113,   7.5164,  12.9689,   7.6580,
         15.3187,  13.5962,  16.1009,  10.2364, 303.2571,  11.0548,  13.0413,
         10.5891,  19.0764,   9.9090], device='cuda:0')
Outer loop valEpocw Maximum [24/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 41.2
model_train val_loss valEpocw [24/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 127.7
model_train val_loss  valEpocw [24/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 303.3
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [88.31885576 97.2137279  90.18408645 97.02489005 97.26733349 96.5997003
 96.99808726 94.61751197 97.44398826 96.46812295 98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.08145612 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.3633484  98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.74618974 97.84237521 92.63410533
 96.90915072 96.22811613 96.9627563  93.87799856 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.99083832 96.13796128 96.24273583 96.9067141
 91.8629159  97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [87.28085671 97.2137279  91.92870457 97.02489005 97.26733349 96.5997003
 96.99808726 94.7332513  97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.16261985
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.05395889 96.13796128 96.24273583 96.9067141
 90.61293113 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [96.81048375  4.21965841 60.80731277 14.81277909 10.21123162 17.66943775
 13.83898667 33.07556199  3.556661   26.11407038  2.43385182  1.63554844
  0.78849805 14.23773339  5.21032601  7.1185907   4.73865632  8.34371663
  1.19143559  5.22941973  5.89751833  0.55598194  2.05140137 16.98982307
 15.80431775 11.28952854 26.60371331 15.41334711  4.62792863  1.3672802
  1.81880475  0.81964072 36.29963568  1.23821672  1.55411134  7.89531521
  7.5138704   1.81463864 20.6124327  29.67575683  5.93238999 45.49938917
 29.11275598 25.87876951 17.05203739 31.93951008  2.83829984  1.69543242
  2.71174159  1.63947485  1.41679846  1.13735107  1.08133297  7.77696759
  1.72109917  8.00058055 64.72302087 25.1139569  16.49536738  9.97877925
 63.28807788 22.68764661 21.39114524 12.16332986  5.86869941  8.70634627
  4.89869787 11.85497305  3.44914683 12.34434212  0.63726193 28.80241745
  6.0968516  24.09917788 11.90624319  7.18431439  1.40804181  2.27895867
  0.20266052  0.93618067]
Accuracy th:0.5 is [45.66708495 97.2137279  71.24304041 97.02489005 97.26733349 75.93962062
 76.24541611 75.59483924 77.19569693 96.34263715 77.47468964 98.52097319
 99.41399349 79.48002583 76.98005629 96.56680596 96.29512311 76.78878181
 98.65376884 98.30776915 79.46418781 77.92424556 98.38695922 76.85335218
 80.63132759 96.65086926 94.0778012  76.52440882 98.01293844 77.35285876
 97.30875598 98.57457877 96.36213009 98.02024829 85.66537932 77.01782386
 76.83873247 88.4492148  97.11504489 74.11337581 78.65157588 92.05906361
 76.27343721 75.84946577 96.9627563  93.87434364 98.02877645 98.57336046
 90.77862112 85.86883688 85.96264665 98.55508583 98.99976852 76.39161316
 98.70615611 76.72055652 71.4501529  92.36729572 96.24273583 96.9067141
 89.79300934 97.17717864 91.68747944 76.68766219 98.42838172 77.27366869
 98.20786784 76.0066276  77.86211182 97.48297414 78.37136487 95.99054592
 77.64525286 95.45083515 76.15404296 81.25266505 85.64101315 77.31021796
 78.42131553 99.14718388]
Accuracy th:0.7 is [45.73287362 97.2137279  71.24304041 97.02489005 97.26733349 75.93962062
 76.24541611 75.87992349 77.19569693 96.4608131  77.47468964 98.52097319
 99.41399349 79.98440565 76.98005629 96.56680596 96.29512311 76.78878181
 98.65376884 98.30776915 79.91739867 77.92424556 98.38695922 77.14452797
 81.35378468 96.65086926 94.0778012  76.52440882 98.01293844 77.35285876
 97.30875598 98.57457877 96.36213009 98.02024829 85.92244247 77.01782386
 76.83873247 88.7562286  97.11504489 74.11337581 79.3094626  92.05906361
 76.27343721 75.84946577 96.9627563  93.87434364 98.02877645 98.57336046
 92.95817546 86.80815292 86.14417466 98.55508583 98.99976852 76.39161316
 98.70615611 76.72055652 71.4501529  92.73766158 96.24273583 96.9067141
 89.79300934 97.17717864 91.87388068 76.68766219 98.42838172 77.27366869
 98.20786784 76.0066276  77.86211182 97.55850928 78.37136487 95.99054592
 77.72931616 95.45083515 76.15404296 81.34038328 85.79208343 77.31021796
 78.42131553 99.14718388]
Avg Prec: is [55.91301531  3.08067357 11.29148126  3.32623584  2.30316983  3.70594233
  3.19759948  5.49383178  2.3179669   3.77874876  1.51791464  1.61207313
  0.59800969  5.04114592  2.69211552  3.17638417  3.66024454  2.67223529
  1.2748335   1.80227147  1.91749197  0.89464546  1.83408091  2.41782716
  5.20894415  3.55728952  6.53687369  3.26509589  2.00757343  1.91682796
  2.65840903  1.32871752  3.62320746  1.6329214   2.37163949  2.38436632
  3.13298352  2.59475053  2.77313503  7.27170663  2.21878041  8.17448136
  3.31717225  3.96472085  3.20760926  6.53388264  2.06976019  1.45636459
  2.10008964  1.57939087  1.84765545  1.64762128  1.02570039  3.08803222
  1.30154869  2.65637025 11.2690752   3.65936332  4.04699032  2.75014443
 10.75489587  2.18417905  3.85759321  3.06935728  1.59843412  2.46009088
  1.85515113  4.34792417  1.25215556  2.41901682  0.16797486  3.40890194
  1.94919967  4.57690758  3.90324281  3.16117942  0.89147172  1.89203549
  0.14761815  0.74467047]
mAP score regular 13.67, mAP score EMA 3.75
starting validation
Accuracy th:0.5 is [89.15215387 97.22450607 89.34399681 96.96290206 97.90716795 96.63651992
 96.80843112 94.47641827 97.38894287 96.41727085 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.46368687
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.31945586 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.40481351 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.76228916 97.82744101 92.8793881
 97.07750953 96.48952338 97.03764606 94.05037746 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.11044672 96.39235618 96.16314124 96.78102499
 92.50815955 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [89.0823928  97.22450607 92.19672621 96.96290206 97.90716795 96.63651992
 96.80843112 94.89249321 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.58290356
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.11542965 96.39235618 96.16314124 96.78102499
 91.46672646 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [97.11001497  5.10877501 64.64000181 16.03717091  9.72765348 16.75510055
 13.99967077 33.67084799  4.51444454 28.76141706  2.40986694  1.54775895
  0.79621682 15.98951989  6.39906602  8.63075288  5.08834212  9.16819544
  1.07125079  6.73627319  5.90973194  0.5314373   1.7852659  16.53649218
 16.27822488 13.29481288 26.41026385 19.09277053  5.33777124  1.45145185
  1.63965644  0.75344617 39.95043984  1.20515787  1.36693465  8.18449086
  7.49566997  2.1776977  27.50540918 31.22250628  6.48836468 45.27789411
 29.84527079 26.11053217 15.4697662  32.1824448   2.97736011  1.72927929
  2.8013817   1.64341076  1.44079393  1.19419619  1.30632094 10.01845752
  1.70656653  8.12644725 59.05045127 26.1411567  15.13194412 10.63074673
 63.57689973 24.66526006 22.43709012 12.34429876  6.63349051  8.42766149
  5.10172276 12.49792467  3.27915549 15.11564272  0.71493605 31.80662053
  6.06206651 26.7631951  14.79471129  6.20600483  1.42343867  2.28315257
  0.20630852  0.87387618]
Accuracy th:0.5 is [45.70097416 97.22450607 69.561751   96.96290206 97.90716795 74.90345566
 75.01557167 74.07878018 76.49799437 96.41228791 76.63253357 98.5325261
 99.34972718 77.41485412 76.5851957  96.31262924 96.21047911 75.99222662
 98.78167277 98.34068316 78.07010987 77.27284052 98.31327703 76.12178289
 77.62164586 96.52938685 94.3393876  76.08939383 97.81747515 76.59017864
 97.52597354 98.67204823 96.39983058 98.18870369 86.18232554 76.2264245
 76.21396716 90.63208511 97.0276802  73.44345616 76.86673144 92.37362035
 75.34444527 75.01806313 97.03764606 94.02795426 98.18621222 98.77668984
 91.8304806  85.89829833 84.31372549 98.55993223 98.87385704 75.32949647
 98.6969629  76.03707302 70.26434462 93.44245958 96.16314124 96.78102499
 90.13379176 97.04761193 91.6809926  76.08441089 98.32075143 76.93898398
 98.13139996 75.26970127 77.31519546 97.50355034 77.70386426 96.07843137
 77.08099758 95.44559882 75.20243167 82.23085931 87.55761517 76.66243117
 77.7835912  99.15040985]
Accuracy th:0.7 is [45.95510377 97.22450607 69.561751   96.96290206 97.90716795 74.90345566
 75.01557167 74.23823405 76.49799437 96.41976231 76.63253357 98.5325261
 99.34972718 77.81099733 76.5851957  96.31262924 96.21047911 75.99222662
 98.78167277 98.34068316 78.39400055 77.27284052 98.31327703 76.22144156
 78.13987094 96.52938685 94.3393876  76.08939383 97.81747515 76.59017864
 97.52597354 98.67204823 96.39983058 98.18870369 86.51119914 76.2264245
 76.21396716 90.80897925 97.0276802  73.44345616 77.38993946 92.37362035
 75.34444527 75.01806313 97.03764606 94.02795426 98.18621222 98.77668984
 93.85604305 86.46136981 84.48563669 98.55993223 98.87385704 75.32949647
 98.6969629  76.03707302 70.26434462 93.76136732 96.16314124 96.78102499
 90.13379176 97.04761193 91.85041234 76.08441089 98.32075143 76.93898398
 98.13139996 75.26970127 77.31519546 97.53344794 77.70386426 96.07843137
 77.11338665 95.44559882 75.20243167 82.31307771 87.74945811 76.66243117
 77.7835912  99.15040985]
Avg Prec: is [54.19666083  3.74019841 14.78974488  4.55533807  1.54987524  4.27944159
 11.8494045   8.69929339  7.47839226  5.11952675  2.29598897  5.17142033
  1.55084199  5.88494873  3.04645476  3.88450768 25.50461162  6.3180885
  1.54104652  2.6207551   3.59248298  1.42154198  1.24399049  5.8297014
  5.74597878 11.27071613  8.15670731  4.55538085  3.90112021  6.98508943
  2.35872021  0.86611576  3.03465995  1.15152632  1.71965458  2.4364594
  2.01065631  2.13973659  2.10377913  6.23078825  1.72667603  6.0659863
  2.19933189  2.72616385  2.38953291  4.95146887  1.7765538   1.06410176
  1.39905939  1.18655626  1.21667672  1.07028267  0.85742513  2.34248505
  0.91621415  1.87170729 10.19341515  3.02306797  3.98816332  2.84655765
  7.94581768  2.02047799  3.37349367  2.55043595  1.38590544  1.89789766
  1.57682491  3.51994692  1.06698565  2.19776892  0.19684365  3.19436909
  1.5441756   4.03021232  3.18428678  2.32040582  0.58289149  1.51504491
  0.12221581  0.60701139]
mAP score regular 14.26, mAP score EMA 4.37
Train_data_mAP: current_mAP = 13.67, highest_mAP = 13.67
Val_data_mAP: current_mAP = 14.26, highest_mAP = 14.26
tensor([0.3382, 0.3887, 0.3712, 0.4280, 0.4370, 0.4462, 0.4707, 0.4388, 0.4351,
        0.4399, 0.4265, 0.4584, 0.4124, 0.4100, 0.4492, 0.3666, 0.3783, 0.4725,
        0.4300, 0.4638, 0.4602, 0.4538, 0.4533, 0.4565, 0.3880, 0.4382, 0.3955,
        0.4612, 0.4065, 0.3811, 0.3726, 0.3776, 0.4393, 0.3818, 0.3731, 0.3846,
        0.3813, 0.3627, 0.3779, 0.3883, 0.4317, 0.4408, 0.4389, 0.4241, 0.4322,
        0.4465, 0.4445, 0.4121, 0.4444, 0.4145, 0.4476, 0.3707, 0.4455, 0.4279,
        0.4486, 0.4312, 0.4310, 0.4228, 0.3682, 0.4662, 0.4273, 0.4544, 0.3967,
        0.4180, 0.4405, 0.4152, 0.4183, 0.4434, 0.4228, 0.4363, 0.4410, 0.4018,
        0.4331, 0.4236, 0.4474, 0.3733, 0.4079, 0.3691, 0.4321, 0.4347],
       device='cuda:0')
Max Train Loss:  tensor([10.7238,  6.4299, 11.9132,  9.5961,  8.0809,  8.7509,  6.4717, 10.9984,
         7.1634,  7.6651,  6.5034,  5.7336,  6.2140,  7.6075,  6.2570,  7.9356,
         7.2314, 12.2118,  8.1623,  4.3509,  4.5662,  9.3065,  9.2266,  7.1555,
         9.0037,  9.7131,  8.0025,  8.3857,  9.5075,  7.1591,  6.9640,  7.2320,
         7.1928,  8.8318,  5.4019,  6.7319,  7.8479,  8.9109,  6.5185, 11.1336,
         7.2705,  7.9432,  7.9338,  7.3917,  6.6833,  7.0547,  8.7200,  8.2558,
         7.9235,  7.7617, 10.0209,  5.0563,  2.7358,  7.3681,  5.5615,  6.4530,
        11.8321,  5.1068,  7.2135,  9.0309, 10.0016,  8.7201,  5.8871,  5.3104,
         7.7768,  6.1038,  4.3375,  6.9360,  6.6070,  6.4879,  7.9969,  9.9225,
        11.0791,  8.3990,  5.4216,  9.8814,  5.6554,  5.7131,  8.5743,  4.6800],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [25/80], Step [000/642], LR 1.0e-04, Loss: 12.2
Max Train Loss:  tensor([13.1690, 13.4030, 17.2989,  9.5570,  8.7303, 15.6852, 16.2229, 15.8033,
         6.7168, 10.0937, 11.5829, 13.1141, 12.1158, 11.1731, 10.8803, 10.1664,
         9.5917,  4.5017,  9.9603,  8.9551, 11.6779,  2.9597, 11.1223, 16.9581,
         8.0883, 10.4673, 14.7711, 17.8273,  7.3200,  8.5129, 11.1510,  9.7669,
        10.0517,  6.4138,  6.3249,  6.7384,  5.4268,  8.5143,  6.4707, 12.5146,
        14.7740, 12.7678,  7.7429,  8.8181, 10.7598, 13.7821,  8.9634, 17.9557,
        11.0541,  9.4082, 10.3671,  8.9942, 12.3397, 10.2178, 13.2678, 16.4511,
        11.0307,  8.5905,  7.6066, 16.8327, 14.0043, 14.5411,  9.7579,  9.4536,
         9.0845,  7.0777,  7.2511, 14.4611, 12.6782, 12.6891,  8.9582, 10.0046,
         5.8101, 11.0281, 10.1843, 10.0122,  5.8688,  4.3141,  4.3664, 16.6493],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [25/80], Step [100/642], LR 1.0e-04, Loss: 18.0
Max Train Loss:  tensor([11.3248,  9.2639, 14.3534,  9.6809, 11.2574, 12.0726,  7.4428, 11.8866,
         7.4641,  8.0276,  9.3112,  5.0011, 11.1478, 10.9195,  7.8492,  6.7464,
         8.9676,  7.9632, 10.3795, 12.8520,  8.6689,  4.4939,  7.3446,  9.1532,
        11.2347, 10.2084, 10.4108,  8.8633,  7.7213,  7.2023,  5.9475,  5.6719,
         8.4728,  6.5241,  5.4320,  5.1730,  7.8495,  7.1090,  4.5989, 10.6148,
         6.0004, 13.4955,  9.5625,  8.7700, 11.0386, 10.0816,  5.3811,  6.5439,
         6.2024,  6.2204,  8.1110,  6.0069,  9.4031,  8.6741,  8.0968,  9.3929,
        12.3183,  9.8079,  7.3094, 10.4490, 13.0973,  9.5524,  9.5704,  9.0357,
         7.3457,  7.7316,  6.2003, 12.1895,  6.9582,  8.1303,  8.4041, 10.1362,
         7.5460,  8.6123, 11.6773,  5.0164,  5.4446,  6.2972,  4.0736,  8.8345],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [25/80], Step [200/642], LR 1.0e-04, Loss: 14.4
Max Train Loss:  tensor([12.1718,  6.2239,  8.4589,  9.3000,  8.0660, 11.8825,  6.1183,  8.2528,
         8.7807,  6.3796,  8.6120,  8.4744,  2.9615, 11.0320, 11.3148,  6.4993,
        13.2456,  0.7578,  9.4261,  4.5515,  7.0890,  4.1906,  9.5486,  6.9691,
        12.2992,  9.0403,  8.5833,  8.9933,  6.5185,  5.0962,  7.6202,  6.3958,
        10.4186,  5.7381,  8.7977, 10.3667,  4.8943,  7.0581,  8.2557, 13.8299,
         7.7395, 13.9063,  7.2508,  7.7850,  9.3612, 10.4229,  8.4492,  8.2035,
         7.7507,  8.3032, 11.2834,  7.3996,  9.8898,  7.5947,  7.3288,  8.7876,
        18.3732,  7.7520,  9.5904, 10.0573, 16.0256, 10.7642, 10.5289,  8.1281,
         9.6273,  7.8359,  9.5611,  9.8109,  7.0197, 10.5811,  8.5707, 12.3213,
         8.7660, 10.6906, 13.3536,  5.8345,  6.4626,  5.4418,  4.9932,  6.1688],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [25/80], Step [300/642], LR 1.0e-04, Loss: 18.4
Max Train Loss:  tensor([10.3837,  8.9927, 11.3480,  7.0973,  8.1055,  7.9326,  8.7790,  8.6738,
         5.5269,  6.8548,  6.7841,  5.0444,  2.6454,  9.3790, 11.8024,  7.1726,
         5.6149, 13.1383, 13.6214, 10.5402,  8.0100,  2.7725,  7.8466, 12.2859,
         9.5475, 10.2214,  7.8249, 10.8840,  6.4019,  5.1938,  8.1906,  9.1929,
        11.5639,  4.3791,  6.1301,  7.6158,  7.0889,  6.0102, 13.9498, 14.1412,
         7.0857, 11.0195,  7.0422,  6.9608,  7.1416,  9.2549,  9.5224,  7.9199,
         8.0337,  6.1417,  6.4664,  4.3734,  7.8601,  7.8504,  6.5910,  8.1499,
        13.6677,  9.8157,  9.0028,  8.5042,  8.9296,  6.4988,  7.3993,  7.0783,
         7.1732,  8.2895,  6.0974,  7.8214,  6.7236,  6.2434,  7.8986,  7.7300,
         8.9720,  9.0314,  7.8606, 11.1955,  5.4994,  5.0567,  4.1202,  7.8223],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [25/80], Step [400/642], LR 1.0e-04, Loss: 14.1
Max Train Loss:  tensor([ 9.3920,  6.4000, 15.0476, 12.7921, 12.1013,  7.3925,  8.0755, 11.1872,
         4.6594,  7.9088,  6.7483,  5.2767,  2.6034,  8.6332,  9.0565,  6.6138,
        10.0155,  7.9139,  8.4206,  5.2279,  5.4517,  2.7267,  7.0383,  8.3502,
         7.2819,  8.6461,  9.0495,  8.9621,  6.4205, 11.0012,  9.5569,  6.3281,
         9.6521,  5.7547,  7.1127,  7.7544,  9.5950,  5.3713,  9.7344,  9.3274,
         6.5344,  7.6758,  9.6093,  7.3373,  8.8821,  7.5982,  4.4212,  7.3054,
         5.6165,  5.4595,  9.1846,  7.2840,  6.9560,  4.6205,  8.9948,  8.8158,
        11.6997,  8.9059,  8.8026,  7.1626, 11.2446,  8.0881,  6.3414,  5.8635,
         7.0244,  8.4626,  6.2348,  8.4899,  8.0053,  8.9855,  7.8629,  7.5142,
        11.3192,  7.1851,  6.1008,  7.6097,  6.4113,  4.2720,  4.0733,  6.2249],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [25/80], Step [500/642], LR 1.0e-04, Loss: 15.0
Max Train Loss:  tensor([13.6512,  7.3818, 10.9730,  9.5059,  9.0196,  8.4809,  8.5355,  9.9062,
        11.6666,  9.0577,  7.6108,  9.1562,  2.7152, 12.5821, 10.7914, 10.3887,
         8.5315,  2.4714,  6.7631,  6.3673,  8.0168,  8.5990,  6.1386,  6.1913,
         8.6109,  8.4660, 10.3155,  8.0699,  9.0703,  6.9154, 10.0354,  6.8694,
         9.4663,  6.2608,  4.3757,  5.6133,  8.4835,  8.5215,  6.0773, 12.9621,
         7.5045, 10.2823,  8.2761,  8.0609,  8.8138,  9.6910,  6.7455,  8.0303,
         8.4621,  8.5347,  6.8152,  5.7300,  8.8927,  4.8030,  7.3919,  9.8019,
        10.9499,  9.4267,  6.2670,  6.5295,  9.2183,  7.7163,  6.5817,  6.9293,
         7.1122,  7.1300,  5.4037,  8.0049,  7.8349,  9.6512,  7.9983, 10.3362,
         9.0437,  9.4775,  7.2564,  4.6959,  5.5859,  6.1140,  4.1997,  7.5843],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [25/80], Step [600/642], LR 1.0e-04, Loss: 13.7
Max_Val Meta Model:  tensor([ 16.1076,  24.8640,  28.0515,  18.3065,   4.5643,   6.9916,   6.7718,
          9.3606,   5.7681,   7.5146,   7.5773,   5.9488,   3.5269,   9.2234,
          5.6379,   5.1018, 117.4349,   3.5044,   2.0499,   4.2421,   5.3451,
          7.7049,   5.1822,   5.3575,  11.3191,   8.9065,  11.2433,   5.5003,
          7.2544,   5.9448,   4.6854,   4.6585,   8.7103,   3.4363,   3.3975,
          4.7814,   6.7194,   5.1801,   4.6486,  17.3796,   6.8262,   9.9846,
          6.4114,   8.8376,   7.7732,  10.7031,   4.5416,   7.5968,   4.8110,
          6.3859,   5.8033,   4.4482,   5.3869,   3.9756,   6.7015,   6.4612,
         11.1468,   6.5232,   9.1199,   4.7642,   6.7732,  12.7973,   5.7689,
          5.3266,   6.5101,   5.3031,   5.4073,   4.8538,   7.7336,  12.0437,
          8.0285,   9.7167,  10.9103,   9.0079,   6.9270,   6.3831,   5.5924,
          4.0594,   4.1978,   6.3759], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 13.5971,  23.8383,  21.8610,  17.9387,   4.9725,   8.3181,   7.9363,
         14.5852,   6.6327,  10.0598,   8.6015,   6.7447,   4.1457,  10.0534,
          5.7061,   5.6078, 110.6380,   5.2850,   2.4557,   4.9185,   5.9902,
          7.4904,   5.4558,   5.6183,  12.0551,  10.6043,  12.2848,   6.4030,
          8.0650,   6.5612,   5.4408,   5.3610,   9.9815,   4.0183,   3.9752,
          5.8216,   9.1728,   5.3953,   5.5363,  16.8759,   6.8764,  10.1262,
          6.6490,   9.0473,   7.8746,   9.7758,   5.0754,   8.2127,   4.9761,
          7.1116,   5.8101,   5.1270,   5.6272,   3.8346,   7.4580,   7.3239,
         12.2756,   6.7452,   9.4037,   5.0615,   6.2849,  11.8719,   6.3920,
          5.7930,   7.2636,   6.0415,   6.1960,   5.8941,   8.1511,  11.9713,
          8.9945,   9.7866,  11.2496,  10.1914,   7.1061,   6.4693,   6.3844,
          4.6056,   4.8805,   7.1558], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 40.2032,  61.3276,  58.8930,  41.9085,  11.3779,  18.6439,  16.8613,
         33.2374,  15.2435,  22.8662,  20.1678,  14.7124,  10.0524,  24.5183,
         12.7035,  15.2980, 292.4750,  11.1858,   5.7113,  10.6039,  13.0162,
         16.5045,  12.0359,  12.3061,  31.0708,  24.1994,  31.0588,  13.8829,
         19.8387,  17.2163,  14.6013,  14.1977,  22.7235,  10.5237,  10.6556,
         15.1367,  24.0556,  14.8761,  14.6514,  43.4581,  15.9295,  22.9707,
         15.1491,  21.3325,  18.2182,  21.8941,  11.4179,  19.9279,  11.1977,
         17.1589,  12.9795,  13.8297,  12.6313,   8.9617,  16.6252,  16.9856,
         28.4800,  15.9526,  25.5423,  10.8566,  14.7086,  26.1259,  16.1112,
         13.8586,  16.4894,  14.5493,  14.8116,  13.2929,  19.2794,  27.4401,
         20.3967,  24.3590,  25.9737,  24.0614,  15.8826,  17.3300,  15.6519,
         12.4769,  11.2947,  16.4621], device='cuda:0')
Outer loop valEpocw Maximum [25/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 117.4
model_train val_loss valEpocw [25/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 110.6
model_train val_loss  valEpocw [25/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 292.5
Max_Val Meta Model:  tensor([30.3070, 32.5669, 37.4677, 34.4540, 40.6504, 45.3261, 44.1656, 35.5474,
        35.2270, 38.3738, 39.1710, 39.9448, 34.6705, 36.3878, 41.7910, 37.2870,
        32.8818, 39.4170, 36.4554, 41.3606, 38.5435, 38.2455, 37.9161, 41.2016,
        32.9347, 35.8743, 33.8376, 38.3711, 34.6518, 31.4580, 31.0495, 31.7495,
        37.0300, 31.1056, 31.5942, 32.2052, 32.3400, 30.4151, 31.8571, 32.9919,
        36.4203, 35.7763, 36.9761, 36.3674, 35.7317, 36.1546, 36.5887, 34.4582,
        37.2607, 34.4779, 37.3751, 31.4239, 36.8260, 35.6401, 36.9750, 36.0348,
        35.7376, 35.6577, 28.3108, 36.7916, 35.6110, 40.2509, 33.0115, 34.5900,
        36.5390, 34.8336, 35.2464, 35.8727, 35.2929, 37.1922, 36.5874, 33.7485,
        36.9793, 35.8657, 39.6143, 30.7104, 34.0910, 31.0122, 37.3272, 36.8866],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([21.0293,  7.0962, 21.3090, 11.7714,  6.2114, 15.1520, 38.3201, 22.4042,
        14.9350, 18.4461,  9.9719, 94.1651,  5.0760,  6.0129,  7.2899,  3.8060,
         5.3672,  8.5091,  1.7154,  9.5893,  4.7373,  8.3921,  4.4584,  6.0358,
         9.8216,  9.6071, 12.1864,  7.3112,  6.9771,  3.2575,  3.7819,  3.6388,
         4.9684,  2.6363,  2.6624,  3.5214,  7.1654,  3.0512,  3.4627,  2.8353,
         2.6198,  2.4486,  3.0851,  2.6083,  2.5013,  2.3301,  3.2809,  5.2394,
         3.1621,  5.1726,  3.6248,  3.5019,  2.3617,  2.0710,  5.1236,  6.0893,
         5.0327,  3.5642,  4.3107,  2.2441,  1.8427,  2.8537,  3.3525,  2.7473,
         5.0441,  4.0629,  4.3119,  6.0978,  3.8898,  4.2400,  6.5214,  3.2781,
         5.7237,  3.9531,  4.6467,  2.6002,  4.4807,  2.5937,  3.3397,  4.9411],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 60.9791,  18.6668,  58.2006,  28.3246,  13.8877,  33.6002,  79.3689,
         52.9956,  35.3198,  41.6975,  22.6243, 208.8597,  12.4833,  14.5262,
         15.7674,  10.3326,  14.1551,  17.8901,   4.0139,  20.4145,  10.4796,
         18.6703,   9.8648,  13.0603,  25.6300,  22.3593,  30.9357,  15.9630,
         17.2824,   8.7288,  10.4095,   9.8478,  11.4290,   7.0611,   7.1423,
          9.2727,  18.9341,   8.5430,   9.2519,   7.4188,   6.0150,   5.7155,
          7.0367,   6.1466,   5.8151,   5.3564,   7.5114,  12.8498,   7.1904,
         12.5821,   8.2208,   9.5608,   5.3828,   4.8982,  11.5912,  14.1939,
         12.0627,   8.5745,  12.5432,   4.9288,   4.4324,   6.6998,   8.6846,
          6.7467,  11.5608,   9.8894,  10.3561,  14.1324,   9.3689,   9.7433,
         15.0753,   8.2908,  13.4096,   9.4774,  10.5318,   7.1727,  11.0454,
          7.1020,   7.6466,  11.4921], device='cuda:0')
Outer loop valEpocw Maximum [25/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 45.3
model_train val_loss valEpocw [25/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 94.2
model_train val_loss  valEpocw [25/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 208.9
Max_Val Meta Model:  tensor([30.3372, 32.4169, 33.3146, 33.7810, 39.2005, 36.8931, 39.5484, 36.6976,
        35.3351, 37.2673, 39.1983, 39.5810, 34.2788, 36.0327, 37.7136, 33.8531,
        32.8567, 37.5157, 36.4263, 39.3308, 36.1995, 38.5928, 37.5517, 38.7229,
        32.6008, 33.3956, 33.5019, 39.1886, 34.3994, 31.0451, 30.6552, 31.3658,
        37.4487, 30.6498, 31.5674, 31.9436, 32.0908, 30.2148, 31.5043, 33.1002,
        36.7456, 36.6770, 38.6007, 36.7678, 36.3826, 37.8220, 39.0574, 34.3591,
        38.4148, 34.2064, 38.5998, 31.3427, 37.0983, 35.6201, 36.6881, 36.9458,
        38.0638, 35.4226, 28.1857, 35.4621, 36.2233, 33.0865, 33.0993, 34.0989,
        36.3530, 34.5674, 35.0812, 35.6654, 35.2940, 37.0229, 36.7506, 33.6463,
        36.8079, 35.8313, 38.0500, 30.4125, 33.9466, 30.7851, 37.3870, 36.6461],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 4.9017,  5.8522,  2.3031,  3.9039,  3.0021,  4.0239,  2.7154,  6.1617,
         3.9978,  2.8195,  7.8744,  3.9093,  3.2050,  6.6386,  2.5464,  5.1855,
         4.0927,  0.6966,  2.0656,  4.1430,  3.9856,  3.2919,  4.4241,  3.9130,
         4.3181,  3.7367, 11.9084, 11.0808,  6.8782,  3.9098,  4.1630,  6.3909,
         7.4710,  4.2286,  4.3362,  5.0962,  7.3409,  5.3122,  5.0028, 22.3794,
        14.3367, 28.1840, 32.6014, 30.9327, 22.4043, 33.8866,  7.7391,  9.8543,
        18.5013,  8.4284, 15.0001, 16.4646, 25.7633, 12.6743, 26.0611, 40.3491,
        39.2444,  8.8201,  8.9603,  6.9907, 36.6007,  6.9029,  9.6959,  9.3859,
        10.1060,  6.5811,  8.0666,  8.8681,  7.8238,  6.0815,  9.2344,  9.9758,
         7.5105, 21.0184,  3.5131,  9.2381,  8.3969,  4.2010,  5.2690,  7.5024],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([14.0949, 15.4498,  6.3631,  9.4570,  6.7793,  9.3319,  5.7922, 14.3343,
         9.4552,  6.4351, 17.9521,  8.5467,  7.9463, 16.2248,  5.7312, 14.1147,
        10.7936,  1.4912,  4.8456,  9.0191,  9.0781,  7.3440,  9.8573,  8.6670,
        11.3289,  8.8971, 30.2311, 24.1183, 17.1235, 10.5906, 11.5771, 17.4892,
        17.1592, 11.4742, 11.6222, 13.5073, 19.5166, 14.9491, 13.4928, 58.5788,
        32.8809, 66.3741, 73.3933, 73.1453, 52.3096, 77.6863, 17.5706, 24.2460,
        41.7968, 20.6555, 33.7395, 45.2634, 59.2252, 30.1544, 59.7295, 94.5901,
        95.4910, 21.3726, 26.1283, 15.5205, 91.0683, 16.1960, 25.0676, 23.4091,
        23.3047, 16.1207, 19.4622, 20.6459, 18.8338, 14.0140, 21.2283, 25.2949,
        17.6877, 50.7096,  8.0449, 25.7994, 20.7930, 11.5675, 12.0231, 17.5371],
       device='cuda:0')
Outer loop valEpocw Maximum [25/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 39.6
model_train val_loss valEpocw [25/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 40.3
model_train val_loss  valEpocw [25/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 95.5
Max_Val Meta Model:  tensor([34.3461, 32.0488, 32.1383, 33.0448, 36.3459, 36.1426, 36.5581, 36.0837,
        34.4795, 37.1126, 36.1351, 35.9894, 33.7574, 35.3374, 36.5434, 33.7555,
        32.6184, 35.9497, 35.5690, 35.9101, 35.5020, 36.1398, 35.7488, 36.7589,
        32.1695, 32.7394, 33.1822, 35.8207, 34.0341, 30.3214, 29.8307, 30.6204,
        36.3181, 29.6321, 31.2377, 31.4554, 31.5205, 29.5723, 30.9203, 32.7586,
        34.3092, 34.7237, 32.4563, 34.9512, 33.9151, 36.0180, 35.6342, 34.0818,
        34.6266, 33.6398, 35.6342, 30.6391, 34.9096, 36.3818, 35.3098, 35.1065,
        37.3838, 36.1163, 28.1947, 34.1534, 33.4311, 31.9473, 36.8335, 33.0468,
        32.6449, 35.7525, 34.7019, 39.6627, 34.8618, 35.0748, 34.4063, 33.1268,
        36.6961, 35.1242, 39.0396, 29.7218, 33.5427, 30.3882, 37.0413, 35.6251],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 11.3060,   4.2990,  13.8001,   8.9863,   7.8986,   8.1633,   8.1140,
         14.2184,   4.7820,  12.6068,   7.3478,   5.2004,   2.4555,   6.6487,
          9.1060,   3.8137,   3.1733,   3.1103,   2.0680,   4.2931,   5.6781,
         10.5480,   5.9599,   6.9022,   7.7868,   4.7078,   8.8272,   3.1291,
          7.3731,   3.3641,   3.6934,   4.0859,   3.9338,   2.9502,   3.0927,
          3.2051,   5.6039,   2.9894,   3.3673,   3.5423,   3.2026,   2.9287,
          3.7379,   3.2966,   3.0490,   2.6876,   3.9148,   6.1296,   4.0164,
          5.0570,   4.7746,   3.9792,   2.8002,   2.5663,   5.7892,   5.8775,
          2.8911,   3.1949,   6.0195,   3.0945,   3.1809,   4.5640,   4.0670,
          3.2451,   5.6208,   4.7173,   4.9327,   3.1246,   4.6769,   3.8493,
          7.3289,   5.2267,   5.0570,   5.0758, 123.8303,   4.1827,   5.1057,
          3.8226,   3.9102,   5.6530], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 31.9042,  11.4453,  39.0433,  21.8181,  18.2388,  19.1069,  17.7092,
         33.3714,  11.4272,  28.6330,  17.3218,  11.7469,   6.0759,  16.4450,
         20.6472,  10.3948,   8.3748,   6.6529,   4.8762,   9.6516,  12.8915,
         24.0684,  13.4860,  15.6231,  20.4875,  11.2729,  22.3889,   7.0072,
         18.3549,   9.1971,  10.4044,  11.2992,   9.1378,   8.1563,   8.2515,
          8.5090,  14.9608,   8.4644,   9.1300,   9.2579,   7.5184,   7.0098,
          9.0187,   7.9081,   7.2534,   6.2925,   9.1413,  15.0085,   9.3392,
         12.4257,  11.0205,  10.9784,   6.5583,   6.0831,  13.3566,  13.8686,
          7.0483,   7.7080,  17.4102,   6.9386,   7.8890,  10.8470,  10.0634,
          8.2267,  13.6061,  11.3780,  11.8632,   7.2118,  11.2434,   9.0820,
         17.0804,  13.3425,  11.9786,  12.3528, 290.9883,  11.7746,  12.6174,
         10.5113,   8.8742,  13.3356], device='cuda:0')
Outer loop valEpocw Maximum [25/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 39.7
model_train val_loss valEpocw [25/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 123.8
model_train val_loss  valEpocw [25/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 291.0
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [88.67338361 97.2137279  91.41823321 96.98468586 97.26733349 96.5997003
 96.99808726 92.85096429 97.44398826 96.45350325 98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.04362764 98.38695922 97.80948088
 95.21935649 96.65086926 94.07901951 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.35482024 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.70720386 97.84237521 92.67187291
 96.90915072 96.22689782 96.9627563  92.96792193 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.96769045 96.13796128 96.24273583 96.9067141
 92.19916911 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.2900184  96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [87.65975073 97.2137279  91.50595144 97.02489005 97.26733349 96.5997003
 96.99808726 94.56390639 97.44398826 96.47908773 98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.27226764
 96.90915072 96.22689782 96.9627563  93.93282246 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 90.94431111 96.13796128 96.24273583 96.9067141
 90.91872662 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [96.93657263  4.03938972 60.47514996 12.68990628 11.35982459 20.5953305
  9.26136338 33.3244232   3.81758522 28.48917358  1.78437912  1.54917127
  0.75156311 14.26515392  8.48620232  7.9923001   8.03066532 11.0888253
  1.15468336  2.16704459  6.03610659 11.00849774  1.65699316  8.04000807
 17.39277127 10.23857386 27.04688184 10.32567406  3.92091311  2.32824022
  6.00672248  0.84633001 31.35870148  1.29672357  1.62895977  9.92097902
  8.96987317 24.41400638  5.36952301 31.26446769 13.20325489 46.12779573
 23.79408422 22.12969759 21.23948865 35.00733849  3.14659007  2.4593025
  9.68660917  2.15669757  8.25294657  1.41387859  1.58974399 19.69868573
  2.041741    3.70640664 64.58843702 19.01060951 13.53772982 10.77547849
 63.47476088 17.58329173 18.55636566 11.53158648  5.40704953  5.07869813
  4.65343024 10.817955    5.14437721  9.19638389  0.60777212 28.09559901
  6.32729922 25.03748355 12.92615815 16.25390281  1.22354506  2.38737161
  0.22444962  0.87752903]
Accuracy th:0.5 is [45.71459899 97.2137279  71.17847005 97.02489005 97.26733349 75.9091629
 76.21495839 75.59971248 77.10432378 96.36213009 77.38087986 98.52097319
 99.41399349 79.57992715 77.03000694 96.56680596 96.29512311 76.80461983
 98.65376884 98.30776915 79.44103995 77.8596752  98.38695922 76.85822541
 80.78848942 96.65086926 94.0778012  76.54024683 98.01293844 77.29072502
 97.30875598 98.57457877 96.36213009 98.02024829 85.80792144 76.8850282
 76.67669741 88.31885576 97.11504489 74.20474897 78.62111816 92.05906361
 76.23079641 75.78976864 96.9627563  93.87434364 98.02877645 98.57336046
 91.13436727 85.7457877  85.90538614 98.55508583 98.99976852 76.3270428
 98.70615611 76.6754791  71.60000487 92.35511263 96.24273583 96.9067141
 89.79300934 97.17717864 91.63631047 76.75223255 98.42838172 77.22128142
 98.20786784 75.93474738 77.83409072 97.47322767 78.33116068 95.99054592
 77.54169662 95.45083515 76.1040923  81.27459461 85.67390748 77.3723517
 78.35918178 99.14718388]
Accuracy th:0.7 is [45.72312715 97.2137279  71.17847005 97.02489005 97.26733349 75.9091629
 76.21495839 75.88966996 77.10432378 96.46934126 77.38087986 98.52097319
 99.41399349 80.02339153 77.03000694 96.56680596 96.29512311 76.80461983
 98.65376884 98.30776915 79.92592683 77.8596752  98.38695922 77.18716877
 81.46465077 96.65086926 94.0778012  76.54024683 98.01293844 77.29072502
 97.30875598 98.57457877 96.36213009 98.02024829 86.05036488 76.8850282
 76.67669741 88.67703854 97.11504489 74.20474897 79.29606121 92.05906361
 76.23079641 75.78976864 96.9627563  93.87434364 98.02877645 98.57336046
 93.3955483  86.75820226 86.12346341 98.55508583 98.99976852 76.3270428
 98.70615611 76.6754791  71.60000487 92.75349959 96.24273583 96.9067141
 89.79300934 97.17717864 91.81418355 76.75345086 98.42838172 77.22128142
 98.20786784 75.93474738 77.83409072 97.55972759 78.33116068 95.99054592
 77.65743595 95.45083515 76.1040923  81.38667901 85.82619607 77.3723517
 78.35918178 99.14718388]
Avg Prec: is [55.93461019  2.92299712 11.16682881  3.34338619  2.2590675   3.70516666
  3.38619937  5.58578441  2.66162429  3.903094    1.66384823  1.61705663
  0.59575189  5.20869643  2.65192153  3.1195683   3.65001877  2.69086295
  1.33725597  1.74847266  1.92877585  0.86943056  1.80697415  2.39962263
  5.16222246  3.60551946  6.61629462  3.17699436  2.07875097  1.82452504
  2.66300501  1.3152961   3.78733421  1.68392911  2.44231955  2.45303437
  3.1131144   2.66430267  2.83824996  7.26418531  2.25287202  8.24725135
  3.28217095  3.95564644  3.17577572  6.48645522  2.05211408  1.60117646
  2.16275526  1.57425311  1.89888455  1.62668987  1.12342768  3.01204572
  1.30505583  2.68648314 11.18207309  3.61789899  3.96378128  2.73700232
 10.87908623  2.16110333  3.95400797  2.94951896  1.65997293  2.59861915
  1.79121822  4.13066866  1.1873713   2.3820677   0.18905089  3.29850741
  1.94435882  4.54590553  3.99130122  3.12662763  0.82613114  1.91795908
  0.14815247  0.74481501]
mAP score regular 14.08, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [89.48102748 97.22450607 91.5688766  96.79597379 97.90716795 96.63651992
 96.80843112 92.29887635 97.38894287 96.42972818 98.5250517  98.5325261
 99.34972718 95.11174228 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 97.77013728 98.31327703 97.88474475
 95.43563296 96.52938685 94.3319132  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.40232205 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.71495129 97.82744101 92.8868625
 97.07750953 96.48703192 97.03764606 92.41348382 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.06560032 96.39235618 96.16314124 96.78102499
 92.61778409 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.23133269 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [89.50594215 97.22450607 92.19921768 96.96290206 97.90716795 96.63651992
 96.80843112 94.34437053 97.38894287 96.41727085 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.68007076
 97.07750953 96.48703192 97.03764606 94.06283479 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.89368911 96.39235618 96.16314124 96.78102499
 91.57136806 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [97.17627211  4.70952413 64.67425218 12.67266674 11.41424355 20.93543642
 10.13324834 33.16163823  4.90992311 31.60851463  1.63722217  1.48291468
  0.68252264 15.1796936   9.02926408  9.43620884  9.24835427 13.87858519
  1.03195294  2.22579088  6.24699237 12.9492389   1.49837936  7.93416676
 16.94592632 10.94293516 26.49177923 10.07306865  4.62771172  2.62817457
  6.60048388  0.78428791 35.70760476  1.25396911  1.42153127 11.07382677
  9.08467262 30.13564975  6.28063669 33.25269052 15.17855455 46.53796323
 26.37835792 23.42686364 21.83629663 35.09827098  3.20005163  2.68637526
 11.35477306  2.17280319 10.92850486  1.63534942  2.00800374 23.45473195
  2.19950153  3.50538087 58.4428426  16.96857335 11.78163085 11.59188479
 63.34829003 18.38557771 18.19988829 11.54056281  5.42772747  4.62696148
  4.43680778 11.32295774  4.87578381  9.70310908  0.5962188  31.13056658
  5.88939281 28.29631707 16.43461948 15.73017943  1.27697723  2.41568574
  0.18605924  0.80986195]
Accuracy th:0.5 is [45.6984827  97.22450607 69.449635   96.96290206 97.90716795 74.77639086
 74.8934898  73.97912151 76.37092957 96.41228791 76.50546877 98.5325261
 99.34972718 77.37000772 76.46311384 96.31262924 96.21047911 75.87512769
 98.78167277 98.34068316 78.01529761 77.15075865 98.31327703 76.00966689
 77.61168    96.52938685 94.3393876  75.97229489 97.81747515 76.46311384
 97.52597354 98.67204823 96.39983058 98.18870369 86.17983407 76.09935969
 76.09686823 90.53990084 97.0276802  73.32635723 76.80693624 92.37362035
 75.21738047 74.89598126 97.03764606 94.02795426 98.18621222 98.77668984
 92.16932008 85.83850313 84.33614869 98.55993223 98.87385704 75.2074146
 98.6969629  75.91000822 70.15222862 93.39761317 96.16314124 96.78102499
 90.13379176 97.04761193 91.71587313 75.96232902 98.32075143 76.82188504
 98.13139996 75.1476194  77.19809652 97.50355034 77.57679946 96.07843137
 76.96639011 95.44559882 75.0952986  82.15611531 87.54266637 76.5403493
 77.6565264  99.15040985]
Accuracy th:0.7 is [45.95510377 97.22450607 69.449635   96.96290206 97.90716795 74.77639086
 74.8934898  74.16349005 76.37092957 96.41976231 76.50546877 98.5325261
 99.34972718 77.80103147 76.46311384 96.31262924 96.21047911 75.87512769
 98.78167277 98.34068316 78.33669681 77.15075865 98.31327703 76.11679996
 78.08755014 96.52938685 94.3393876  75.97229489 97.81747515 76.46311384
 97.52597354 98.67204823 96.39983058 98.18870369 86.49126741 76.09935969
 76.09686823 90.72925231 97.0276802  73.32635723 77.31021252 92.37362035
 75.21738047 74.89598126 97.03764606 94.02795426 98.18621222 98.77668984
 94.21979719 86.41901487 84.50307696 98.55993223 98.87385704 75.2074146
 98.6969629  75.91000822 70.15222862 93.72150385 96.16314124 96.78102499
 90.13379176 97.04761193 91.86286967 75.96482049 98.32075143 76.82188504
 98.13139996 75.1476194  77.19809652 97.53344794 77.57679946 96.07843137
 77.00625358 95.44559882 75.0952986  82.24580811 87.74696664 76.5403493
 77.6565264  99.15040985]
Avg Prec: is [54.20429388  3.74370624 14.80964115  4.55402491  1.55596825  4.28389172
 11.75007148  8.70778709  7.4545236   5.14152741  2.29520564  5.1951697
  1.5551303   5.8774955   3.05230098  3.92244497 25.51272852  6.3516748
  1.53440231  2.62241733  3.59538053  1.41372157  1.15853991  5.80336323
  5.74780991 11.31855597  8.17933954  4.54767305  3.9106732   7.15817885
  2.3590381   0.86680401  3.0057043   1.15084692  1.73259485  2.44199846
  2.01501186  2.15181778  2.20251995  6.23587604  1.73452733  6.09162675
  2.2036665   2.7402465   2.36007231  4.93152031  1.78058706  1.06476377
  1.44845536  1.18900293  1.21771888  1.04000583  0.76792879  2.30403181
  0.91770332  1.87119865 10.18623617  3.05477105  3.9747265   2.88390759
  7.93611672  2.00948366  3.36449399  2.5609936   1.3536058   1.89602658
  1.57415936  3.52642152  1.06345278  2.19082672  0.19658211  3.18729553
  1.53847268  4.05213413  3.18503579  2.31485362  0.58078821  1.51186944
  0.122499    0.60768408]
mAP score regular 14.70, mAP score EMA 4.37
Train_data_mAP: current_mAP = 14.08, highest_mAP = 14.08
Val_data_mAP: current_mAP = 14.70, highest_mAP = 14.70
tensor([0.3434, 0.3840, 0.3693, 0.4154, 0.4365, 0.4359, 0.4587, 0.4359, 0.4281,
        0.4387, 0.4276, 0.4466, 0.4083, 0.4140, 0.4432, 0.3696, 0.3798, 0.4628,
        0.4303, 0.4519, 0.4400, 0.4449, 0.4438, 0.4504, 0.3833, 0.4229, 0.3958,
        0.4476, 0.4037, 0.3736, 0.3642, 0.3693, 0.4332, 0.3732, 0.3721, 0.3804,
        0.3778, 0.3580, 0.3745, 0.3834, 0.4210, 0.4261, 0.4156, 0.4214, 0.4218,
        0.4412, 0.4374, 0.4100, 0.4317, 0.4108, 0.4381, 0.3671, 0.4358, 0.4262,
        0.4381, 0.4291, 0.4241, 0.4195, 0.3519, 0.4477, 0.4090, 0.4231, 0.4031,
        0.4083, 0.4198, 0.4186, 0.4166, 0.4312, 0.4163, 0.4256, 0.4152, 0.3968,
        0.4287, 0.4184, 0.4410, 0.3645, 0.4060, 0.3664, 0.4291, 0.4268],
       device='cuda:0')
Max Train Loss:  tensor([ 9.6862,  6.4696, 12.1709,  7.7239,  7.9673, 10.2248,  7.8700,  9.2527,
         9.5424,  8.6950,  8.7452,  4.9591,  2.6679,  9.7026,  6.3383, 10.1738,
         8.7163,  8.5933,  2.0134,  5.2735,  7.7118,  8.3917,  6.0289,  9.5661,
        10.4291,  9.9301, 10.8667,  8.7489,  7.7530,  8.4724,  6.2283,  4.5395,
         8.2013,  6.3540,  7.1528,  6.4861,  9.3519,  6.9511,  5.4044, 10.5414,
         6.9618, 10.7200,  7.3240,  9.0802,  8.9077, 11.0665,  5.2648,  9.5072,
         5.4906,  6.2839,  8.4129,  6.1021,  4.3263,  7.8633,  7.3296,  8.0804,
        11.6589, 11.3329,  6.2696,  8.5023, 13.3238,  7.3799,  6.6105,  7.9488,
         7.4856,  7.1707,  7.0540,  7.2252,  5.2483,  6.9378,  7.5059,  9.7758,
         7.8120,  9.6687, 10.9000,  6.7051,  6.2549,  4.1409,  4.1536,  8.5937],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [26/80], Step [000/642], LR 1.0e-04, Loss: 13.3
Max Train Loss:  tensor([12.0278,  8.4247, 11.8640,  8.1933, 13.0344,  5.7752, 12.2778, 14.2493,
         8.9506, 10.5780,  7.7033, 18.0211, 16.7093, 10.3462, 13.0507,  6.5604,
         7.7129, 10.6411, 17.1432, 11.8853,  9.2399, 13.0356, 11.9239, 17.4721,
         7.8730,  8.7541,  8.6885, 12.7420, 13.3611,  7.4804,  7.0133,  5.2769,
        11.3883,  9.0907,  8.0039,  9.1785,  7.3207, 11.2715,  7.6356,  5.3300,
         7.4819, 13.7663, 11.5246,  9.6634,  9.5166, 17.3097, 14.9996, 11.5940,
         8.3367, 15.2872,  8.5012,  5.4452, 13.6154,  8.5099,  8.7065,  7.0933,
        17.2156, 10.1646,  7.9461, 12.5618, 10.1019,  6.9919, 10.3048,  7.4661,
        11.1288, 19.3121, 14.2044, 13.9975,  8.4308, 11.6170,  5.9940,  9.7166,
        13.0583, 12.6442, 13.3738,  9.5126,  6.1121,  6.0620, 16.7740,  9.2896],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [26/80], Step [100/642], LR 1.0e-04, Loss: 19.3
Max Train Loss:  tensor([10.1358,  7.1433, 11.7317,  5.5029,  6.8676,  5.5874, 13.1004,  8.4167,
        12.3070,  7.8624,  6.0517,  9.7102,  6.7549, 11.2732,  8.1436, 10.3706,
         9.1695, 10.6698,  6.1793, 15.5273,  8.9714,  4.5546,  7.1089, 14.5338,
        11.3803, 10.1032, 10.5718,  8.3480,  4.2390,  4.7324,  7.4031,  5.6215,
        11.6572,  8.5482,  6.0828,  8.3867,  6.4890,  5.6610,  7.9736, 10.2281,
         6.7406, 11.2121,  5.7793,  8.6441,  6.7438, 10.0770,  6.3151,  8.0326,
         6.0688,  7.3174,  6.4185,  3.9382,  7.2143,  7.7600,  7.1124,  5.1515,
        12.0266,  7.0023,  9.3250, 11.2864,  7.0260,  8.0179,  9.4765,  6.8724,
        10.3716,  7.4696,  7.6777, 10.8985,  6.4444,  3.6148,  4.5067,  7.0506,
         5.4369,  7.2758,  7.2904, 11.4590,  8.0841,  6.6983,  5.7175,  9.7820],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [26/80], Step [200/642], LR 1.0e-04, Loss: 15.5
Max Train Loss:  tensor([13.2836,  4.6806,  8.3203,  7.3945,  7.2977,  4.2729, 16.6352,  8.2739,
         4.5510, 10.1536,  9.4061,  7.8937,  6.1091, 11.8340,  7.1339,  6.8440,
         6.4029,  6.3072,  9.7362, 12.8017,  6.2890,  6.7315,  8.1661,  6.7818,
         7.1422,  6.9204,  7.4113,  7.5719,  5.0630,  4.8966,  4.6911,  6.9042,
         6.3396,  8.4165,  7.1436,  6.1425,  6.9455,  8.2894,  8.2484,  7.2069,
         7.5728,  9.6876,  7.8564, 10.4044,  9.7233,  9.2815,  6.3264,  8.1988,
         6.1617,  5.6586,  7.7754,  5.2457,  8.1969,  6.4552, 11.7293,  7.7946,
        10.4027,  6.6000,  3.7524,  9.3007,  8.7021,  6.3228,  6.6130,  9.6465,
         7.2057,  9.7996,  7.3211, 11.1161,  6.1096,  7.6198,  4.5239,  7.4383,
         6.2666,  6.8014, 10.6983,  4.2849,  5.3317,  5.8972,  5.7459, 10.3245],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [26/80], Step [300/642], LR 1.0e-04, Loss: 16.6
Max Train Loss:  tensor([11.1420,  9.1767, 10.8988,  8.9475,  7.6115,  7.4253,  4.7966,  8.1846,
         8.8664,  6.7666,  8.2874,  7.7822,  6.1457,  8.8790,  9.0265,  8.3074,
        10.0455,  6.4639,  6.2368,  8.1254,  6.3448,  4.6012,  7.4382,  9.6694,
        10.0183, 11.3903, 10.5182, 10.1799,  7.8692,  3.6990,  7.3576,  6.7391,
         8.4950,  4.5848,  8.5609,  7.3926,  5.1175,  8.3448,  6.7846, 10.7675,
         6.5551, 11.0254,  6.7501,  8.0588,  7.5248, 11.7099,  6.3285,  9.9073,
         7.3906,  7.4779,  6.8119,  4.5109,  6.4807, 10.8913,  7.0307,  9.2730,
         8.3235,  7.9526,  6.1965,  7.0481,  8.9637,  7.3995,  9.4860,  6.6594,
         4.7566,  7.2727,  8.1999,  5.6231,  7.8006,  7.2425,  4.5830,  9.9091,
         9.5347,  8.7275,  8.2573,  5.8835,  6.3599,  6.0728,  5.7901,  9.1262],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [26/80], Step [400/642], LR 1.0e-04, Loss: 11.7
Max Train Loss:  tensor([12.0548,  9.1285, 11.1075,  8.5535,  7.6075,  7.7763, 12.0619,  8.1232,
         6.2831,  6.9282,  7.9854,  7.6757,  7.5815, 14.1747,  8.7446,  7.8043,
         6.1107,  6.6295,  7.7269,  6.7814,  6.2956,  5.6707,  5.9605,  9.6627,
         7.5258,  5.1619,  9.0251,  7.7675,  8.0229,  6.3955,  6.8773,  6.6321,
         7.4867,  7.7232,  5.2740,  4.6886, 11.9327,  6.8785,  8.3625,  8.5986,
         9.3416,  8.4488,  7.0518,  7.5042,  6.5187,  6.9553,  6.1861,  8.5049,
         7.8781,  9.2832, 10.3582,  5.3579,  6.5090,  9.8316,  8.4350,  8.9774,
        11.0329,  8.5953,  6.7660,  7.7753, 13.6718,  5.7596,  8.5996,  8.2101,
         5.6001, 10.3396,  7.4516,  9.6009,  5.4458,  4.6711,  4.6366,  6.7794,
         7.3380,  8.5959,  6.9014,  6.3373,  5.4451,  5.8536,  5.8483,  8.5770],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [26/80], Step [500/642], LR 1.0e-04, Loss: 14.2
Max Train Loss:  tensor([14.1222,  8.9978,  8.7311,  8.3856,  7.2525,  6.5392,  6.1721,  9.4504,
         5.3269,  8.1206, 10.3105,  4.3017,  7.0624,  6.7696,  5.3199,  7.7537,
         8.5306,  7.7334,  8.2313,  4.6519,  8.7202,  4.7631,  7.4967,  5.8799,
        14.8878,  9.6153,  7.5552,  8.2794,  5.5027,  5.2383,  9.1826,  4.6416,
         6.1395,  7.2175,  8.5030,  6.1554,  8.6586,  8.4175,  7.9279,  8.3333,
         7.1953, 10.4592,  6.9403,  7.0684,  8.2825, 13.2213,  7.0353,  8.3638,
         6.5083,  6.7275,  8.8084,  5.8542,  7.3475, 12.0227,  8.2520,  9.1062,
         9.0645,  7.4205,  6.2076,  9.1235, 13.4472,  7.5517,  6.6642,  8.2479,
         5.7401,  7.4008,  7.5191, 12.3572,  6.3339,  7.3735,  4.7328,  9.6800,
         6.3970, 10.3016, 10.8986,  4.4484,  8.0268,  5.2012,  6.7411,  9.4131],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [26/80], Step [600/642], LR 1.0e-04, Loss: 14.9
Max_Val Meta Model:  tensor([ 19.1902,  24.1039,  29.7085,  20.6658,   5.6321,   5.8256,   6.0350,
          8.5551,   5.4681,   7.6198,   6.3221,   6.4687,   6.9452,   8.1290,
          5.9610,   4.7401, 119.7858,   5.1153,   6.4413,   4.4598,   4.3427,
          4.7255,   3.8578,   5.1713,  10.5072,   8.8467,  11.5006,   3.8912,
          6.4745,   6.4434,   5.8712,   4.6239,   6.3962,   3.3431,   4.5516,
          4.5599,   4.3303,   6.4453,   5.5718,  20.6941,   7.0009,   9.1883,
          5.7331,   8.1862,   7.9097,   9.7934,   5.3338,   8.3357,   5.4869,
          6.5749,   5.8446,   3.3134,   8.0932,   4.4206,   6.3158,   7.5029,
         14.1360,   8.6597,   7.9281,   4.4756,   6.2929,  11.9390,   5.8137,
          5.3697,   4.1409,   4.8973,   6.2282,   5.4470,   7.8962,  11.5055,
          4.7177,   9.2793,  10.0114,   6.7545,   7.4515,   5.9701,   5.5336,
          4.9453,   5.9592,   8.7264], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 14.6945,  23.3636,  23.7468,  20.0622,   6.3952,   7.3002,   6.8061,
         12.0339,   6.0552,   8.7923,   6.9529,   7.0385,   7.6577,   9.0370,
          5.9171,   5.2356, 113.6939,   5.7738,   7.1078,   4.9489,   4.9229,
          5.3540,   4.2412,   5.3788,  11.1285,   9.7078,  13.6099,   4.4957,
          7.2698,   6.7197,   6.3886,   5.1836,   7.0714,   3.8037,   5.1051,
          5.1751,   5.0844,   6.7174,   6.2594,  20.6954,   6.9051,   9.3820,
          5.8044,   8.0006,   8.1254,   9.6799,   5.6983,   8.8759,   6.0744,
          7.0813,   6.4797,   3.7740,   8.5280,   4.1642,   6.8958,   7.1148,
         14.9667,   8.8887,   8.1359,   4.8543,   5.1485,  11.3249,   6.1247,
          5.7931,   4.5674,   5.2158,   6.8383,   6.1075,   8.4260,  11.3782,
          5.3013,   9.0842,  10.3886,   7.7436,   7.5402,   6.3015,   6.1682,
          5.4847,   6.6110,   9.4249], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 42.7905,  60.8395,  64.3094,  48.2907,  14.6515,  16.7467,  14.8385,
         27.6089,  14.1452,  20.0431,  16.2607,  15.7593,  18.7573,  21.8273,
         13.3520,  14.1641, 299.3826,  12.4760,  16.5195,  10.9506,  11.1879,
         12.0332,   9.5560,  11.9418,  29.0331,  22.9553,  34.3886,  10.0436,
         18.0093,  17.9845,  17.5431,  14.0346,  16.3233,  10.1934,  13.7193,
         13.6043,  13.4573,  18.7625,  16.7154,  53.9841,  16.4033,  22.0208,
         13.9672,  18.9857,  19.2627,  21.9420,  13.0283,  21.6474,  14.0719,
         17.2383,  14.7913,  10.2801,  19.5682,   9.7694,  15.7407,  16.5826,
         35.2939,  21.1910,  23.1205,  10.8425,  12.5867,  26.7645,  15.1943,
         14.1889,  10.8796,  12.4598,  16.4155,  14.1635,  20.2385,  26.7371,
         12.7696,  22.8944,  24.2354,  18.5095,  17.0968,  17.2880,  15.1928,
         14.9683,  15.4056,  22.0843], device='cuda:0')
Outer loop valEpocw Maximum [26/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 119.8
model_train val_loss valEpocw [26/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 113.7
model_train val_loss  valEpocw [26/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 299.4
Max_Val Meta Model:  tensor([30.2160, 29.8468, 35.5130, 39.7990, 37.0562, 36.9971, 42.3222, 37.8888,
        36.2575, 36.6661, 36.7877, 43.2233, 34.8616, 37.8034, 40.2621, 31.2153,
        32.9647, 37.9716, 40.6126, 42.6808, 37.3470, 38.1409, 37.1146, 36.7793,
        32.8287, 35.9158, 33.7453, 36.9653, 34.6459, 31.3445, 31.1713, 31.7030,
        36.6811, 31.0707, 31.4820, 32.0624, 32.2163, 30.4758, 31.8888, 32.9196,
        35.2442, 35.1593, 35.0326, 36.1190, 34.8923, 36.0009, 36.8348, 34.2578,
        36.4300, 34.2918, 36.9619, 31.2966, 36.9123, 35.6927, 36.3711, 36.0638,
        36.1500, 35.8353, 29.6619, 36.4047, 34.6020, 33.8099, 33.9990, 34.6966,
        34.9771, 35.1695, 35.0520, 35.6369, 35.1888, 36.3311, 34.3912, 33.5910,
        36.0789, 35.5397, 36.9600, 30.6294, 33.9604, 31.0863, 35.3984, 36.8175],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([20.4488,  7.3254, 18.9589,  6.1151, 10.2405, 15.6600, 37.2488, 20.6773,
        15.2174, 15.6758,  9.0775, 70.9060,  7.5114,  6.0758,  8.0637,  4.1988,
         5.2387,  9.9834,  6.3256, 10.7623,  4.4270,  4.1795,  3.7481,  6.7900,
        10.0361,  8.8705, 12.6595,  6.9680,  7.5156,  3.3928,  4.7170,  4.1772,
         3.8973,  2.9781,  4.0951,  3.6753,  4.8973,  4.4715,  4.3988,  0.9663,
         3.0934,  1.6947,  3.1649,  2.5221,  3.1414,  2.0938,  4.3604,  6.8197,
         4.8619,  6.0004,  5.2078,  2.9395,  5.9002,  2.6078,  5.5127,  3.9938,
         6.9905,  3.7692,  4.2677,  2.4798,  1.8813,  3.9485,  3.3756,  3.4355,
         3.2444,  3.2538,  5.3460,  6.3336,  4.9223,  4.2503,  4.2066,  3.7201,
         4.9283,  3.8345,  6.6681,  3.0989,  5.0121,  3.9100,  5.2190,  7.9261],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 60.5592,  19.8367,  50.7948,  14.3678,  23.3630,  36.3133,  79.9285,
         47.5452,  35.5609,  36.1277,  21.1885, 158.9310,  18.5207,  14.1627,
         17.9945,  11.3811,  13.8165,  21.6637,  13.9762,  22.9848,  10.1008,
          9.3506,   8.4398,  15.3693,  26.2751,  21.1541,  32.1446,  15.6530,
         18.6113,   9.1192,  12.9771,  11.3156,   9.0095,   7.9780,  11.0513,
          9.7061,  12.9347,  12.5262,  11.7518,   2.5227,   7.3466,   3.9785,
          7.6151,   5.9652,   7.4751,   4.7586,   9.9228,  16.8429,  11.3258,
         14.6694,  11.9568,   8.0275,  13.5090,   6.1536,  12.6691,   9.3199,
         16.3848,   9.0350,  12.2047,   5.5770,   4.6603,   9.5037,   8.4947,
          8.3988,   7.7265,   7.8109,  12.9201,  14.7878,  11.8936,   9.9670,
         10.2895,   9.4027,  11.6237,   9.1958,  15.2576,   8.5559,  12.3944,
         10.7030,  12.6379,  18.5296], device='cuda:0')
Outer loop valEpocw Maximum [26/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 43.2
model_train val_loss valEpocw [26/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 70.9
model_train val_loss  valEpocw [26/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 158.9
Max_Val Meta Model:  tensor([28.8424, 30.4521, 33.6518, 37.1506, 37.0247, 35.5547, 36.7933, 36.6761,
        37.7696, 36.0568, 34.8371, 38.4341, 34.6991, 37.2286, 37.5538, 31.6537,
        33.0697, 35.9219, 36.1655, 38.6510, 37.1359, 34.5303, 36.3559, 37.3068,
        32.5590, 37.9333, 33.4572, 36.0066, 34.5168, 31.3999, 31.2888, 31.7057,
        36.3899, 31.4758, 31.2338, 32.0569, 32.1467, 30.8792, 31.9025, 33.4357,
        34.9353, 37.5142, 38.3501, 36.8768, 35.4886, 39.2086, 38.8152, 34.0371,
        37.5829, 34.1972, 38.5558, 31.4467, 37.5914, 35.7979, 37.1290, 39.2406,
        40.3270, 36.0178, 30.2872, 36.8640, 35.7820, 34.4362, 34.2414, 35.2588,
        34.1695, 35.0724, 34.9853, 36.0615, 35.0923, 36.3323, 33.6522, 34.0706,
        36.7988, 35.9203, 38.1115, 31.0283, 34.1129, 31.0816, 34.9639, 36.8853],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 5.4860,  5.8740,  3.0158,  4.5236,  3.1478,  1.9379,  1.8243,  5.9264,
         3.5122,  3.5693,  5.9757,  3.9718,  7.2403,  6.9956,  2.3852,  4.5555,
         4.4042,  2.2087,  7.0768,  3.8199,  3.9011,  5.7683,  4.0555,  3.4688,
         2.9877,  3.8807, 12.6903,  7.6545,  4.8694,  4.7559,  5.1382,  6.3893,
         5.4160,  4.2666,  5.5168,  4.9286,  7.8370,  6.8416,  5.8361, 13.5060,
        14.5435, 27.0200, 30.3958, 29.8698, 22.4832, 32.3889,  9.0323, 10.6448,
        19.2120,  8.6340, 16.3807, 16.8349, 23.8028, 13.0829, 26.7603, 31.2174,
        34.6443, 17.8431,  5.5210,  6.1859, 35.9654,  6.5788, 12.3736,  9.5071,
         8.2763,  7.2637,  9.1887,  9.1211,  7.5387,  4.9005,  5.4898, 11.2419,
         6.7908, 16.5501,  4.2080,  6.9840,  8.2090,  5.3330,  6.6359, 10.0583],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([16.6249, 15.4837,  7.7789, 10.5726,  7.1666,  4.4686,  4.0583, 13.4979,
         7.9814,  8.2367, 14.2808,  8.7962, 17.9153, 16.2578,  5.3927, 12.1672,
        11.5704,  4.9255, 16.3509,  8.2909,  8.8570, 13.3938,  9.2176,  7.7396,
         7.8307,  9.0410, 32.3822, 17.3430, 12.0545, 12.7541, 14.0716, 17.3055,
        12.4944, 11.2767, 14.9996, 13.0102, 20.7669, 18.9232, 15.5753, 34.8723,
        35.1233, 61.9352, 73.4587, 70.4272, 53.7523, 71.2387, 20.0708, 26.4880,
        44.4894, 21.1770, 37.0276, 46.0364, 54.1603, 30.9429, 61.3454, 73.2887,
        78.3313, 42.6579, 15.4757, 13.7042, 89.2924, 15.5443, 30.9966, 22.9151,
        19.8136, 17.4811, 22.2693, 20.9910, 18.2691, 11.4795, 13.7161, 28.0490,
        15.7265, 39.4802,  9.3176, 19.0615, 20.2685, 14.5941, 16.2606, 23.4633],
       device='cuda:0')
Outer loop valEpocw Maximum [26/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 40.3
model_train val_loss valEpocw [26/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 36.0
model_train val_loss  valEpocw [26/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 89.3
Max_Val Meta Model:  tensor([31.3051, 29.7529, 32.0324, 36.5826, 36.7746, 34.6249, 36.9366, 36.0427,
        35.5666, 36.1896, 34.4481, 36.3150, 34.6708, 36.2501, 35.3274, 31.2170,
        32.9744, 36.4914, 35.4885, 36.4967, 35.6308, 33.9780, 34.3088, 35.9849,
        32.7516, 34.7213, 33.6695, 35.9056, 34.5250, 30.9361, 30.7289, 31.1977,
        33.9284, 30.5027, 31.6119, 31.9652, 31.8690, 30.4389, 31.5796, 32.9584,
        35.3631, 35.4268, 34.8574, 36.4885, 34.6481, 35.0480, 35.4783, 34.2469,
        36.2772, 33.9674, 35.4004, 31.0815, 35.5729, 36.5311, 34.9117, 34.0827,
        35.1153, 35.0455, 29.4440, 35.2449, 34.5034, 34.0731, 35.8927, 35.6660,
        33.4006, 36.2647, 36.4743, 37.5671, 35.1437, 34.1999, 34.9370, 33.4943,
        35.9083, 33.4170, 41.4353, 30.3472, 33.9712, 30.9581, 35.6644, 36.0918],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 10.7126,   4.8786,  12.7352,   4.4057,  11.3354,   7.6484,   9.6179,
         15.2351,   4.9108,  11.6990,   6.4808,   7.2625,   6.6155,   7.0074,
          9.5670,   4.2993,   4.0500,   6.6114,   6.8746,   5.6209,   5.2606,
          4.6380,   4.7350,   7.9896,   7.6809,   4.1623,   8.2664,   2.3727,
          8.0211,   3.9104,   4.4192,   4.7389,   2.8881,   3.4211,   4.7424,
          3.4355,   4.0184,   4.3413,   3.7589,   1.8057,   4.2758,   2.0588,
          3.8713,   3.3297,   4.1090,   2.9577,   5.2565,   7.8637,   5.7044,
          5.9658,   5.9448,   3.4196,   6.7490,   3.7127,   6.2710,   4.1034,
          3.8420,   4.2437,   5.7524,   3.8693,   3.7817,   6.1104,   4.2318,
          4.2592,   4.0057,   4.4254,   6.4904,   4.2352,   5.7191,   3.9481,
          4.9556,   5.9856,   4.9697,   4.9281, 109.3460,   4.3126,   5.7439,
          5.1623,   6.0725,   8.9252], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 31.2108,  13.2327,  34.6820,  10.4661,  26.0632,  18.0222,  21.4380,
         35.4551,  11.6279,  27.0651,  15.6351,  16.6045,  16.4153,  16.7545,
         22.1686,  11.6634,  10.6945,  14.5602,  16.2066,  12.6540,  12.1549,
         10.9328,  11.0673,  18.3673,  20.1405,  10.0735,  21.0182,   5.3804,
         19.9249,  10.6659,  12.3502,  13.0652,   6.8588,   9.3505,  12.7653,
          9.1142,  10.7396,  12.1958,  10.1566,   4.7187,  10.1360,   4.8535,
          9.3757,   7.8236,   9.8602,   6.8526,  12.2762,  19.4573,  13.3492,
         14.7355,  13.9787,   9.4184,  15.8329,   8.7776,  14.6805,   9.8682,
          9.1783,  10.3526,  16.6134,   8.7850,   9.4289,  14.6664,  10.4140,
         10.4185,   9.7746,  10.4737,  15.3208,   9.7599,  13.8579,   9.5934,
         11.9508,  15.1943,  11.8037,  12.3149, 257.0390,  12.0440,  14.2217,
         14.2184,  14.6171,  21.0836], device='cuda:0')
Outer loop valEpocw Maximum [26/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 41.4
model_train val_loss valEpocw [26/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 109.3
model_train val_loss  valEpocw [26/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 257.0
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [88.85491161 97.2137279  91.79956385 97.02489005 97.26733349 96.5997003
 96.99808726 93.90480135 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 93.83779437 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.53298571
 96.90915072 96.22689782 96.9627563  93.93769569 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46104458 91.39021211 96.13917959 96.24273583 96.9067141
 92.55004203 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [86.53403345 97.2137279  91.60585276 97.02489005 97.26733349 96.5997003
 96.99808726 94.68817388 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.12653355 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.06759177
 96.90915072 96.22689782 96.9627563  93.87921687 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 92.02251435 96.13796128 96.24273583 96.9067141
 91.58635982 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [97.06653207  4.0177147  62.0122416   7.28127077 10.03172024 24.29061989
 11.19392236 34.08764148  4.74936694 26.19486253  1.74950295  2.62750072
  0.94068978 13.96377512  8.70369923  6.99688934  5.54800412  5.35613058
  0.89120139  1.65726059  1.82159577  0.47218215  1.1439441  11.70217568
 17.91012529 10.09771652 28.45582006 15.27096014  4.47108163  1.22776374
 30.66286952  0.83871226 28.55456313  1.31138893  1.36324205  4.30563782
  3.6424109  10.90692522 17.69658591 33.65807416 13.34795868 46.74199223
 27.9090691  29.15871102 23.09313166 36.70462291  2.92926616  2.05262447
  2.87212589  2.00532176  1.322       1.29223875  1.28098225 16.52553191
  1.87832938 12.55526411 65.28219501 24.20212916 11.64418648 10.64746312
 65.7274179  14.43438339 19.07662707 11.86752638  7.92327705  9.36271901
  7.16516287 10.58690585  4.36970994  8.6493419   0.65557197 32.67117265
  9.02919441 25.52068625 16.60565377  7.70227252  1.43748685  2.29423462
  0.21688925  1.03876436]
Accuracy th:0.5 is [45.68657789 97.2137279  71.27106151 97.02489005 97.26733349 75.98713466
 76.26369074 75.79951511 77.25295744 96.33167237 77.52707691 98.52097319
 99.41399349 79.93445499 77.1908237  96.56680596 96.29512311 76.86797188
 98.65376884 98.30776915 79.68835662 78.02292857 98.38695922 77.03122525
 81.1478905  96.65086926 94.0778012  76.57923271 98.01293844 77.41986574
 97.30875598 98.57457877 96.36213009 98.02024829 85.70802013 77.08970407
 76.81558461 88.26890511 97.11504489 74.22667852 78.76122367 92.05906361
 76.26978229 75.84581085 96.9627563  93.87434364 98.02877645 98.57336046
 91.46209232 85.912696   86.07229444 98.55508583 98.99976852 76.46836661
 98.70615611 76.79243674 71.71939913 92.12972552 96.24273583 96.9067141
 89.79300934 97.17717864 91.95063413 76.9130493  98.42838172 77.30656303
 98.20786784 76.11505708 78.02171026 97.48053752 78.45299156 95.99054592
 77.78048513 95.45083515 76.24054288 81.38911563 85.92487908 77.49661919
 78.51756192 99.14718388]
Accuracy th:0.7 is [45.66464834 97.2137279  71.27106151 97.02489005 97.26733349 75.98713466
 76.26369074 76.09921906 77.25295744 96.46690464 77.52707691 98.52097319
 99.41399349 80.4376165  77.1908237  96.56680596 96.29512311 76.86797188
 98.65376884 98.30776915 80.16349703 78.02292857 98.38695922 77.38697141
 81.87156589 96.65086926 94.0778012  76.57923271 98.01293844 77.41986574
 97.30875598 98.57457877 96.36213009 98.02024829 85.93828048 77.08970407
 76.81558461 88.62465126 97.11504489 74.22667852 79.42763855 92.05906361
 76.26978229 75.84581085 96.9627563  93.87434364 98.02877645 98.57336046
 93.60144248 86.85688527 86.26356891 98.55508583 98.99976852 76.46836661
 98.70615611 76.79243674 71.71939913 92.55613358 96.24273583 96.9067141
 89.79300934 97.17717864 92.12607059 76.9130493  98.42838172 77.30656303
 98.20786784 76.11505708 78.02171026 97.55972759 78.45299156 95.99054592
 77.89622446 95.45083515 76.24054288 81.49998173 86.0808226  77.49661919
 78.51756192 99.14718388]
Avg Prec: is [56.05724811  3.10326035 11.20724505  3.38965136  2.23142637  3.85870476
  3.34144654  5.6683369   2.55020039  3.93912599  1.58292571  1.56368886
  0.64487468  5.07509841  2.62722228  3.07750894  3.48228944  2.65971349
  1.38279879  1.73720179  2.10454255  0.88640728  1.84319196  2.42119321
  5.20782135  3.64143141  6.49662658  3.4040583   1.99809509  1.85542108
  2.59462478  1.37289517  3.83445263  1.62134924  2.39157481  2.40780974
  3.22453849  2.52557874  2.8395911   7.43267834  2.3111574   8.27757801
  3.36358788  4.06358585  3.25051611  6.35528558  2.04337713  1.50071211
  2.13823316  1.59824349  1.9146882   1.68529132  1.03022902  2.93840079
  1.38700068  2.69727672 11.15823709  3.71686936  3.8281314   2.84435867
 10.69519795  2.10653233  3.7908741   2.98642526  1.53700496  2.51400659
  1.76547828  4.07238211  1.19990176  2.4627888   0.18898584  3.31160421
  1.99125355  4.53512678  3.88192339  3.15957818  0.83459851  1.87362425
  0.15027892  0.75912256]
mAP score regular 14.31, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [89.59812642 97.22450607 92.03478088 96.96290206 97.90716795 96.63651992
 96.80843112 93.68911478 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 93.55208411 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74734036 97.82744101 92.89184543
 97.07750953 96.48703192 97.03764606 94.01798839 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.56334554 90.3181603  96.38488178 96.16314124 96.78102499
 92.76228916 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44809029 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [88.55669333 97.22450607 92.49570222 96.96290206 97.90716795 96.63651992
 96.80843112 94.67822707 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3767596  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.40600942
 97.07750953 96.48703192 97.03764606 94.03293719 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.38450806 96.39235618 96.16314124 96.78102499
 92.10703341 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [97.22805053  5.05334955 66.51329267  8.6100575   8.20171548 23.1626876
 11.83973457 34.4140218   6.29010041 30.22180772  1.84181328  2.81201252
  1.0049699  15.04920822 10.00830757  7.90076739  5.94777823  5.83578288
  0.79390829  1.67574589  1.63571894  0.47844673  1.17696674 13.5167411
 17.39339232 10.41258183 27.75137554 16.9826931   5.80523714  1.27707484
 36.03773792  0.77988893 32.6002292   1.24662172  1.18266123  3.83546001
  3.4003836  16.386584   22.93171892 35.03671294 16.00491013 46.99903664
 28.73100942 30.07663341 22.90546671 36.99150327  2.99159548  1.9695068
  3.08546936  1.88105645  1.33701185  1.44952819  1.5240221  17.9017939
  1.99365902 12.95233717 59.51431821 25.28563448  9.30085125 11.48559211
 65.48402539 16.32835147 18.81034495 12.45750328  8.59798697  9.35697033
  6.77555611 11.99527299  3.33535874  7.73460393  0.48525065 35.72594286
  8.1617614  28.15028271 24.43141823  5.87537244  1.33327     2.18154869
  0.20168771  0.93137553]
Accuracy th:0.5 is [45.6984827  97.22450607 69.28021526 96.96290206 97.90716795 74.60198819
 74.71908713 73.82714204 76.2114757  96.41228791 76.33604903 98.5325261
 99.34972718 77.28529785 76.2936941  96.31262924 96.21047911 75.71069088
 98.78167277 98.34068316 77.90816454 76.98133891 98.31327703 75.86516182
 77.52946159 96.52938685 94.3393876  75.80287515 97.81747515 76.29867703
 97.52597354 98.67204823 96.39983058 98.18870369 86.00792286 75.92993996
 75.92744849 90.48508857 97.0276802  73.17686922 76.69731171 92.37362035
 75.04796073 74.73154446 97.03764606 94.02795426 98.18621222 98.77668984
 92.41597528 85.75379326 84.22901562 98.55993223 98.87385704 75.03301193
 98.6969629  75.74058848 70.00772355 93.34778384 96.16314124 96.78102499
 90.13379176 97.04761193 91.84044647 75.79789222 98.32075143 76.65744824
 98.13139996 74.9981314  77.03365972 97.50105887 77.40239679 96.07843137
 76.80942771 95.44559882 74.94082767 82.1187433  87.6074445  76.3759125
 77.48212373 99.15040985]
Accuracy th:0.7 is [45.99496724 97.22450607 69.28021526 96.96290206 97.90716795 74.60198819
 74.71908713 74.02895084 76.2114757  96.41976231 76.33604903 98.5325261
 99.34972718 77.69888133 76.2936941  96.31262924 96.21047911 75.71069088
 98.78167277 98.34068316 78.21461494 76.98133891 98.31327703 75.98226076
 78.05516107 96.52938685 94.3393876  75.80287515 97.81747515 76.29867703
 97.52597354 98.67204823 96.39983058 98.18870369 86.3193562  75.92993996
 75.92744849 90.68938884 97.0276802  73.17686922 77.20806239 92.37362035
 75.04796073 74.73154446 97.03764606 94.02795426 98.18621222 98.77668984
 94.44153773 86.29444154 84.37601216 98.55993223 98.87385704 75.03301193
 98.6969629  75.74058848 70.00772355 93.67914891 96.16314124 96.78102499
 90.13379176 97.04761193 92.00986621 75.80038369 98.32075143 76.65744824
 98.13139996 74.9981314  77.03365972 97.53344794 77.40239679 96.07843137
 76.84929118 95.44559882 74.94082767 82.17853851 87.82669357 76.3759125
 77.48212373 99.15040985]
Avg Prec: is [54.24764029  3.73632549 14.84626336  4.56846937  1.56326352  4.28884523
 11.70835771  8.72589983  7.37951238  5.16298259  2.28178483  5.19554283
  1.55762382  5.92096188  3.06593447  3.92757212 25.55316264  6.35693769
  1.52755551  2.6212698   3.6186181   1.41447241  1.28032175  5.75843806
  5.7539079  11.48315401  8.17137005  4.55706816  3.90475179  6.79117059
  2.35488336  0.86774237  3.03038689  1.11159465  1.69628255  2.35664371
  2.03043717  2.14470418  2.11313813  6.37346454  1.7010289   6.02786816
  2.19510947  2.73221098  2.37484244  4.87236721  1.76213473  1.0587325
  1.4558425   1.18904357  1.22339252  1.00221972  0.77537611  2.32353142
  0.91631099  1.87530556 10.17492081  3.02935932  4.02098621  2.87838147
  7.96558684  2.04033651  3.35921699  2.59603269  1.38469041  1.89242499
  1.63562559  3.51919782  1.07711754  2.18813381  0.19639926  3.18903814
  1.58517678  4.11817726  3.18888799  2.31978069  0.65953482  1.50156836
  0.12125542  0.60564541]
mAP score regular 14.96, mAP score EMA 4.37
Train_data_mAP: current_mAP = 14.31, highest_mAP = 14.31
Val_data_mAP: current_mAP = 14.96, highest_mAP = 14.96
tensor([0.3431, 0.3658, 0.3646, 0.4166, 0.4354, 0.4227, 0.4496, 0.4303, 0.4210,
        0.4302, 0.4135, 0.4399, 0.4040, 0.4195, 0.4337, 0.3663, 0.3794, 0.4547,
        0.4221, 0.4442, 0.4317, 0.4247, 0.4254, 0.4354, 0.3820, 0.4146, 0.3955,
        0.4411, 0.4009, 0.3675, 0.3604, 0.3650, 0.4202, 0.3665, 0.3713, 0.3773,
        0.3752, 0.3544, 0.3713, 0.3804, 0.4227, 0.4202, 0.4140, 0.4285, 0.4198,
        0.4307, 0.4276, 0.4047, 0.4285, 0.4079, 0.4265, 0.3650, 0.4295, 0.4255,
        0.4273, 0.4197, 0.4152, 0.4088, 0.3452, 0.4361, 0.4014, 0.4131, 0.4043,
        0.4119, 0.4108, 0.4243, 0.4234, 0.4268, 0.4137, 0.4131, 0.4144, 0.3932,
        0.4210, 0.3991, 0.4313, 0.3575, 0.4040, 0.3634, 0.4164, 0.4232],
       device='cuda:0')
Max Train Loss:  tensor([ 9.7489,  5.9460,  9.3158,  6.0212,  6.9676,  5.7277,  9.9043,  7.3810,
         4.4403,  6.8213,  6.9101,  5.1575,  6.2825,  8.4080,  9.5816,  7.8814,
         7.1170,  8.7203,  8.7816,  9.6089,  6.6206,  6.4260,  5.9359,  6.5455,
         7.4249, 12.6671, 12.6270,  6.2480,  7.9339,  5.5233,  7.9775,  6.1059,
         9.2389,  3.3381,  6.5898,  7.1510,  8.5431,  9.0419,  7.8668,  7.0997,
         6.7298,  6.7657,  7.4255,  7.1639,  8.2348,  8.6448,  7.0040,  9.0591,
         6.3562,  8.3969,  8.1217,  4.6668,  7.4809,  6.2620,  7.0098,  8.9487,
        11.5987,  9.1918,  6.8919,  7.4121,  8.1318,  8.4192,  7.2456,  9.9035,
         6.6113,  7.4299,  9.3250,  9.5684,  7.4877,  5.8738,  4.7596,  7.5279,
         6.3645,  6.3313,  7.8284,  5.8366,  6.3348,  5.1620,  5.8185,  9.3568],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [27/80], Step [000/642], LR 1.0e-04, Loss: 12.7
Max Train Loss:  tensor([16.0939,  9.4738, 16.0569, 10.1970, 11.7427, 12.9356, 15.2362, 15.3010,
         8.9403,  6.5912, 16.1800, 10.6725,  9.3574, 14.1653, 14.7620,  9.1127,
         7.8204, 14.2513,  8.2526, 13.6883, 17.6679,  9.3827, 16.0162, 12.4228,
        11.6668, 10.0939, 13.2651, 14.2616,  5.4726,  6.9749,  8.9701,  5.3729,
        12.9570,  8.8449,  5.2395,  5.6181,  7.2635,  8.7987,  6.5695, 13.6014,
         6.0944, 14.0174,  9.8227, 15.7908,  7.0606, 14.8081, 11.3889,  6.0618,
        12.7016, 14.0684, 13.8670,  7.8715,  7.2702, 10.4018,  4.9854,  9.2378,
        10.9425,  9.4688,  6.7588,  8.7569, 11.9736,  8.8575, 10.2216,  7.1757,
         6.0921,  8.1631,  8.7622, 10.8994, 15.0833, 11.1065, 16.4139,  9.9059,
        10.9664, 10.4972, 14.4649,  6.9935,  7.2911,  5.9190,  9.7347,  3.4263],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [27/80], Step [100/642], LR 1.0e-04, Loss: 17.7
Max Train Loss:  tensor([15.9148,  6.4156, 14.2622,  6.1000,  9.7367,  8.4556,  6.1968, 13.0369,
         9.5466,  8.5501,  5.5921,  7.5917,  9.1264, 10.6037,  5.6167, 11.3899,
         7.1489, 11.0088,  5.0312, 10.1741,  9.4006,  6.6395,  6.3709,  5.1630,
        11.7816,  8.8778,  9.4858,  9.1910,  7.3106,  5.7317,  6.0402,  4.6924,
         9.6385,  8.3222,  5.3104,  5.6741, 10.0900,  9.5743,  7.3303, 12.2849,
         7.7192, 13.1759, 13.6255, 11.1510, 11.4171, 10.5944,  8.1417,  3.5164,
        10.5190,  5.7834,  6.7837,  5.2465,  8.6955,  8.6588,  6.6989,  8.1768,
        14.0239,  7.1706,  6.9452,  6.2876, 11.7592,  8.1244,  6.5481,  7.8248,
         6.6921,  7.3096,  7.9052, 14.9785,  7.0595,  9.8479,  5.6881,  8.4176,
         9.8648,  7.8313, 12.7269,  4.8913,  5.2276,  4.0526,  9.2352,  5.6399],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [27/80], Step [200/642], LR 1.0e-04, Loss: 15.9
Max Train Loss:  tensor([12.4970,  3.1031, 10.3386,  6.8335,  9.0623,  6.8148,  8.5944, 10.6513,
         6.3080,  8.7260,  7.5120, 10.1520,  8.4476, 11.1339,  9.5161,  5.4159,
         5.7159,  8.4737,  6.6632, 11.4561,  6.8038,  6.7974,  8.0446,  7.1260,
         9.7614,  8.5697, 11.0045, 10.4551,  6.6991,  5.7527,  8.3014,  9.3259,
         6.9697,  7.6236,  6.4663,  5.8172,  6.0254,  8.0376,  6.4736, 10.7164,
         5.8640, 10.4934,  7.6205,  8.9717,  9.2413, 10.9684,  6.6523,  3.6288,
         5.0466,  5.6942,  6.2334,  8.1432,  6.7662,  5.8146,  4.7598,  7.4774,
        11.3215,  9.2823,  3.6235,  6.4640,  9.2069,  9.6605,  8.0281,  7.9786,
         5.7806,  8.5407,  7.9549,  9.9835,  6.2882,  9.2220,  6.6004, 10.1752,
        11.0087,  7.0059, 10.2798,  7.8129,  7.7611,  3.1569,  9.9146,  3.2445],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [27/80], Step [300/642], LR 1.0e-04, Loss: 12.5
Max Train Loss:  tensor([12.5573,  4.6185, 10.6125,  8.6915,  7.0313,  8.6165,  7.4228,  7.4334,
         8.1106,  7.8333,  9.0022,  6.7933,  9.0013,  7.0685, 11.8992,  8.5927,
         8.2747,  6.7026,  4.9281,  7.7296,  7.5343,  8.6420,  5.8517,  7.4551,
         8.9628, 11.1636,  8.1250,  5.8574,  6.5644,  7.2985,  4.4034,  6.1916,
         9.0183,  3.9300,  6.6793,  8.9154, 10.7391,  8.6098,  4.9331, 12.5926,
         6.5299,  7.8976,  7.8952, 10.0715,  8.2987,  9.3673,  7.1639,  4.7587,
         6.6446,  7.0501,  6.1726,  6.5399,  6.0400,  6.4768,  7.0319,  7.4758,
         9.2411,  7.4350,  6.7687,  6.5143,  8.5839,  7.8197,  8.7282,  9.0585,
         6.4833,  8.4634,  9.7304,  5.3955,  7.6936,  8.2359,  5.7207,  6.2247,
         9.2966, 11.5334, 11.8366,  7.8101,  5.2650,  6.5850,  9.2682,  4.4264],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [27/80], Step [400/642], LR 1.0e-04, Loss: 12.6
Max Train Loss:  tensor([ 8.5947,  4.0647,  9.4713,  7.9128,  9.8776,  6.7439, 10.1591,  7.2195,
         5.4907,  8.1628,  6.3383,  6.7802,  9.7697, 14.7549,  6.0785,  7.2208,
        12.8263,  4.4749,  8.6780,  8.8965,  7.7281,  7.8402,  7.2544,  7.4698,
         7.2731, 11.2078, 11.5047,  9.1632,  8.3216,  6.9086,  6.7978,  3.2437,
         8.6332,  5.1344,  7.4547,  8.7092,  9.3660,  5.6334,  8.3768, 12.1433,
         6.7700,  8.5214,  6.5191, 10.4658,  7.8554,  9.2805,  6.2994,  4.5341,
         6.9565,  5.5851,  5.0086,  4.9840,  6.0280,  6.9546,  5.3228,  5.9879,
        12.7401,  7.9235,  7.0268,  9.1701,  8.3984,  8.0194,  6.6982,  9.6527,
         5.6977,  6.5550,  3.7451, 10.0440,  7.3271,  5.3033,  5.7081,  6.9848,
         8.3520,  8.1544,  9.8125,  6.4133,  5.2427,  7.2550,  9.8885,  4.5170],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [27/80], Step [500/642], LR 1.0e-04, Loss: 14.8
Max Train Loss:  tensor([12.5798,  9.6159, 10.9473,  6.0996,  8.5237,  7.5198,  9.5509,  9.1977,
         5.4709,  8.7228,  7.9036,  9.0963,  9.3419,  8.3302,  4.2535,  7.4884,
         7.7863,  6.2723,  9.4035,  6.4436,  7.1264,  7.7668,  5.6396,  6.5440,
         9.3875,  5.1058,  7.9539,  6.0999,  9.7891,  4.6433,  8.0386,  5.8557,
         9.5094,  5.9317,  7.4771,  5.1476,  5.6202, 10.1689,  8.4736, 11.4903,
         8.3533,  9.0285,  7.4002,  9.2133,  6.3686,  9.6533,  7.0830,  6.4277,
         6.3258,  8.2007,  6.1873,  5.9709,  6.3546,  9.4914,  7.0478,  7.8809,
        11.3103,  7.4353,  9.2956, 12.8781, 12.0618,  9.4965,  9.9593,  8.2275,
         7.4897,  6.8959,  7.3866, 11.6002,  7.2469,  6.6932,  7.4737,  7.6240,
         9.3071, 10.4104, 10.1254,  3.9710,  6.9410,  5.9470,  9.5826,  4.7394],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [27/80], Step [600/642], LR 1.0e-04, Loss: 12.9
Max_Val Meta Model:  tensor([ 22.1425,  24.2421,  27.4323,  19.1979,   6.0640,   8.2463,   8.4900,
          8.6630,   5.4804,   8.8000,   6.6278,   6.1608,   9.1253,   8.4806,
          5.7054,   7.7151, 118.4707,   4.2786,   4.0084,   4.7330,   4.8098,
          6.9445,   4.8028,   5.5428,  10.4610,   8.2521,  11.0012,   5.0166,
          6.8897,   6.1463,   7.2248,   3.4398,   6.0019,   4.0761,   4.1186,
          4.1518,   4.3322,   5.9304,   6.0105,  16.7528,   7.8612,   9.7303,
          4.8791,   9.4020,   7.4875,   9.4644,   4.3042,   4.6725,   4.4812,
          5.9025,   3.9433,   4.5047,   7.6506,   4.5846,   4.8846,   4.5078,
         11.3641,   7.8173,   7.9793,   4.7048,   7.0320,  11.9674,   6.7333,
          6.1999,   5.0896,   4.3950,   2.9002,   6.3554,   8.1194,  10.7655,
          5.9744,   9.5714,  11.6171,   6.7549,   9.4358,   5.5026,   5.4923,
          3.9621,   9.5628,   3.3574], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 15.0863,  23.6244,  22.5130,  18.2287,   5.9306,  11.3573,  10.0125,
         12.2312,   5.7532,  11.7644,   7.1541,   6.8606,   9.8392,   9.6813,
          5.5799,   7.0725, 116.8676,   5.0140,   4.4323,   5.0081,   5.1904,
          7.5593,   4.9593,   5.4103,  11.5149,   8.5670,  12.0448,   6.0325,
          7.4576,   6.1929,   8.9526,   3.8205,   6.8059,   4.5812,   4.9546,
          4.8140,   6.1795,   6.2558,   7.0729,  16.9575,   8.0176,   9.4860,
          4.8267,   9.2427,   7.6388,   9.0051,   4.4627,   5.0414,   4.4977,
          6.3244,   4.1927,   4.9558,   8.0764,   4.0440,   5.2954,   4.5089,
         12.6769,   7.6574,   8.1962,   4.7051,   6.3464,  11.2947,   6.4360,
          6.3993,   5.5451,   4.5462,   2.9023,   6.9451,   8.4020,  10.8130,
          6.5047,   9.3662,  12.0036,   6.8489,   9.5269,   5.6851,   6.0147,
          4.2837,  10.2375,   3.7160], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 43.9645,  64.5913,  61.7421,  43.7601,  13.6198,  26.8675,  22.2713,
         28.4249,  13.6660,  27.3458,  17.3002,  15.5974,  24.3571,  23.0793,
         12.8661,  19.3073, 307.9934,  11.0269,  10.5015,  11.2741,  12.0230,
         17.8003,  11.6591,  12.4260,  30.1436,  20.6643,  30.4527,  13.6764,
         18.6003,  16.8514,  24.8408,  10.4660,  16.1959,  12.4999,  13.3443,
         12.7587,  16.4699,  17.6533,  19.0493,  44.5761,  18.9677,  22.5772,
         11.6594,  21.5693,  18.1981,  20.9069,  10.4357,  12.4571,  10.4952,
         15.5034,   9.8307,  13.5760,  18.8031,   9.5029,  12.3935,  10.7436,
         30.5324,  18.7328,  23.7442,  10.7886,  15.8096,  27.3435,  15.9202,
         15.5356,  13.4983,  10.7145,   6.8541,  16.2707,  20.3098,  26.1782,
         15.6959,  23.8233,  28.5140,  17.1624,  22.0883,  15.9027,  14.8872,
         11.7877,  24.5842,   8.7805], device='cuda:0')
Outer loop valEpocw Maximum [27/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 118.5
model_train val_loss valEpocw [27/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 116.9
model_train val_loss  valEpocw [27/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 308.0
Max_Val Meta Model:  tensor([28.1117, 31.6097, 30.2756, 36.0717, 38.7544, 38.3660, 42.7111, 35.4672,
        41.8570, 37.9587, 37.0335, 45.3296, 34.3464, 37.7501, 39.3530, 30.6942,
        33.0318, 37.5359, 36.2520, 40.2074, 36.5545, 36.2401, 35.5550, 38.4510,
        32.4356, 35.0645, 33.5120, 36.8533, 34.1349, 30.6917, 30.8897, 30.8904,
        35.6969, 30.4360, 31.3893, 31.6434, 31.6778, 30.1587, 31.2806, 32.2319,
        35.2917, 34.0987, 34.6576, 36.4726, 34.4429, 34.9703, 35.4249, 33.6784,
        35.9281, 33.8111, 35.8408, 30.9154, 36.0565, 35.4678, 35.5137, 34.8138,
        34.5534, 34.1528, 28.8663, 36.2234, 34.0485, 33.0465, 33.9324, 34.7837,
        34.0520, 35.5689, 35.3007, 34.5748, 34.8691, 35.0995, 34.8272, 33.4770,
        36.1736, 33.8556, 36.3946, 29.7417, 33.6987, 30.4647, 35.9224, 35.7801],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([19.7751,  6.5347, 19.9514,  8.1990, 11.1308, 19.8395, 34.1235, 19.0408,
        14.5159, 19.1220,  9.4138, 72.8664,  9.3846,  6.5232,  7.6599,  5.2381,
         6.3224,  9.6164,  3.6185, 10.4426,  4.5630,  6.2902,  4.9557,  7.2177,
         9.7018,  7.9951, 11.1277,  7.2630,  7.5150,  3.3408,  6.4155,  2.9767,
         3.9165,  3.7924,  3.6925,  3.4377,  5.4204,  4.2356,  5.2513,  2.8494,
         4.4527,  1.9288,  2.1542,  3.4911,  2.6824,  1.8797,  3.2530,  3.1904,
         3.2134,  5.2358,  3.2783,  3.9750,  5.5484,  2.5584,  4.1990,  3.9685,
         3.8421,  3.4806,  4.0859,  2.8561,  2.2295,  4.1474,  3.5828,  3.8506,
         4.4427,  3.0893,  1.9167,  6.2432,  5.0027,  3.6038,  5.4491,  3.5927,
         7.2941,  3.2746,  9.7427,  2.6636,  4.9164,  2.8037,  8.9623,  2.8295],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 61.7712,  17.7154,  56.8334,  19.5285,  25.0373,  46.1249,  75.1406,
         45.0527,  34.7158,  44.0761,  22.4462, 166.7262,  23.5346,  15.2522,
         17.3384,  14.4203,  16.5890,  21.2671,   8.5243,  22.9631,  10.6377,
         14.8489,  11.6477,  16.4341,  25.6090,  19.3285,  28.2283,  16.4688,
         18.8421,   9.1525,  17.8912,   8.2319,   9.3192,  10.3719,   9.9714,
          9.1850,  14.5419,  11.9442,  14.3005,   7.5937,  10.5599,   4.6534,
          5.2135,   8.1840,   6.4509,   4.3804,   7.6621,   7.9382,   7.5504,
         12.9311,   7.7105,  11.0037,  12.9783,   6.0609,   9.8402,   9.5138,
          9.2984,   8.6014,  11.9455,   6.5569,   5.6460,  10.2376,   8.9906,
          9.3977,  10.8794,   7.3111,   4.5389,  15.0257,  12.1716,   8.7286,
         13.1677,   9.1238,  17.2977,   8.2483,  22.6285,   7.5497,  12.2287,
          7.7896,  21.4743,   6.7231], device='cuda:0')
Outer loop valEpocw Maximum [27/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 45.3
model_train val_loss valEpocw [27/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 72.9
model_train val_loss  valEpocw [27/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 166.7
Max_Val Meta Model:  tensor([29.4504, 31.5915, 29.1107, 35.7964, 36.2734, 36.5239, 36.8927, 36.4655,
        36.1818, 36.6380, 36.6107, 36.0869, 33.9570, 38.6545, 37.2119, 30.6478,
        33.0133, 34.9734, 36.0341, 36.6526, 34.7912, 35.9058, 35.5754, 36.6631,
        31.7141, 33.8931, 32.9292, 34.9476, 33.6862, 30.2516, 30.3448, 30.4100,
        35.3555, 30.0174, 31.1098, 31.1901, 31.2286, 29.8488, 30.7833, 33.4094,
        35.2572, 35.5372, 35.5653, 35.8913, 34.8554, 37.7123, 36.7607, 33.5019,
        36.5855, 34.7428, 37.1987, 30.6623, 37.4939, 35.3186, 35.9654, 35.7513,
        36.5193, 33.7826, 28.8356, 35.5170, 35.5882, 33.1158, 33.2902, 34.2470,
        33.7823, 35.1372, 34.9946, 34.2237, 34.7486, 34.8347, 34.6815, 33.5835,
        35.8081, 33.7841, 36.2787, 29.3117, 33.3650, 30.0577, 35.8962, 35.1751],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 5.4927,  3.6530,  2.8264,  3.9080,  2.1073,  2.9540,  2.8649,  5.7630,
         2.4618,  3.0151,  6.0627,  3.5295,  8.9808,  6.6823,  2.3401,  6.9967,
         2.6378,  2.2474,  4.1420,  3.4263,  3.9962,  7.5196,  3.4869,  3.2725,
         3.8105,  2.3974, 10.1138,  8.2247,  5.6742,  4.2388,  5.4666,  4.6701,
         5.9233,  3.7890,  4.3314,  4.7018,  5.9717,  6.5967,  5.6540, 18.5446,
        13.3958, 26.2465, 31.4970, 29.6428, 23.1623, 31.1853,  7.2395,  6.7673,
        18.6921,  7.2112, 15.3448, 16.2276, 23.5279, 12.7296, 26.4970, 40.1195,
        45.0470, 12.8463,  4.9421,  6.0743, 34.9064,  5.9809, 12.1705,  9.8707,
         8.2236,  6.6509,  5.7594, 12.9516,  6.8284,  5.9912,  6.4601,  8.5096,
         9.0092, 15.6464,  5.0047,  6.2157,  7.7783,  3.6292, 10.2260,  3.7009],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 17.0735,   9.9068,   8.0369,   9.3304,   4.8326,   6.9850,   6.3859,
         13.4031,   5.8347,   7.0618,  14.5356,   8.1418,  22.6715,  15.7492,
          5.4304,  19.2869,   6.9178,   5.0872,   9.7965,   7.8211,   9.5361,
         17.8206,   8.2290,   7.5953,  10.1420,   5.8555,  25.6888,  19.0377,
         14.3080,  11.6980,  15.3913,  13.0264,  14.1092,  10.4267,  11.6979,
         12.6357,  16.1014,  18.6996,  15.5221,  49.3648,  31.8338,  63.6052,
         76.3035,  71.2364,  55.9203,  72.3233,  16.8687,  16.8588,  43.9260,
         17.5426,  35.6952,  45.2615,  55.4285,  30.2850,  62.3041,  96.3853,
        109.3596,  32.0259,  14.3756,  14.0204,  89.4814,  14.7191,  30.7037,
         24.3715,  20.2302,  15.8303,  13.6748,  31.2236,  16.5806,  14.5202,
         15.5869,  21.5505,  21.4743,  39.4436,  11.6692,  17.7753,  19.4539,
         10.1410,  24.4291,   8.8724], device='cuda:0')
Outer loop valEpocw Maximum [27/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 38.7
model_train val_loss valEpocw [27/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 45.0
model_train val_loss  valEpocw [27/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 109.4
Max_Val Meta Model:  tensor([30.6664, 31.1961, 28.2947, 35.5800, 35.4860, 34.3578, 35.3375, 35.6145,
        35.3466, 33.2466, 33.9407, 35.5997, 33.9429, 34.0589, 34.9333, 30.6529,
        32.8911, 34.8523, 35.6110, 34.2081, 34.5609, 35.5267, 33.5647, 34.3237,
        31.6965, 33.3152, 32.9866, 34.5517, 33.7021, 29.8527, 29.8343, 30.0096,
        33.8554, 29.1888, 31.1851, 31.1468, 31.0774, 29.4375, 30.6159, 32.2401,
        34.6069, 33.0385, 34.1323, 33.2689, 33.9459, 34.5267, 34.6671, 33.3492,
        33.9327, 34.5805, 35.0446, 30.3138, 34.8055, 33.3178, 33.7120, 34.4093,
        35.3890, 33.6614, 28.3812, 35.0285, 33.7674, 33.1106, 34.6519, 33.5572,
        33.2441, 34.8901, 33.7455, 37.0390, 34.6159, 34.4532, 35.3782, 33.1988,
        36.0179, 33.3542, 39.5284, 29.0112, 33.2900, 30.1271, 32.9753, 34.4130],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([10.7428,  3.5743, 14.7527,  6.9518, 14.2125, 11.6619, 14.7477, 12.7139,
         5.9848, 13.5609,  7.5758,  6.4706,  9.3239,  7.2360,  9.1244,  7.4106,
         5.2796,  6.1079,  4.5966,  5.8367,  5.7871,  7.5416,  6.5679,  8.0246,
         8.0119,  3.7264,  8.8047,  4.3121,  8.2489,  4.1647,  6.1617,  3.7551,
         4.0078,  4.3998,  4.0334,  3.8553,  4.6818,  5.0305,  5.4463,  4.5432,
         5.6707,  3.4962,  3.1083,  4.7778,  3.6567,  2.8235,  4.4340,  4.1823,
         4.4147,  5.7624,  4.3652,  4.9154,  6.7775,  3.8479,  5.2264,  4.4589,
         2.7785,  4.4068,  5.8036,  4.5471,  4.4442,  6.1443,  5.4039,  5.1147,
         5.5092,  4.2064,  2.9921,  4.7795,  6.3085,  4.1944,  6.8693,  5.7731,
         7.7933,  4.8045, 82.7289,  4.1157,  6.0438,  4.5381, 10.1122,  3.6819],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 31.0887,   9.8169,  43.1002,  16.6370,  32.9566,  28.3867,  33.3793,
         29.9949,  14.4084,  32.8347,  18.7088,  15.0622,  23.5048,  17.8232,
         21.5957,  20.4607,  13.8630,  13.7316,  10.9767,  13.7200,  13.7726,
         18.0243,  15.7962,  19.1403,  21.2698,   9.2013,  22.3263,   9.9917,
         20.7723,  11.6126,  17.5244,  10.5760,   9.7472,  12.4020,  10.8290,
         10.3465,  12.6293,  14.3809,  14.9919,  12.0918,  13.4478,   8.6231,
          7.5602,  11.8006,   8.8411,   6.7158,  10.6400,  10.3965,  10.5782,
         14.0646,  10.4176,  13.7507,  16.2236,   9.3941,  12.5173,  10.7559,
          6.8233,  11.0449,  17.1564,  10.5068,  11.3228,  15.1911,  13.4347,
         12.8686,  13.6972,  10.0888,   7.2561,  11.4240,  15.3312,  10.2589,
         16.2075,  14.7990,  18.6293,  12.3041, 198.7657,  11.8369,  15.0892,
         12.6466,  25.1444,   8.8896], device='cuda:0')
Outer loop valEpocw Maximum [27/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 39.5
model_train val_loss valEpocw [27/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 82.7
model_train val_loss  valEpocw [27/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 198.8
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [88.80861588 97.2137279  90.47160731 97.02489005 97.26733349 96.22324289
 96.99808726 94.37263191 97.44398826 96.3073062  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.17039266 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.74497143 97.84237521 92.78761224
 96.90915072 96.22689782 96.9627563  93.90967459 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.973782   96.1818204  96.24273583 96.9067141
 92.3928802  97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [86.8885613  97.2137279  92.04688052 97.02489005 97.26733349 96.6009186
 96.99808726 94.75030762 97.44398826 96.47665111 98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.08023781 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.12972552
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 90.7761845  96.13796128 96.24273583 96.9067141
 91.32320513 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [97.0036511   3.53935182 62.03448235 14.14758859 28.84095073 22.26633177
 12.01141768 34.15312734  9.74529488 27.0786448   1.69713056  2.28013229
  0.7688814  13.64795901  7.438295   12.25572723  7.43933919  4.17492729
  0.88958586  2.26950856  1.65999952  0.47845064  5.23279777 17.48177138
 18.59317664 12.95997931 30.18607413  8.87093365  3.93417995  1.24043691
 36.82548468  0.87333538 24.76643617  1.62573905  3.8067573   2.63495909
  9.27805668  2.2823003   6.98811686 33.32906316  5.60839126 47.37073944
 23.21284736 26.52074253 18.73474853 36.55746299  2.97358501  2.01010684
  4.80279772  1.91600544  1.37027624  1.18416692  1.0676735  16.43282267
  1.6625919   6.2450152  64.32838195 28.91340154 11.71318338 10.88481731
 64.20702773  8.68236497 25.47702743 13.16904806  7.05780032  7.34501693
 10.46662051 10.2312965   4.72924447 14.49071989  0.78483235 30.19278271
  7.12627957 24.94988121 17.23147598  8.63834639  1.62053044  2.43359415
  0.46394503  1.08919366]
Accuracy th:0.5 is [45.60982444 97.2137279  71.18334328 97.02489005 97.26733349 75.82875452
 76.2295781  75.71910674 77.13356319 96.34872869 77.42230236 98.52097319
 99.41399349 79.85404661 77.08361253 96.56680596 96.29512311 76.66816925
 98.65376884 98.30776915 79.67008199 77.95470328 98.38695922 76.86675357
 81.1905313  96.65086926 94.0778012  76.56948624 98.01293844 77.36382354
 97.30875598 98.57457877 96.36213009 98.02024829 85.56669631 76.9678732
 76.80340152 88.13123622 97.11504489 74.03174913 78.8209208  92.05906361
 76.26003582 75.82144467 96.9627563  93.87434364 98.02877645 98.57336046
 91.60097952 85.97604805 86.24651259 98.55508583 98.99976852 76.32947942
 98.70615611 76.74126777 71.48792047 92.05906361 96.24273583 96.9067141
 89.79300934 97.17717864 91.84098634 76.76319733 98.42838172 77.29194332
 98.20786784 76.01759238 77.85114704 97.44033333 78.34821701 95.99054592
 77.56362617 95.45083515 76.06754304 81.20027777 85.80792144 77.37478832
 78.42253384 99.14718388]
Accuracy th:0.7 is [45.66343003 97.2137279  71.18334328 97.02489005 97.26733349 75.82875452
 76.2295781  76.03830363 77.13356319 96.46812295 77.42230236 98.52097319
 99.41399349 80.34502504 77.08361253 96.56680596 96.29512311 76.66816925
 98.65376884 98.30776915 80.14400409 77.95470328 98.38695922 77.26270391
 81.94466442 96.65086926 94.0778012  76.56948624 98.01293844 77.36382354
 97.30875598 98.57457877 96.36213009 98.02024829 85.78599189 76.9678732
 76.80340152 88.48941899 97.11504489 74.03174913 79.53484972 92.05906361
 76.26003582 75.82144467 96.9627563  93.87434364 98.02877645 98.57336046
 93.77078739 86.98968092 86.42682229 98.55508583 98.99976852 76.32947942
 98.70615611 76.74126777 71.48792047 92.45988718 96.24273583 96.9067141
 89.79300934 97.17717864 92.01520449 76.76319733 98.42838172 77.29194332
 98.20786784 76.01759238 77.85114704 97.55729097 78.34821701 95.99054592
 77.68302043 95.45083515 76.06754304 81.29530586 85.96386496 77.37478832
 78.42253384 99.14718388]
Avg Prec: is [56.03727472  3.04992363 11.32937749  3.36118619  2.24079737  3.83049927
  3.32048798  5.72183461  2.45229753  3.94013047  1.58478051  1.66908384
  0.58377968  5.14965527  2.66226532  3.13795523  3.68715928  2.7662713
  1.36853614  1.7098637   2.06786744  0.85672292  1.89059824  2.52718926
  5.06895081  3.72410049  6.53560858  3.23862837  2.02651781  1.85256908
  2.64872318  1.36775363  3.7223868   1.67014899  2.27367938  2.35294945
  3.06571774  2.67772523  2.75470063  7.40973974  2.28534063  8.24908691
  3.32841224  3.99182416  3.16327879  6.41450627  2.09622543  1.50938131
  2.1542227   1.5764263   1.78912794  1.5563852   1.0913801   2.97263417
  1.36010936  2.74734699 11.1664255   3.52244221  4.03772871  2.86844966
 10.83581976  2.11399839  3.78652598  3.03407243  1.58126618  2.40621043
  1.81986978  4.19081067  1.25474118  2.34668977  0.19136164  3.39194659
  1.912136    4.59703092  3.93638721  3.12717643  0.80386774  1.83481171
  0.1283145   0.71922438]
mAP score regular 14.51, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [89.58816055 97.22450607 90.91112938 96.96290206 97.90716795 95.95884097
 96.80843112 94.35184493 97.38894287 96.20051324 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.42409747 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.81211849 97.82744101 93.10860303
 97.07750953 96.48703192 97.03764606 94.07529212 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.14034432 96.49201485 96.16314124 96.78102499
 92.53058275 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [88.58659093 97.22450607 92.60781822 96.96290206 97.90716795 96.63651992
 96.80843112 94.89249321 97.38894287 96.44218551 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.52310835
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.68938884 96.39235618 96.16314124 96.78102499
 91.78314274 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [97.1403597   4.16343791 66.35290187 14.52277723 27.72114115 21.41945269
 12.14581961 35.45165981 13.80977571 30.50011209  1.67230207  2.2928074
  0.74154165 15.33007064  7.97145979 14.78732601  8.88558463  4.11902049
  0.77029521  2.19094988  1.41750595  0.46553924  4.20360132 18.11347095
 18.81490961 14.96786985 29.84401283  8.72249649  4.55598719  1.28286154
 38.94399309  0.80086555 26.31585442  1.60220706  3.04985165  2.3150234
  8.9742614   2.67762499  7.96737344 35.11183255  6.79256284 47.20748317
 25.3764555  27.73798266 17.88587141 36.32234655  2.87902387  2.17291517
  5.24088418  1.80812287  1.42068201  1.30121964  1.25969407 18.15128926
  1.7463685   6.52721332 57.31805875 29.55951591 10.07303197 11.70736201
 63.40840445  8.86520248 28.64924887 14.16182937  7.93946162  7.26756919
 12.08031155 11.16997237  4.55229239 18.23430508  0.83540661 32.28921758
  6.65759905 27.27380695 21.75500513  7.50438727  1.66305192  2.45338167
  0.23731278  0.96267428]
Accuracy th:0.5 is [45.73087176 97.22450607 69.15065899 96.96290206 97.90716795 74.45748312
 74.57956499 73.7100431  76.06697063 96.40979645 76.1965269  98.5325261
 99.34972718 77.18813065 76.14918903 96.31262924 96.21047911 75.56618581
 98.78167277 98.34068316 77.8134888  76.83683384 98.31327703 75.73062262
 77.50703839 96.52938685 94.3393876  75.65837008 97.81747515 76.16413783
 97.52597354 98.67204823 96.39983058 98.18870369 86.00293993 75.78543489
 75.79290929 90.4228019  97.0276802  73.06724469 76.60512744 92.37362035
 74.9084386  74.59202232 97.03764606 94.02795426 98.18621222 98.77668984
 92.54304009 85.71392979 84.21406682 98.55993223 98.87385704 74.8934898
 98.6969629  75.59608341 69.92799661 93.28549717 96.16314124 96.78102499
 90.13379176 97.04761193 91.87781847 75.66584448 98.32075143 76.5328749
 98.13139996 74.86857513 76.88915465 97.50105887 77.25789172 96.07843137
 76.67987144 95.44559882 74.81625433 82.03403344 87.61990184 76.2413733
 77.33761866 99.15040985]
Accuracy th:0.7 is [45.94762937 97.22450607 69.15065899 96.96290206 97.90716795 74.45748312
 74.57956499 73.91434337 76.06697063 96.41976231 76.1965269  98.5325261
 99.34972718 77.6042056  76.14918903 96.31262924 96.21047911 75.56618581
 98.78167277 98.34068316 78.12492214 76.83683384 98.31327703 75.88260209
 78.05017814 96.52938685 94.3393876  75.65837008 97.81747515 76.16413783
 97.52597354 98.67204823 96.39983058 98.18870369 86.31686474 75.78543489
 75.79290929 90.64703391 97.0276802  73.06724469 77.12335252 92.37362035
 74.9084386  74.59202232 97.03764606 94.02795426 98.18621222 98.77668984
 94.55863667 86.24959514 84.38099509 98.55993223 98.87385704 74.8934898
 98.6969629  75.59608341 69.92799661 93.63181105 96.16314124 96.78102499
 90.13379176 97.04761193 91.99242594 75.66584448 98.32075143 76.5328749
 98.13139996 74.86857513 76.88915465 97.53344794 77.25789172 96.07843137
 76.72471784 95.44559882 74.81625433 82.13120064 87.84164237 76.2413733
 77.33761866 99.15040985]
Avg Prec: is [54.68400065  3.73860276 14.94151163  4.55572606  1.51434342  4.29000114
 11.56379045  8.7389142   7.3513016   5.17964088  2.27791648  5.23786791
  1.56215666  5.91720511  3.09325779  3.97198754 25.62198671  6.32185313
  1.54501986  2.61658029  3.63593208  1.43520192  1.18730079  5.7688204
  5.76640358 11.62038496  8.15886542  4.49054912  3.8796533   6.9786093
  2.33392304  0.88089605  3.07931761  1.10744234  1.70394416  2.42618946
  2.0380917   2.20646731  2.25575484  6.20990522  1.73255913  6.02983184
  2.20102802  2.7390412   2.36225927  4.83785476  1.72726993  1.00876922
  1.38851624  1.18814977  1.20395104  0.97424966  0.73677391  2.35018993
  0.85507491  1.83019171 10.03395815  2.98412216  3.84453314  2.8436538
  7.86195678  2.02155288  3.24112163  2.57875939  1.363901    1.84819606
  1.58755093  3.44296084  1.07150108  2.19594349  0.19528917  3.1513483
  1.56283539  3.99799069  3.5143957   2.33154057  0.63642814  1.54044294
  0.12726414  0.59212595]
mAP score regular 15.03, mAP score EMA 4.37
Train_data_mAP: current_mAP = 14.51, highest_mAP = 14.51
Val_data_mAP: current_mAP = 15.03, highest_mAP = 15.03
tensor([0.3455, 0.3631, 0.3420, 0.4159, 0.4317, 0.4089, 0.4409, 0.4229, 0.4149,
        0.4112, 0.4034, 0.4295, 0.3955, 0.4060, 0.4222, 0.3606, 0.3803, 0.4451,
        0.4177, 0.4244, 0.4184, 0.4182, 0.4148, 0.4183, 0.3761, 0.4051, 0.3947,
        0.4301, 0.3950, 0.3581, 0.3521, 0.3540, 0.4104, 0.3541, 0.3717, 0.3719,
        0.3699, 0.3492, 0.3621, 0.3737, 0.4200, 0.4014, 0.4107, 0.4031, 0.4142,
        0.4208, 0.4150, 0.4009, 0.4166, 0.4087, 0.4186, 0.3573, 0.4178, 0.4085,
        0.4168, 0.4150, 0.4042, 0.3970, 0.3373, 0.4305, 0.3893, 0.4039, 0.4012,
        0.3966, 0.4014, 0.4162, 0.4092, 0.4133, 0.4108, 0.4096, 0.4214, 0.3901,
        0.4203, 0.3899, 0.4222, 0.3462, 0.4005, 0.3573, 0.4028, 0.4131],
       device='cuda:0')
Max Train Loss:  tensor([11.7751,  6.2502, 12.0204,  7.7334,  7.6934,  7.7866,  9.5415, 10.2130,
         8.4067, 10.1499,  8.0969,  7.9206,  8.7860,  9.2610, 12.4896,  7.8232,
         8.8379,  4.0858,  4.9629,  5.3620,  6.3562, 11.0948,  5.9736,  7.1943,
         6.6718,  7.0637, 13.7508,  6.6937,  8.2031,  6.6462,  8.0143,  3.2708,
        12.1908,  4.5532,  9.7955,  6.4257, 11.0386,  4.8166,  9.5859,  9.8208,
         5.6500,  7.5694,  6.5705,  7.4525,  6.6232,  5.9552,  5.3668,  7.8083,
         6.6006,  6.1699,  6.1148,  5.2202,  6.8210,  6.6390,  4.6785,  5.1027,
         9.8188,  7.5610,  6.5330,  5.4327,  6.9942,  7.5755,  7.3348,  7.1271,
         4.8820,  5.2202,  3.6015,  7.9365,  7.8583,  6.8958,  5.9644,  9.7554,
         9.9740,  9.0284, 12.4626,  5.5637,  5.3508,  4.8601,  9.1229,  3.2031],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [28/80], Step [000/642], LR 1.0e-04, Loss: 13.8
Max Train Loss:  tensor([23.1327,  7.0889, 18.0735,  9.7096, 12.3266,  9.5410, 11.6446, 10.3711,
        13.6045, 13.2850,  8.5068, 12.2256,  7.6377,  9.2550, 12.1710,  6.2190,
         9.5368, 13.0501, 11.5110,  6.5474,  6.8639,  7.5670,  6.9102, 11.9931,
        13.3060,  7.8086, 12.2142, 10.7964, 13.7821,  7.5305,  9.4225,  5.6191,
         8.2925,  6.7587,  5.9702,  7.1108, 10.2974, 10.6932,  5.4222, 10.9608,
        13.2037, 15.8556, 11.0808,  6.4385,  8.7153, 14.8089,  7.5887, 16.8107,
        10.1447, 16.6110,  9.7997,  7.6394,  7.4558,  7.6843, 15.9943, 10.2531,
        14.6287, 12.6614,  7.7503, 16.4903, 14.2737, 11.0649, 10.0690,  6.3295,
        14.0997, 11.9033, 15.1404, 14.0664, 10.2358,  8.9722,  7.6282, 12.7533,
         7.4528, 10.6381, 10.5563,  7.4278, 11.3660,  6.1888,  4.8188, 12.4207],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [28/80], Step [100/642], LR 1.0e-04, Loss: 23.1
Max Train Loss:  tensor([11.1111,  8.0194,  9.3563, 11.1059,  8.0111,  8.4796,  5.0604, 10.1476,
         6.4035,  9.2828,  7.8200,  8.6654,  6.9604, 12.2038,  8.8014, 10.8158,
         6.7737, 11.1640,  7.7987,  7.7708,  6.2943,  7.4636,  8.0351,  7.1219,
         6.8260, 11.1602,  7.6265,  6.9640,  4.1104, 10.8765,  5.8081,  4.9623,
         7.2320,  5.4356,  5.2272,  6.4821,  8.5834,  9.6999,  7.4141, 11.2301,
         7.2979,  8.8610,  8.4832, 10.1244,  8.5119, 11.6070,  5.7763,  7.6289,
         3.5688,  4.7663,  8.6261,  4.9668,  7.5059,  6.9756,  8.2424,  7.5205,
        10.4485,  8.6134,  8.2238,  7.1348,  9.5362,  7.2443,  8.0188,  7.8929,
         9.7284,  8.6332,  9.7636, 11.8343,  6.2539,  4.9059,  6.9086,  7.8500,
         6.5442, 11.7841, 12.6481,  7.1924, 11.7087,  6.5178,  3.4118,  7.3775],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [28/80], Step [200/642], LR 1.0e-04, Loss: 12.6
Max Train Loss:  tensor([13.5446,  6.6128, 10.5664,  7.4284,  8.8883,  6.9917,  9.8740,  8.1011,
         9.4632,  8.3764,  7.8309,  8.8921,  6.9998,  9.1326,  9.2886,  6.6023,
        13.4359,  7.2041,  5.6703,  6.8703,  5.1401,  6.4394,  8.1418,  7.1151,
         7.4571,  7.2003, 10.1157, 11.8363,  6.1018,  6.5584,  8.2422,  6.0600,
         7.4307,  5.7631,  6.9010,  6.9165,  6.1599,  3.6451,  6.9370,  7.2813,
         6.7445, 10.4882,  8.4495,  7.1210,  7.9663,  9.3486,  9.7914,  7.4391,
         6.1642,  5.6424, 10.0430,  4.1479,  7.9679,  7.3939,  6.5337, 10.5361,
        13.8920,  7.2975,  4.3102,  5.6367, 11.7248, 10.6962,  8.9560,  7.0557,
         7.2322,  8.7546,  6.3763,  7.6237,  4.4150,  8.2492,  7.5461,  9.6993,
         7.7747,  8.4366,  8.8844,  5.4909,  7.7599,  3.6869,  3.4460,  6.5306],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [28/80], Step [300/642], LR 1.0e-04, Loss: 13.9
Max Train Loss:  tensor([12.1535,  5.7644, 10.0946,  6.5233, 10.6450,  9.7048,  9.9562,  9.0571,
         6.8581,  6.3530,  8.5400,  7.9535,  5.0409, 10.4560,  8.4795,  7.2058,
        10.6091, 11.2654,  5.4628,  7.7270,  7.5769,  5.1937,  4.9080,  6.3909,
         7.3535,  7.1460, 12.2221,  9.3753,  5.7676,  9.1812,  4.9762,  4.6257,
         8.2609,  5.3880,  7.1463,  7.1953,  9.5559,  6.2072,  7.4994,  7.8364,
         5.7919, 14.7722,  7.1909,  8.8178,  7.3790,  9.8574,  4.6618,  6.9361,
         5.8927,  5.4965,  7.8949,  4.0036,  6.5747,  4.6073,  8.2741,  8.3358,
        13.0335,  6.9358,  8.2139,  6.8520,  9.3772,  8.7372,  6.2796,  7.7445,
         6.6807,  8.0071,  7.9952, 13.6966,  3.6287,  5.8602,  6.9409, 10.7912,
         5.4427,  6.6382, 12.3385,  5.3692,  2.3866,  5.3981,  3.4428,  7.1378],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [28/80], Step [400/642], LR 1.0e-04, Loss: 14.8
Max Train Loss:  tensor([12.7317,  6.8505,  8.8620,  8.3241,  7.3598,  9.4807, 10.8285,  9.8581,
         7.8651,  7.4503,  7.9561,  7.1094,  5.1490, 11.3045,  9.0577, 10.9438,
        10.0229,  8.7716,  7.8079,  7.4693,  5.2197,  6.5573,  7.0597,  6.5025,
         9.1964,  5.9809, 11.5333,  8.6640,  7.4426,  4.4744,  7.0745,  7.7239,
         8.1681,  6.8372,  9.6128,  6.6134,  6.3817,  6.2161,  5.8465, 11.8705,
         9.1391, 13.6960,  6.7660, 11.0845,  8.7800,  6.1399,  5.9675,  8.3631,
         7.8841,  6.7028,  6.7785,  4.1721,  7.2227,  6.5218,  5.7514,  7.5011,
        12.4052,  8.2546,  4.4229,  7.6151,  8.6730,  6.9418,  8.4266, 10.3264,
         8.3412,  9.2810,  8.4445,  9.0622,  5.7049,  7.6575,  7.0507,  9.6926,
         6.8499,  7.8715,  7.8734,  7.3529,  3.5775,  4.8966,  3.5457,  6.6531],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [28/80], Step [500/642], LR 1.0e-04, Loss: 13.7
Max Train Loss:  tensor([12.5278,  7.5474, 13.1308,  7.5591,  8.1199,  5.6838, 12.4987,  9.5816,
         6.6303, 10.9896,  3.3820,  8.3107,  5.2403,  9.7644,  8.0315,  7.2324,
         6.2314,  8.1758,  6.5831,  8.2981,  7.8555,  8.3549,  3.9941,  7.7143,
        10.4605,  8.5067, 11.5018,  7.7997,  8.2451,  8.4801,  6.5726,  4.7814,
         7.3753,  5.7360,  5.4925,  6.6839, 10.1735,  6.4870,  5.0527,  8.3913,
         6.7420,  7.4163,  7.8649,  5.9284,  6.6991,  9.4516,  8.3117,  9.6062,
         6.5257,  9.3939,  7.8764,  5.9135,  6.7481,  6.0704,  8.7325,  8.9607,
        10.5374, 10.2944,  6.5641,  7.5709, 10.6708,  5.5606,  7.1268,  6.7795,
         6.1287,  8.1870,  6.5118,  8.1629,  3.7759,  6.2620,  7.1356,  4.8595,
         6.7524, 11.1106,  8.8916,  4.7712,  3.8113,  6.1491,  4.4886,  6.7348],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [28/80], Step [600/642], LR 1.0e-04, Loss: 13.1
Max_Val Meta Model:  tensor([ 18.7402,  23.2068,  30.4160,  19.0504,   4.9624,   4.6045,   5.0040,
          8.4648,   5.5628,   7.4190,   3.2378,   8.5919,   5.7114,   8.2382,
          6.1203,   4.9758, 111.3599,   4.4164,   4.5667,   5.0391,   4.1588,
          5.2266,   4.0156,   5.4239,  11.2617,   7.7672,  11.0498,   3.2857,
          5.7798,   5.3163,   4.1145,   3.5999,   9.5428,   2.9985,   4.1029,
          4.1757,   5.6358,   5.1727,   3.9063,  15.6959,   7.5901,   9.8206,
          6.3917,   7.8104,   7.7282,   9.0406,   3.8816,   6.8319,   3.6085,
          5.6720,   5.9335,   3.2462,   7.2748,   4.2669,   5.6796,   5.0341,
         11.9262,   7.0380,   7.3639,   4.8045,   6.2853,  10.6052,   5.1404,
          4.5517,   5.9861,   4.7043,   5.7814,   4.6411,   6.4293,  11.7314,
          6.9914,   8.1799,   9.0586,   6.0475,   7.3072,   5.3571,   2.3975,
          4.4374,   3.4600,   6.5918], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 15.9091,  22.4459,  24.9325,  17.8713,   5.0037,   4.6029,   5.4588,
         10.1191,   6.3922,   8.4267,   3.7432,   9.4744,   6.4720,   9.3859,
          6.1717,   5.6230, 108.0401,   5.1617,   5.1219,   5.6722,   4.7454,
          5.9452,   4.3411,   5.7459,  11.3957,   7.7425,  12.2696,   4.1974,
          6.5715,   5.5011,   4.6692,   4.1429,  10.6916,   3.4751,   4.6856,
          5.0257,   6.9448,   5.5805,   4.5668,  15.6947,   7.9661,   9.4860,
          6.4082,   7.9200,   8.0278,   9.2322,   4.1390,   7.5471,   4.0917,
          6.2400,   6.4767,   3.7572,   7.8535,   4.5314,   6.3999,   5.6909,
         13.2641,   7.5523,   7.3060,   5.4669,   5.5512,   9.8157,   5.5879,
          5.2028,   6.6993,   5.3811,   6.4825,   5.4974,   6.8054,  12.0991,
          7.7872,   8.3202,   9.3926,   6.8966,   7.5643,   5.6830,   2.8135,
          4.9425,   4.0066,   7.3514], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 46.0432,  61.8199,  72.8997,  42.9744,  11.5896,  11.2562,  12.3819,
         23.9257,  15.4075,  20.4927,   9.2786,  22.0592,  16.3624,  23.1189,
         14.6168,  15.5923, 284.0947,  11.5969,  12.2610,  13.3637,  11.3412,
         14.2164,  10.4653,  13.7354,  30.2961,  19.1109,  31.0841,   9.7599,
         16.6362,  15.3604,  13.2594,  11.7040,  26.0544,   9.8143,  12.6071,
         13.5122,  18.7740,  15.9813,  12.6134,  41.9937,  18.9672,  23.6305,
         15.6040,  19.6478,  19.3807,  21.9407,   9.9727,  18.8245,   9.8223,
         15.2667,  15.4707,  10.5157,  18.7965,  11.0922,  15.3554,  13.7128,
         32.8170,  19.0248,  21.6597,  12.6999,  14.2593,  24.3001,  13.9296,
         13.1199,  16.6898,  12.9297,  15.8416,  13.3014,  16.5645,  29.5354,
         18.4804,  21.3283,  22.3462,  17.6904,  17.9149,  16.4165,   7.0250,
         13.8328,   9.9477,  17.7968], device='cuda:0')
Outer loop valEpocw Maximum [28/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 111.4
model_train val_loss valEpocw [28/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 108.0
model_train val_loss  valEpocw [28/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 284.1
Max_Val Meta Model:  tensor([31.2778, 29.3791, 30.2278, 35.9400, 32.9010, 36.4123, 44.4727, 35.8417,
        34.5433, 34.7066, 33.0721, 40.9573, 33.5374, 32.8636, 36.1820, 29.9061,
        32.8764, 35.6439, 34.0324, 39.2748, 35.0163, 35.2787, 34.3194, 34.8764,
        31.4131, 34.2017, 33.2207, 34.7284, 32.7105, 29.6448, 29.6440, 29.8908,
        34.4672, 29.1922, 31.2132, 30.9099, 31.1380, 29.2131, 30.2018, 31.1143,
        34.5587, 32.5830, 34.0908, 33.5908, 33.8538, 34.2316, 34.2275, 33.5799,
        34.4493, 33.7314, 34.8338, 30.0191, 34.8675, 33.9304, 34.5522, 34.2649,
        33.5075, 33.1294, 27.9570, 35.2540, 32.3594, 31.9379, 33.2917, 33.1947,
        33.1333, 33.3642, 34.3769, 33.0340, 34.1935, 34.5441, 35.2518, 32.1826,
        33.1122, 32.9315, 35.2116, 28.8186, 32.7068, 29.8937, 33.8274, 34.7647],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([20.4590,  6.9387, 16.4532,  7.8758,  7.2389, 15.2648, 39.3953, 17.8476,
        14.2893, 15.1278,  6.9935, 69.2146,  6.7186,  6.9517,  8.1253,  4.6026,
         6.4756,  9.2045,  4.4695,  9.7808,  3.8542,  4.8021,  4.1851,  7.0248,
         9.4353,  8.1509, 11.7897,  6.5601,  6.2182,  2.8874,  3.5887,  3.3014,
         5.2272,  2.7514,  3.8105,  3.6420,  6.0889,  3.4666,  3.3990,  3.0974,
         4.4244,  2.6925,  3.6655,  2.4268,  3.6715,  2.0158,  3.3826,  5.6126,
         3.1926,  5.1492,  5.2872,  2.9639,  5.2986,  2.9615,  5.1812,  5.3752,
         5.6663,  4.5568,  3.0302,  2.9318,  2.2936,  3.8053,  3.0875,  3.0743,
         5.4909,  3.3895,  5.3895,  5.7183,  3.2271,  5.3033,  6.5702,  2.3908,
         3.9686,  4.2456,  6.8993,  2.6642,  2.2198,  3.3918,  3.1681,  6.0319],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 59.2114,  19.3650,  47.8689,  18.8065,  17.4176,  36.7177,  87.5924,
         42.5164,  34.4960,  36.7377,  17.5376, 161.9690,  16.9650,  17.4149,
         19.2335,  12.8482,  17.0130,  20.7786,  10.7746,  22.8208,   9.2431,
         11.4612,  10.0670,  16.8606,  25.1342,  20.0162,  29.8372,  15.2760,
         15.9084,   8.0553,  10.1820,   9.3117,  12.6981,   7.7054,  10.2013,
          9.7997,  16.4127,   9.9269,   9.3923,   8.3779,  10.5720,   6.7126,
          8.9261,   6.0510,   8.8660,   4.7436,   8.1272,  13.9164,   7.6969,
         12.5794,  12.5799,   8.2981,  12.6536,   7.2389,  12.3485,  12.9462,
         14.0087,  11.5457,   8.9842,   6.8114,   5.8999,   9.5461,   7.7660,
          7.7114,  13.6757,   8.3199,  13.0947,  14.0416,   7.8450,  12.9295,
         15.5389,   6.1693,   9.8546,  10.8348,  16.4849,   7.6747,   5.5446,
          9.4863,   7.8559,  14.6912], device='cuda:0')
Outer loop valEpocw Maximum [28/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 44.5
model_train val_loss valEpocw [28/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 69.2
model_train val_loss  valEpocw [28/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 162.0
Max_Val Meta Model:  tensor([30.3438, 29.0896, 29.3445, 35.3693, 32.5807, 34.2549, 34.3907, 31.5587,
        34.4157, 34.4084, 34.2712, 34.8821, 33.5907, 32.0386, 35.2694, 29.9473,
        32.9115, 34.9085, 33.8760, 33.8641, 34.8077, 33.6538, 35.0121, 35.2697,
        31.3612, 35.0344, 33.7394, 34.5947, 32.3870, 29.6130, 29.6138, 29.8341,
        35.4326, 29.0106, 31.6360, 31.0474, 31.1681, 29.3208, 30.1830, 31.8849,
        35.3500, 33.9641, 35.5595, 34.2895, 34.9025, 36.7181, 36.3897, 33.9517,
        35.9644, 33.8335, 37.1702, 30.3443, 35.1276, 34.4425, 35.3289, 35.8299,
        35.1527, 32.8770, 28.3031, 35.4417, 34.3011, 32.2553, 33.9390, 32.2549,
        33.2296, 32.9555, 34.5698, 33.2433, 34.5984, 34.6318, 35.7924, 32.4721,
        32.2666, 33.2853, 34.3414, 28.7112, 32.9834, 29.9826, 34.5687, 34.7884],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 6.1112,  4.7846,  1.8017,  2.8580,  2.7296,  0.8956,  1.3932,  5.2896,
         3.8879,  2.6470,  1.7298,  7.5671,  5.9165,  5.9346,  2.6232,  4.6123,
         3.0045,  1.7371,  4.2042,  4.3076,  4.4030,  6.0173,  3.5585,  3.2408,
         3.3861,  2.7024, 10.1712,  7.1223,  4.3456,  3.8681,  4.9603,  5.1919,
         6.8189,  3.7159,  4.9669,  4.1427,  6.3940,  5.5705,  4.1565, 21.7839,
        13.8307, 27.0048, 28.0494, 28.1922, 22.3191, 29.2632,  6.7733,  9.0876,
        18.0497,  7.4041, 15.6955, 16.3822, 22.7935, 13.6029, 25.9053, 40.3230,
        35.1277, 10.5430,  4.4366,  7.0700, 36.1891,  5.4719,  9.1554,  7.3215,
         9.0915,  7.6833,  8.1661,  8.0082,  5.2932,  6.0999,  8.1338,  6.6593,
         6.3236, 14.5223,  3.4077,  6.0942,  4.9022,  4.4949,  4.2409,  7.5302],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([17.3006, 13.5191,  5.3462,  6.8986,  6.5852,  2.2078,  3.2020, 13.2389,
         9.5125,  6.4475,  4.2903, 17.8975, 15.0030, 15.2273,  6.2560, 12.9525,
         7.8965,  3.9354, 10.2579, 10.4775, 10.6787, 14.7881,  8.5717,  7.8200,
         9.0781,  6.6409, 25.8373, 16.6897, 11.1945, 10.8995, 14.2156, 14.8055,
        16.5621, 10.5669, 13.2282, 11.1987, 17.3589, 16.0632, 11.5973, 59.0368,
        32.8537, 68.3931, 68.2245, 71.0916, 54.0862, 68.7367, 16.2038, 22.4881,
        43.1973, 18.1925, 36.7469, 46.1200, 55.2565, 33.3709, 62.1903, 97.4816,
        89.4431, 27.0035, 13.1859, 16.5402, 93.8102, 13.8083, 22.9836, 18.9566,
        22.8208, 19.1666, 19.9176, 19.7925, 12.8363, 14.9525, 19.0032, 17.2575,
        15.8357, 37.3200,  8.3073, 17.8196, 12.2825, 12.6424, 10.3810, 18.4669],
       device='cuda:0')
Outer loop valEpocw Maximum [28/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 37.2
model_train val_loss valEpocw [28/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 40.3
model_train val_loss  valEpocw [28/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 97.5
Max_Val Meta Model:  tensor([33.4660, 28.4928, 28.7615, 32.8648, 32.2288, 33.4597, 33.7492, 30.9767,
        33.8215, 34.4884, 33.5261, 34.4370, 33.2273, 31.4898, 33.7924, 29.8784,
        32.7866, 33.6728, 33.5701, 32.9468, 34.3080, 32.8058, 32.9130, 33.7204,
        31.0393, 34.2964, 33.0874, 33.2697, 32.1002, 29.1053, 29.2103, 29.3548,
        32.2219, 28.3314, 31.3863, 30.7720, 30.7510, 28.9174, 29.7882, 31.5308,
        31.1845, 32.3401, 33.5473, 33.0547, 33.7087, 31.7164, 33.7532, 33.5965,
        33.2220, 33.3076, 32.3865, 29.7111, 32.7682, 34.1235, 33.7793, 32.7277,
        32.0446, 33.6895, 27.9404, 33.1024, 32.5800, 31.9629, 32.4678, 31.3865,
        32.5945, 33.6058, 32.5769, 35.6563, 34.1799, 34.0964, 33.4060, 31.9762,
        31.5241, 32.6132, 39.3207, 28.2208, 32.4532, 29.7341, 31.9049, 33.5564],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([12.5325,  3.8179,  8.0113,  5.5859,  9.2319,  4.7517,  5.5583,  9.3626,
         4.5100, 10.6297,  3.8902,  7.5992,  4.9785,  6.5498,  9.1235,  4.2645,
         4.7079,  4.8243,  4.7847,  5.2193,  4.1894,  4.9128,  5.0522,  7.1662,
         7.6283,  2.7415,  8.8431,  2.0434,  6.1987,  3.0198,  3.6296,  3.4481,
         3.5201,  2.8446,  4.0899,  3.3215,  4.7785,  3.2885,  3.2952,  3.7707,
         4.4880,  2.7398,  4.1397,  2.5729,  4.4948,  1.6409,  4.0217,  5.9814,
         3.4078,  4.7462,  5.5731,  3.1245,  5.4740,  3.0430,  5.3782,  4.6959,
         3.3343,  4.3557,  4.6980,  3.5791,  3.2630,  6.7257,  3.7934,  3.2109,
         5.7588,  3.5203,  5.4785,  3.3549,  3.6638,  4.7438,  6.8808,  3.4268,
         2.9634,  4.9928, 95.5026,  3.6274,  2.4200,  4.3211,  3.3575,  6.3288],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 35.1259,  10.8485,  24.0731,  13.8407,  22.4180,  11.7990,  12.7984,
         23.6003,  11.1202,  25.8735,   9.6759,  18.0776,  12.6427,  16.9677,
         22.0160,  11.9509,  12.4233,  10.9771,  11.7218,  12.8025,  10.1550,
         12.1667,  12.3840,  17.6036,  20.5087,   6.7996,  22.5431,   4.8362,
         15.9969,   8.5601,  10.4362,   9.8834,   8.7908,   8.1884,  10.8681,
          8.9761,  13.0019,   9.5162,   9.2284,  10.2076,  11.2627,   6.9850,
         10.1697,   6.5297,  10.9045,   4.0606,   9.8313,  14.8116,   8.3579,
         11.7082,  13.7921,   8.8176,  13.6109,   7.4994,  13.0417,  11.6486,
          8.6337,  11.0474,  14.0329,   8.5388,   8.4599,  17.0239,   9.6518,
          8.4210,  14.5699,   8.7147,  13.7477,   8.1668,   8.8883,  11.6983,
         16.4558,   8.9262,   7.4309,  12.9403, 238.6141,  10.6535,   6.0646,
         12.1398,   8.3990,  15.6999], device='cuda:0')
Outer loop valEpocw Maximum [28/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 39.3
model_train val_loss valEpocw [28/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 95.5
model_train val_loss  valEpocw [28/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 238.6
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [88.86100316 97.2137279  91.80809201 97.02489005 97.26733349 96.5997003
 96.99808726 94.65771616 97.44398826 96.4742145  98.53193796 98.52097319
 99.41399349 95.31560288 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.15699126 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.26832032 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.61339409 97.84237521 92.80588687
 96.90915072 96.22689782 96.9627563  93.88165349 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.90921163 96.13796128 96.24273583 96.9067141
 92.22353529 97.17839695 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [86.87028667 97.2137279  90.62511422 97.02489005 97.26733349 96.5997003
 96.99808726 94.74908931 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.09729414 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.42060891 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.73766158 97.84237521 92.48303505
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.00400824 96.13796128 96.24273583 96.9067141
 91.1380222  97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [97.08717055  4.78974084 61.76320267 13.91388922 18.32836608 21.89107554
 10.85618881 33.82742686  4.43823363 26.69375191  3.93611152  1.27855469
  0.87668962 16.33347547  6.96115651  8.57135435  7.02759985  8.69245721
  1.38378236  2.82821789  1.81560346  0.50770216  2.75745015 19.05216475
 17.61087542 12.81986059 29.47569905  9.82846657  5.23226279  1.33389935
  2.48113587  0.86144626 36.98971937  1.32642507  1.50584178  4.88579186
  9.6850022   8.69544117  2.86532149 34.2505238   8.11070021 47.5284075
 32.98644074 30.00995473 19.88431772 35.47492965  3.13732722  1.6799782
  2.74184631  1.77840634  1.50381739  1.23967702  1.26186102 11.42857053
  1.58478795  3.96026711 64.0814461  26.56497097 13.15937718 11.97410111
 64.18933162 33.40561187 26.28189878  9.40709983  3.63094124  8.57061224
  2.98248464 11.75837868  3.34421819  4.4863419   0.33249328 22.48368276
  7.39143095 23.40527168 23.09043832  5.82609657  1.15484341  2.36359083
  0.18563998  0.82072561]
Accuracy th:0.5 is [45.6463737  97.2137279  70.93846323 97.02489005 97.26733349 75.58143785
 75.91647275 75.48153653 76.84482402 96.36943994 77.17254907 98.52097319
 99.41399349 79.62865949 76.76563395 96.56680596 96.29512311 76.4671483
 98.65376884 98.30776915 79.41058223 77.61235852 98.38695922 76.65476785
 81.08819337 96.65086926 94.0778012  76.21983163 98.01293844 77.07021113
 97.30875598 98.57457877 96.36213009 98.02024829 85.4680133  76.65964109
 76.55364823 88.03498983 97.11504489 73.92819288 78.59066045 92.05906361
 75.86164886 75.50102947 96.9627563  93.87434364 98.02877645 98.57336046
 92.01885942 85.72142152 85.97117482 98.55508583 98.99976852 76.09434583
 98.70615611 76.57435947 71.45502613 92.04078898 96.24273583 96.9067141
 89.79300934 97.17717864 91.96159891 76.51709896 98.42838172 77.10310547
 98.20786784 75.70936027 77.52098537 97.45373473 78.06435107 95.99054592
 77.38087986 95.45083515 75.85433901 81.15520035 85.78111865 77.13721811
 78.09968202 99.14718388]
Accuracy th:0.7 is [45.65733848 97.2137279  70.93846323 97.02489005 97.26733349 75.58143785
 75.91647275 75.81900805 76.84482402 96.46568633 77.17254907 98.52097319
 99.41399349 80.16715196 76.76563395 96.56680596 96.29512311 76.4671483
 98.65376884 98.30776915 79.87719448 77.61235852 98.38695922 77.09701393
 81.81430538 96.65086926 94.0778012  76.21983163 98.01293844 77.07021113
 97.30875598 98.57457877 96.36213009 98.02024829 85.69461873 76.65964109
 76.55364823 88.36393319 97.11504489 73.92819288 79.25220209 92.05906361
 75.86164886 75.50102947 96.9627563  93.87434364 98.02877645 98.57336046
 94.11922369 86.72043469 86.16488591 98.55508583 98.99976852 76.09434583
 98.70615611 76.57435947 71.45502613 92.45988718 96.24273583 96.9067141
 89.79300934 97.17717864 92.13216213 76.51709896 98.42838172 77.10310547
 98.20786784 75.70936027 77.52098537 97.55850928 78.06435107 95.99054592
 77.49052765 95.45083515 75.85433901 81.23926365 85.97726636 77.13721811
 78.09968202 99.14718388]
Avg Prec: is [55.77832921  3.07135068 11.29925982  3.31220072  2.2231258   3.9108886
  3.40201679  5.52611659  2.53788826  3.82333219  1.5473362   1.65252709
  0.64595797  5.13246561  2.6769402   3.14490986  3.68247775  2.62315495
  1.41354568  1.74853387  2.07433935  0.90443116  1.84518061  2.39376817
  5.0503179   3.62550552  6.48077158  3.38571932  2.0454805   1.82727637
  2.6818138   1.31014259  3.79869737  1.6622158   2.39044924  2.42291011
  3.01221064  2.52801188  2.90199938  7.31526548  2.28814116  8.34191737
  3.56684252  4.16821812  3.33649901  6.34863835  2.06057211  1.48795539
  2.17723723  1.56669837  1.77317171  1.56658718  1.01709641  3.03477928
  1.28799158  2.72567275 11.27890641  3.5649006   3.9991181   2.76256778
 10.90515741  2.15894854  3.69599536  2.98612006  1.5702744   2.41516987
  1.81039435  4.21344411  1.26084417  2.33921196  0.21310941  3.38528117
  1.85412171  4.51362185  3.97660205  3.01219273  0.83550796  1.85234488
  0.13564153  0.73140463]
mAP score regular 14.13, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [89.44614695 97.22450607 92.45833022 96.96290206 97.90716795 96.63651992
 96.80843112 94.73802227 97.38894287 96.43969405 98.5250517  98.5325261
 99.34972718 95.10426788 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.35931933 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.22791938 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.31382515 97.82744101 92.77225503
 97.07750953 96.48703192 97.03764606 94.04290306 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.00829658 96.39235618 96.16314124 96.78102499
 92.50317662 97.05508633 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [88.75601066 97.22450607 91.14532725 96.96290206 97.90716795 96.63651992
 96.80843112 94.89498468 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.34935346 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.43221965 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.82208436 97.82744101 92.80215263
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.69935471 96.39235618 96.16314124 96.78102499
 91.71587313 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [97.14639185  6.05354838 65.57650838 14.58408718 16.9169411  21.86037446
 12.04338757 34.23271539  5.01060603 29.89506628  4.29739811  1.27180828
  0.7129168  17.37779502  7.14687871 10.38285307  8.13955875 10.16919223
  1.13818528  3.1265949   1.50835531  0.50665947  2.72898603 17.90525455
 17.75018911 14.04503717 29.71470819  9.79355839  6.21945468  1.37540384
  2.27543328  0.76545219 41.20823254  1.25200569  1.29941977  4.78442544
  9.68488465 13.2792145   3.07654857 36.37580333  9.73132795 46.47266047
 32.13615354 29.61809778 20.4776151  35.41939863  3.12221627  1.49784251
  3.29280516  1.67955338  1.73812615  1.38512186  1.46878376 14.3036774
  1.68242452  4.33017455 57.69105282 28.61624864 11.57257811 12.99535393
 63.42040421 38.07747275 29.28168283  9.65984416  3.51814948  8.42954483
  2.82019097 12.3760042   2.92916776  4.23504257  0.29839461 23.95047044
  7.06546566 25.23813323 31.25677883  4.92100997  1.13971033  2.37152358
  0.1706798   0.75690252]
Accuracy th:0.5 is [45.71841443 97.22450607 68.98871366 96.96290206 97.90716795 74.26065725
 74.37775619 73.57052097 75.88011062 96.41228791 75.99471809 98.5325261
 99.34972718 77.09594638 75.96731196 96.31262924 96.21047911 75.36437701
 98.78167277 98.34068316 77.67396666 76.63502504 98.31327703 75.56618581
 77.42232852 96.52938685 94.3393876  75.46652714 97.81747515 75.96731196
 97.52597354 98.67204823 96.39983058 98.18870369 85.81358846 75.59359195
 75.60604928 90.3256347  97.0276802  72.90031642 76.46311384 92.37362035
 74.72157859 74.42509405 97.03764606 94.02795426 98.18621222 98.77668984
 92.97406383 85.60679672 84.12437402 98.55993223 98.87385704 74.69666393
 98.6969629  75.39925754 69.76605127 93.21324464 96.16314124 96.78102499
 90.13379176 97.04761193 91.90024167 75.46403568 98.32075143 76.34103197
 98.13139996 74.68669806 76.70229464 97.49607594 77.06106585 96.07843137
 76.4805541  95.44559882 74.61942846 81.9318833  87.67969704 76.05949623
 77.13580985 99.15040985]
Accuracy th:0.7 is [45.9376635  97.22450607 68.98871366 96.96290206 97.90716795 74.26065725
 74.37775619 73.77232977 75.88011062 96.41976231 75.99471809 98.5325261
 99.34972718 77.52946159 75.96731196 96.31262924 96.21047911 75.36437701
 98.78167277 98.34068316 78.01280614 76.63502504 98.31327703 75.72314822
 77.9704512  96.52938685 94.3393876  75.46652714 97.81747515 75.96731196
 97.52597354 98.67204823 96.39983058 98.18870369 86.10010713 75.59359195
 75.60604928 90.56481551 97.0276802  72.90031642 76.97635598 92.37362035
 74.72157859 74.42509405 97.03764606 94.02795426 98.18621222 98.77668984
 94.94232255 86.15741087 84.29628522 98.55993223 98.87385704 74.69666393
 98.6969629  75.39925754 69.76605127 93.57201585 96.16314124 96.78102499
 90.13379176 97.04761193 92.07215288 75.46403568 98.32075143 76.34103197
 98.13139996 74.68669806 76.70229464 97.53344794 77.06106585 96.07843137
 76.5328749  95.44559882 74.61942846 82.01659317 87.90143758 76.05949623
 77.13580985 99.15040985]
Avg Prec: is [54.19895637  3.74775205 14.89399109  4.56828169  1.59221402  4.29823269
 11.18665883  8.77544941  7.27136169  5.168877    2.26844467  5.27778003
  1.54245499  5.90555945  3.09193756  4.18838518 26.59720358  6.22299879
  1.53556379  2.58314389  3.62801307  1.43974554  1.30888725  5.82018143
  5.7743091  11.9947933   8.21904765  4.56389812  3.91078373  7.24106685
  2.32637502  0.87116883  3.00037197  1.15543269  1.71701754  2.43685181
  2.04028996  2.21844394  2.20104613  6.21307706  1.73781438  6.04970778
  2.20074339  2.73480285  2.37142602  4.87607122  1.76753052  1.05315869
  1.41875885  1.19304155  1.22436476  1.00239152  0.75444364  2.32419839
  0.91802268  1.88073661 10.18299304  3.02631143  3.92572493  2.88244118
  7.93544782  2.04057573  3.38516133  2.60897514  1.36186514  1.89345515
  1.58997745  3.53392689  1.05561448  2.18725362  0.19399668  3.16192265
  1.53319161  4.03677169  3.19203177  2.31443024  0.55406663  1.50052462
  0.12382546  0.61997179]
mAP score regular 14.67, mAP score EMA 4.39
Train_data_mAP: current_mAP = 14.13, highest_mAP = 14.51
Val_data_mAP: current_mAP = 14.67, highest_mAP = 15.03
tensor([0.3575, 0.3511, 0.3330, 0.4027, 0.4121, 0.4026, 0.4346, 0.3963, 0.4053,
        0.4097, 0.4015, 0.4210, 0.3938, 0.3859, 0.4143, 0.3561, 0.3788, 0.4398,
        0.4073, 0.4068, 0.4115, 0.4041, 0.4075, 0.4065, 0.3718, 0.4040, 0.3928,
        0.4204, 0.3872, 0.3527, 0.3477, 0.3492, 0.3998, 0.3473, 0.3765, 0.3699,
        0.3679, 0.3456, 0.3571, 0.3688, 0.3983, 0.3912, 0.4078, 0.3942, 0.4129,
        0.4010, 0.4088, 0.4039, 0.4074, 0.4058, 0.4038, 0.3545, 0.4020, 0.4071,
        0.4114, 0.4041, 0.3834, 0.3932, 0.3346, 0.4182, 0.3847, 0.3948, 0.3922,
        0.3805, 0.3958, 0.4042, 0.3964, 0.4043, 0.4123, 0.4074, 0.4170, 0.3841,
        0.4012, 0.3859, 0.4094, 0.3396, 0.4028, 0.3551, 0.3979, 0.4027],
       device='cuda:0')
Max Train Loss:  tensor([11.0759,  6.5274,  9.0527,  8.7890,  7.5189,  6.4284,  2.7254, 10.9169,
         6.0830,  6.7692,  2.2611,  9.8903,  5.1369,  7.2952,  9.7643, 12.2739,
         8.6842,  6.3154,  5.5543,  7.5924,  6.9376,  8.5422,  4.9791,  6.5566,
         6.6945,  6.7434,  9.5061,  6.8857,  7.5482,  5.1804,  7.4622,  3.6407,
         9.5441,  4.9793, 10.2617,  8.1296,  7.3237,  4.9420,  5.8885,  8.5571,
         7.5105, 10.1425,  9.1316,  8.1419,  9.4806, 12.0498,  7.4433,  8.0586,
         7.3014,  4.8699,  9.1800,  6.3113,  5.6804,  7.5134,  5.6904,  7.4564,
        13.3488,  8.1924,  5.4334,  8.1544,  9.8211,  7.7250,  8.5566,  5.3880,
         7.2284,  7.2097,  5.6661,  7.3136,  8.2448, 10.8209,  6.9913,  5.3940,
         7.7101, 11.0550,  7.4722,  4.4014,  3.9318,  5.9667,  3.5121,  6.5034],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [29/80], Step [000/642], LR 1.0e-04, Loss: 13.3
Max Train Loss:  tensor([12.8095,  6.1682, 12.3704,  8.9731, 10.6730, 11.1295, 13.5009, 14.6648,
         7.3245, 14.5365, 11.2708,  4.6782, 15.7348, 12.0463, 11.1841,  7.6075,
         6.5141, 11.9631,  7.1148,  9.5500,  9.8269, 13.5957,  7.7770, 10.1163,
         9.4939,  9.3164, 10.3748, 13.9162,  5.9029,  6.8025,  8.9293,  5.0251,
        14.9049,  4.6917,  6.0637,  6.3541,  7.9568,  9.2194,  8.0503, 10.1345,
         9.0608, 12.9478,  7.5200, 12.1499, 10.4035, 14.9485,  5.8137,  7.2634,
        11.8846, 13.4243, 10.7807,  5.9972, 13.6114,  9.5609, 11.4132, 11.6054,
         9.3311,  7.6107,  8.6547,  8.9983, 11.7116,  6.2148,  7.6044,  9.3027,
        10.1657,  6.0543,  7.1807, 11.3533, 11.2270, 14.8624,  4.4621,  4.4762,
        12.1769,  6.9060,  7.7192,  7.9852, 15.5557,  7.0012, 15.9279,  6.3214],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [29/80], Step [100/642], LR 1.0e-04, Loss: 15.9
Max Train Loss:  tensor([13.5630,  5.6312, 10.1408,  8.1117,  8.0738,  8.6304,  7.1718, 11.7963,
         5.7764,  7.0281,  7.2510,  4.8399,  2.4691, 11.7926,  8.1973,  4.9296,
         9.0518,  8.8072,  7.8155,  4.9321,  7.1870,  5.6785,  6.9794,  7.8703,
         8.6926,  9.6611, 11.5670,  9.7187,  6.4637,  6.9482,  8.8159,  4.9965,
         8.8328,  5.9459,  6.5406,  5.9541,  6.9996,  5.8987,  4.5787, 10.2958,
         6.4946, 12.9049, 11.0147,  7.9463,  6.4585,  9.8882,  8.6044, 10.0560,
         8.7335,  9.5100,  9.1122,  3.8888,  6.6708,  6.6385,  5.2881,  9.9173,
        13.0601, 10.8741,  6.9301,  8.1627, 10.7357, 11.1984, 11.4507,  5.8233,
         8.5500, 10.3131,  6.9829,  7.8212,  7.6531,  5.7521,  4.1565, 10.0921,
         8.6775, 12.5225,  6.9388,  7.1737,  5.8234,  4.5802,  5.7145,  7.7892],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [29/80], Step [200/642], LR 1.0e-04, Loss: 13.6
Max Train Loss:  tensor([13.9312,  5.8108, 10.7508,  8.1638,  5.2738,  8.5532,  4.8060,  9.1025,
         5.9478, 11.4657,  9.3243,  8.3215,  3.5977, 10.0814, 11.3861,  9.7025,
        10.7031, 10.3412,  5.9701,  5.8513,  5.2120,  5.9588,  7.1667,  5.9421,
         8.6516,  5.7876, 10.7166,  7.3012,  7.2075,  5.0371,  8.9051,  5.3778,
         8.2627,  4.9561,  7.6033,  8.2405,  7.0122,  8.7065,  3.6395,  9.6740,
         6.6701, 12.3380,  8.6740,  9.8860, 12.7217, 11.3030,  9.8949,  7.6092,
         9.3587,  6.8567,  5.0556,  6.6327,  4.8177,  6.5481,  7.8428,  7.1708,
        11.3149,  9.3230,  6.1544,  9.6078, 12.0153,  7.2510,  9.4978,  8.4543,
         9.1736,  7.5635,  8.0814,  7.1367,  7.8508,  7.4213,  5.2941,  8.3049,
         8.7880, 10.0568,  9.6285,  7.2206,  5.9095,  4.4640,  5.7950,  7.7575],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [29/80], Step [300/642], LR 1.0e-04, Loss: 13.9
Max Train Loss:  tensor([11.1689,  6.0203,  8.5100,  7.2830,  8.5102,  7.4694,  9.2545,  7.4146,
        10.5708,  5.7849,  8.1030,  5.7875,  2.6902, 12.8559,  9.9106,  5.9954,
        11.3664, 10.3697,  5.2039,  6.2389,  8.3931,  4.6400,  7.4315,  4.0543,
         9.3135,  9.0397,  8.4596,  6.8376,  9.4325,  9.2929,  7.5601,  4.3109,
         6.7217,  5.2084,  4.9124,  7.2622,  7.2911,  7.8196,  4.9454, 12.5388,
         6.7583, 13.0117,  9.9226, 10.4435,  9.9082, 12.0227,  7.2774,  8.0525,
        10.8560,  7.0548,  4.9141,  6.4517,  5.0179,  8.1361,  6.2759,  7.9340,
        11.3941,  9.8561,  6.9525, 11.5288,  9.7759,  8.2485,  8.2984,  8.4562,
         8.8101,  5.1071,  4.5080,  9.3020,  8.3514,  9.2297,  4.4387,  8.7464,
         8.6936,  7.2401, 11.2078,  5.8425,  6.0897,  7.8454,  5.9760,  6.9831],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [29/80], Step [400/642], LR 1.0e-04, Loss: 13.0
Max Train Loss:  tensor([10.5642,  8.1723,  8.3801,  5.9678,  5.9554,  6.2376,  6.5907,  7.3885,
         6.9848,  6.3979,  8.9251,  5.0314,  2.5252,  4.3851, 11.0629,  7.3552,
         7.1462,  3.2895,  8.0954, 10.2493,  4.5850,  4.4609,  6.7690,  7.7684,
         6.1815,  5.7930, 10.6179,  9.0834,  7.0826,  6.5507,  5.2046,  3.1541,
         8.4382,  4.9436,  6.9066,  6.8104,  9.7381,  3.5979,  8.7134,  7.8547,
         6.5174, 11.9280,  7.6826,  7.5510,  7.5112,  9.6845,  6.4838,  4.7911,
         6.4395,  6.8852,  4.4116,  5.1678,  5.9904,  6.5275,  6.1256,  6.7145,
        13.3302,  9.1876,  5.5693, 11.7075, 10.9265,  7.6478,  7.0108,  8.0967,
         9.4364,  4.4400,  7.9741, 10.6962,  8.3017,  6.9142,  4.2357,  5.7657,
         8.2226,  9.9555,  8.2285,  7.8775,  6.7132,  6.1241,  6.5765,  8.0134],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [29/80], Step [500/642], LR 1.0e-04, Loss: 13.3
Max Train Loss:  tensor([11.1138,  6.1957, 10.1171,  7.5007, 12.2606,  8.5560,  6.6292,  9.8755,
         6.0755,  7.2000,  4.8155,  6.3943,  2.6355,  9.0967,  6.4484,  7.7537,
         8.4581,  6.7819,  6.4408,  5.1210,  5.5132,  6.4122,  8.5491,  7.5091,
        10.6333,  6.4473,  8.9037,  8.8424,  7.7628,  6.5697,  6.3929,  4.5343,
         6.6718, 10.2562,  5.8010,  5.6200,  7.3796,  7.7003,  7.1379,  9.0429,
         6.8469,  8.7958,  4.2172,  7.7907,  6.5849,  5.7995,  7.6471,  4.8231,
         5.1858,  5.8215,  4.5087,  5.9626,  4.9108,  7.9043,  6.4699,  5.9517,
         8.7075,  8.9190,  4.6302, 11.2629,  8.3052,  8.0842,  6.8606,  6.5632,
         9.2840,  6.0147,  5.2708,  6.3460,  7.1538,  7.4698,  4.3563,  7.7760,
         7.3855, 10.9737,  5.3805,  4.7381,  7.5500,  4.5484,  5.9068,  6.8896],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [29/80], Step [600/642], LR 1.0e-04, Loss: 12.3
Max_Val Meta Model:  tensor([ 22.7593,  24.2365,  31.0685,  19.9940,   5.4227,   7.7749,   7.5289,
          8.2212,   4.9379,   7.5308,   4.0106,   7.1774,   3.6043,   6.9777,
          5.5719,   5.1290, 131.1335,   6.1105,   5.1103,   5.0907,   3.8308,
          4.5509,   4.0778,   3.8400,  10.5124,   8.7501,  11.3693,   4.8493,
          6.6334,   5.4610,   3.9150,   4.5752,   6.1278,   2.8887,   4.2538,
          4.6830,   4.7313,   5.5958,   5.3889,  16.8962,   7.5862,   9.7009,
          5.4286,   8.5421,   7.0745,   8.2742,   4.8205,   3.4913,   4.2021,
          4.8749,   3.5599,   4.0182,   6.5440,   4.7413,   4.3699,   4.3458,
         11.3761,   7.7913,   7.7434,   1.8998,  10.7813,  10.9027,   4.7849,
          5.1039,   8.1415,   3.7246,   4.4167,   5.3044,   8.6953,  10.7855,
          4.3085,   8.8960,  11.0919,   6.5684,   8.2795,   5.2569,   6.0011,
          4.2932,   5.8854,   6.1498], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 15.8635,  23.2386,  24.1661,  19.6913,   5.0302,   8.9295,   8.3919,
         10.5448,   5.3428,   8.4821,   4.3928,   7.6823,   4.0058,   7.5701,
          5.5593,   5.6129, 124.4480,   7.7149,   5.6990,   5.7122,   4.2927,
          5.0908,   4.3329,   4.1265,  11.0669,   9.6267,  12.2490,   6.3277,
          7.1706,   5.7117,   5.0997,   5.2336,   7.5676,   3.3025,   5.2304,
          5.6702,   5.7881,   5.8131,   6.7099,  17.1791,   7.8984,   9.4335,
          5.6356,   8.2405,   7.2650,   8.6265,   5.3455,   3.7863,   4.3446,
          5.0604,   3.6419,   4.5316,   6.7847,   4.4442,   4.8126,   4.3026,
         11.1378,   7.8105,   7.9926,   2.1001,   7.3756,  10.6027,   5.2369,
          5.7178,   8.8452,   4.0820,   4.9658,   6.9167,   9.1862,  10.9749,
          4.8587,   9.0700,  11.6182,   7.3013,   8.1506,   5.4195,   6.5947,
          4.7644,   6.5114,   6.7889], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 44.3730,  66.1946,  72.5668,  48.9037,  12.2064,  22.1780,  19.3102,
         26.6106,  13.1812,  20.7022,  10.9412,  18.2461,  10.1724,  19.6156,
         13.4183,  15.7612, 328.4975,  17.5410,  13.9929,  14.0429,  10.4330,
         12.5990,  10.6331,  10.1507,  29.7687,  23.8275,  31.1821,  15.0512,
         18.5178,  16.1943,  14.6685,  14.9871,  18.9307,   9.5104,  13.8920,
         15.3304,  15.7326,  16.8219,  18.7910,  46.5866,  19.8296,  24.1150,
         13.8201,  20.9025,  17.5958,  21.5141,  13.0774,   9.3750,  10.6650,
         12.4687,   9.0191,  12.7844,  16.8762,  10.9169,  11.6980,  10.6480,
         29.0507,  19.8657,  23.8902,   5.0220,  19.1716,  26.8583,  13.3514,
         15.0282,  22.3458,  10.0998,  12.5271,  17.1061,  22.2801,  26.9419,
         11.6505,  23.6165,  28.9583,  18.9203,  19.9071,  15.9589,  16.3741,
         13.4169,  16.3662,  16.8570], device='cuda:0')
Outer loop valEpocw Maximum [29/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 131.1
model_train val_loss valEpocw [29/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 124.4
model_train val_loss  valEpocw [29/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 328.5
Max_Val Meta Model:  tensor([33.2985, 29.7246, 29.0783, 36.1703, 34.1261, 36.5856, 47.2961, 33.9602,
        33.5181, 34.7335, 34.3332, 44.6475, 33.6204, 34.4379, 38.6301, 30.5743,
        32.8656, 35.7695, 35.4428, 36.8827, 35.0878, 34.6050, 34.1950, 37.2177,
        31.9518, 34.8587, 33.5328, 35.3464, 31.3872, 29.9316, 30.0040, 30.3086,
        34.0434, 29.2502, 31.9485, 31.4429, 31.3884, 29.5833, 30.6135, 31.5468,
        33.4484, 32.8483, 34.4730, 33.8518, 34.3028, 33.1929, 34.5366, 33.9915,
        34.5853, 33.9058, 34.3863, 30.4379, 34.0617, 34.4390, 34.4236, 34.0348,
        32.7607, 34.1130, 27.3541, 33.8325, 33.4057, 32.5283, 33.4647, 32.6669,
        33.4254, 34.3998, 31.5190, 32.8444, 35.1375, 35.1357, 35.0085, 32.4633,
        33.6792, 31.6871, 33.1508, 28.8661, 34.6194, 30.3215, 33.9123, 34.5529],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([21.8801,  6.4228, 14.5504,  6.1962,  9.3447, 16.8777, 33.3602, 18.8412,
        14.1969, 15.5062,  7.1114, 46.2599,  4.5983,  4.3644,  7.0192,  4.5272,
         4.7977, 10.8758,  4.4041,  9.4765,  3.9088,  3.8387,  3.8038,  5.1441,
         9.3248,  9.0646, 12.2355,  7.5077,  6.5157,  2.8336,  3.5555,  3.6279,
         3.7016,  2.3781,  3.4538,  3.7218,  5.1975,  3.8143,  4.2199,  1.6680,
         4.2643,  1.8254,  2.6906,  2.8412,  2.4417,  1.5537,  4.0251,  1.8293,
         2.8958,  3.8932,  2.5203,  3.3493,  3.9405,  2.7190,  3.5098,  3.2326,
         3.9764,  4.4794,  3.9999,  0.8907,  3.1299,  2.9712,  2.9883,  3.7066,
         7.1422,  2.3440,  3.6160,  6.4697,  5.4842,  3.8396,  3.5807,  2.8497,
         6.7907,  2.1554,  9.7828,  2.3199,  5.2051,  2.9621,  4.9773,  5.2101],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 64.9950,  18.2845,  43.5011,  15.1280,  22.7976,  40.9380,  75.2121,
         47.3353,  35.0924,  37.8435,  17.6746, 109.3416,  11.6717,  11.0538,
         16.3866,  12.7287,  12.5942,  24.9023,  10.6956,  22.7234,   9.4936,
          9.4806,   9.3217,  12.3456,  25.0348,  22.3217,  31.0127,  17.7699,
         17.2926,   7.9802,  10.1429,  10.3133,   9.2295,   6.7704,   9.1870,
         10.0418,  14.1164,  11.0183,  11.7531,   4.5453,  10.7053,   4.6202,
          6.5875,   7.1906,   5.9142,   3.8698,   9.7816,   4.5023,   7.0873,
          9.5856,   6.1929,   9.4368,   9.7680,   6.6686,   8.5038,   7.9439,
         10.3047,  11.3065,  12.1437,   2.1332,   8.0981,   7.6328,   7.6615,
          9.6531,  17.9535,   5.7554,   9.3589,  16.4349,  13.2955,   9.3289,
          8.6084,   7.4302,  16.9457,   5.6848,  23.9206,   6.7964,  12.6419,
          8.3071,  12.6035,  12.9499], device='cuda:0')
Outer loop valEpocw Maximum [29/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 47.3
model_train val_loss valEpocw [29/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 46.3
model_train val_loss  valEpocw [29/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 109.3
Max_Val Meta Model:  tensor([30.3057, 29.4523, 28.3791, 35.5918, 33.3876, 34.6490, 32.4585, 32.9189,
        29.8894, 33.9444, 33.5885, 34.8967, 32.8222, 33.4696, 34.3688, 30.8685,
        33.8490, 33.4929, 35.1130, 36.0762, 34.4103, 35.4346, 34.0846, 34.4398,
        31.1242, 32.8678, 32.7713, 34.7134, 30.7116, 29.3419, 29.5840, 29.8394,
        34.8830, 28.6857, 31.4700, 30.8993, 30.9938, 29.2341, 30.1602, 31.3195,
        33.2236, 33.0980, 35.0694, 34.1521, 34.6673, 34.5267, 35.8531, 30.5547,
        35.9231, 33.9383, 35.2069, 30.2636, 35.8284, 34.0976, 34.7005, 33.7134,
        35.7033, 33.6198, 26.8367, 31.8060, 34.3073, 32.5717, 33.0549, 32.1831,
        33.1746, 33.7953, 30.7241, 32.3115, 34.7006, 34.5535, 34.3923, 32.2853,
        33.0740, 31.5193, 32.2138, 28.3614, 34.2098, 29.7689, 33.5169, 34.0210],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 4.0562,  3.4732,  2.6591,  4.9070,  1.7873,  3.3137,  2.4213,  5.4731,
         3.1487,  2.3755,  2.5572,  1.9587,  2.7338,  6.6192,  2.0779,  4.5144,
         3.1993,  2.5967,  5.5802,  5.5866,  2.3121,  4.8855,  3.0433,  2.3864,
         3.6874,  3.5904, 10.0751,  7.0640,  5.2674,  3.9255,  3.8464,  4.9719,
         5.3912,  3.3596,  3.9414,  4.5330,  6.5118,  5.3144,  5.9335, 15.8542,
        12.8858, 24.5959, 31.1794, 28.1223, 21.5523, 27.8648,  7.2159,  5.2005,
        17.3092,  6.2673, 13.2924, 16.0134, 22.3850, 13.5116, 26.4926, 34.2332,
        36.5158, 10.9851,  4.9163,  1.9898, 23.1872,  5.8766,  7.3266,  7.3961,
        10.6874,  5.9192,  6.3159,  7.9816,  7.6284,  5.7980,  4.7795,  6.8355,
         7.8139, 15.2507,  1.5984,  5.9548,  8.2568,  4.0934,  6.3989,  6.7222],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([12.0594,  9.8778,  7.9210, 12.0156,  4.3594,  8.1705,  5.8049, 13.7704,
         8.1176,  5.8289,  6.3563,  4.6389,  6.9601, 16.9608,  5.0479, 12.6844,
         8.2207,  6.0528, 13.5481, 13.4942,  5.6481, 11.9381,  7.4647,  5.8465,
         9.9495,  8.9818, 25.5071, 16.8484, 14.0162, 11.0893, 10.9623, 14.1590,
        13.3784,  9.5791, 10.4802, 12.2628, 17.6933, 15.3544, 16.5550, 43.1405,
        32.3934, 63.2750, 76.3303, 71.1827, 52.2524, 68.8708, 17.2943, 13.4701,
        42.1623, 15.2845, 32.3709, 45.0635, 55.4494, 33.1436, 64.2901, 87.1563,
        94.9274, 27.8006, 14.8667,  4.8135, 60.3381, 15.0683, 18.7620, 19.3054,
        26.8594, 14.5451, 16.4206, 20.2772, 18.4905, 14.0777, 11.5060, 17.7828,
        19.5097, 40.4834,  3.9128, 17.4905, 20.0603, 11.5001, 16.1710, 16.7404],
       device='cuda:0')
Outer loop valEpocw Maximum [29/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 36.1
model_train val_loss valEpocw [29/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 36.5
model_train val_loss  valEpocw [29/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 94.9
Max_Val Meta Model:  tensor([30.6948, 29.3242, 27.6316, 33.0615, 32.5137, 32.3650, 32.0801, 32.2779,
        29.1952, 32.0578, 33.1087, 34.0963, 32.4551, 30.6224, 33.5027, 28.9362,
        33.4279, 33.0049, 32.9087, 32.6473, 33.7966, 31.0135, 32.2995, 33.5818,
        30.9402, 31.9934, 32.4895, 33.3503, 30.5405, 28.8522, 28.9531, 31.5269,
        33.5181, 27.9687, 31.3092, 30.6818, 30.3758, 31.3005, 29.7798, 32.1998,
        32.8602, 33.0377, 33.6748, 34.1685, 32.0226, 32.9973, 33.9048, 30.1368,
        32.7154, 33.4313, 33.5094, 29.6311, 32.6380, 32.5093, 32.5308, 30.5837,
        34.5091, 33.1233, 26.6082, 33.1867, 33.3487, 31.3459, 34.0570, 33.2510,
        32.5743, 30.9562, 31.9322, 34.7540, 31.8524, 33.9550, 32.0113, 32.2798,
        32.6432, 31.0208, 41.6728, 27.9014, 31.9354, 29.6473, 33.5479, 32.9080],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([11.0623,  2.4189,  7.8818,  4.0780, 11.9427,  9.4799, 10.2635, 12.6948,
         3.6283, 10.5910,  3.9285, 13.1588,  2.1338,  5.7100,  8.1074,  3.6453,
         2.8425,  6.2245,  4.3064,  4.2435,  4.2095,  3.7509,  4.4936,  4.5828,
         7.1082,  4.2310,  8.9991,  3.3646,  6.3360,  2.7687,  2.6950,  3.2370,
         2.4553,  2.3081,  2.7328,  2.9551,  3.6590,  3.5222,  3.1397,  2.1755,
         4.2431,  1.5774,  2.7217,  3.0995,  2.4736,  1.1845,  4.0349,  1.7936,
         3.0300,  3.2244,  2.7997,  3.3005,  3.8948,  2.8952,  3.4973,  2.5026,
         2.3298,  4.2226,  4.9317,  1.0892,  4.4090,  3.8785,  3.1817,  3.6852,
         7.0545,  2.2892,  3.5812,  3.7607,  5.2663,  3.0734,  3.5185,  4.1887,
         5.9228,  2.6535, 79.1150,  3.0951,  5.0306,  3.6509,  5.0114,  5.1427],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 32.2356,   6.8947,  23.8797,  10.1361,  29.3411,  23.8911,  24.5831,
         32.0764,   9.4233,  26.4665,   9.7748,  31.3892,   5.4210,  15.1132,
         19.8306,  10.3981,   7.2839,  14.4099,  10.6425,  10.5910,  10.2591,
          9.5364,  11.1172,  11.3211,  19.1949,  10.6817,  22.8074,   8.0649,
         16.8156,   7.8669,   7.7405,   8.9720,   6.1650,   6.6715,   7.2179,
          7.9636,   9.9884,   9.7589,   8.7924,   5.8444,  10.5844,   4.0083,
          6.6535,   7.8178,   6.1356,   2.9619,   9.8741,   4.6207,   7.5013,
          7.9224,   6.9204,   9.3065,   9.8205,   7.2393,   8.6386,   6.4965,
          6.0332,  10.7768,  14.9439,   2.6046,  11.3965,  10.0486,   8.0684,
          9.5207,  17.8677,   5.8662,   9.1532,   9.3948,  13.3521,   7.5220,
          8.5497,  10.9343,  14.8102,   7.1003, 199.4447,   9.1172,  12.6803,
         10.1990,  12.5356,  12.9336], device='cuda:0')
Outer loop valEpocw Maximum [29/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 41.7
model_train val_loss valEpocw [29/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 79.1
model_train val_loss  valEpocw [29/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 199.4
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [89.31908724 97.2137279  91.79103568 97.02489005 97.26733349 96.59726368
 96.99808726 94.19719545 97.44398826 96.48030604 98.53193796 98.51853657
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.10704061 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.7413165  97.84237521 92.69014754
 96.90915072 96.23420767 96.9627563  93.9108929  98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 92.11997905 96.13796128 96.24273583 96.9067141
 90.26814975 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.25225083 96.16354577 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [87.18704694 97.2137279  90.47038901 97.02489005 97.26733349 96.5997003
 96.99808726 94.72350483 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.09073964
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.445036   96.13796128 96.24273583 96.9067141
 91.88850038 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [97.37578171  4.28962148 61.54991206  6.80596046 21.77550456 23.03231949
 12.27386208 35.26317797  4.79361191 29.47787279  2.3894993   6.5419403
  0.8457129  13.67868456  9.10981694  6.41628892  6.65804869  7.6693624
  0.91434479  1.35496935  7.78486116  0.50581034  2.68595546  7.76602084
 18.65152054 10.98464946 28.0317551   9.2764738   4.3772481   1.33166513
  8.05608892  3.14623475 38.261633    1.3071124   8.15763903 10.00484217
  6.80667848 46.82505165 17.47319853 34.41618467  6.19559752 48.97847431
 25.77654823 30.65215157 21.87120146 35.63059736  2.58026241  2.10520313
  9.52044986  2.49826159  3.29565315  1.26225638  1.36775404 17.60863381
  1.87147672 11.73915071 65.79684375 26.56554682 10.60843396 13.12660246
 59.88303135 17.6162113  21.17763117  8.61234313  5.36193386  9.27678265
  4.36274825 10.21218652  3.42706851  9.75003939  0.58462157 39.66834674
  4.8769399  24.84382276 16.59883241  7.65948617  1.48444337  2.65123165
  0.26550582  0.92848628]
Accuracy th:0.5 is [45.58789488 97.2137279  70.81541404 97.02489005 97.26733349 75.44864219
 75.84215592 75.29148037 76.71933821 96.32679914 76.96909151 98.52097319
 99.41399349 79.38256113 76.59872565 96.56680596 96.29512311 76.35871883
 98.65376884 98.30776915 79.27047672 77.45763331 98.38695922 76.44887367
 81.07601028 96.65086926 94.0778012  76.05048671 98.01293844 76.87650004
 97.30875598 98.57457877 96.36213009 98.02024829 85.2998867  76.50004264
 76.3404442  87.88026462 97.11504489 73.67112974 78.46273803 92.05906361
 75.74834615 75.33168456 96.9627563  93.87434364 98.02877645 98.57336046
 91.96890876 85.58131602 85.87249181 98.55508583 98.99976852 75.92012768
 98.70615611 76.24663442 71.08344197 92.14556353 96.24273583 96.9067141
 89.79300934 97.17717864 92.03226082 76.3270428  98.42838172 76.85822541
 98.20786784 75.45717036 77.35895031 97.41840377 77.87794983 95.99054592
 77.13843642 95.45083515 75.68255747 81.01143992 85.8164496  76.88259159
 77.92790049 99.14718388]
Accuracy th:0.7 is [45.66830326 97.2137279  70.81541404 97.02489005 97.26733349 75.44864219
 75.84215592 75.6021491  76.71933821 96.46690464 76.96909151 98.52097319
 99.41399349 79.90034234 76.59872565 96.56680596 96.29512311 76.35871883
 98.65376884 98.30776915 79.73343405 77.45763331 98.38695922 76.90939438
 81.84841803 96.65086926 94.0778012  76.05048671 98.01293844 76.87650004
 97.30875598 98.57457877 96.36213009 98.02024829 85.54476675 76.50004264
 76.3404442  88.23357415 97.11504489 73.67112974 79.12915291 92.05906361
 75.74834615 75.33168456 96.9627563  93.87434364 98.02877645 98.57336046
 94.20450531 86.56327286 86.08569584 98.55508583 98.99976852 75.92012768
 98.70615611 76.24663442 71.08344197 92.50496461 96.24273583 96.9067141
 89.79300934 97.17717864 92.1979508  76.3270428  98.42838172 76.85822541
 98.20786784 75.45717036 77.35895031 97.55729097 77.87794983 95.99054592
 77.28707009 95.45083515 75.68255747 81.09428491 85.98579452 76.88259159
 77.92790049 99.14718388]
Avg Prec: is [55.94655766  3.10854362 11.27536526  3.43986449  2.20907388  3.77827696
  3.37137057  5.71237601  2.42504916  3.94403621  1.55743846  1.62247857
  0.65076882  5.24344832  2.68583376  3.10298747  3.60627089  2.61122024
  1.36208592  1.69563161  2.10124127  0.83900715  1.79741992  2.42094359
  5.13849351  3.66921105  6.49069455  3.35604081  2.10346518  1.92152618
  2.50041364  1.25311832  3.71442925  1.66425695  2.30677209  2.33599502
  3.03519612  2.5820923   2.8005377   7.37226286  2.26755543  8.36449709
  3.58171337  4.2485827   3.37623571  6.48877538  2.09078685  1.47473275
  2.02301984  1.55821358  1.77957547  1.58966382  1.04879657  3.0058167
  1.29616342  2.78093554 11.37945624  3.60559686  4.08378278  2.79270015
 10.86572982  2.16756326  3.89834755  3.08205253  1.58860808  2.47171693
  1.86714212  4.29256065  1.27379766  2.43773359  0.23293171  3.34899014
  1.93282994  4.46975809  3.92550849  3.0919105   0.88373583  1.86397465
  0.14392148  0.72789922]
mAP score regular 15.08, mAP score EMA 3.78
starting validation
Accuracy th:0.5 is [90.03911603 97.22450607 92.47826195 96.96290206 97.90716795 96.63901139
 96.80843112 94.00054812 97.38894287 96.45215138 98.5250517  98.53003463
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.34187906 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.77723796 97.82744101 93.04880783
 97.07750953 96.48703192 97.03764606 94.08525799 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.21757979 96.39235618 96.16314124 96.78102499
 89.84229016 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.18150335 95.77447243 97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [89.44116401 97.22450607 91.09051499 96.96290206 97.90716795 96.63651992
 96.80843112 94.78536014 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.74484889 97.82744101 92.44836435
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.17522485 96.39235618 96.16314124 96.78102499
 91.98744301 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [97.43772678  4.68270063 65.70368027  6.69878408 20.83742011 22.64746908
 13.33126832 34.85381237  5.44772734 32.37742474  2.42970184  7.67864143
  0.81849155 15.75731182  8.97560166  7.2069901   7.70572621  8.95550368
  0.82631127  1.32748762  8.43359115  0.52408658  3.03881051  7.58948651
 18.7843801  11.74867398 27.08316932  9.19116893  5.18848797  1.38588175
  8.44365123  3.18186999 39.88682468  1.22204618  8.29396779 11.12829027
  6.31582114 51.96529002 18.95253191 35.87682829  7.45542871 48.2711777
 26.20485468 31.26492957 21.06810558 36.122188    2.54283793  1.96765625
 12.49630678  2.40960898  4.22239882  1.4205886   1.79545063 18.0757671
  2.07334151 13.00563239 59.168786   27.95931291  9.22233918 15.16389246
 59.51164882 18.33796349 22.77810212  8.79558884  5.62174996  9.48266827
  3.95234545 10.6903885   2.84877897  9.96010113  0.46903948 40.73542993
  4.02249366 27.35100301 22.16633759  6.75469231  1.50939876  2.64399631
  0.18547785  0.81197461]
Accuracy th:0.5 is [45.70844856 97.22450607 68.84919152 96.96290206 97.90716795 74.11116925
 74.22328525 73.43598176 75.71567382 96.41228791 75.83526422 98.5325261
 99.34972718 77.01621945 75.80785809 96.31262924 96.21047911 75.1999402
 98.78167277 98.34068316 77.574308   76.47557117 98.31327703 75.40922341
 77.44226026 96.52938685 94.3393876  75.32202207 97.81747515 75.80287515
 97.52597354 98.67204823 96.39983058 98.18870369 85.74382739 75.43413808
 75.46652714 90.22597603 97.0276802  72.74086255 76.35099783 92.37362035
 74.56212472 74.27560605 97.03764606 94.02795426 98.18621222 98.77668984
 93.07870543 85.52956125 84.04713855 98.55993223 98.87385704 74.53222712
 98.6969629  75.24976954 69.636495   93.1783641  96.16314124 96.78102499
 90.13379176 97.04761193 91.92515634 75.32700501 98.32075143 76.18656103
 98.13139996 74.53222712 76.5478237  97.49109301 76.89662905 96.07843137
 76.3310661  95.44559882 74.45499165 81.86212223 87.71457757 75.90502529
 76.97137305 99.15040985]
Accuracy th:0.7 is [45.95012084 97.22450607 68.84919152 96.96290206 97.90716795 74.11116925
 74.22328525 73.65273937 75.71567382 96.41976231 75.83526422 98.5325261
 99.34972718 77.47714079 75.80785809 96.31262924 96.21047911 75.1999402
 98.78167277 98.34068316 77.92311334 76.47557117 98.31327703 75.59110048
 77.96795974 96.52938685 94.3393876  75.32202207 97.81747515 75.80287515
 97.52597354 98.67204823 96.39983058 98.18870369 86.05526073 75.43413808
 75.46652714 90.47263124 97.0276802  72.74086255 76.86922291 92.37362035
 74.56212472 74.27560605 97.03764606 94.02795426 98.18621222 98.77668984
 95.04696415 86.0652266  84.21655829 98.55993223 98.87385704 74.53222712
 98.6969629  75.24976954 69.636495   93.52218651 96.16314124 96.78102499
 90.13379176 97.04761193 92.11201634 75.32700501 98.32075143 76.18656103
 98.13139996 74.53222712 76.5478237  97.53095647 76.89662905 96.07843137
 76.3833869  95.44559882 74.45499165 81.92939183 87.93382664 75.90502529
 76.97137305 99.15040985]
Avg Prec: is [54.57194053  3.7604587  14.96897638  4.56356208  1.53113171  4.29309872
 10.89073408  8.76493189  7.22047057  5.15349536  2.26454017  5.29767061
  1.53932459  5.90746357  3.08013717  4.30083729 26.48831335  6.27655911
  1.54855875  2.62023586  3.63465957  1.42806038  1.27206973  5.76292117
  5.77680994 12.34366713  8.21020607  4.46624108  3.89870836  6.89664667
  2.37881579  0.87561131  3.08022131  1.10782054  1.71224445  2.42739356
  2.03199552  2.21563605  2.248539    6.20500553  1.72958454  6.02526751
  2.20687323  2.73891282  2.36182113  4.86775466  1.78516828  1.04822989
  1.3779571   1.17046744  1.21201326  0.99233744  0.73726386  2.36289275
  0.87132309  1.83979376 10.10903391  2.98135634  3.82732086  2.86074664
  7.87436066  2.01713334  3.25579048  2.61431135  1.36702371  1.86769724
  1.5838895   3.44740376  1.07129583  2.19943424  0.19215246  3.13137995
  1.57829175  3.95159335  3.52539334  2.33342359  0.59579122  1.54863021
  0.12759758  0.60999141]
mAP score regular 15.56, mAP score EMA 4.39
Train_data_mAP: current_mAP = 15.08, highest_mAP = 15.08
Val_data_mAP: current_mAP = 15.56, highest_mAP = 15.56
tensor([0.3434, 0.3504, 0.3305, 0.4014, 0.4068, 0.3972, 0.4179, 0.3962, 0.3849,
        0.3995, 0.4019, 0.4191, 0.3945, 0.3777, 0.4093, 0.3504, 0.3897, 0.4325,
        0.4032, 0.4004, 0.4100, 0.3939, 0.4041, 0.4045, 0.3702, 0.3972, 0.3954,
        0.4149, 0.3772, 0.3517, 0.3483, 0.3606, 0.3978, 0.3456, 0.3790, 0.3714,
        0.3666, 0.3610, 0.3575, 0.3714, 0.4009, 0.3934, 0.4093, 0.3968, 0.3997,
        0.4006, 0.4083, 0.3887, 0.4040, 0.4072, 0.4047, 0.3546, 0.3945, 0.3983,
        0.4046, 0.3855, 0.3854, 0.3911, 0.3295, 0.4181, 0.3869, 0.3859, 0.3941,
        0.3881, 0.3951, 0.3881, 0.3909, 0.3960, 0.3918, 0.4093, 0.4109, 0.3831,
        0.4022, 0.3736, 0.4060, 0.3393, 0.3928, 0.3574, 0.4003, 0.3969],
       device='cuda:0')
Max Train Loss:  tensor([14.5392,  3.9736,  6.2212,  8.1227,  5.8277,  7.3786,  8.2848,  8.3093,
         5.5622,  7.3580,  6.3187,  6.7295,  4.7209,  5.2091,  7.8651,  5.7318,
        12.3963,  7.4612,  8.5375,  7.6163,  5.7115,  4.4585,  5.8890,  5.5988,
         8.0478, 12.4867, 10.5215,  9.2013,  7.0778,  4.6359,  3.9340,  6.1978,
         6.6556,  5.1622,  6.3786,  8.0995,  7.7017,  6.7634,  7.3461,  9.9425,
         7.5661, 10.7212,  8.7500, 10.4816,  6.4069, 11.0080,  8.7493,  7.2534,
         4.2717,  7.5724,  4.7690,  5.6720,  5.4498,  8.8813,  6.1185,  8.3623,
        11.3164,  8.8861,  6.1861,  3.9789, 13.2653,  7.2582, 10.9735,  8.0980,
        10.3678,  5.2174,  7.6871, 10.1489,  8.0556,  5.7949,  5.0842,  8.0723,
         8.7829,  9.4100,  6.9470,  8.3392,  5.8754,  6.0782,  5.9484,  6.0892],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [30/80], Step [000/642], LR 1.0e-04, Loss: 14.5
Max Train Loss:  tensor([16.4668,  4.8447, 10.4256, 11.3047, 11.5854,  7.2819,  8.1340, 11.6932,
         6.7804,  6.0816, 11.1234, 10.1844, 16.3420, 11.9028, 12.4747,  7.8605,
         8.9336, 10.0594, 12.1471, 12.7338,  6.6781, 10.9323,  8.7214, 10.7119,
        12.9570,  8.3026,  8.8591,  7.9738,  6.7350,  6.1737, 10.1235,  8.6334,
         9.7307,  9.3648,  6.9854,  6.2803,  5.9649,  8.5538,  7.2554, 12.9534,
         9.8293, 15.0950,  9.2183,  7.3753,  6.7266, 15.3560, 13.8067, 12.3093,
        10.7064, 15.6280, 14.4031,  4.0945, 13.5239,  8.2949, 11.7405,  6.6359,
        12.8086,  7.1394,  3.7225, 13.6599,  9.4164,  6.5782,  9.0888,  7.5481,
         5.7708, 11.2340,  8.2507,  8.2756,  7.6122, 12.3680, 14.3233,  7.0441,
         8.6065, 10.4768, 12.2914,  6.2204, 10.0096,  5.6138,  7.3431,  5.1177],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [30/80], Step [100/642], LR 1.0e-04, Loss: 16.5
Max Train Loss:  tensor([12.7454,  3.9461, 10.2936,  8.5504,  5.1004, 10.1910,  9.4230, 13.4574,
         8.8660,  9.4760,  7.2627,  6.4501,  6.3580, 13.4299, 10.6919,  6.8250,
        12.1264,  6.1859, 12.4546,  9.4283,  6.8808,  7.1427,  4.5352,  6.3978,
        12.2097,  9.5674, 11.0020,  7.1630,  6.2639,  5.5964,  8.2599,  6.1467,
         7.0315,  6.0450,  7.3183,  6.2357,  3.9660,  7.4397,  8.4458, 10.9043,
        10.1508,  9.1078,  6.7205,  7.7662,  5.9216, 11.7860,  8.3662,  6.9598,
         8.0869,  6.5489,  2.6322,  4.1975,  4.6066,  7.2122,  7.0210,  4.1856,
        13.3032,  8.2577,  5.5393,  6.1706, 13.5044,  9.3074,  8.6767,  5.6991,
         3.9221,  8.7067,  4.9473,  8.0081,  6.4615,  9.4731,  6.0164,  8.1301,
         8.6147,  6.8658,  7.9310,  7.4299, 10.0040,  5.0865,  6.8765,  7.6587],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [30/80], Step [200/642], LR 1.0e-04, Loss: 13.5
Max Train Loss:  tensor([11.5709,  7.7427,  8.6817,  8.6628,  4.9843,  9.7591,  7.5168,  8.8224,
         7.0429,  9.6688,  7.9239,  8.3553,  6.2364,  5.0757,  6.3951,  6.8018,
         6.0322, 10.0376,  6.0197,  5.9432,  5.8133,  6.9381,  4.3519,  7.9857,
         7.1471,  9.4073, 10.7565, 12.3887,  7.0548,  4.9176,  7.7742,  4.6706,
         6.6467,  3.6754,  4.9252,  6.2055,  4.8521,  6.8983,  7.1627,  9.4786,
         3.6981,  9.2514,  7.3015,  8.0211,  8.7033, 12.6538,  8.2760,  8.4219,
         8.1057,  7.3301,  9.3736,  7.9115,  5.5395,  8.6098,  6.2103, 10.6863,
        12.6335,  6.5751,  8.0945,  8.7590,  9.5287,  5.9706,  7.0789,  6.7436,
         7.3828,  7.5990,  9.6063,  9.4550,  5.7145,  8.7599,  5.1916,  6.5471,
         4.0969,  9.2048,  6.6052,  6.5125,  7.9437,  5.9084,  7.4505,  5.4606],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [30/80], Step [300/642], LR 1.0e-04, Loss: 12.7
Max Train Loss:  tensor([12.0240,  4.8114,  9.8829,  8.7686,  5.3360, 12.0155,  8.5449, 11.1304,
         6.9035,  5.8812,  6.6345,  6.1085,  6.5235,  8.0455,  6.5178,  8.8414,
         6.1482,  8.1160,  5.4676,  6.1168,  8.9937,  6.1742,  5.5140,  8.3767,
         9.1987,  7.1204, 10.0774,  9.2761,  7.9293,  4.0442,  7.5457,  5.4409,
         6.5401,  3.7758,  8.3065,  9.9567,  5.6657,  7.8590,  3.7343, 13.6271,
         7.8513, 13.4838,  7.7009,  9.3069,  9.1174, 13.2122,  5.3562,  7.5631,
        10.0380,  6.3687,  6.6333,  5.0461,  6.6199,  5.8385,  7.1960,  7.6088,
        12.8626,  4.3847,  4.8568,  5.5530, 10.0066,  7.7148,  9.0868,  7.3005,
         6.6708,  5.2006,  6.5712,  9.3928,  8.0887, 10.0363,  5.2937, 10.6025,
         9.1098,  8.0646,  9.3015,  7.3074,  8.0265,  4.0631,  6.8972,  4.7475],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [30/80], Step [400/642], LR 1.0e-04, Loss: 13.6
Max Train Loss:  tensor([12.6707,  6.3066, 10.4938,  7.9880,  6.4721,  8.4889,  7.7341,  9.8725,
         5.4368,  8.7769,  7.4027,  7.1780,  6.8654,  9.6396,  6.3478,  6.6308,
         8.6746, 10.8041,  5.7637,  5.7700,  5.3743,  3.6811,  8.9686,  4.5078,
         9.4955,  6.8967,  7.2609,  6.3352,  5.6050,  3.8239,  7.8149,  4.6397,
         5.3072,  6.2660,  4.9707,  5.8858,  7.6861,  8.6766,  5.3133,  8.5213,
         2.5685, 10.5028,  5.5653,  8.3563,  6.3501,  9.0494,  6.4349,  8.1466,
         5.0975,  9.4954,  4.0497,  4.3621,  7.8673,  4.3702,  8.8146,  4.8685,
        10.3895,  9.9253,  4.3792,  6.3762,  9.4923,  7.2658,  7.7635,  3.4839,
         3.7688,  5.8847,  4.7831,  7.3603,  6.3448,  5.4529,  5.0856,  6.6992,
         6.9395,  7.9170,  8.0331,  7.4554,  7.8211,  4.0511,  6.6877,  4.5468],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [30/80], Step [500/642], LR 1.0e-04, Loss: 12.7
Max Train Loss:  tensor([12.9573,  6.8837,  8.8621,  5.1432,  6.3457,  7.8362,  8.9998,  7.4732,
         6.1100,  5.9268,  7.3171,  6.0823,  6.2601,  4.8492, 11.0591,  7.5481,
         6.0687,  7.4755,  7.6189,  7.7635,  5.4627,  4.5463,  7.4706,  5.1465,
         7.8596,  8.3437,  7.0726,  7.9958,  9.5357,  5.4240,  7.4137,  5.0450,
         6.9158,  7.9060,  3.9082,  6.0908,  6.9132,  9.1307,  6.8452,  8.3573,
         5.6356,  8.5961,  5.9682,  7.5436,  6.2282,  7.4566,  6.5839,  6.9540,
         4.7900,  7.4373,  4.8129,  4.5197,  4.5446,  6.8241,  8.5796,  6.4591,
        10.5045,  5.0962,  5.9023,  8.1700,  8.4407,  6.0194,  7.4066,  7.3194,
         5.5932,  5.6621,  6.4327, 10.4669,  8.0039,  4.8800,  5.9752,  5.9432,
         6.9302,  9.0269,  4.1866,  3.7455,  7.9489,  6.1869,  7.4983,  4.6613],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [30/80], Step [600/642], LR 1.0e-04, Loss: 13.0
Max_Val Meta Model:  tensor([ 20.2569,  23.6150,  28.2048,  19.3034,   4.6887,   7.6857,   6.7940,
          8.3123,   4.4270,   6.4796,   4.6959,   6.9869,   6.2253,   7.8955,
          6.0780,   4.1259, 131.2788,   4.5552,   3.7644,   5.0688,   3.6426,
          3.8230,   4.4845,   4.8820,  10.6932,   7.8456,  10.9359,   3.5791,
          6.7444,   5.1905,   5.7039,   3.7776,   7.7422,   2.8191,   4.0184,
          4.1152,   4.9613,   6.6553,   4.8342,  16.6051,   5.8651,   8.9899,
          5.7658,   8.6757,   7.3119,  10.0416,   3.7020,   7.1335,   3.9328,
          6.4698,   3.5860,   3.2489,   6.3381,   4.0177,   6.3691,   4.4306,
         11.7025,   6.3202,   7.0265,   4.6529,   6.1737,  10.4396,   5.3197,
          4.4501,   3.9833,   3.7238,   5.0146,   5.3698,   8.1368,  10.1614,
          5.3403,   8.2765,   9.7649,   5.5221,   7.6075,   5.1698,   8.1049,
          4.1410,   6.9646,   4.7893], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 14.0338,  22.5588,  20.6932,  19.2868,   4.8846,  10.2800,   8.3372,
          9.6518,   4.6721,   7.7681,   5.4142,   7.5151,   6.5805,   7.7844,
          6.2433,   4.3112, 129.2210,   5.2825,   3.9593,   5.3307,   3.9312,
          3.9660,   4.5822,   4.7530,  10.5015,   7.7403,  11.6822,   4.1072,
          6.9841,   5.1421,   5.8812,   4.0365,   8.3627,   3.0229,   4.2874,
          4.5210,   6.0699,   6.4186,   5.3528,  16.0465,   5.7762,   8.8531,
          5.6061,   8.3766,   7.3359,   8.7651,   3.6227,   7.4419,   3.8641,
          6.8916,   3.4256,   3.4773,   6.3833,   3.8338,   6.6739,   4.4158,
         10.4473,   6.2855,   7.2338,   4.6426,   5.4413,   9.9348,   5.3192,
          4.6950,   4.2373,   3.9495,   5.3157,   5.9724,   8.3279,   9.9971,
          5.6373,   8.4808,  10.0612,   5.7729,   7.5359,   5.3217,   8.4469,
          4.3905,   7.3173,   5.0750], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 40.8661,  64.3759,  62.6147,  48.0473,  12.0067,  25.8806,  19.9515,
         24.3619,  12.1370,  19.4455,  13.4728,  17.9312,  16.6793,  20.6112,
         15.2522,  12.3030, 331.6248,  12.2151,   9.8188,  13.3134,   9.5892,
         10.0695,  11.3402,  11.7495,  28.3648,  19.4880,  29.5480,   9.8996,
         18.5157,  14.6213,  16.8837,  11.1926,  21.0234,   8.7467,  11.3113,
         12.1729,  16.5581,  17.7784,  14.9721,  43.2095,  14.4070,  22.5026,
         13.6972,  21.1121,  18.3551,  21.8797,   8.8718,  19.1437,   9.5639,
         16.9227,   8.4655,   9.8058,  16.1817,   9.6263,  16.4935,  11.4561,
         27.1101,  16.0715,  21.9551,  11.1041,  14.0648,  25.7456,  13.4970,
         12.0974,  10.7240,  10.1753,  13.5996,  15.0837,  21.2539,  24.4252,
         13.7184,  22.1382,  25.0172,  15.4541,  18.5593,  15.6833,  21.5058,
         12.2839,  18.2810,  12.7852], device='cuda:0')
Outer loop valEpocw Maximum [30/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 131.3
model_train val_loss valEpocw [30/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 129.2
model_train val_loss  valEpocw [30/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 331.6
Max_Val Meta Model:  tensor([30.4588, 28.9913, 32.6831, 34.6750, 32.7206, 35.9397, 43.0559, 33.0498,
        32.0785, 35.5752, 34.2343, 40.1041, 33.6705, 37.1772, 33.7854, 29.7977,
        33.7512, 34.4947, 37.5185, 37.5707, 34.6612, 33.4432, 33.6484, 34.4592,
        31.6783, 33.0271, 33.5863, 33.0722, 30.2906, 29.6547, 30.1002, 31.0779,
        33.8613, 28.9767, 31.8532, 31.2650, 31.3292, 30.8419, 30.3273, 33.5161,
        31.7295, 31.0206, 34.2834, 33.9538, 32.9993, 33.5088, 33.5737, 32.6369,
        33.9620, 34.2713, 34.1550, 30.3092, 33.4137, 33.4758, 33.8089, 32.3193,
        32.6261, 33.3881, 33.1483, 34.7013, 32.8057, 32.9905, 32.8314, 33.2032,
        32.9018, 32.8009, 33.0384, 32.2279, 33.0816, 34.8724, 33.9554, 31.8669,
        32.8189, 31.8055, 33.2571, 28.6189, 32.9953, 30.1957, 34.0363, 33.7962],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([20.6010,  5.9729, 14.6939,  5.9566,  7.3878, 18.3211, 33.2774, 16.6217,
        13.1620, 15.2248,  7.5820, 80.4054,  6.3274,  5.0050,  7.4789,  3.5356,
         5.7153,  8.3206,  3.0728,  8.9793,  3.3483,  3.1710,  3.7717,  5.6293,
         9.2233,  7.9068, 11.4530,  6.6755,  6.0797,  2.3328,  4.1614,  2.9339,
         3.5558,  2.1421,  3.0975,  2.9029,  5.2226,  4.1988,  3.1652,  2.1026,
         1.8126,  1.5199,  2.4134,  2.3855,  2.5610,  2.1054,  2.2948,  5.0938,
         2.5011,  5.2567,  2.1390,  2.4656,  3.5975,  2.3650,  5.0588,  3.5327,
         4.2077,  3.3795,  3.3887,  2.7150,  1.4157,  2.7538,  2.2155,  2.3893,
         3.0338,  2.3836,  3.9401,  5.7060,  4.6502,  2.7491,  4.1080,  2.6405,
         4.3931,  2.5741,  7.9779,  2.1615,  6.6625,  2.5811,  5.5854,  3.6896],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 60.7259,  17.1711,  43.4829,  14.7057,  18.3516,  45.7117,  78.3239,
         42.1993,  34.1256,  38.2849,  18.8154, 192.0565,  16.0615,  12.6780,
         18.5218,  10.0802,  14.6136,  19.4749,   7.4567,  21.7811,   8.1715,
          8.0298,   9.3484,  13.8819,  24.8737,  20.0386,  28.9351,  16.2211,
         16.6293,   6.5777,  11.8294,   8.0438,   8.9113,   6.1100,   8.2033,
          7.8234,  14.1610,  11.5714,   8.8269,   5.6038,   4.6349,   3.9227,
          5.8895,   5.9695,   6.4031,   5.1747,   5.5998,  13.1285,   6.1840,
         12.7933,   5.2540,   6.9137,   9.0317,   5.9255,  12.4591,   9.0929,
         10.9246,   8.6206,  10.0255,   6.5043,   3.6650,   7.1053,   5.6983,
          6.0610,   7.6458,   6.0992,  10.0396,  14.6684,  11.9059,   6.6592,
         10.1421,   6.9397,  11.0774,   6.8399,  19.7079,   6.3415,  16.9549,
          7.2150,  14.0430,   9.2990], device='cuda:0')
Outer loop valEpocw Maximum [30/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 43.1
model_train val_loss valEpocw [30/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 80.4
model_train val_loss  valEpocw [30/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 192.1
Max_Val Meta Model:  tensor([29.1174, 28.8623, 30.8790, 34.5810, 32.3924, 34.9326, 33.1515, 32.2097,
        32.4117, 32.7586, 33.8426, 34.2401, 33.3107, 33.8033, 32.5110, 30.0064,
        34.0571, 31.8624, 33.0593, 33.7111, 34.1523, 34.1095, 33.6387, 35.2384,
        31.0685, 32.4642, 31.3725, 33.2113, 29.8896, 29.4341, 29.9024, 30.9811,
        34.7928, 28.9247, 31.5195, 30.8662, 31.0354, 30.6812, 30.1057, 34.0417,
        31.7240, 33.1173, 35.3944, 34.8341, 33.8705, 35.0464, 35.2503, 32.4272,
        35.5351, 35.1245, 35.3227, 30.4611, 35.5100, 33.4189, 34.4081, 33.5627,
        34.3388, 33.2292, 31.2576, 34.3403, 34.1049, 33.3427, 32.8815, 32.1563,
        32.8398, 32.6843, 32.8553, 32.0088, 32.8304, 34.7780, 33.3089, 32.0170,
        32.8890, 31.9993, 33.2043, 28.4467, 32.8949, 29.8667, 33.7195, 33.6103],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 4.6516,  3.3215,  2.6361,  3.4819,  2.0040,  2.9081,  2.1290,  5.7685,
         2.4429,  2.1566,  2.7602,  5.5250,  5.6407,  6.2066,  2.8709,  3.8317,
         2.5109,  1.9839,  3.3265,  4.8747,  2.6429,  3.1123,  3.5379,  3.0469,
         3.2268,  2.9054,  9.7329,  6.6458,  5.1551,  3.2779,  4.0677,  4.8916,
         5.6630,  3.0447,  4.1360,  3.8231,  5.7553,  5.9300,  4.6764, 20.4292,
        12.2366, 26.1041, 31.0061, 28.1063, 22.0954, 30.4575,  6.6407,  8.8381,
        17.2490,  7.7018, 14.0511, 16.3615, 23.0380, 10.7449, 25.1188, 36.8448,
        36.8181,  8.0460,  5.2015,  4.9638, 40.0628,  4.3962,  8.9909,  7.1693,
         6.9382,  5.0073,  7.0000,  9.0804,  6.8139,  5.3341,  5.4420,  6.9378,
         5.2039, 13.7091,  3.1662,  5.8420,  9.6767,  3.5861,  7.1236,  5.0017],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 13.8832,   9.5050,   7.7059,   8.6136,   4.9697,   7.2374,   5.1772,
         14.6365,   6.2914,   5.4367,   6.8505,  13.2284,  14.3871,  15.9286,
          7.1650,  10.9347,   6.4039,   4.7886,   8.3283,  12.1104,   6.5276,
          7.8251,   8.8423,   7.4318,   8.7355,   7.3846,  25.6340,  16.2400,
         14.1510,   9.2534,  11.5266,  13.3837,  14.1351,   8.6440,  11.0060,
         10.3485,  15.6047,  16.3214,  13.0412,  54.4614,  31.5954,  67.2523,
         75.5891,  69.8939,  55.2493,  74.7157,  15.9520,  22.8694,  42.4964,
         18.4004,  34.1011,  45.8459,  57.4808,  26.9947,  61.8780,  94.7728,
         96.0816,  20.5954,  15.3358,  11.9214, 107.2587,  11.2939,  23.1690,
         18.4307,  17.4701,  12.7896,  17.8673,  23.3408,  17.5075,  12.8871,
         13.6275,  18.1557,  13.1021,  36.3654,   7.7690,  17.1711,  24.6575,
         10.0734,  18.0052,  12.6077], device='cuda:0')
Outer loop valEpocw Maximum [30/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 35.5
model_train val_loss valEpocw [30/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 40.1
model_train val_loss  valEpocw [30/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 107.3
Max_Val Meta Model:  tensor([31.1564, 28.7329, 29.7921, 33.1166, 32.1957, 32.6313, 33.6917, 32.1281,
        31.6849, 33.7670, 33.8753, 34.0158, 33.6375, 32.1771, 32.6255, 30.1162,
        33.9011, 32.3217, 32.9547, 33.3851, 32.6587, 32.4800, 32.5102, 33.1651,
        31.5063, 32.2628, 31.3240, 33.1636, 30.3268, 29.3635, 29.5198, 30.6544,
        32.5554, 28.5244, 31.9854, 31.1448, 30.9042, 30.4966, 30.0897, 32.5544,
        31.5309, 32.3563, 33.0626, 32.5870, 32.4985, 31.3649, 33.4917, 32.7703,
        32.9144, 32.5424, 33.9013, 30.0982, 32.1191, 33.2181, 33.4139, 31.6691,
        31.9479, 33.2773, 30.5314, 32.9586, 31.8030, 33.2791, 32.7355, 31.3426,
        34.3199, 33.5573, 32.9888, 34.8285, 33.1341, 32.1011, 32.4845, 31.8691,
        33.1519, 31.6658, 38.1490, 28.4428, 32.8843, 30.1863, 32.0394, 32.7916],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 9.4763,  2.4031,  8.0970,  3.6712,  8.7132,  9.6056, 11.1239,  9.0184,
         3.0193,  9.4234,  4.7162,  5.1208,  4.5695,  5.2902,  8.3944,  2.6513,
         2.8506,  4.0766,  3.1978,  4.2510,  3.4528,  3.4083,  4.4831,  6.0301,
         7.1045,  2.8161,  8.4945,  1.7289,  6.1604,  2.3714,  3.4217,  2.9727,
         2.1685,  2.1679,  3.1948,  2.4317,  3.4823,  3.8297,  2.3900,  2.5707,
         1.8192,  1.6666,  2.7431,  2.9638,  2.8651,  2.2815,  2.6355,  5.2544,
         2.7578,  4.4967,  2.4833,  2.5129,  3.6314,  2.5457,  5.1877,  2.8876,
         2.6442,  2.8952,  4.6204,  3.5768,  2.8242,  4.2803,  3.3487,  2.5613,
         3.1536,  2.2813,  3.9977,  3.1551,  4.8068,  2.2636,  4.1728,  4.2479,
         3.3716,  3.1101, 72.0884,  3.0369,  6.7860,  3.4634,  5.5693,  3.7598],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 27.3594,   6.9270,  24.3699,   9.1983,  21.7701,  24.4724,  26.8570,
         23.0003,   7.8943,  23.6081,  11.7226,  12.3612,  11.5705,  14.0058,
         21.0078,   7.5527,   7.2793,   9.6859,   8.0397,  10.6903,   8.6187,
          8.7269,  11.2551,  15.1163,  19.1289,   7.2121,  22.4663,   4.2003,
         16.7597,   6.7280,   9.8379,   8.2367,   5.5371,   6.2574,   8.3989,
          6.5431,   9.4825,  10.6097,   6.6890,   6.9373,   4.6445,   4.2694,
          6.8530,   7.6098,   7.2273,   5.8277,   6.4912,  13.4684,   6.9114,
         11.2706,   6.1444,   7.0702,   9.3454,   6.4341,  12.8549,   7.5346,
          7.0227,   7.4403,  13.7807,   8.6598,   7.4636,  11.0582,   8.6414,
          6.7358,   7.8634,   5.8072,  10.1749,   8.0156,  12.2616,   5.7597,
         10.3953,  11.2010,   8.5082,   8.3286, 180.9318,   8.9387,  17.3138,
          9.6574,  14.2958,   9.5785], device='cuda:0')
Outer loop valEpocw Maximum [30/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 38.1
model_train val_loss valEpocw [30/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 72.1
model_train val_loss  valEpocw [30/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 180.9
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [89.41777025 97.2137279  91.92261303 97.02489005 97.26733349 96.17085562
 96.88478454 94.68208233 97.44398826 96.49858067 98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.16673773 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.43644692 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72426018 97.84237521 92.70842217
 96.90915072 96.22689782 96.9627563  93.16406964 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.9603806  96.13796128 96.24273583 96.9067141
 92.06271853 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.24517245 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [87.10176533 97.2137279  90.54957907 97.02489005 97.26733349 96.5997003
 96.99808726 94.73690623 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.41086244 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.76446437 97.84237521 92.16018323
 96.90915072 96.22689782 96.9627563  93.92673091 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.74961319 96.13796128 96.24273583 96.9067141
 90.59343819 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.1537993  96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [97.34221917  5.07079195 62.18056732  9.58753452 17.60742032 23.08792364
 11.15609699 31.55729923  5.4456278  29.71691409  4.21631239  1.15471542
  0.73703449 15.40891631  6.49172119  5.68533981  8.20646211  8.08425297
  1.18386539  1.62252037  2.61104438  1.00751014  1.48692878 16.02067273
 19.15788088 13.47184482 31.34147037 12.680867    5.09514728  1.32768531
 31.79451937  0.86927612 39.36275583  1.46883401  1.51837329  4.79194655
  9.62186189 47.12398311 21.57627318 34.85093873  6.74526584 47.06448019
 31.00590777 29.63176864 19.40504461 34.59006845  3.64479793  1.87002969
  5.80798358  1.82377461  3.32366502  1.2496185   1.17373572 12.4245225
  1.52058203  7.43017544 65.65654132 24.99737369 16.42368206 11.50792063
 65.96626705 13.65928129 25.79364321 11.33852813  4.17734154  7.68266331
  3.97946006 11.80108562  2.60093927 13.41367397  0.39909066 38.05121249
  4.01048368 25.20051023 25.69336981  6.67698498  1.15689136  2.60870009
  0.19454367  0.82335648]
Accuracy th:0.5 is [45.71825392 97.2137279  70.56200582 97.02489005 97.26733349 75.20010721
 75.50590271 75.25980434 76.43425397 96.34385546 76.74735932 98.52097319
 99.41399349 79.45809627 76.30145832 96.56680596 96.29512311 76.08094443
 98.65376884 98.30776915 79.24367393 77.18473215 98.38695922 76.23323303
 81.15641866 96.65086926 94.0778012  75.79464188 98.01293844 76.63040168
 97.30875598 98.57457877 96.36213009 98.02024829 85.23775295 76.36359206
 76.03099377 87.79132808 97.11504489 73.43721446 78.25440723 92.05906361
 75.48031822 75.06609325 96.9627563  93.87434364 98.02877645 98.57336046
 92.25399301 85.42902742 85.81279468 98.55508583 98.99976852 75.6715927
 98.70615611 76.00053606 70.92749845 91.88484546 96.24273583 96.9067141
 89.79300934 97.17717864 91.97256369 76.16866266 98.42838172 76.50735249
 98.20786784 75.15746641 77.11285194 97.42084039 77.61966838 95.99054592
 76.91670423 95.45083515 75.48762807 80.80920067 85.71167505 76.60725381
 77.68911197 99.14718388]
Accuracy th:0.7 is [45.69510605 97.2137279  70.56200582 97.02489005 97.26733349 75.20010721
 75.50590271 75.55707167 76.43425397 96.46568633 76.74735932 98.52097319
 99.41399349 79.97465918 76.30145832 96.56680596 96.29512311 76.08094443
 98.65376884 98.30776915 79.69079324 77.18473215 98.38695922 76.76076071
 81.97390383 96.65086926 94.0778012  75.79464188 98.01293844 76.63040168
 97.30875598 98.57457877 96.36213009 98.02024829 85.47532316 76.36359206
 76.03099377 88.20311643 97.11504489 73.43721446 78.96468123 92.05906361
 75.48031822 75.06609325 96.9627563  93.87434364 98.02877645 98.57336046
 94.4408572  86.50235743 85.98457621 98.55508583 98.99976852 75.6715927
 98.70615611 76.00053606 70.92749845 92.30516197 96.24273583 96.9067141
 89.79300934 97.17717864 92.15652831 76.16988097 98.42838172 76.50735249
 98.20786784 75.15746641 77.11285194 97.55729097 77.61966838 95.99054592
 77.04462665 95.45083515 75.48762807 80.90057382 85.89685798 76.60725381
 77.68911197 99.14718388]
Avg Prec: is [55.85277033  3.12335958 11.17040898  3.32895835  2.25984244  3.76808065
  3.37651623  5.58138924  2.58576379  3.85373735  1.52231453  1.60326813
  0.64294079  5.10728461  2.8005436   3.21726047  3.73339222  2.66333854
  1.4015072   1.66035152  1.89916533  0.9146513   1.78292038  2.41834571
  5.22907512  3.73047939  6.6134965   3.3446367   2.0265623   1.91311356
  2.62166752  1.3625145   3.61088213  1.5632138   2.35745938  2.32879748
  3.05685788  2.60799983  2.66884005  7.37686418  2.26405084  8.20158896
  3.34581521  4.02429182  3.1729421   6.4400777   2.02901269  1.59176347
  2.08605543  1.72086002  1.82131486  1.65437634  1.02261252  3.07412027
  1.34333616  2.695824   11.11599521  3.72555243  4.00159372  2.83845595
 10.7071524   2.14254751  3.81161034  2.85472195  1.53454755  2.53477715
  1.69728986  4.13971591  1.25233897  2.41950935  0.21172364  3.3487389
  1.90290461  4.55154149  3.86103194  3.13385099  0.84085975  1.9278063
  0.12170572  0.7562629 ]
mAP score regular 15.44, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [89.83232429 97.22450607 92.60532676 96.96290206 97.90716795 95.90153723
 96.47706605 94.70812467 97.38894287 96.50198072 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.45399507 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.43720258 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.57792062 97.82744101 92.94416623
 97.07750953 96.48703192 97.03764606 92.90679423 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.97341605 96.39235618 96.16314124 96.78102499
 92.35618008 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.9638239  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [88.8880584  97.22450607 91.27239206 96.96290206 97.90716795 96.62904552
 96.80843112 94.88003588 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.42474525 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.88187956 97.82744101 92.51812542
 97.07750953 96.48703192 97.03764606 94.01798839 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.36208486 96.39235618 96.16314124 96.78102499
 91.10297232 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.75703216 97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [97.33230816  6.35116858 66.81382249 10.68655382 16.10157527 22.72051215
 11.68198484 32.16568649  6.70074177 34.08394838  5.28396993  1.18282073
  0.68783008 16.78417637  6.97559033  6.32434247  9.37974866  9.05889741
  1.08245862  1.6754255   2.45991413  1.08232268  1.40193592 15.89470787
 19.4922649  14.30825891 31.01241027 12.93120613  6.0188855   1.35517406
 36.69136452  0.76795245 41.92416423  1.38439421  1.2959066   4.40976375
  9.34552019 50.77347229 25.95795844 36.47005214  7.01160232 46.35954146
 31.69343518 29.88213651 18.30526702 33.96083591  3.4458251   1.64234924
  6.57566317  1.60205068  3.83438516  1.37046052  1.34150574 15.07143282
  1.46633499  7.66413272 59.63444959 25.80657617 15.05542368 12.47647053
 65.5181511  14.21475981 28.26642605 11.6048722   4.02375098  7.34273093
  3.70621363 12.02325495  2.09970879 14.47134729  0.34193607 40.0711511
  3.33464113 26.97210133 33.31428023  5.9146392   1.12750019  2.72835618
  0.17293931  0.75992194]
Accuracy th:0.5 is [45.7209059  97.22450607 68.71465232 96.96290206 97.90716795 73.96168124
 74.07379724 73.32884869 75.56120288 96.40979645 75.68079328 98.5325261
 99.34972718 76.98632185 75.65837008 96.31262924 96.21047911 75.04546927
 98.78167277 98.34068316 77.46966639 76.3161173  98.31327703 75.29959887
 77.38744799 96.52938685 94.3393876  75.1625682  97.81747515 75.64840422
 97.52597354 98.67204823 96.39983058 98.18870369 85.68901512 75.28465007
 75.31703914 90.2060443  97.0276802  72.60632334 76.2563221  92.37362035
 74.40267085 74.12113511 97.03764606 94.02795426 98.18621222 98.77668984
 93.37768144 85.46478312 84.01724095 98.55993223 98.87385704 74.37775619
 98.6969629  75.10028154 69.52687047 93.14348357 96.16314124 96.78102499
 90.13379176 97.04761193 92.01484914 75.18249994 98.32075143 76.04703889
 98.13139996 74.40267085 76.39335277 97.48860154 76.73717518 96.07843137
 76.18656103 95.44559882 74.30550365 81.77990383 87.7046117  75.76052022
 76.81191918 99.15040985]
Accuracy th:0.7 is [45.95510377 97.22450607 68.71465232 96.96290206 97.90716795 73.96168124
 74.07379724 73.5381319  75.56120288 96.41976231 75.68079328 98.5325261
 99.34972718 77.46468346 75.65837008 96.31262924 96.21047911 75.04546927
 98.78167277 98.34068316 77.82594613 76.3161173  98.31327703 75.49891621
 77.98041707 96.52938685 94.3393876  75.1625682  97.81747515 75.64840422
 97.52597354 98.67204823 96.39983058 98.18870369 86.0129058  75.28465007
 75.31703914 90.4377507  97.0276802  72.60632334 76.76458131 92.37362035
 74.40267085 74.12113511 97.03764606 94.02795426 98.18621222 98.77668984
 95.33099135 86.00293993 84.18416922 98.55993223 98.87385704 74.37775619
 98.6969629  75.10028154 69.52687047 93.47484864 96.16314124 96.78102499
 90.13379176 97.04761193 92.17430301 75.18249994 98.32075143 76.04703889
 98.13139996 74.40267085 76.39335277 97.53095647 76.73717518 96.07843137
 76.2413733  95.44559882 74.30550365 81.8571393  87.92635224 75.76052022
 76.81191918 99.15040985]
Avg Prec: is [54.58437047  3.76149438 15.00456304  4.56766371  1.53114506  4.29332595
 10.64254227  8.7984714   7.16737447  5.15643529  2.26211325  5.31428367
  1.55776278  5.90445049  3.06473635  4.2468681  26.19894608  6.27419557
  1.55246448  2.68457012  3.65281316  1.43371102  1.30048706  5.57160414
  5.77779005 12.64798278  8.22699028  4.46991403  3.89743001  6.76969535
  2.3276144   0.88393388  3.07183779  1.10852177  1.71188325  2.41735684
  2.03549788  2.21019467  2.24698982  6.23822577  1.73419363  6.02176691
  2.20940916  2.77198657  2.36450564  4.88833397  1.81462376  1.06507239
  1.37924898  1.17879363  1.20232609  0.99475172  0.73727018  2.35202234
  0.95114079  1.89234504 10.13919641  2.95933852  3.809842    2.81883998
  7.89684716  2.01501347  3.2173664   2.63551505  1.36661976  1.8626728
  1.57961935  3.44480147  1.06730535  2.19651903  0.19277013  3.12740847
  1.5787218   3.92888123  3.52483748  2.3243928   0.59875872  1.54231967
  0.12849423  0.5920701 ]
mAP score regular 16.00, mAP score EMA 4.38
Train_data_mAP: current_mAP = 15.44, highest_mAP = 15.44
Val_data_mAP: current_mAP = 16.00, highest_mAP = 16.00
tensor([0.3449, 0.3473, 0.3344, 0.3976, 0.4002, 0.3935, 0.4141, 0.3925, 0.3839,
        0.3983, 0.4023, 0.4146, 0.3950, 0.3786, 0.4001, 0.3511, 0.3914, 0.4205,
        0.3978, 0.3981, 0.3989, 0.3909, 0.3966, 0.3992, 0.3704, 0.3919, 0.3789,
        0.4092, 0.3674, 0.3521, 0.3479, 0.3613, 0.3904, 0.3465, 0.3795, 0.3714,
        0.3674, 0.3610, 0.3582, 0.3702, 0.3911, 0.3913, 0.4003, 0.3880, 0.3964,
        0.3885, 0.4062, 0.3898, 0.3985, 0.3999, 0.4045, 0.3560, 0.3887, 0.3962,
        0.4033, 0.3839, 0.3777, 0.3880, 0.3358, 0.4130, 0.3775, 0.3876, 0.3869,
        0.3809, 0.4023, 0.3932, 0.3920, 0.3912, 0.3912, 0.3932, 0.3987, 0.3798,
        0.3993, 0.3732, 0.4050, 0.3393, 0.3936, 0.3578, 0.3874, 0.3928],
       device='cuda:0')
Max Train Loss:  tensor([11.1569,  4.8590,  7.0964,  7.5067,  7.8847,  8.7185,  8.3091,  6.4359,
         6.3558,  7.8809,  6.9943,  6.5109,  6.3357, 12.1575,  9.3803,  8.4117,
         9.1742,  6.8248,  4.7811,  5.9495,  5.8328,  5.5422,  6.3346,  7.6001,
         8.9505,  7.1192,  5.5803,  8.0745,  5.5639,  3.9466,  7.2697,  3.7842,
         7.7387,  4.7144,  7.2472,  5.6129,  6.1396,  7.4484,  5.9832,  9.7460,
         5.6678,  8.0908,  5.4291,  7.3746,  5.7301,  9.5399,  4.5532,  7.3008,
         4.6525,  7.3670,  8.5751,  6.3790,  4.5899,  6.6167,  7.1345,  9.1547,
         8.6357, 10.1754,  6.6867,  7.0823,  9.7660,  8.1684,  7.5511,  5.1531,
         4.0523,  6.7810,  5.0205, 10.4212,  7.2011,  4.7688,  5.8980,  8.5776,
         7.5004,  6.4611,  5.2011,  7.2404,  8.6750,  5.4026,  7.4001,  4.7324],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [31/80], Step [000/642], LR 1.0e-04, Loss: 12.2
Max Train Loss:  tensor([10.9559,  5.5567, 11.3795,  7.6108,  7.7760,  6.4239, 12.8928, 10.9783,
         6.1900,  9.8061, 15.6342, 11.8972,  9.8582,  8.5908,  9.3654,  6.9275,
        10.5141, 12.7503, 11.1823, 12.4004,  5.9859,  6.0962,  9.0467,  9.0503,
        12.2637,  7.9995,  7.4618, 15.0436,  6.5169,  6.9363,  5.8548,  5.6273,
        13.1539,  6.7709,  5.7854,  7.2100,  7.8265,  8.2531,  7.6460, 12.9730,
        10.2556, 14.3072,  4.9661,  9.0291,  6.3951, 13.3777, 13.8259,  7.6041,
        11.2960,  7.3649,  9.9907,  4.9940,  8.5877,  6.5386,  7.7752,  9.4744,
        10.8102, 12.7898,  5.9131,  9.0482, 11.9416,  9.5343, 12.1765,  8.5517,
        10.6891,  7.2668, 14.3326,  7.0075,  9.6397, 12.4908, 11.4816,  8.7555,
        10.9425,  8.0295, 13.9193,  7.3561,  4.9596,  8.3541,  4.2862, 14.6908],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [31/80], Step [100/642], LR 1.0e-04, Loss: 15.6
Max Train Loss:  tensor([15.2071,  9.0992,  8.6602,  7.8524,  6.9598,  9.4939,  7.7997,  7.5906,
         6.1447,  7.1820,  5.5249,  5.2673,  8.8356, 10.5713,  4.5447,  4.9994,
         8.7044,  5.0281,  6.9552,  5.9963,  8.1608,  6.9543,  4.5888,  5.9295,
        11.9207,  9.3706,  8.3087,  5.9675,  5.4407,  9.0993, 12.3861,  9.1354,
         7.1757,  3.8108,  4.5643,  8.2163,  6.6194,  8.2113,  4.3615, 12.8937,
        10.6122, 13.4720, 10.8204,  7.3830, 12.1797, 15.0114,  8.8533,  6.7875,
         7.8368,  7.9358,  9.6680,  7.2061,  6.0320,  8.7873,  6.3975,  6.8936,
        13.3422, 14.3700,  8.6440, 10.2294,  9.9882,  5.8746,  5.5835,  5.0917,
         4.9934, 14.7302,  5.3383,  9.5784,  8.3875,  8.6064,  4.3922,  9.3997,
         8.9894,  8.1801,  6.9919, 11.1916,  3.6776,  3.4017,  3.9073,  5.6170],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [31/80], Step [200/642], LR 1.0e-04, Loss: 15.2
Max Train Loss:  tensor([10.3633,  4.9135, 12.5865,  7.4696,  6.9208,  7.3783,  6.3111,  7.3276,
         9.8820, 10.6991,  7.4988,  6.6201,  8.9730,  3.8813,  9.3458,  7.3524,
        12.8330,  6.6534,  7.2796,  7.0877,  6.9947,  4.5098,  5.6574,  9.7893,
         8.0715,  7.6006,  9.0982,  6.9369,  7.7542,  4.2447,  5.5789,  4.3312,
         8.8728,  4.9796,  6.8573,  8.0130,  7.9317,  7.5460,  5.7802, 11.1983,
         9.8347,  9.9841,  7.5144,  6.1972,  8.6996,  9.2863,  7.9676,  6.4834,
        11.5756,  9.1055,  7.8504,  3.7391,  7.0468,  8.4883,  8.1818,  8.6532,
        10.7517,  8.7298,  4.3485,  9.2365, 11.1266,  6.3950,  6.2012,  7.2985,
         6.5542,  5.1188,  7.7949, 11.7829,  8.4377,  7.0320,  3.3904,  8.6827,
         7.5404,  7.0717,  7.2586,  4.3155,  6.0441,  4.3875,  4.0454,  5.9022],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [31/80], Step [300/642], LR 1.0e-04, Loss: 12.8
Max Train Loss:  tensor([ 9.4400,  4.7730,  8.3171,  6.6488,  8.3569,  7.7292,  7.1997,  7.4545,
         5.0924,  6.2973, 10.0016,  7.4714,  9.7548,  8.6465,  7.9418,  6.1465,
         7.9631,  6.4089,  6.7084,  8.5897,  4.5991,  8.9976,  5.5241,  7.5386,
         7.3455,  7.2420,  7.4046,  6.0124,  7.6463,  5.4883,  5.1197,  5.5978,
         9.8762,  8.9837,  6.5295,  5.5127,  5.4521,  5.8929,  9.7805,  8.3182,
         7.1138,  8.3145,  6.1757,  7.0881,  6.2534,  9.1796,  7.1565,  6.5582,
         6.3788,  9.0423,  4.3601,  4.5434,  6.2467,  5.2096,  3.4758,  7.3958,
        10.6159,  5.5534,  7.7159,  6.9811, 10.2790,  7.5050,  8.2220,  5.9937,
         7.2454,  6.2712,  7.6297,  7.7739,  9.6275,  7.8397,  3.4528,  7.0644,
         8.2564,  4.8903, 10.5395,  5.2964,  9.8215,  6.1755,  5.8127,  5.7737],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [31/80], Step [400/642], LR 1.0e-04, Loss: 10.6
Max Train Loss:  tensor([11.2008,  6.3698, 10.6432,  6.0398,  9.1668,  8.3375, 11.3695,  4.9091,
         6.5433,  5.5731,  7.8125,  6.7064,  9.0723,  6.6467,  7.2030,  6.6207,
         6.3263,  3.3115,  5.2021,  3.4742,  7.2602,  5.9027,  4.7676,  4.8928,
         8.9045,  8.5095,  9.3196,  7.3152,  7.1176,  5.6955,  7.4188,  8.5019,
         6.4637,  5.1911,  7.4832,  8.1853,  6.6522,  5.9492,  8.5925,  9.8329,
        10.3285,  8.9135,  9.3430,  5.6405, 10.1223,  8.6507,  6.8463,  4.9499,
         9.2852,  7.3198,  6.1359,  5.1811,  6.9602,  7.2328,  4.4253,  7.0951,
         8.6293,  6.9179,  9.4221,  8.6516,  9.0010, 11.1291,  7.2377,  6.3002,
         5.1523,  4.4304,  6.4873,  8.3122,  8.5178,  8.6724,  3.4849, 10.1453,
         8.3687,  7.9183,  8.9661,  5.5778,  5.3802,  6.6188,  4.1412,  5.0528],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [31/80], Step [500/642], LR 1.0e-04, Loss: 11.4
Max Train Loss:  tensor([12.0904,  4.5746,  8.2264,  4.7695,  6.9625,  7.4644, 10.8847,  6.6407,
         4.9165,  6.3971,  6.7727,  4.2221,  9.7482,  4.2821, 10.6431,  6.7298,
         9.3803,  6.4808,  5.3903,  5.4346,  6.9641,  5.7512,  6.8488, 11.4417,
         9.0219,  8.0077, 10.9112, 10.1392,  8.4721,  6.4855,  6.5665,  6.8357,
         5.4569,  5.1909,  4.6630,  5.1891,  8.2022,  4.6533,  8.7212,  9.6411,
        10.3197, 10.8709,  7.8697,  7.1760,  7.8577, 10.7001,  6.7042,  6.6808,
         8.8273, 10.2914,  5.7983,  4.9706,  7.1970,  8.2465,  6.8585,  8.2388,
        10.6544,  6.8374,  6.1933,  7.2609, 10.5752,  5.7352,  6.4848,  6.1087,
         5.8669,  9.7800,  5.6077,  7.4342,  8.0089,  6.4466,  4.3273,  6.2729,
         8.7020, 10.3851,  7.8674,  6.1766,  3.9283,  4.5247,  4.1655,  5.6766],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [31/80], Step [600/642], LR 1.0e-04, Loss: 12.1
Max_Val Meta Model:  tensor([ 20.1330,  23.5300,  30.0115,  17.7974,   5.2274,   8.1279,   5.7427,
          7.6508,   4.8723,   6.6798,   6.5931,   5.1415,   9.7149,   7.6603,
          6.0129,   4.9789, 131.4328,   3.4192,   4.4386,   3.5384,   4.6448,
          4.6455,   3.8190,   3.9493,  10.4102,   8.0721,  11.2840,   3.7722,
          6.4800,   5.2438,   4.7513,   4.4480,   6.0234,   2.8676,   3.8354,
          4.8071,   6.2615,   4.8327,   7.5994,  15.1750,   8.6455,   9.5653,
          6.0673,   8.1922,   6.9355,   8.9702,   3.9503,   5.8373,   6.4725,
          3.9368,   4.6264,   3.8412,   7.5617,   5.0539,   3.0081,   4.5538,
         10.1902,   6.7134,   7.7338,   3.1822,   6.1970,  10.7338,   5.7096,
          5.3852,   5.1832,   3.5176,   5.6332,   4.5890,   9.8962,  11.1534,
          3.5006,   8.5684,  10.9801,   6.2470,   7.3268,   5.2813,   3.9236,
          4.3864,   4.1629,   5.1007], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 14.6140,  22.4314,  23.3267,  16.7813,   5.4216,  10.8819,   6.2052,
          7.7621,   5.3689,   8.1163,   7.1706,   5.7091,  10.5894,   7.6200,
          6.4642,   5.7015, 120.8654,   3.9803,   4.9433,   4.1642,   5.2869,
          5.2624,   4.3052,   4.4245,  11.1009,   8.8907,  11.3880,   4.7526,
          7.0543,   5.5913,   5.1793,   5.0441,   6.7483,   3.3189,   4.5409,
          5.7411,   8.1465,   4.7343,   8.0130,  14.8128,   9.0772,   9.3932,
          6.3288,   8.3306,   6.7594,   8.5714,   4.2730,   6.3817,   7.2017,
          4.2591,   4.3812,   4.3881,   8.1458,   4.4053,   3.0965,   5.1536,
          9.3747,   7.0684,   7.9221,   3.6569,   5.6073,  10.0623,   6.1759,
          5.8573,   5.7668,   3.9449,   6.2750,   5.6897,  10.6038,  11.4416,
          4.0140,   8.8092,  11.6519,   6.8569,   7.7322,   5.5194,   4.4709,
          4.8676,   4.7425,   5.7070], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 42.3674,  64.5814,  69.7517,  42.2042,  13.5461,  27.6522,  14.9841,
         19.7773,  13.9867,  20.3756,  17.8251,  13.7689,  26.8088,  20.1279,
         16.1562,  16.2393, 308.7842,   9.4658,  12.4261,  10.4602,  13.2532,
         13.4626,  10.8562,  11.0831,  29.9733,  22.6872,  30.0549,  11.6146,
         19.2030,  15.8786,  14.8873,  13.9605,  17.2848,   9.5774,  11.9651,
         15.4572,  22.1721,  13.1139,  22.3680,  40.0132,  23.2082,  24.0073,
         15.8095,  21.4708,  17.0525,  22.0647,  10.5189,  16.3723,  18.0704,
         10.6516,  10.8322,  12.3253,  20.9551,  11.1176,   7.6781,  13.4241,
         24.8234,  18.2163,  23.5935,   8.8547,  14.8531,  25.9587,  15.9638,
         15.3762,  14.3352,  10.0315,  16.0074,  14.5426,  27.1057,  29.0955,
         10.0670,  23.1926,  29.1798,  18.3718,  19.0928,  16.2674,  11.3602,
         13.6043,  12.2427,  14.5277], device='cuda:0')
Outer loop valEpocw Maximum [31/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 131.4
model_train val_loss valEpocw [31/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 120.9
model_train val_loss  valEpocw [31/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 308.8
Max_Val Meta Model:  tensor([33.0031, 29.9029, 30.2943, 34.0650, 34.0091, 29.7988, 38.9451, 32.8307,
        32.1945, 35.6179, 33.6135, 40.9825, 33.6775, 34.7550, 34.9163, 28.9533,
        33.4550, 34.3291, 35.9950, 36.0181, 33.5435, 33.1777, 32.8587, 34.5371,
        31.6556, 33.3584, 31.8810, 33.6739, 31.1012, 29.5369, 29.8149, 30.9601,
        32.9309, 28.7720, 31.8166, 31.1254, 31.2883, 30.5752, 30.2344, 33.8512,
        32.5056, 32.3595, 33.5390, 33.0572, 32.6125, 32.2166, 33.7919, 32.4845,
        33.5239, 32.8015, 33.9502, 30.2187, 32.7760, 33.1113, 33.1289, 31.9933,
        30.9711, 32.9565, 28.0210, 33.1712, 31.7331, 31.2375, 32.3926, 32.4134,
        33.4107, 31.8862, 32.9802, 31.6640, 33.1579, 33.3967, 32.6367, 31.6177,
        34.1832, 31.8545, 33.7054, 28.4905, 33.0420, 30.0824, 32.6575, 33.3003],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([21.1543,  6.3115, 16.4177,  6.8357,  8.4345, 17.7504, 35.5286, 16.1763,
        13.2662, 14.6620,  8.3035, 65.7158,  9.5133,  3.9511,  7.3826,  4.0661,
         5.4249,  7.3184,  4.0605,  8.9766,  4.4397,  4.0029,  3.6717,  4.9509,
         9.7857,  8.6393, 10.9725,  6.4121,  6.5683,  2.7991,  3.5540,  3.8246,
         3.5479,  2.3993,  2.9518,  3.3565,  6.9148,  2.5351,  4.6370,  2.2288,
         5.5696,  1.7453,  3.3314,  2.6568,  2.1557,  2.7460,  3.0316,  4.1981,
         5.6076,  3.2438,  2.8710,  3.2471,  5.5440,  2.5417,  1.9891,  4.7165,
         3.5714,  3.7411,  3.7726,  1.9475,  1.6720,  3.0506,  3.5403,  3.2825,
         4.4142,  2.3060,  4.8063,  5.7941,  7.2052,  4.1308,  2.8926,  2.9294,
         6.4821,  3.3048,  6.9631,  2.4086,  3.3282,  3.0474,  3.4979,  4.3354],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 64.4124,  18.1116,  48.3719,  17.0568,  20.8925,  47.8415,  84.9645,
         41.4476,  34.6457,  36.3000,  21.0233, 158.1894,  24.2468,  10.2078,
         18.3442,  11.6969,  13.8954,  17.5861,   9.9882,  21.9972,  11.2079,
         10.2363,   9.2685,  12.3248,  26.3502,  22.0684,  29.0179,  15.7236,
         17.9929,   7.9181,  10.1324,  10.5321,   9.0671,   6.8822,   7.8134,
          9.0658,  18.7648,   7.0007,  12.9982,   5.9781,  14.3103,   4.4648,
          8.3249,   6.8255,   5.4429,   7.0173,   7.4500,  10.8196,  14.1282,
          8.0802,   7.0954,   9.1350,  14.2113,   6.4307,   4.9352,  12.2617,
          9.5729,   9.6702,  11.2986,   4.7512,   4.4693,   7.9686,   9.2759,
          8.5438,  10.9530,   5.9529,  12.2572,  15.0716,  18.4485,  10.4648,
          7.3804,   7.7686,  16.1774,   8.7995,  17.3459,   7.0912,   8.3700,
          8.5425,   9.0985,  11.0758], device='cuda:0')
Outer loop valEpocw Maximum [31/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 41.0
model_train val_loss valEpocw [31/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 65.7
model_train val_loss  valEpocw [31/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 158.2
Max_Val Meta Model:  tensor([28.8684, 29.9149, 29.8593, 33.8539, 34.2342, 29.3830, 33.4523, 32.1682,
        32.6707, 34.8245, 32.3061, 34.4706, 33.8619, 33.0761, 34.7100, 29.2362,
        34.3551, 33.8234, 34.1177, 34.6358, 32.7872, 35.5542, 33.7824, 34.8540,
        31.7756, 34.4323, 31.9967, 33.6100, 31.2915, 29.8222, 30.1066, 31.2368,
        33.5874, 29.0677, 32.0809, 31.3558, 31.5282, 30.9994, 30.4876, 33.8886,
        32.9581, 33.8125, 34.5942, 33.9196, 33.6791, 33.5593, 35.2594, 32.7825,
        34.7681, 33.1482, 35.5275, 30.6651, 33.4699, 33.6045, 33.1677, 33.3592,
        34.0878, 32.1326, 28.3304, 33.2883, 33.3409, 31.6641, 32.7895, 32.7637,
        33.8508, 31.6637, 33.2421, 31.9433, 33.4556, 33.7499, 32.8666, 32.0388,
        34.4558, 32.3423, 34.0292, 28.7955, 33.3917, 30.3201, 32.9501, 33.5718],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 5.4127,  3.7304,  2.1267,  3.3663,  2.9934,  3.2090,  2.2115,  5.4316,
         2.5470,  2.8527,  6.5754,  2.5125,  9.9894,  7.1529,  3.0215,  4.2978,
         2.8517,  2.6493,  4.2625,  3.2191,  3.8398,  5.4461,  3.4788,  3.1071,
         4.7748,  3.4433,  9.2396,  7.5382,  5.7980,  4.0356,  4.5568,  6.0459,
         4.9172,  3.5223,  3.7241,  5.0230,  6.5079,  4.7876,  7.3297, 19.8092,
        13.2813, 26.7739, 30.4103, 28.3363, 19.7966, 28.8377,  7.1662,  8.1905,
        17.3351,  5.7512, 13.7986, 15.8795, 21.7803, 14.6433, 25.2160, 38.8752,
        39.1544, 10.0139,  5.1701,  3.7153, 36.1065,  5.4358,  9.2494,  9.0261,
         8.0405,  5.0862,  8.0406,  8.0885,  9.6565,  5.0765,  3.9535,  8.2149,
         8.0649, 14.6921,  2.9339,  7.7561,  6.4683,  4.3459,  4.8809,  5.8215],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 16.5275,  10.7044,   6.2492,   8.4450,   7.4065,   8.6490,   5.4463,
         13.9661,   6.6569,   7.1042,  16.9400,   6.1010,  25.4930,  18.6001,
          7.5289,  12.3682,   7.2603,   6.4509,  10.6814,   7.9982,   9.8718,
         13.6869,   8.7861,   7.7333,  12.8790,   8.7428,  24.4393,  18.6805,
         15.9048,  11.4183,  12.9884,  16.6581,  12.5482,  10.1030,   9.8678,
         13.5819,  17.6630,  13.2141,  20.5423,  53.1448,  34.1598,  68.4495,
         76.0331,  72.6685,  49.9810,  73.8008,  17.3985,  21.1364,  43.3663,
         14.3531,  33.6981,  44.7010,  55.7948,  37.0453,  63.3854, 101.0098,
        104.9859,  26.2563,  15.4727,   9.0804,  96.8193,  14.1772,  24.2445,
         23.5247,  19.9375,  13.2234,  20.5474,  21.0441,  24.7071,  12.8530,
         10.1190,  21.7759,  20.1390,  39.1127,   7.3034,  22.8775,  16.2797,
         12.2040,  12.7036,  14.8870], device='cuda:0')
Outer loop valEpocw Maximum [31/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 35.6
model_train val_loss valEpocw [31/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 39.2
model_train val_loss  valEpocw [31/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 105.0
Max_Val Meta Model:  tensor([29.8668, 29.8240, 29.2493, 33.7916, 33.0802, 29.4290, 33.4247, 31.8753,
        32.3033, 32.4960, 32.2837, 32.0296, 31.5521, 32.7324, 33.6821, 29.3563,
        33.4380, 34.0007, 32.6263, 33.2105, 32.6693, 32.4479, 32.6109, 33.4710,
        31.8433, 33.0536, 31.8096, 32.9737, 31.5436, 29.5863, 29.6419, 30.9305,
        33.4018, 28.6532, 31.9950, 31.3669, 30.9015, 32.2706, 30.3951, 32.4657,
        32.8817, 31.9937, 32.2383, 32.9824, 32.8712, 32.2766, 32.4525, 32.8541,
        32.7000, 32.6429, 32.7655, 30.2393, 33.4461, 33.1864, 33.1804, 31.9765,
        32.9733, 32.9626, 28.1230, 31.6837, 32.6760, 31.7209, 33.2550, 32.1591,
        32.8157, 32.3966, 33.1706, 34.7810, 33.4980, 33.4301, 33.5935, 31.6659,
        33.1669, 32.1848, 37.9530, 28.7755, 31.1289, 30.4695, 33.1325, 33.9351],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([10.6494,  2.6917,  8.4469,  4.3795, 10.4039, 10.0385,  5.7592,  6.5713,
         4.4900,  9.6333,  5.7222,  5.4154,  8.2976,  5.9391,  8.4516,  4.4982,
         2.6988,  3.2371,  4.3413,  3.4420,  4.8792,  4.1738,  4.5137,  4.6722,
         7.6863,  4.0622,  8.7273,  2.0008,  6.6713,  2.9444,  3.1896,  3.9894,
         2.7282,  2.5163,  2.5544,  2.5564,  4.8159,  2.2902,  3.7111,  2.3541,
         5.8508,  1.6721,  3.6196,  3.0294,  2.7144,  3.1792,  3.3895,  4.4802,
         5.8228,  2.5642,  3.6767,  3.4031,  5.8953,  3.1634,  2.5461,  4.0511,
         2.4864,  3.6523,  5.0638,  3.0215,  2.8803,  5.2221,  4.6039,  3.6695,
         5.0955,  2.6812,  5.1306,  3.4589,  7.5840,  4.0084,  3.2236,  4.5650,
         6.0383,  4.1019, 92.1715,  3.7547,  3.4347,  4.0530,  3.7394,  4.6604],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 31.7600,   7.7487,  25.2812,  10.9125,  26.0537,  27.0106,  14.0698,
         16.8849,  11.7884,  24.5520,  14.7127,  13.5382,  21.8988,  15.5940,
         21.2616,  12.8993,   6.9243,   7.7622,  11.0563,   8.6821,  12.4339,
         10.8032,  11.4326,  11.8111,  20.5983,  10.4802,  23.1171,   4.9406,
         18.1168,   8.3383,   9.1529,  11.0317,   6.9964,   7.2662,   6.7365,
          6.8589,  13.2434,   6.1219,  10.3727,   6.3925,  14.9146,   4.3355,
          9.2683,   7.8467,   6.8517,   8.1886,   8.5067,  11.4551,  14.7915,
          6.4320,   9.2591,   9.5959,  15.1004,   8.0265,   6.3158,  10.5626,
          6.5953,   9.4441,  15.2187,   7.4458,   7.6044,  13.6991,  11.9648,
          9.6753,  12.8338,   6.9152,  13.0570,   8.8605,  19.3016,  10.1842,
          8.0105,  12.1738,  15.2231,  10.9544, 234.1674,  11.0271,   8.9094,
         11.2556,   9.6153,  11.8161], device='cuda:0')
Outer loop valEpocw Maximum [31/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 38.0
model_train val_loss valEpocw [31/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 92.2
model_train val_loss  valEpocw [31/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 234.2
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [89.48721385 97.2137279  92.16018323 97.02489005 97.26733349 95.99785578
 96.99808726 94.75152593 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.18744898 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.66815706
 96.94082674 97.14793923 97.21981945 92.86436569 97.84237521 92.74497143
 96.90915072 96.22689782 96.97128446 93.75251276 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31606584
 98.70615611 97.46591781 91.93723273 96.13796128 96.24273583 96.9067141
 92.11876074 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [86.75942057 97.2137279  91.11731095 97.02489005 97.26733349 96.60579184
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.10825891 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.79004885 97.84237521 92.60852085
 96.90915072 96.22689782 96.9627563  93.95597032 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 90.68237473 96.13796128 96.24273583 96.9067141
 91.10512786 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [97.57836839  3.70427314 63.22020224 14.41456177 18.58590303 24.33616406
  9.0119048  32.19957955  7.04769151 29.39422787  1.42077469  4.29145565
  0.78917168 15.18416018  8.15632507 11.49738016 11.97199678  3.66871418
  1.16724708  1.85752435  3.34667193  0.48052771  1.27266226  3.14690183
 18.15887595 10.92455646 31.38182751 14.6301995   4.17469642  1.3862251
 21.43024638  0.8748789  37.03473028  1.35884355  7.16654886 22.76341053
  9.75208706 43.00674316 39.23246756 37.05280733  5.69792595 47.46711145
 21.51299339 28.0046389  25.68695531 37.28669096  3.06236501  2.03399051
  2.68758983  3.2280037  11.03926668  1.45595996  1.33866952 20.4789231
  3.64899615  4.06765743 64.38956404 26.40840776 12.68705215 11.63721081
 64.76968002 17.74417691 25.53793833 11.53427988  8.52889176  8.30003703
  5.81240223 11.961623    3.8576608  11.23855802  0.65204625 31.1933606
  6.30937528 22.99118043 20.98172624 16.34218753  1.38271347  2.71988598
  0.21107693  0.81878588]
Accuracy th:0.5 is [45.76454965 97.2137279  70.31103422 97.02489005 97.26733349 74.89065679
 75.33046625 74.9064948  76.21983163 96.35482024 76.43790889 98.52097319
 99.41399349 79.32651893 76.08703598 96.56680596 96.29512311 75.8250996
 98.65376884 98.30776915 79.01219527 76.92888732 98.38695922 75.97616988
 81.11377785 96.65086926 94.0778012  75.58265616 98.01293844 76.38917655
 97.30875598 98.57457877 96.36213009 98.02024829 85.2170417  76.04439517
 75.94571216 87.81082102 97.11504489 73.32269344 78.03145673 92.05906361
 75.18305089 74.91014973 96.9627563  93.87434364 98.02877645 98.57336046
 92.45135902 85.3279078  85.57400616 98.55508583 98.99976852 75.39138168
 98.70615611 75.78124048 70.90313227 91.91773979 96.24273583 96.9067141
 89.79300934 97.17717864 91.94819751 75.8957615  98.42838172 76.28805692
 98.20786784 75.14771994 76.89111975 97.42449532 77.3857531  95.99054592
 76.7595424  95.45083515 75.24883956 80.77630633 85.75553417 76.42938073
 77.44788684 99.14718388]
Accuracy th:0.7 is [45.77795105 97.2137279  70.31103422 97.02489005 97.26733349 74.89065679
 75.33046625 75.26102265 76.21983163 96.47177788 76.43790889 98.52097319
 99.41399349 79.82358889 76.08703598 96.56680596 96.29512311 75.8250996
 98.65376884 98.30776915 79.49708215 76.92888732 98.38695922 76.50369757
 81.96415736 96.65086926 94.0778012  75.58265616 98.01293844 76.38917655
 97.30875598 98.57457877 96.36213009 98.02024829 85.47654147 76.04439517
 75.94571216 88.19702489 97.11504489 73.32269344 78.74051242 92.05906361
 75.18305089 74.91014973 96.9627563  93.87434364 98.02877645 98.57336046
 94.62847675 86.34397729 85.77990034 98.55508583 98.99976852 75.39138168
 98.70615611 75.78124048 70.90313227 92.31003521 96.24273583 96.9067141
 89.79300934 97.17717864 92.13459875 75.8957615  98.42838172 76.28805692
 98.20786784 75.14771994 76.89111975 97.55607266 77.38697141 95.99054592
 76.93254224 95.45083515 75.24883956 80.8701161  85.94559033 76.42938073
 77.44788684 99.14718388]
Avg Prec: is [55.62901193  3.06906462 11.13929506  3.4015565   2.20675587  3.67122011
  3.31369806  5.44302852  2.46140445  4.03240707  1.67668364  1.61924086
  0.76560727  5.28678659  2.74751954  3.13878214  3.71916584  2.77230573
  1.37868049  1.67257625  2.00861462  0.9071751   1.91799842  2.42513451
  5.13845478  3.47959655  6.49759216  3.31516746  2.04658852  1.90576305
  2.61781249  1.29099782  3.64938228  1.62138398  2.30349305  2.3362343
  2.97294131  2.54045501  2.87025419  7.38359621  2.25201044  8.27602131
  3.43172173  4.04518975  3.30152916  6.41413466  2.09433412  1.53912849
  2.17865963  1.57884146  1.87262602  1.57768388  1.10158844  3.17167186
  1.26908983  2.68234731 11.14313553  3.64423565  3.98027907  2.75378208
 10.8424831   2.18999258  3.78814344  2.96332266  1.56657859  2.50361852
  1.72701888  4.17657101  1.20058747  2.2741486   0.1830142   3.35999389
  1.85951556  4.43824379  3.86794526  3.0751258   0.84205124  1.90683494
  0.12583253  0.70329502]
mAP score regular 15.93, mAP score EMA 3.75
starting validation
Accuracy th:0.5 is [90.0715051  97.22450607 92.80215263 96.96290206 97.90716795 95.55024043
 96.80843112 94.88003588 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.40416573 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39733911 98.18870369 98.00931809 97.75518848
 97.27931833 96.79846526 97.2593866  92.87689663 97.82744101 93.06375663
 97.07750953 96.48703192 97.0276802  93.64925131 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.2145402
 98.6969629  97.58576874 91.21508832 96.39235618 96.16314124 96.78102499
 92.50317662 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [88.90549867 97.22450607 91.8603782  96.96290206 97.90716795 96.62904552
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.40914867 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.9242345  97.82744101 92.9242345
 97.07750953 96.48703192 97.03764606 94.07280066 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.4751227  96.39235618 96.16314124 96.78102499
 91.61621447 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [97.45771706  4.09681493 67.06090682 16.06117717 15.39745332 22.92883956
  9.45160886 32.5258004   8.97430847 32.30984626  1.42671074  5.08271828
  0.75243718 16.5283523   8.29017186 13.43272325 13.51836038  4.04274182
  1.14531831  2.12891951  3.41948468  0.49391305  1.41253922  3.33294001
 18.14014861 11.42772496 30.81459403 15.34743796  4.95352739  1.42081677
 23.65699763  0.7845424  39.9945549   1.30106365  7.70028544 23.16150518
  9.48516779 47.98638184 47.12332061 38.98640916  5.75601671 47.70880957
 21.67420548 27.75558379 24.59867036 37.04883658  2.88781103  1.89527705
  2.67547328  3.08195611 11.93838963  1.50197035  1.47294668 22.17674595
  4.0288825   3.87697148 58.43080975 27.20084889 11.60182938 12.21133221
 64.99465199 19.01916805 28.27081548 12.75150304  9.50321619  8.15001949
  6.39300655 12.71772373  3.4734497  12.08041497  0.6721742  33.50718795
  6.25306957 25.75240365 27.77488447 16.00259533  1.40542625  2.74691613
  0.1803883   0.77779081]
Accuracy th:0.5 is [45.7657523  97.22450607 68.55768991 96.96290206 97.90716795 73.77980417
 73.88195431 73.17936069 75.37434288 96.40979645 75.48895035 98.5325261
 99.34972718 76.87171438 75.48645888 96.31262924 96.21047911 74.85860926
 98.78167277 98.34068316 77.34509306 76.12925729 98.31327703 75.15758527
 77.36253332 96.52938685 94.3393876  74.9757082  97.81747515 75.46154421
 97.52597354 98.67204823 96.39983058 98.18870369 85.57440765 75.1177218
 75.13017914 90.14874056 97.0276802  72.43939507 76.11181703 92.37362035
 74.21581085 73.96915564 97.03764606 94.02795426 98.18621222 98.77668984
 93.53713531 85.39751352 83.91509081 98.55993223 98.87385704 74.19587911
 98.6969629  74.9383362  69.39482273 93.11358597 96.16314124 96.78102499
 90.13379176 97.04761193 92.04723821 75.0056058  98.32075143 75.88509355
 98.13139996 74.22577671 76.20150983 97.48112714 76.54533224 96.07843137
 76.00219249 95.44559882 74.12860951 81.68771956 87.76938984 75.57366021
 76.62007624 99.15040985]
Accuracy th:0.7 is [45.9451379  97.22450607 68.55768991 96.96290206 97.90716795 73.77980417
 73.88195431 73.42352443 75.37434288 96.41976231 75.48895035 98.5325261
 99.34972718 77.33761866 75.48645888 96.31262924 96.21047911 74.85860926
 98.78167277 98.34068316 77.686424   76.12925729 98.31327703 75.37683434
 77.96795974 96.52938685 94.3393876  74.9757082  97.81747515 75.46154421
 97.52597354 98.67204823 96.39983058 98.18870369 85.92570446 75.1177218
 75.13017914 90.3854299  97.0276802  72.43939507 76.62007624 92.37362035
 74.21581085 73.96915564 97.03764606 94.02795426 98.18621222 98.77668984
 95.47549642 85.90078979 84.10693375 98.55993223 98.87385704 74.19587911
 98.6969629  74.9383362  69.39482273 93.41007051 96.16314124 96.78102499
 90.13379176 97.04761193 92.20170915 75.00809727 98.32075143 75.88509355
 98.13139996 74.22577671 76.20150983 97.53095647 76.54533224 96.07843137
 76.05949623 95.44559882 74.12860951 81.76993796 87.96372424 75.57366021
 76.62007624 99.15040985]
Avg Prec: is [54.48456412  3.77507627 14.91777853  4.59451088  1.63141305  4.29403404
 10.49394105  8.79798673  7.1158705   5.16076305  2.26478222  5.31095531
  1.55872039  5.880916    3.01372817  4.20526117 26.04966778  6.15691634
  1.54252474  2.74754683  3.67503731  1.42840542  1.45671871  5.50272135
  5.76422211 12.9874062   8.28359364  4.54433063  3.90611115  6.91509825
  2.32377633  0.86720226  3.03935119  1.15561128  1.69724173  2.35717572
  2.01718014  2.21283357  2.11719581  6.37422958  1.74200177  6.04382894
  2.20788111  2.72343453  2.36007335  4.86609145  1.78313564  1.08649824
  1.45337833  1.20347867  1.248443    1.0043321   0.77549472  2.32185038
  0.92742509  1.8830509  10.21244141  3.10244814  3.96615607  2.85931791
  7.9533134   2.02167387  3.28401173  2.61492465  1.37776068  1.87532793
  1.58442508  3.53358132  1.07334487  2.17715182  0.19404009  3.14212926
  1.58627256  4.00711664  3.19767367  2.33826216  0.55170065  1.51201835
  0.12135346  0.59529429]
mAP score regular 16.57, mAP score EMA 4.39
Train_data_mAP: current_mAP = 15.93, highest_mAP = 15.93
Val_data_mAP: current_mAP = 16.57, highest_mAP = 16.57
tensor([0.3356, 0.3471, 0.3341, 0.3987, 0.3986, 0.3709, 0.4087, 0.3876, 0.3801,
        0.3905, 0.3877, 0.3977, 0.3740, 0.3794, 0.3978, 0.3480, 0.3897, 0.4157,
        0.3917, 0.3945, 0.3905, 0.3859, 0.3929, 0.3950, 0.3711, 0.3874, 0.3776,
        0.4030, 0.3670, 0.3518, 0.3468, 0.3592, 0.3893, 0.3442, 0.3783, 0.3712,
        0.3629, 0.3732, 0.3565, 0.3674, 0.3917, 0.3834, 0.3885, 0.3858, 0.3953,
        0.3873, 0.3972, 0.3901, 0.3922, 0.3974, 0.3953, 0.3546, 0.3899, 0.3933,
        0.4012, 0.3826, 0.3760, 0.3832, 0.3319, 0.4048, 0.3760, 0.3822, 0.3842,
        0.3777, 0.3958, 0.3866, 0.3915, 0.3874, 0.3924, 0.3929, 0.4013, 0.3760,
        0.3992, 0.3727, 0.4009, 0.3381, 0.3834, 0.3574, 0.3891, 0.3932],
       device='cuda:0')
Max Train Loss:  tensor([ 9.3673,  5.9571,  8.5712,  8.9091,  7.7381,  7.4258,  7.0633,  5.5058,
         7.9507,  6.8502,  8.3023,  4.1412,  8.6779,  9.8072,  6.0563,  7.4513,
         5.6958,  4.4511,  6.2279,  6.5496,  6.0672,  5.4219,  8.6483, 10.0738,
         7.3654,  7.5107,  9.3085,  6.8244,  4.9836,  3.3560,  7.9263,  6.3374,
         7.8224,  8.7238,  6.6430,  6.3882,  6.8610,  4.3346,  8.4576,  9.5072,
         9.3029,  8.8094,  8.8401, 10.0530,  7.7233,  9.2703,  3.9088,  5.0111,
         8.8889,  2.8962,  5.4020,  6.3955,  6.9961,  9.3105,  3.8771,  8.8256,
        13.4356,  8.7305,  5.6532,  4.1487, 12.8271,  5.3008,  8.9142,  7.0659,
         5.8701,  5.7212,  7.8966,  7.6708,  9.1905,  7.7699,  6.2536,  8.1754,
        10.4680,  9.9419, 10.0886,  3.7350,  4.5587,  6.3973,  4.2125,  5.1213],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [32/80], Step [000/642], LR 1.0e-04, Loss: 13.4
Max Train Loss:  tensor([13.2112,  6.1832, 11.3603, 12.3903, 12.4174,  6.9564, 13.2331, 10.1481,
         5.3248,  9.1179,  8.7311, 12.4798,  5.1081, 10.8606, 13.9159,  7.7865,
        12.8589, 10.9911,  6.1670,  9.6522,  5.9540, 13.8863,  7.7036,  7.5045,
         6.5039,  7.7216, 11.0768, 13.6734, 13.3894,  3.9169,  4.8708,  5.7824,
        11.9409,  3.8521,  9.3156,  8.7174,  8.5749, 13.1280,  5.1243, 11.6158,
         7.7400,  9.7634,  6.6383,  7.0769,  6.3082, 11.5294, 10.1274, 12.7381,
         6.2097, 15.1549,  6.1561,  4.7676,  6.2952, 11.0343, 14.2939, 12.6283,
        13.6794, 11.7876,  7.0750, 13.6831, 14.1335,  5.6628,  9.2933,  8.5070,
        11.6468, 14.5899,  9.2792, 10.1596,  3.4588, 11.5706, 14.7733,  9.7174,
         3.3825,  8.2752, 11.8867,  5.4575, 15.3188,  6.2687, 13.8068,  9.5240],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [32/80], Step [100/642], LR 1.0e-04, Loss: 15.3
Max Train Loss:  tensor([11.5004,  3.8683, 10.0772, 10.6591,  7.0216,  5.4800,  6.1569,  9.2166,
         6.0604,  9.9420,  8.2823,  9.8955,  3.0999,  7.8228,  5.8260,  6.1049,
        11.8372,  6.5424,  6.6256,  5.5114,  6.7109,  4.1151,  7.0199,  8.4048,
         8.7236,  6.3889,  7.8166, 13.3637,  6.1274,  5.1655,  4.0063,  5.2928,
         8.8323,  7.5890,  8.8484,  8.7066,  7.4478,  6.8681,  8.0994,  9.9209,
         5.4643,  8.8811,  8.9546,  8.8836,  9.4055,  8.5710,  7.8417,  4.2329,
         6.2486,  7.4988,  8.9834,  8.4590,  5.7626, 11.0157,  7.0079, 10.7581,
         7.2264, 10.5175,  9.1361,  9.8425, 11.3021,  9.3053,  6.8979,  6.3104,
         9.8267, 10.0627,  8.4599,  4.9983,  3.4298,  6.3852,  5.1768,  5.7900,
         6.2558,  7.0625, 10.5509,  3.8128,  5.4485,  7.2225,  4.8448,  8.3150],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [32/80], Step [200/642], LR 1.0e-04, Loss: 13.4
Max Train Loss:  tensor([11.9274,  4.7663,  9.8480,  9.4932,  7.8091,  7.6546, 11.7431,  8.9578,
         8.7059,  7.9382,  7.6635,  7.5194,  4.2771,  6.5940,  9.7896,  4.8620,
         9.9596,  6.5365,  7.5168,  9.3775,  8.7234,  4.8524,  4.5114,  4.2391,
         6.1774,  9.0803,  7.0524,  4.6612,  5.2575,  5.3964,  9.5996,  6.7074,
        10.5305,  2.8188,  6.6777,  5.6913,  8.2935,  7.7647,  6.3450,  9.6980,
         7.1563,  8.1655,  7.7744,  7.1198,  6.8633,  7.6656,  8.3583,  8.4477,
         8.1168,  6.8049,  6.5112,  6.1554,  7.2228,  8.8210,  6.0181,  8.2900,
        14.4699,  7.5887,  7.7074,  7.4799,  7.0740,  6.0524,  9.0079,  7.6462,
        10.2039,  9.9386,  9.5112,  6.8737,  4.6208,  7.3014,  5.3191,  6.8192,
         6.1716,  9.4782,  4.3031,  4.2065,  7.1438,  6.2444,  4.9987,  9.0445],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [32/80], Step [300/642], LR 1.0e-04, Loss: 14.5
Max Train Loss:  tensor([11.9595,  6.4856,  7.7422,  4.9441,  6.5525,  8.3864,  5.8574,  8.8474,
         7.7036,  6.2163,  4.5944,  5.7738,  3.0546,  7.8426,  8.7814,  8.2571,
         7.8731,  7.7275,  4.6283,  4.8983,  8.2887,  5.4366,  5.3240,  8.2181,
         7.5964,  7.6763,  7.6144,  5.6419,  5.3139,  4.2752,  7.5810,  5.3874,
         7.8433,  3.7626,  6.5825,  4.5772,  6.8412,  7.7424,  4.4997,  7.7332,
         4.4836, 12.4541, 10.3050, 10.8634, 10.0034,  7.5760,  8.6162,  7.1780,
         6.3013,  5.1993,  7.6553,  6.0319,  5.7217,  9.1688,  5.8545,  8.2295,
        12.2689,  8.5660,  6.6663,  8.7735,  9.7033,  5.9885,  9.7404,  9.8175,
         5.4733,  8.3558,  8.9783,  6.8037,  3.4512,  7.3594,  5.1437,  7.0416,
         7.5358,  8.1116,  7.4870,  6.1500,  6.3505,  5.1851,  4.8259,  8.2718],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [32/80], Step [400/642], LR 1.0e-04, Loss: 12.5
Max Train Loss:  tensor([11.3024,  4.0214,  8.5777,  8.3413,  7.4857,  8.5037,  5.8805,  7.3725,
         5.2169,  5.2225,  5.8973,  4.4905,  3.2197,  7.1423, 10.4353,  6.6728,
         6.8824,  6.6370,  8.1171,  5.2418,  4.7347,  6.4663,  4.5274,  6.5397,
        10.1157,  8.6571,  9.8971,  7.8533,  6.8036,  7.3888,  5.3967,  5.5795,
         5.9682,  7.2326,  8.4303,  5.5898,  7.8233,  7.2357,  6.0997, 12.9472,
         5.5062, 11.4710,  9.0221,  7.9703,  9.0154,  7.9136,  7.6993,  7.2787,
         5.0892,  5.9907,  4.4963,  6.7478,  7.4510,  6.4329,  7.8689,  5.8460,
        10.3303,  7.6572,  5.2752,  7.1192,  8.9048,  8.0336,  8.5008,  8.1047,
         4.1886,  7.1536,  8.3048,  9.1053,  5.2230,  8.0982,  6.2205, 13.9497,
         9.7836, 10.7716,  7.1148,  6.3309,  7.0587,  5.2720,  4.9982,  1.7589],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [32/80], Step [500/642], LR 1.0e-04, Loss: 13.9
Max Train Loss:  tensor([ 7.8330,  4.7879,  6.1970,  6.1018,  6.1160,  5.8200,  8.4345,  8.6147,
         5.3638,  8.0926,  3.5314,  4.5640,  3.2148, 10.9590,  9.0341,  9.0899,
         7.9499,  7.8590,  5.7608,  3.9287,  5.8620,  5.3535,  7.7719,  5.7637,
         9.4282,  6.1318,  6.7785,  5.1428,  5.3784,  4.2431,  8.6136,  5.7664,
         7.9551,  4.0683,  7.8334,  6.9481,  7.3552,  6.9299,  9.3602,  9.9990,
         4.9743,  7.5436,  6.9860,  6.7365,  6.6463,  9.6677,  5.7004,  4.3672,
         5.8858,  5.1963,  5.3628,  5.0355,  5.8841,  4.7578,  5.0703,  5.8095,
         7.5376,  6.6996,  8.1330,  9.4441,  8.1964,  8.6446,  6.0349,  7.3941,
         3.3380,  6.9542,  7.7677,  6.9723,  2.2816,  6.1025,  5.3038,  8.7157,
         6.3113,  6.6297,  8.4901,  9.4679,  5.5710,  5.9605,  4.9830,  2.8309],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [32/80], Step [600/642], LR 1.0e-04, Loss: 11.0
Max_Val Meta Model:  tensor([ 18.2834,  23.2793,  28.9601,  19.0339,   3.4990,   6.3514,   6.6951,
          7.6286,   4.5268,   6.8052,   3.5150,   6.0605,   3.9821,   8.4472,
          4.9162,   4.9123, 133.2540,   4.2619,   3.1478,   4.0370,   4.8410,
          4.2977,   4.5762,   4.3302,  10.4557,   7.8264,  10.1855,   4.3172,
          6.2440,   5.1182,   5.9512,   4.7280,   6.3128,   2.8253,   4.3549,
          4.0636,   2.1873,   5.0930,   4.4472,  16.6556,   6.7525,   9.5282,
          5.7501,   8.0530,   6.3953,   8.3339,   3.7812,   5.3156,   3.8669,
          5.2001,   4.5422,   4.3227,   7.3339,   3.3223,   5.1514,   4.1719,
         11.2510,   6.2741,   8.1141,   4.6140,   6.1512,  11.1167,   5.8476,
          4.7804,   2.2773,   3.2072,   6.5407,   4.1089,   5.5191,  10.8618,
          5.3869,   8.8437,   8.9213,   6.7573,   7.1307,   5.4017,   5.6503,
          4.4430,   5.0488,   1.7621], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 15.5381,  22.0585,  22.0583,  18.3788,   3.7222,   7.7274,   7.8388,
         10.1256,   5.0808,   8.9704,   4.2458,   7.3209,   4.9877,   9.1308,
          5.1682,   5.6639, 117.3088,   5.1466,   3.8913,   5.1438,   5.9264,
          5.3163,   5.3278,   4.7294,  11.9461,   8.5765,  12.5314,   5.9034,
          7.1448,   5.6087,   6.9619,   5.8222,   7.2184,   3.6315,   5.5320,
          5.0305,   3.4702,   5.5963,   5.5607,  15.3670,   7.3572,   9.7625,
          6.3188,   8.5722,   6.4835,   8.1826,   4.3080,   6.2030,   3.7981,
          5.8782,   5.5872,   5.3636,   8.3676,   3.8142,   6.3039,   4.8851,
         10.6358,   6.9003,   8.7546,   5.3452,   6.1651,  11.0252,   6.6201,
          5.6563,   2.9376,   4.0793,   7.7719,   5.8173,   5.8544,  11.4165,
          6.4907,   9.6592,   9.6172,   7.7277,   7.7388,   5.9115,   6.7722,
          5.0305,   6.1084,   2.3001], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 46.3003,  63.5527,  66.0137,  46.0983,   9.3378,  20.8322,  19.1800,
         26.1213,  13.3682,  22.9743,  10.9506,  18.4084,  13.3350,  24.0670,
         12.9925,  16.2760, 301.0055,  12.3802,   9.9334,  13.0398,  15.1776,
         13.7767,  13.5604,  11.9728,  32.1948,  22.1385,  33.1860,  14.6485,
         19.4683,  15.9451,  20.0745,  16.2096,  18.5410,  10.5503,  14.6234,
         13.5527,   9.5637,  14.9940,  15.5983,  41.8231,  18.7804,  25.4601,
         16.2626,  22.2174,  16.3993,  21.1291,  10.8447,  15.9015,   9.6833,
         14.7906,  14.1340,  15.1265,  21.4591,   9.6974,  15.7113,  12.7678,
         28.2881,  18.0047,  26.3791,  13.2038,  16.3966,  28.8490,  17.2299,
         14.9764,   7.4223,  10.5513,  19.8502,  15.0152,  14.9205,  29.0583,
         16.1743,  25.6880,  24.0890,  20.7346,  19.3046,  17.4843,  17.6658,
         14.0756,  15.7005,   5.8495], device='cuda:0')
Outer loop valEpocw Maximum [32/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 133.3
model_train val_loss valEpocw [32/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 117.3
model_train val_loss  valEpocw [32/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 301.0
Max_Val Meta Model:  tensor([32.6894, 28.7230, 35.3678, 33.9094, 37.6812, 31.5448, 44.3082, 32.7556,
        32.1340, 35.1366, 34.1014, 42.5523, 31.4366, 34.8303, 37.3375, 35.3958,
        33.2890, 33.1391, 34.2943, 34.5506, 32.9184, 32.8073, 32.5019, 37.0615,
        31.7271, 32.8884, 32.1928, 33.3302, 31.0246, 29.4706, 29.7890, 30.8277,
        32.9468, 28.7284, 31.7077, 31.0476, 30.4887, 31.5162, 30.0576, 32.9410,
        32.3234, 31.5311, 32.5878, 32.9444, 32.3757, 30.5910, 32.0337, 32.3940,
        32.6949, 33.0981, 33.2927, 30.3374, 33.1329, 32.8660, 33.2707, 31.8556,
        32.4814, 32.6930, 27.7789, 33.3921, 31.7468, 32.4262, 32.3257, 32.2446,
        32.6360, 32.5576, 33.4513, 31.6883, 32.5949, 33.4333, 32.9587, 31.8689,
        31.1554, 31.7897, 32.3598, 28.3345, 32.0301, 29.9108, 32.7253, 32.8320],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([19.7077,  6.3124, 16.2832,  6.4408,  6.2250, 13.3198, 31.7693, 17.4434,
        13.0219, 15.0627,  7.0689, 47.7540,  5.0941,  6.2764,  6.5541,  4.4383,
         5.8197,  7.6188,  3.0254,  9.3707,  4.4420,  3.9052,  4.4179,  5.6063,
         9.4118,  7.9291, 10.5507,  6.8008,  6.5538,  2.8081,  4.3213,  4.3328,
         3.3705,  2.5567,  3.6185,  3.1790,  3.2094,  3.1741,  3.3282,  2.5442,
         3.5257,  1.9372,  3.0549,  3.2469,  2.0027,  2.2697,  2.8571,  3.8260,
         2.3066,  4.4966,  4.0422,  3.9443,  5.5046,  2.4481,  4.7383,  4.2001,
         4.2333,  4.0627,  4.7506,  3.1134,  2.3293,  3.8777,  3.4022,  3.0772,
         1.9412,  2.6820,  5.8208,  5.7706,  2.0097,  4.9769,  4.8122,  4.3850,
         3.9301,  3.7592,  8.3115,  2.6986,  5.1334,  3.0068,  4.4713,  1.4779],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 59.3986,  18.2437,  47.7139,  16.0208,  15.1845,  35.9015,  76.7112,
         44.9506,  33.9761,  37.6561,  17.8884, 120.2623,  13.7106,  15.9277,
         15.9263,  12.5379,  14.8682,  18.5549,   7.5525,  23.3220,  11.4191,
         10.0576,  11.2695,  13.7050,  25.3318,  20.4298,  27.8327,  16.8720,
         17.9251,   7.9292,  12.3674,  11.9585,   8.6245,   7.3191,   9.6010,
          8.5792,   8.7901,   8.4634,   9.3300,   6.8443,   9.0313,   5.0225,
          7.8425,   8.3648,   5.0543,   5.8845,   7.2029,   9.8417,   5.8707,
         11.2187,  10.1856,  11.0364,  13.9144,   6.1901,  11.7847,  10.9022,
         11.1295,  10.5691,  14.3311,   7.6995,   6.2093,  10.0930,   8.9328,
          8.0193,   4.8688,   6.8701,  15.1521,  14.9905,   5.1389,  12.6049,
         12.1815,  11.6035,  10.1251,  10.0310,  20.9110,   7.9607,  13.3390,
          8.4370,  11.5920,   3.7628], device='cuda:0')
Outer loop valEpocw Maximum [32/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 44.3
model_train val_loss valEpocw [32/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 47.8
model_train val_loss  valEpocw [32/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 120.3
Max_Val Meta Model:  tensor([29.5252, 28.7983, 30.5999, 33.9731, 34.5141, 31.0729, 29.0787, 32.1407,
        31.2506, 34.3189, 34.3259, 32.0013, 31.5604, 34.8642, 34.2172, 31.8880,
        34.0927, 33.2355, 34.3039, 34.0699, 33.1306, 34.8842, 33.8444, 34.1798,
        31.5591, 31.7357, 32.1124, 34.2906, 31.0976, 29.5706, 29.8044, 30.8650,
        33.4417, 28.7983, 31.8903, 31.1979, 30.5595, 31.6933, 30.1583, 33.2228,
        32.7582, 33.0005, 33.4558, 33.5807, 31.9992, 33.7694, 33.3014, 32.6187,
        34.5452, 34.0936, 34.7815, 30.5918, 33.9209, 33.1644, 34.0121, 33.1038,
        33.7083, 32.8244, 27.9657, 33.6010, 34.5753, 33.0625, 32.5791, 32.2983,
        32.9239, 32.5919, 31.7480, 31.7427, 32.9242, 33.5835, 33.2883, 32.1659,
        31.7749, 32.0448, 32.4078, 28.4688, 32.1521, 30.1048, 32.9590, 33.0653],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 4.7097,  3.5080,  2.6066,  4.3867,  1.5457,  3.5261,  1.9433,  4.8668,
         1.9572,  2.5843,  2.3002,  2.2702,  3.6310,  5.9575,  2.0414,  3.2996,
         1.8531,  2.1376,  2.9517,  3.1779,  5.1891,  4.8222,  3.9946,  2.6880,
         3.9082,  2.9289,  9.5783,  6.7685,  4.8614,  3.6388,  4.4134,  6.1205,
         6.0433,  3.3289,  3.9911,  3.5126,  5.1282,  5.2777,  3.8994, 18.6863,
        12.9647, 26.1203, 28.3992, 27.9415, 22.4735, 28.0603,  6.4572,  7.8217,
        16.7872,  7.2813, 14.5713, 16.1576, 21.1340, 11.0408, 26.0663, 40.3909,
        31.6318,  9.0766,  6.4619,  5.9991, 26.5649,  5.7247, 10.5592,  9.5420,
         5.8652,  4.7647,  8.3376,  7.0726,  3.7531,  6.9456,  5.6929, 10.1077,
         4.5287, 15.3212,  2.6397,  5.9482,  7.7316,  4.0899,  5.5188,  1.8507],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 14.1472,  10.1458,   7.6478,  10.9337,   3.8067,   9.5454,   5.1241,
         12.5791,   5.2032,   6.5261,   5.8252,   5.7720,   9.7888,  15.1701,
          5.1434,   9.3215,   4.7068,   5.2287,   7.3929,   7.9845,  13.3479,
         12.2022,  10.1229,   6.7650,  10.5552,   7.6507,  25.2945,  16.7288,
         13.3263,  10.3088,  12.6750,  16.9812,  15.4656,   9.5717,  10.5868,
          9.4843,  14.1191,  14.1054,  10.9464,  50.2544,  33.2294,  67.8010,
         73.0951,  72.0188,  58.4324,  72.3135,  16.1210,  20.1355,  42.4454,
         18.1833,  36.3761,  45.3672,  53.6487,  28.0067,  65.0270, 105.1836,
         83.5365,  23.6766,  19.4895,  14.8579,  71.0677,  14.8816,  27.7226,
         25.0313,  14.7436,  12.2698,  21.8476,  18.3955,   9.5802,  17.6129,
         14.3553,  26.7535,  11.6685,  40.9927,   6.6705,  17.6176,  20.1617,
         11.4856,  14.2951,   4.7201], device='cuda:0')
Outer loop valEpocw Maximum [32/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 34.9
model_train val_loss valEpocw [32/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 40.4
model_train val_loss  valEpocw [32/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 105.2
Max_Val Meta Model:  tensor([31.2483, 28.2633, 29.7372, 32.3270, 31.9668, 30.6480, 28.6977, 31.6307,
        30.5752, 32.3292, 31.7514, 31.3894, 31.4296, 31.3360, 33.0597, 31.6242,
        32.7928, 33.0837, 31.9275, 33.2056, 33.0050, 33.0357, 32.5569, 32.3068,
        31.4573, 31.1677, 31.8872, 32.5978, 31.0275, 29.0257, 29.2067, 31.5114,
        31.6846, 28.0842, 31.7550, 30.9754, 29.8261, 32.2660, 29.8724, 32.6347,
        32.3905, 31.4837, 32.4191, 30.7478, 30.2295, 31.0579, 32.8768, 32.4248,
        30.6259, 32.7113, 33.0639, 30.0248, 31.2380, 33.3079, 32.7861, 31.3933,
        33.0656, 32.6935, 27.5719, 32.2099, 31.7183, 30.7211, 32.8381, 31.3473,
        32.2687, 32.6291, 31.3724, 34.4492, 32.5257, 33.0400, 31.6452, 31.8337,
        31.4765, 31.4525, 37.9225, 28.0383, 31.7447, 29.8433, 33.0856, 31.8447],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([10.6644,  2.5744,  8.8095,  3.9391,  8.6254,  7.2279,  8.9470,  9.6577,
         3.0712,  8.8646,  3.3909,  8.3145,  2.8256,  5.6252,  7.6187,  4.1862,
         2.8407,  3.8356,  2.9731,  3.9613,  4.3919,  3.8482,  4.8790,  5.6192,
         6.9021,  2.7316,  7.9371,  2.5366,  6.5677,  2.6763,  3.1937,  4.2315,
         2.2511,  2.4161,  3.0430,  2.5653,  1.7371,  2.8699,  2.4869,  2.2327,
         3.4382,  1.7448,  2.7975,  3.2357,  2.0746,  2.3094,  3.2532,  3.8459,
         2.9037,  3.9738,  3.9635,  3.7923,  5.2708,  2.4397,  4.5531,  3.5497,
         2.3282,  3.4732,  5.7363,  4.1600,  3.0359,  4.9561,  4.1907,  3.2274,
         1.9775,  2.4690,  5.7145,  2.7663,  1.9551,  4.3831,  4.7895,  7.0222,
         2.6880,  4.4257, 65.0862,  3.4317,  5.0691,  4.0534,  4.6069,  1.5216],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 31.2596,   7.4905,  26.3666,   9.9481,  21.7115,  19.6226,  23.4065,
         25.1012,   8.2334,  22.8234,   8.8002,  21.2158,   7.5472,  14.9787,
         19.3570,  11.8247,   7.2708,   9.2790,   7.6043,  10.0630,  11.2107,
          9.9156,  12.4156,  14.3790,  18.5960,   7.1655,  21.0010,   6.3362,
         17.8826,   7.6242,   9.2702,  11.5803,   5.8535,   7.0276,   8.0191,
          6.8970,   4.7991,   7.5171,   6.9824,   6.0542,   8.7397,   4.5577,
          7.1983,   8.6349,   5.4241,   6.0178,   8.1675,   9.8342,   7.5618,
         10.0213,  10.0637,  10.6717,  13.8047,   6.1586,  11.4286,   9.2949,
          6.1762,   9.0718,  17.4261,  10.3965,   8.0613,  13.1701,  10.9314,
          8.6220,   4.9883,   6.3482,  15.0038,   7.0743,   4.9666,  11.1854,
         12.1439,  18.6953,   6.9383,  11.9388, 166.6691,  10.1767,  13.2435,
         11.3449,  11.7626,   3.9078], device='cuda:0')
Outer loop valEpocw Maximum [32/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 37.9
model_train val_loss valEpocw [32/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 65.1
model_train val_loss  valEpocw [32/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 166.7
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [88.45652465 97.2137279  92.04809883 97.02489005 97.26733349 96.5997003
 96.99808726 94.69426542 97.44398826 96.47665111 98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.20937854 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36700333 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.7681193  97.84237521 92.59511946
 96.90915072 96.22689782 96.9627563  93.92429429 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 92.09317625 96.13796128 96.24273583 96.9067141
 92.66456305 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99298254
 97.96420609 95.45083515 96.3633484  96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [85.65319623 97.2137279  91.51691622 97.02489005 97.26733349 96.5997003
 96.99808726 94.74177946 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.72913342 97.84237521 92.06028192
 96.90915072 96.22689782 96.9627563  93.87799856 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.07832507 96.13796128 96.24273583 96.9067141
 91.31467697 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.19156687 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [97.37520458  4.49766283 61.97565341  9.0663206  18.34565748 14.01998582
 10.57856478 31.87519422  5.10585573 24.91049405  2.22340626  6.61406918
  0.7181174  14.92998545  8.42209768 16.67255439 11.72092474  6.94537153
  1.10526269  1.88067761  1.61323182  0.49538502  1.15840569  8.84594129
 17.9463749  12.06598717 30.3307466  16.09427816  4.74986279  1.47569356
 33.35056021  0.95078216 30.73395214  1.29937703  6.59102354 15.14073869
  7.94384907 33.72765242 21.11722134 32.19897475  5.03944448 48.93312479
 21.8995268  28.73507049 21.34973729 36.16624492  3.66007481  1.96265254
 14.81336092  2.8380368   1.22011167  1.24073001  1.19860212 14.07180307
  1.44274497  5.314299   65.69098618 24.07592456 12.55206769 12.30653477
 66.17153541  8.46191983 25.17225744 10.31635475  4.39454022  6.36790345
  3.88685274 11.97260929  2.55409283  7.86367256  0.42268837 41.25893978
  4.27288104 23.40774579 27.81534215  6.16925154  1.21404624  3.18181751
  0.23866564  0.89389337]
Accuracy th:0.5 is [45.56962025 97.2137279  70.47794252 97.02489005 97.26733349 74.92355113
 75.24396633 74.99543134 76.11140215 96.32070759 76.37577515 98.52097319
 99.41399349 79.39474422 76.06632473 96.56680596 96.29512311 75.83606438
 98.65376884 98.30776915 79.08042056 76.8984296  98.38695922 76.04317686
 81.53165775 96.65086926 94.0778012  75.48640977 98.01293844 76.29536677
 97.30875598 98.57457877 96.36213009 98.02024829 85.23897126 75.94571216
 75.81047989 87.77427176 97.11504489 73.10217955 78.1240482  92.05906361
 75.23300155 74.7944104  96.9627563  93.87434364 98.02877645 98.57336046
 92.59633776 85.18414737 85.55329492 98.55508583 98.99976852 75.30731838
 98.70615611 75.76540247 70.76302677 91.93114119 96.24273583 96.9067141
 89.79300934 97.17717864 92.13338044 75.80804327 98.42838172 76.40135963
 98.20786784 75.01005105 76.76076071 97.41353054 77.30656303 95.99054592
 76.71933821 95.45083515 75.24274802 80.73732045 85.78355527 76.35993714
 77.33702075 99.14718388]
Accuracy th:0.7 is [45.58423996 97.2137279  70.47794252 97.02489005 97.26733349 74.92355113
 75.24396633 75.33899441 76.11140215 96.46203141 76.37577515 98.52097319
 99.41399349 79.96734933 76.06632473 96.56680596 96.29512311 75.83606438
 98.65376884 98.30776915 79.55190604 76.8984296  98.38695922 76.6194369
 82.41127667 96.65086926 94.0778012  75.48640977 98.01293844 76.29536677
 97.30875598 98.57457877 96.36213009 98.02024829 85.50456257 75.94571216
 75.81047989 88.17387702 97.11504489 73.10217955 78.78315323 92.05906361
 75.23300155 74.7944104  96.9627563  93.87434364 98.02877645 98.57336046
 94.7198499  86.21361826 85.76406233 98.55508583 98.99976852 75.30731838
 98.70615611 75.76540247 70.76302677 92.33805631 96.24273583 96.9067141
 89.79300934 97.17717864 92.30272536 75.80804327 98.42838172 76.40135963
 98.20786784 75.01005105 76.76076071 97.55607266 77.30656303 95.99054592
 76.87771835 95.45083515 75.24274802 80.83600346 85.98823114 76.35993714
 77.33702075 99.14718388]
Avg Prec: is [55.87832088  2.96569687 11.05571471  3.37310292  2.2227621   3.74374382
  3.35496359  5.56688225  2.42086494  3.76392439  1.52917339  1.60442623
  0.61720571  4.95678409  2.67907392  3.12778635  3.6775851   2.63947584
  1.42585636  1.67859349  1.91704847  0.85753055  1.95934655  2.39931486
  5.11554061  3.72003485  6.52922779  3.23944093  2.0629873   1.99425431
  2.60496699  1.3446682   3.7980685   1.59447819  2.37720763  2.41665637
  3.11135565  2.61536068  2.82937194  7.48751856  2.38205173  8.26500522
  3.38474886  4.15993669  3.25676027  6.35907764  2.08330223  1.62078928
  2.16798135  1.72138613  1.81401495  1.62519699  1.11289171  3.13941782
  1.41178676  2.66161541 11.10197746  3.57918669  3.92005292  2.78515604
 10.84664255  2.13698475  3.70991703  2.98892598  1.52169162  2.34714122
  1.75848459  4.14302993  1.33986657  2.39863422  0.2155346   3.31714128
  1.93783388  4.52121167  3.87632914  3.0885437   0.8355765   1.81769471
  0.14139574  0.71285174]
mAP score regular 15.09, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [89.57570322 97.22450607 92.61778409 96.96290206 97.90716795 96.63651992
 96.80843112 94.65331241 97.38894287 96.43221965 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43314149 96.52938685 94.2571692  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.40481351 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.82955876 97.82744101 93.04133343
 97.07750953 96.48703192 97.03764606 93.72897825 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.22505419 96.39235618 96.16314124 96.78102499
 92.58041209 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.14570097
 98.03174129 95.44559882 96.16314124 97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [88.7759424  97.22450607 92.29887635 96.96290206 97.90716795 96.63651992
 96.80843112 94.88003588 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.34935346 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.75481476 97.82744101 92.37362035
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.03071978 96.39235618 96.16314124 96.78102499
 92.36116302 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.84672497 97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [97.29978371  5.25880946 66.73638666 10.70244977 16.2936501  15.36198186
 11.66922971 33.01955615  6.54773662 29.06535106  2.44720752  7.76951106
  0.66312362 15.98373376  9.78910889 21.48870252 12.06619059  8.39294991
  1.08197117  2.02420169  1.35298011  0.49799975  1.18494117  9.72270065
 18.73965451 13.22520368 30.38909414 18.28474131  5.51732039  1.51689972
 38.45685031  0.80684669 36.13533009  1.22305516  6.19149717 20.42393971
  7.7356049  46.88812998 24.66735037 34.88995065  4.8885028  48.04412871
 20.34133178 28.55215221 20.42673914 34.7149024   3.41411769  1.68884362
 14.69089571  2.68413752  1.17449895  1.2668645   1.38397806 14.85973778
  1.35839362  4.90796232 59.41929373 25.06835231 11.39014089 12.74062533
 65.52502638  9.14051663 28.01628654 10.64468554  4.43217707  5.94111671
  3.78743119 11.97940151  2.30118193  8.74728495  0.41001787 45.40061814
  4.29366579 25.77339501 36.46142999  5.34720573  1.32946341  3.47489643
  0.20221548  0.82580352]
Accuracy th:0.5 is [45.7956499  97.22450607 68.34840671 96.96290206 97.90716795 73.55058923
 73.66270523 72.99499215 75.16007674 96.40979645 75.25475247 98.5325261
 99.34972718 76.76458131 75.26222687 96.31262924 96.21047911 74.63437726
 98.78167277 98.34068316 77.15574159 75.90004235 98.31327703 74.9757082
 77.33263572 96.52938685 94.3393876  74.75645913 97.81747515 75.23731221
 97.52597354 98.67204823 96.39983058 98.18870369 85.52457832 74.89847273
 74.9009642  90.02665869 97.0276802  72.2500436  75.95734609 92.37362035
 74.00154471 73.76485537 97.03764606 94.02795426 98.18621222 98.77668984
 93.72897825 85.21812791 83.83287241 98.55993223 98.87385704 73.97663004
 98.6969629  74.71908713 69.23536886 93.0438249  96.16314124 96.78102499
 90.13379176 97.04761193 92.05222114 74.803797   98.32075143 75.67581035
 98.13139996 74.01151058 75.97229489 97.47863567 76.31113436 96.07843137
 75.78543489 95.44559882 73.91932631 81.59055236 87.76440691 75.34444527
 76.38587837 99.15040985]
Accuracy th:0.7 is [45.99496724 97.22450607 68.34840671 96.96290206 97.90716795 73.55058923
 73.66270523 73.23915589 75.16007674 96.41976231 75.25475247 98.5325261
 99.34972718 77.22301119 75.26222687 96.31262924 96.21047911 74.63437726
 98.78167277 98.34068316 77.53444453 75.90004235 98.31327703 75.21239754
 77.9555024  96.52938685 94.3393876  74.75645913 97.81747515 75.23731221
 97.52597354 98.67204823 96.39983058 98.18870369 85.86840073 74.89847273
 74.9009642  90.2733139  97.0276802  72.2500436  76.49799437 92.37362035
 74.00154471 73.76485537 97.03764606 94.02795426 98.18621222 98.77668984
 95.60505269 85.76375912 83.97488602 98.55993223 98.87385704 73.97663004
 98.6969629  74.71908713 69.23536886 93.34529237 96.16314124 96.78102499
 90.13379176 97.04761193 92.23160675 74.80628846 98.32075143 75.67581035
 98.13139996 74.01151058 75.97229489 97.52846501 76.31113436 96.07843137
 75.85519595 95.44559882 73.91932631 81.69768543 87.97369011 75.34444527
 76.38587837 99.15040985]
Avg Prec: is [54.4234004   3.76058902 15.00566259  4.56991785  1.54143487  4.28631638
 10.20699997  8.7998512   6.95462118  5.19473777  2.2789611   5.3260979
  1.56008216  5.90235149  2.99386394  4.23051491 25.41198831  6.04087853
  1.56228767  2.95785764  3.6919233   1.41653284  1.49060784  5.43518456
  5.78467608 13.59020482  8.32403726  4.50114494  3.88398954  7.20733231
  2.31009449  0.8658855   3.13909608  1.09083226  1.70542388  2.41069697
  2.04179442  2.18994785  2.26476871  6.22100537  1.73564209  6.05087505
  2.2282637   2.74166671  2.37480137  4.85284559  1.76809357  1.03853993
  1.41136572  1.1783775   1.19784773  0.98018481  0.73842871  2.34084512
  0.86620083  1.84354289 10.13030598  2.99007476  3.80930367  2.77656964
  7.89925909  2.03274615  3.2165892   2.61334919  1.36347481  1.86359692
  1.56516653  3.47413461  1.06695985  2.18596565  0.19342255  3.1400818
  1.56427013  3.93311677  3.51971828  2.30824137  0.59140885  1.4990447
  0.12692105  0.58659636]
mAP score regular 15.98, mAP score EMA 4.38
Train_data_mAP: current_mAP = 15.09, highest_mAP = 15.93
Val_data_mAP: current_mAP = 15.98, highest_mAP = 16.57
tensor([0.3373, 0.3443, 0.3372, 0.3939, 0.3979, 0.3681, 0.3806, 0.3847, 0.3741,
        0.3870, 0.3848, 0.3925, 0.3726, 0.3774, 0.3940, 0.3534, 0.3902, 0.4107,
        0.3912, 0.3949, 0.3904, 0.3883, 0.3906, 0.3914, 0.3702, 0.3819, 0.3781,
        0.3979, 0.3662, 0.3513, 0.3457, 0.3668, 0.3840, 0.3447, 0.3777, 0.3706,
        0.3629, 0.3828, 0.3564, 0.3681, 0.3922, 0.3828, 0.3883, 0.3748, 0.3832,
        0.3871, 0.3995, 0.3896, 0.3829, 0.3986, 0.3945, 0.3559, 0.3839, 0.3966,
        0.3982, 0.3830, 0.3793, 0.3815, 0.3295, 0.3995, 0.3746, 0.3772, 0.3824,
        0.3767, 0.3958, 0.3904, 0.3805, 0.3879, 0.3923, 0.3924, 0.3884, 0.3765,
        0.3906, 0.3712, 0.3987, 0.3366, 0.3836, 0.3559, 0.3897, 0.3891],
       device='cuda:0')
Max Train Loss:  tensor([ 8.2976,  7.0685,  8.7176,  5.0460,  7.6452,  8.8832,  8.9999,  9.8411,
         6.1796,  6.8567,  4.6098,  5.3707,  3.2189,  7.2491,  5.4413,  9.1158,
         8.0897, 10.7073,  7.4003,  8.2156,  4.8139,  4.2931,  7.3416,  7.9873,
         8.4618,  6.9095, 10.3682,  7.3321,  5.1837,  6.3420,  6.6940,  8.0048,
         5.9348,  5.2009,  5.7439,  4.6357,  5.8907,  6.8029,  5.2640, 12.6390,
         3.9240, 10.2273,  6.7976,  7.5497,  6.4445,  9.3142,  3.7817,  4.3843,
         5.3125,  5.1198,  6.6685,  5.0038,  5.8329,  5.2978,  6.7625,  8.7554,
         8.9100,  6.5210,  8.9324,  6.8491,  6.8229,  7.7806,  7.1243,  5.6466,
         4.2072,  5.9720,  7.4778,  6.9310,  8.7781, 11.9780,  5.1762, 11.5644,
         8.8968,  8.7463,  9.7440,  7.2340,  5.6113,  5.9131,  5.7627,  4.1304],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [33/80], Step [000/642], LR 1.0e-04, Loss: 12.6
Max Train Loss:  tensor([10.8952,  7.7261, 13.3468,  9.2538, 13.7854,  8.8469,  7.2608, 11.9367,
         5.8726,  9.8601, 16.2152,  9.1087, 13.2056,  9.0064, 12.1406,  6.5268,
         9.7924,  6.5898,  6.1909,  6.3269, 11.7776, 13.9130,  6.3997, 11.7240,
        15.0904,  9.2965, 11.1280,  5.5555,  6.6732,  7.2495,  6.9899, 12.0795,
         8.0311,  8.2118,  6.9767,  6.2087, 10.0978,  5.5214,  8.4014, 10.4613,
        15.5002, 14.5908,  9.1401,  9.2296, 10.6715, 10.8640,  7.1496, 14.6107,
        13.3211,  8.9892, 10.8358,  8.3322,  4.7981,  9.6535,  7.0208,  6.4189,
        15.0638,  7.3006,  6.0084,  9.6974, 12.5301, 10.0812, 10.6960,  5.7089,
        12.4244, 13.6447,  4.2935, 11.4653, 15.3069,  7.0667,  7.7157,  9.1348,
        11.6528,  8.0764, 15.2298,  6.5932,  5.4299,  4.7726,  8.8841, 10.0508],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [33/80], Step [100/642], LR 1.0e-04, Loss: 16.2
Max Train Loss:  tensor([11.3977,  5.1092, 10.0159,  9.7015,  4.7779,  8.1810,  9.9712,  8.8682,
         5.4618,  8.9696,  4.8420,  6.8939,  6.0937,  9.2791, 10.4781,  5.6172,
         7.6599,  6.7757,  5.2465,  7.4159,  8.8162,  5.2260,  8.2754,  6.7601,
        10.7624,  8.7926, 10.2537,  7.6891,  6.1035,  4.7359,  9.5299,  4.7096,
         6.7744,  7.2215,  5.1621,  5.0850,  8.8043,  5.3641,  7.3361,  9.2760,
         7.1721,  8.4948,  9.1863,  8.9898, 10.0879,  9.4055,  9.2081,  8.1769,
        10.1529,  8.0705,  6.5026,  6.1276,  4.7734, 10.2438,  7.6523,  5.2266,
         8.0801,  9.1154, 10.9960,  6.1327, 10.7625,  8.1400,  6.2201,  8.1400,
         7.5357,  9.8980,  3.9469,  9.8552,  6.6967,  7.1675,  7.1920,  7.3680,
         8.8044,  7.6402,  7.2613,  9.3028,  4.2871,  4.3586,  8.3145,  8.1451],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [33/80], Step [200/642], LR 1.0e-04, Loss: 11.4
Max Train Loss:  tensor([12.1936,  2.7998,  7.0036,  7.3110,  4.6355,  5.5039,  5.5502,  7.3290,
         5.7855,  8.3321,  3.9838,  4.9001,  6.0027,  8.3597,  8.6481,  7.8444,
         9.5155, 11.0422,  6.7882, 12.5618,  4.6841,  7.1665,  7.5211,  7.5900,
         9.7052,  4.8128,  6.7331,  9.8641,  7.0104,  4.7643,  8.5550,  5.8152,
         7.4913,  4.5578,  6.3009,  7.1497,  6.8454,  6.7516,  4.6845, 14.5051,
         8.0251,  8.7079,  7.4613,  7.4339,  6.6351, 10.9316,  5.2129,  7.0055,
         6.9511,  7.0319,  4.7589,  3.8808,  3.4993,  7.2858,  6.6280,  6.7475,
         9.9539,  8.0694,  5.7275,  7.2879,  7.6622,  7.7573,  9.1466,  4.8566,
         7.3830,  8.7050,  4.8501,  7.4339,  7.2123,  7.3162,  7.1474, 10.0806,
         8.9963,  9.0534, 10.7153,  5.1210,  4.2356,  6.6564,  8.2862,  6.4785],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [33/80], Step [300/642], LR 1.0e-04, Loss: 14.5
Max Train Loss:  tensor([11.3338,  4.4888,  7.5554,  6.4601,  6.5063,  4.7848,  4.3044,  9.5145,
         6.2630,  6.0671,  5.8556,  7.3485,  5.9586,  8.4100,  6.9927,  4.9726,
         8.9537,  4.9138,  4.3009,  9.7142,  6.8760,  3.7898,  6.5701,  8.4781,
         8.4727,  5.7982,  7.0861,  5.3417,  6.0310,  6.1567,  9.0263,  7.1569,
         5.9219,  4.8831,  5.0520,  5.4539,  6.8287,  6.1914,  5.3126,  7.4211,
         8.3848,  9.6191,  7.6681,  8.1806,  8.3105, 12.1368,  8.8944,  7.4226,
         5.5224,  6.8558,  4.3798,  5.1025,  8.5423,  7.3342,  4.9722,  4.4716,
         8.6790,  5.9645,  9.0947, 10.6663,  9.4383,  6.1169,  9.6297,  8.5384,
         6.7278,  5.7709,  5.5427,  8.2598,  5.8148,  7.4096,  7.1044,  8.7730,
         7.1318,  7.3630,  8.0481,  9.4808,  5.8386,  4.1979,  8.2471,  7.1940],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [33/80], Step [400/642], LR 1.0e-04, Loss: 12.1
Max Train Loss:  tensor([11.1344,  5.8413, 10.5639,  6.2244, 10.0536,  8.3118,  8.0507,  7.9089,
         9.6263,  6.1162,  3.0896,  3.2414,  6.6938,  7.5047,  7.7851,  6.4277,
         5.4833,  3.8008,  6.6594,  4.7903,  5.0190,  4.7171,  4.1641,  4.9663,
         9.1983,  7.2635,  9.9104,  9.6412,  7.9116,  3.8978,  7.7272,  7.6294,
         7.9283,  4.7430,  6.1794,  5.2079,  6.5577,  7.5414,  6.5394,  9.7175,
         5.4152, 12.4414,  5.9106,  8.8520,  8.0771, 12.5049,  6.7027,  7.7992,
         5.5389,  7.4070,  5.4955,  7.3933,  5.3549,  5.8301,  7.1406, 10.1748,
        10.2246, 10.6769,  7.9209,  4.9175, 11.6684,  7.3964, 10.8045,  5.2794,
         6.8006,  8.8018,  3.8394,  8.4421,  9.1720, 10.9119,  7.2526,  9.5338,
        10.2205, 10.0564, 10.2069,  7.0704,  4.3333,  4.3604,  8.9975,  6.5900],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [33/80], Step [500/642], LR 1.0e-04, Loss: 12.5
Max Train Loss:  tensor([10.8091,  4.7156,  7.6397,  6.8035,  5.4517,  5.7540,  7.9622,  8.9241,
         9.3542,  7.8137,  3.1368,  3.2730,  6.1383,  6.9688,  8.4044,  6.6377,
         9.5823,  4.7992,  5.8754,  8.6522,  9.1173,  4.8305,  6.5133,  6.6130,
         8.3397,  7.4709,  9.1076,  4.8715,  4.6420,  6.2435,  7.8471,  4.0110,
         8.5127,  7.7239,  4.9694,  6.5543,  5.0919,  8.6073,  7.0295,  9.9265,
         9.9517, 11.8757,  8.2635,  7.6510,  7.4137,  7.9021,  4.5430,  5.0916,
         5.7138,  7.0908,  5.6819,  3.1879,  4.5968,  8.3716,  5.1489,  6.4688,
         8.8452,  8.0291,  6.9272,  6.2988,  9.8223,  6.0964,  5.6138,  5.8089,
         6.2196,  5.2299,  3.0994, 10.9122,  7.4796,  6.2521,  7.2747,  7.8433,
         8.1405,  8.5401,  9.1711,  4.2208,  5.1734,  4.9443,  8.4138,  7.9905],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [33/80], Step [600/642], LR 1.0e-04, Loss: 11.9
Max_Val Meta Model:  tensor([ 19.1785,  21.3911,  26.6389,  19.4351,   4.2339,   5.9965,   6.2737,
          6.7801,   4.3074,   5.3780,   4.0707,   4.8724,   6.8638,   8.1263,
          5.7600,   4.6840, 144.1469,   3.9888,   4.2842,   4.8601,   4.4331,
          3.9603,   4.3310,   4.7703,   9.8490,   8.4575,  10.7625,   3.9673,
          4.7314,   4.9849,   4.2177,   4.0225,   6.0507,   3.9629,   4.3686,
          4.5190,   4.2886,   6.7334,   3.9949,  15.5086,   8.0013,   8.8718,
          5.7867,   7.9553,   6.6127,   8.4294,   2.8346,   6.0019,   4.2436,
          7.0839,   3.5341,   3.1690,   5.3069,   3.8376,   3.2179,   4.6632,
          9.6416,   7.0530,   8.0483,   4.9380,   6.8298,  10.2573,   4.2960,
          2.5700,   6.2626,   3.8019,   3.0798,   5.7787,   8.2860,   9.5296,
          7.3449,   8.1784,  10.4033,   5.6645,   6.3677,   5.3833,   4.3814,
          4.0697,   8.4885,   6.6554], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 15.7726,  20.1013,  20.8240,  18.5919,   4.7396,   7.1114,   7.5850,
          9.0619,   5.0189,   5.9825,   4.7381,   5.4906,   7.9094,   8.7812,
          5.6182,   5.0713, 137.3919,   4.5475,   4.8613,   5.7816,   4.9194,
          4.4771,   4.7284,   4.5480,  10.3522,   9.6649,  11.8543,   4.9922,
          5.4737,   5.5131,   4.8969,   4.7928,   7.0280,   4.6698,   5.2979,
          5.2463,   5.3443,   5.9885,   4.8090,  15.0523,   8.5962,   8.6460,
          6.3646,   8.5042,   6.8124,   8.3165,   3.3716,   6.6852,   5.0143,
          7.9705,   4.1684,   3.8693,   5.8891,   4.0588,   3.7625,   5.4902,
         10.4380,   7.3966,   8.7034,   5.6444,   6.6989,  10.0040,   5.1266,
          3.1906,   7.2123,   4.6120,   3.7513,   6.8965,   8.9942,   9.4368,
          8.3236,   8.9956,  11.2151,   6.6861,   6.6726,   5.7545,   5.1822,
          4.7638,   9.5376,   7.5937], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 46.7570,  58.3838,  61.7538,  47.2022,  11.9113,  19.3175,  19.9281,
         23.5535,  13.4149,  15.4578,  12.3125,  13.9882,  21.2258,  23.2700,
         14.2594,  14.3499, 352.1137,  11.0721,  12.4274,  14.6406,  12.6013,
         11.5314,  12.1044,  11.6194,  27.9635,  25.3092,  31.3495,  12.5457,
         14.9457,  15.6943,  14.1665,  13.0661,  18.3034,  13.5487,  14.0269,
         14.1561,  14.7247,  15.6448,  13.4944,  40.8865,  21.9186,  22.5872,
         16.3900,  22.6909,  17.7776,  21.4852,   8.4397,  17.1574,  13.0953,
         19.9970,  10.5661,  10.8715,  15.3393,  10.2341,   9.4487,  14.3364,
         27.5158,  19.3875,  26.4155,  14.1295,  17.8835,  26.5197,  13.4067,
          8.4690,  18.2223,  11.8126,   9.8592,  17.7804,  22.9289,  24.0512,
         21.4292,  23.8932,  28.7117,  18.0137,  16.7359,  17.0966,  13.5110,
         13.3851,  24.4766,  19.5156], device='cuda:0')
Outer loop valEpocw Maximum [33/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 144.1
model_train val_loss valEpocw [33/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 137.4
model_train val_loss  valEpocw [33/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 352.1
Max_Val Meta Model:  tensor([29.6081, 28.8766, 32.1564, 34.7350, 33.9365, 31.6654, 35.8958, 32.8060,
        32.0018, 32.9433, 32.8842, 40.4853, 31.8016, 33.8919, 34.9006, 34.4698,
        33.7209, 32.7735, 34.7891, 36.5812, 33.1039, 33.2972, 32.5829, 33.3440,
        31.8830, 33.0093, 32.2649, 33.0892, 30.9806, 29.7380, 29.9139, 31.8209,
        32.7530, 29.2203, 31.8309, 31.3697, 30.8798, 33.0570, 30.3635, 31.3711,
        32.5312, 32.1484, 32.7726, 32.4030, 31.7138, 31.4644, 32.4995, 32.5736,
        32.2519, 33.6612, 33.3948, 30.3741, 32.6956, 33.4389, 33.1776, 32.2131,
        31.1044, 32.8467, 27.9439, 33.2099, 31.9219, 32.9252, 32.6000, 32.0412,
        33.5116, 33.1062, 31.9732, 31.7607, 33.1970, 31.6111, 31.9091, 32.5943,
        33.7258, 31.8898, 32.3571, 28.5495, 32.1237, 30.2338, 33.0437, 33.4351],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([18.7031,  7.2638, 18.3259,  5.8908,  7.0886, 12.8291, 30.2557, 17.7034,
        13.3280, 12.8604,  6.7531, 61.0061,  7.3878,  5.5029,  6.9980,  3.2615,
         4.8011,  7.6740,  4.3393,  9.0387,  3.9759,  3.2960,  4.1720,  6.9258,
         9.1796,  9.1082, 11.0115,  6.1158,  5.2112,  2.5629,  3.4722,  3.4766,
         3.4494,  3.4372,  3.2767,  3.2332,  4.6223,  3.5257,  3.0639,  2.7585,
         4.7324,  1.8403,  2.9065,  2.5250,  2.2576,  2.0145,  2.0355,  4.2041,
         3.3235,  6.2033,  2.8374,  2.6697,  3.0883,  2.1707,  2.2366,  4.8026,
         4.1387,  3.4911,  4.6499,  2.7391,  2.3547,  3.1570,  2.3276,  0.9532,
         5.5072,  2.6945,  2.5361,  5.8510,  5.1475,  1.9618,  6.3361,  2.6072,
         6.2344,  2.6177,  5.5535,  2.3298,  3.7174,  2.8696,  7.4022,  5.6360],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 56.8390,  21.1553,  53.4724,  14.7851,  18.4811,  34.6127,  78.9307,
         45.8341,  35.2269,  33.0614,  17.4913, 154.4873,  19.9571,  14.1072,
         17.4747,   9.0644,  12.2965,  18.9188,  10.8392,  22.3178,  10.2318,
          8.4215,  10.7056,  17.6351,  24.6751,  23.6842,  29.0634,  15.3916,
         14.3392,   7.2376,   9.9132,   9.3531,   8.9583,   9.7912,   8.7273,
          8.7109,  12.7149,   9.1082,   8.5576,   7.5378,  12.1896,   4.7702,
          7.4687,   6.6496,   5.8824,   5.1794,   5.1008,  10.8604,   8.7070,
         15.4298,   7.1697,   7.5016,   7.9380,   5.4617,   5.5941,  12.4664,
         10.9148,   9.1118,  14.0874,   6.8692,   6.3227,   8.2498,   6.1480,
          2.4912,  13.7363,   6.8594,   6.6727,  15.3219,  13.1827,   5.1021,
         16.7640,   6.8703,  15.8466,   6.9920,  14.1235,   6.8816,   9.6859,
          8.0453,  19.2683,  14.4529], device='cuda:0')
Outer loop valEpocw Maximum [33/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 40.5
model_train val_loss valEpocw [33/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 61.0
model_train val_loss  valEpocw [33/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 154.5
Max_Val Meta Model:  tensor([28.8248, 28.7829, 29.7196, 33.1515, 29.8515, 31.3905, 32.1732, 34.4089,
        33.4303, 33.5772, 33.8909, 31.7224, 31.8236, 33.1867, 33.1975, 32.1336,
        34.2536, 32.3921, 33.3517, 34.0372, 34.3311, 33.9835, 32.7822, 34.3832,
        31.6435, 34.4701, 32.0952, 32.7724, 30.9864, 29.7176, 29.9171, 31.8228,
        33.1814, 29.1274, 31.9061, 31.3695, 30.9541, 31.3851, 30.3798, 31.5353,
        32.8549, 33.2278, 34.2543, 32.8737, 32.5110, 34.2038, 34.0755, 32.7249,
        33.3560, 35.4163, 34.6684, 30.6027, 33.1192, 33.6436, 33.8662, 33.3149,
        33.6013, 32.9351, 27.9886, 33.0049, 34.1654, 32.9565, 32.8146, 32.0537,
        33.5223, 33.0859, 32.0947, 31.8083, 33.3286, 31.6593, 32.1440, 32.7811,
        33.7133, 32.0442, 32.1954, 28.5901, 32.1944, 30.2635, 33.2303, 33.4397],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 5.4620,  3.3769,  1.7953,  2.9571,  1.6996,  3.6788,  2.3793,  4.9972,
         3.8792,  1.2795,  3.0925,  2.3877,  6.0862,  6.3336,  1.8727,  3.7571,
         1.8746,  2.5791,  2.7715,  5.0588,  3.1011,  3.4038,  3.1683,  1.7001,
         3.1783,  2.9640,  9.1858,  6.1752,  2.8763,  3.3476,  4.2539,  5.1040,
         6.0819,  3.6593,  4.4576,  3.9061,  5.9111,  4.8237,  4.2687, 17.4457,
        12.7808, 28.1409, 28.4719, 29.0098, 20.0064, 26.6041,  6.2152,  7.6688,
        16.5687,  8.5258, 14.4060, 16.9955, 22.5190, 12.3323, 25.9183, 39.2471,
        33.7944, 11.2181,  6.8457,  7.3650, 27.4214,  4.9052,  8.5215,  5.8343,
         8.9563,  5.2377,  5.2024, 10.0328,  7.1710,  3.6479,  7.4025,  8.1292,
         7.2470, 13.9747,  2.2873,  5.7473,  6.4130,  3.6988,  8.8158,  7.0194],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 16.5277,   9.8527,   5.2611,   7.5033,   4.4486,   9.9585,   6.2732,
         12.6551,  10.1514,   3.2664,   7.9000,   6.1572,  16.4417,  16.4334,
          4.7781,  10.4455,   4.7889,   6.4155,   7.0501,  12.7340,   7.9029,
          8.6625,   8.1735,   4.3031,   8.5714,   7.6425,  24.2916,  15.6707,
          7.9103,   9.4761,  12.1614,  13.7587,  15.7884,  10.4719,  11.8622,
         10.5374,  16.2579,  12.8135,  11.9343,  47.7063,  32.8776,  73.1812,
         72.2133,  76.6434,  52.1764,  67.9781,  15.3765,  19.7829,  43.0492,
         20.9141,  36.0097,  47.7863,  58.0289,  31.0649,  64.9852, 102.0873,
         89.7923,  29.3138,  20.7667,  18.5363,  73.6677,  12.8421,  22.4535,
         15.3230,  22.4240,  13.3656,  13.6788,  26.2703,  18.3332,   9.5185,
         19.4764,  21.4453,  18.4591,  37.4242,   5.8396,  17.0145,  16.7278,
         10.3773,  22.8620,  18.0281], device='cuda:0')
Outer loop valEpocw Maximum [33/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 35.4
model_train val_loss valEpocw [33/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 39.2
model_train val_loss  valEpocw [33/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 102.1
Max_Val Meta Model:  tensor([31.3641, 28.4907, 29.1631, 32.4569, 29.6234, 31.3241, 32.4281, 32.3292,
        32.9880, 33.6097, 31.9354, 31.4514, 32.0098, 32.5364, 33.0676, 32.0079,
        32.9445, 32.5215, 33.0266, 31.0586, 32.8663, 32.4703, 31.0715, 32.0289,
        31.5356, 32.6916, 31.9314, 32.6311, 31.0298, 29.3051, 29.3151, 31.1483,
        30.4809, 28.4972, 31.9889, 31.3238, 30.4633, 31.6687, 30.0975, 31.2674,
        32.3429, 30.3386, 33.0066, 31.7004, 31.3831, 31.4782, 32.1302, 32.7439,
        31.1117, 32.4316, 31.9707, 30.0465, 30.9680, 31.6503, 32.6220, 31.7190,
        31.0370, 33.1260, 27.7091, 32.1051, 32.1208, 32.6594, 32.3694, 30.9883,
        32.7852, 32.3991, 31.7626, 34.0208, 33.3476, 31.1573, 33.2074, 32.6212,
        32.5031, 31.4142, 36.1398, 28.2798, 31.7724, 30.2166, 33.5910, 31.5164],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 9.5405,  4.2989, 13.4258,  4.6509,  8.9713,  7.0860,  8.3547, 11.4704,
         3.5279,  8.7456,  3.9611,  6.1907,  6.7734,  5.8248,  8.6860,  3.5454,
         2.4716,  4.1179,  5.0126,  4.7264,  4.8848,  3.9908,  5.8209,  8.7851,
         7.4319,  5.0399,  8.2106,  2.4544,  5.5993,  2.9427,  3.4728,  3.9557,
         2.8984,  3.4453,  3.1051,  3.0069,  3.5557,  3.1620,  3.0925,  3.3815,
         5.4013,  2.1558,  3.7151,  3.2412,  2.9297,  2.1407,  2.6956,  5.0879,
         3.7799,  6.3569,  3.3634,  3.0745,  3.4979,  2.7458,  2.7624,  4.5361,
         2.1527,  4.0997,  5.9375,  4.3438,  3.4430,  5.7150,  2.6806,  1.4668,
         6.1744,  2.9076,  2.9966,  4.2782,  6.0134,  1.5203,  7.4889,  4.7462,
         6.0604,  3.6291, 78.5515,  3.5475,  4.3050,  4.0953,  8.5773,  6.2592],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 28.0121,  12.5976,  40.3239,  11.8719,  23.6290,  19.2182,  21.8204,
         29.8583,   9.3188,  22.2736,  10.3689,  16.0689,  18.0835,  15.3704,
         22.2222,   9.8465,   6.3748,  10.0770,  12.8314,  12.3377,  12.5689,
         10.3552,  15.1954,  22.8508,  20.0039,  13.2636,  21.7176,   6.1735,
         15.2613,   8.3703,  10.0489,  10.8029,   7.7501,  10.0037,   8.1768,
          8.0592,   9.8374,   8.4093,   8.6573,   9.2139,  13.8480,   5.7697,
          9.4925,   8.6663,   7.6627,   5.5575,   6.8418,  13.0041,  10.0082,
         16.2269,   8.6561,   8.6693,   9.3293,   7.0664,   6.9623,  11.8793,
          5.7494,  10.6870,  18.0985,  10.9610,   9.1432,  15.0417,   7.0638,
          3.9223,  15.6726,   7.5094,   7.8758,  11.0080,  15.2592,   3.9893,
         18.9718,  12.5914,  15.5780,   9.7798, 201.7787,  10.5075,  11.2720,
         11.4158,  21.9016,  16.4409], device='cuda:0')
Outer loop valEpocw Maximum [33/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 36.1
model_train val_loss valEpocw [33/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 78.6
model_train val_loss  valEpocw [33/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 201.8
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [89.65534046 97.2137279  92.09561287 97.02489005 97.26733349 96.5997003
 96.99808726 94.50786418 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.15699126 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.26246025 97.11504489 93.00934443 97.84237521 92.73400665
 96.90915072 96.22689782 96.9627563  93.93525907 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 92.10779596 96.1964401  96.24273583 96.9067141
 92.42821116 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.4715464  96.2524823  96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [87.75112389 97.2137279  91.19771933 97.02489005 97.26733349 96.5997003
 96.99808726 94.72837807 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.18083357 97.11504489 92.74253481 97.84237521 92.38069712
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.17700808 96.13796128 96.24273583 96.9067141
 91.46209232 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15623591 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [97.3798234  10.66010428 62.0107522  10.39163685 11.68337103 13.84384364
  7.96838186 34.19148571  3.27600678 29.53505439  2.19201054  4.64902029
  1.30389356 16.45277562  8.15248585 24.60629883 10.46585839  5.52054922
  4.60626832  1.56953418  7.30361164  1.14008444  7.08731226 17.61156722
 19.31421024 10.73769721 31.65453061 11.91728126  5.1729141   1.4668583
  3.71345301  0.87597911 21.12909406  2.45762214 10.4672394  24.89020389
  7.71711331 37.87224604  4.55207748 38.47077983  4.14895191 47.39880344
 26.90296633 21.02150773 25.78808981 35.34581776  3.35177572  1.70811011
  4.10901942  1.59855026  1.28785901  1.12955624  1.10083203 15.7074727
  2.07777889  3.44102904 65.39622077 30.93905631 15.68677335 11.15968006
 64.86963455 15.16282881 20.35896303 12.13094088  3.18239838  8.89305482
  3.25266542 11.56974163  2.33019842 12.94898346  0.33340151 32.72499464
  3.92661713 26.49322731 28.49069017  6.03048214  1.10820487  2.43960406
  0.179774    0.80635505]
Accuracy th:0.5 is [45.97531706 97.2137279  70.21235121 97.02489005 97.26733349 74.79684702
 75.18061427 74.95279054 76.0212473  96.31217943 76.30998648 98.52097319
 99.41399349 79.25463871 75.90063474 96.56680596 96.29512311 75.69230394
 98.65376884 98.30776915 79.13646276 76.76441564 98.38695922 75.88845165
 81.46099585 96.65086926 94.0778012  75.46935344 98.01293844 76.20521192
 97.30875598 98.57457877 96.36213009 98.02024829 85.09033759 75.87992349
 75.61555049 87.43680023 97.11504489 73.14360205 78.06556938 92.05906361
 75.14528332 74.84070613 96.9627563  93.87434364 98.02877645 98.57336046
 92.86436569 85.21095016 85.70558351 98.55508583 98.99976852 75.29269868
 98.70615611 75.64844483 70.71673103 91.79834554 96.24273583 96.9067141
 89.79300934 97.17717864 92.20160573 75.64235329 98.42838172 76.21374009
 98.20786784 74.80050194 76.72177483 97.3940376  77.20422509 95.99054592
 76.59872565 95.45083515 75.07949465 80.76412324 85.79939328 76.21861332
 77.24930252 99.14718388]
Accuracy th:0.7 is [45.94485935 97.2137279  70.21235121 97.02489005 97.26733349 74.79684702
 75.18061427 75.32559301 76.0212473  96.47055957 76.30998648 98.52097319
 99.41399349 79.7821664  75.90063474 96.56680596 96.29512311 75.69230394
 98.65376884 98.30776915 79.5738356  76.76441564 98.38695922 76.53293698
 82.39543865 96.65086926 94.0778012  75.46935344 98.01293844 76.20521192
 97.30875598 98.57457877 96.36213009 98.02024829 85.34740074 75.87992349
 75.61555049 87.83153227 97.11504489 73.14360205 78.74294904 92.05906361
 75.14528332 74.84070613 96.9627563  93.87434364 98.02877645 98.57336046
 94.93183563 86.23676612 85.8992946  98.55508583 98.99976852 75.29269868
 98.70615611 75.64844483 70.71673103 92.23206345 96.24273583 96.9067141
 89.79300934 97.17717864 92.3782605  75.64235329 98.42838172 76.21374009
 98.20786784 74.80050194 76.72177483 97.55729097 77.20422509 95.99054592
 76.76076071 95.45083515 75.07949465 80.86280625 85.9687382  76.21861332
 77.24930252 99.14718388]
Avg Prec: is [55.63295691  3.06262834 11.35534808  3.40627055  2.23163571  3.75184329
  3.29841964  5.73178547  2.59754839  3.74214144  1.6403568   1.57347542
  0.60016253  5.2057692   2.63696761  3.16115104  3.72605737  2.67022354
  1.42369666  1.77582731  1.82540406  0.88044162  1.88728295  2.49964583
  5.12148457  3.56127324  6.57341204  3.26033267  2.1146621   1.87975046
  2.61564996  1.35410671  3.67284915  1.70218166  2.34379989  2.35683467
  3.17183088  2.56964428  2.78717394  7.37648442  2.33192731  8.37154047
  3.35480041  3.8714739   3.17279554  6.43823187  2.13847072  1.48932981
  2.14583542  1.62653045  1.78419286  1.51870011  1.09672161  3.04243542
  1.3425073   2.63537519 11.2092611   3.70920835  3.90126824  2.87070717
 10.7885046   2.09676171  3.90160831  3.20596198  1.62763866  2.45692961
  1.89771135  4.19985736  1.23908204  2.4196292   0.18340864  3.34425329
  1.90702001  4.67005311  3.82543129  3.094146    0.81761938  1.97209443
  0.14061709  0.70479802]
mAP score regular 15.03, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [89.85723896 97.22450607 92.48324489 96.96290206 97.90716795 96.63651992
 96.80843112 94.24720333 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88723622
 95.44310736 96.52938685 94.47641827 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.94047886 97.0276802  93.0737225  97.82744101 92.68256222
 97.07750953 96.48703192 97.03764606 93.94075292 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.01826245 96.60163939 96.16314124 96.78102499
 92.34870568 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.48297082 95.97628124 97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [89.43368961 97.22450607 91.84293794 96.96290206 97.90716795 96.63651992
 96.80843112 94.78536014 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.89563246 97.0276802  92.83454169 97.82744101 92.76727209
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.15778459 96.39235618 96.16314124 96.78102499
 92.23908115 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7694895  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [97.25452382 12.09105166 65.71054199 10.30175824  9.66894056 14.24242443
  8.49727787 33.2972403   4.37389313 32.43605614  2.0867334   5.2769633
  1.32454764 17.34584142  8.30266167 30.48130751 12.45151026  6.22292062
  6.9058842   1.68106493  7.62054411  1.4516232   9.46639297 18.2532559
 20.03644328 10.98844892 30.54664593 13.68343175  6.04615273  1.54145222
  3.95515809  0.78904349 25.3286168   2.71185778  9.91983098 28.42642501
  7.55627316 42.78957668  5.37107246 40.1878917   4.03999884 46.42410558
 28.25656908 23.46163664 25.58166434 34.68308874  3.20343058  1.63058971
  4.65412913  1.49075913  1.30667475  1.26542838  1.25292045 18.14426695
  2.04178466  3.35206695 58.40031598 32.44079704 14.07007972 12.06562995
 64.33469931 16.12463179 22.14052589 12.0721436   3.10815849  8.60434501
  3.05291749 12.58277555  1.81940908 16.76071191  0.27326202 35.71209454
  3.46843273 28.3988715  36.62958246  4.95856058  1.09538479  2.50772191
  0.1737464   0.74182138]
Accuracy th:0.5 is [45.76076936 97.22450607 68.17898697 96.96290206 97.90716795 73.37618656
 73.49826843 72.86543588 75.01557167 96.40979645 75.08533274 98.5325261
 99.34972718 76.70229464 75.102773   96.31262924 96.21047911 74.47492339
 98.78167277 98.34068316 77.04860852 75.73062262 98.31327703 74.84615193
 77.25540025 96.52938685 94.3393876  74.59700526 97.81747515 75.0728754
 97.52597354 98.67204823 96.39983058 98.18870369 85.38505618 74.72905299
 74.73154446 90.01420136 97.0276802  72.11052146 75.81284102 92.37362035
 73.83710791 73.6054015  97.03764606 94.02795426 98.18621222 98.77668984
 93.99058226 85.10102898 83.72324788 98.55993223 98.87385704 73.8072103
 98.6969629  74.54966739 69.09584672 93.0064529  96.16314124 96.78102499
 90.13379176 97.04761193 92.13942248 74.65680046 98.32075143 75.50639061
 98.13139996 73.84707377 75.79789222 97.47365274 76.13673169 96.07843137
 75.62348955 95.44559882 73.75987244 81.49587662 87.80925331 75.17502554
 76.2114757  99.15040985]
Accuracy th:0.7 is [45.9526123  97.22450607 68.17898697 96.96290206 97.90716795 73.37618656
 73.49826843 73.10959962 75.01557167 96.41976231 75.08533274 98.5325261
 99.34972718 77.13331838 75.102773   96.31262924 96.21047911 74.47492339
 98.78167277 98.34068316 77.42232852 75.73062262 98.31327703 75.12270474
 77.87079254 96.52938685 94.3393876  74.59700526 97.81747515 75.0728754
 97.52597354 98.67204823 96.39983058 98.18870369 85.73137006 74.72905299
 74.73154446 90.22597603 97.0276802  72.11052146 76.37092957 92.37362035
 73.83710791 73.6054015  97.03764606 94.02795426 98.18621222 98.77668984
 95.80187857 85.68652366 83.87771881 98.55993223 98.87385704 73.8072103
 98.6969629  74.54966739 69.09584672 93.30792037 96.16314124 96.78102499
 90.13379176 97.04761193 92.28143608 74.65929193 98.32075143 75.50639061
 98.13139996 73.84707377 75.79789222 97.52846501 76.13673169 96.07843137
 75.70570795 95.44559882 73.75987244 81.59055236 87.98614745 75.17502554
 76.2114757  99.15040985]
Avg Prec: is [54.4223924   3.76189626 14.98953739  4.57383465  1.51648115  4.30333358
 10.04867909  8.76820204  6.87734002  5.20221609  2.26931354  5.32268652
  1.55641269  5.88874024  2.96366505  4.24967104 24.88568206  5.94510806
  1.56933327  3.1279763   3.69766347  1.41730624  1.529382    5.31888522
  5.77001801 13.87832304  8.32031766  4.4394828   3.89284338  6.95271695
  2.29370801  0.87740002  3.0858358   1.11411495  1.69739773  2.41314293
  2.0507665   2.21732906  2.25096658  6.21121676  1.73630727  6.05598814
  2.21538733  2.73919327  2.37862545  4.84738535  1.74090734  1.02879861
  1.40749807  1.18718541  1.23119728  0.9807215   0.73769082  2.35216661
  0.86487829  1.84733885 10.120965    3.01830984  3.81399534  2.76550337
  7.93850118  2.01279773  3.23534419  2.61300945  1.3705993   1.8556959
  1.57083819  3.45803341  1.07668892  2.18727232  0.1932835   3.10386572
  1.57683308  3.92519536  3.53247796  2.317262    0.59114907  1.49744751
  0.12836637  0.58909155]
mAP score regular 15.74, mAP score EMA 4.37
Train_data_mAP: current_mAP = 15.03, highest_mAP = 15.93
Val_data_mAP: current_mAP = 15.74, highest_mAP = 16.57
tensor([0.3355, 0.3421, 0.3371, 0.3898, 0.3810, 0.3692, 0.3806, 0.3843, 0.3807,
        0.3913, 0.3814, 0.3868, 0.3715, 0.3820, 0.3917, 0.3594, 0.3869, 0.4042,
        0.3920, 0.3849, 0.3860, 0.3863, 0.3797, 0.3857, 0.3701, 0.3807, 0.3782,
        0.3947, 0.3646, 0.3526, 0.3474, 0.3676, 0.3730, 0.3464, 0.3769, 0.3709,
        0.3628, 0.3786, 0.3573, 0.3660, 0.3870, 0.3735, 0.3906, 0.3764, 0.3830,
        0.3875, 0.3964, 0.3884, 0.3758, 0.3948, 0.3884, 0.3557, 0.3791, 0.3888,
        0.3956, 0.3833, 0.3761, 0.3822, 0.3285, 0.3945, 0.3727, 0.3817, 0.3778,
        0.3772, 0.3968, 0.3898, 0.3800, 0.3900, 0.3919, 0.3815, 0.3857, 0.3788,
        0.3911, 0.3712, 0.3934, 0.3363, 0.3833, 0.3559, 0.3885, 0.3800],
       device='cuda:0')
Max Train Loss:  tensor([ 9.0736,  6.4150,  7.7907,  7.3126,  6.9582,  6.2158,  5.4058,  7.4299,
         5.5608,  5.1778,  3.0169,  5.6090,  6.0357,  6.0316,  5.3589,  6.7810,
         8.8479, 10.9092,  9.8505,  7.7559,  6.5468,  4.8254,  7.6095,  6.0932,
         7.4662,  7.5310,  9.5282,  6.0790,  3.6358,  6.8664,  5.5833,  6.5631,
         8.7278,  5.1339,  6.5026,  6.9801,  7.2408,  7.1443,  7.5029, 11.3193,
         9.0432,  9.0459,  7.6874,  6.3967,  5.9601,  7.4959,  5.7601,  5.7428,
         5.6062,  7.7022,  5.0712,  4.7232,  3.4931,  6.2810,  5.2615,  6.9932,
         9.5170,  9.5527,  8.3430,  7.9938,  9.1214,  6.9454,  6.1727,  5.4679,
         9.4694,  7.3076,  5.6762,  8.3162,  8.2345,  6.0072,  7.1460,  9.2377,
        10.4248,  7.2534,  6.6039,  4.2900,  4.2729,  3.2692,  8.3139,  6.3611],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [34/80], Step [000/642], LR 1.0e-04, Loss: 11.3
Max Train Loss:  tensor([15.0904,  7.3941, 14.0441,  6.4714,  6.1282,  6.2889,  6.0641, 13.9535,
        10.9748, 15.4244, 11.1665, 12.6201, 12.3330, 12.4822, 12.4180,  7.4209,
        10.2879, 10.1344,  8.4081, 15.2324,  6.0117,  7.1324,  8.0645, 12.9583,
         9.7394,  9.6684, 14.4314,  8.9811,  6.7552,  4.9553,  5.9894,  5.7994,
        10.9095,  6.3045,  7.8117,  6.5043,  8.9139, 12.8441,  8.3781,  9.0322,
        11.7285, 13.6471,  8.2714,  6.8710,  7.7320,  8.0381,  9.7607, 13.4138,
         7.9759,  7.1868, 12.0209,  5.0101, 13.3270,  7.5074, 15.1648, 12.6566,
        13.9000, 11.0961,  8.1932, 10.1946, 13.4441,  7.4704,  7.5747, 13.1482,
         8.1964,  7.1817, 10.8794, 10.9711, 10.4371, 11.3042,  4.7431,  7.8532,
        11.5150, 11.8840,  9.1533,  6.6464, 14.3287,  6.1904,  4.3654,  7.8713],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [34/80], Step [100/642], LR 1.0e-04, Loss: 15.4
Max Train Loss:  tensor([ 9.8967,  4.4868,  8.2483, 10.2199,  6.9069,  8.6454,  7.9649,  7.1846,
         7.4278,  7.3215,  7.7732,  7.7447,  3.7906,  8.0959,  9.2414,  6.5203,
         6.0668,  5.8014,  6.9707,  8.1831,  6.2077,  6.6795,  8.7710,  7.4205,
         6.2221,  7.9941,  8.8160,  7.6540,  5.4209,  3.8931,  5.7245,  6.5262,
         5.3865,  5.9032,  9.4122,  8.2461,  5.3784,  6.1572,  5.5514, 12.0275,
         7.1429, 11.2141,  8.3274,  6.5862,  7.7399, 10.4864, 12.0869,  5.6414,
         5.9479,  5.5885,  9.1268,  6.7437,  6.1979,  5.7135,  7.7282,  8.9984,
         6.9216,  6.4460,  4.7615,  8.7528,  8.5271,  9.6990,  7.5164, 10.0668,
         8.1937,  5.9503,  8.5939,  8.5560,  8.6168,  7.2094,  5.0553,  9.9897,
         5.3905,  6.6481,  8.8887,  6.5484,  6.8236,  3.1847,  4.9961,  8.1916],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [34/80], Step [200/642], LR 1.0e-04, Loss: 12.1
Max Train Loss:  tensor([10.9321,  6.0072,  9.0786,  7.5909,  6.5288,  6.9869,  7.0957,  9.9553,
         7.6232,  7.8211,  6.8947,  8.0913,  5.3953,  8.1932,  5.8525,  7.6819,
         8.1360,  7.1823,  7.6152,  6.8646,  6.7976,  6.4530,  5.7579,  5.6226,
         8.1700,  9.5343,  9.4455,  8.2683,  3.9563,  5.1031,  9.5265,  5.3625,
         8.9850,  6.0033,  7.0782,  5.9167,  6.6489,  9.7631,  6.1527, 10.7354,
         8.8047,  9.1666,  6.2380,  5.5906,  7.5100,  6.9990,  5.4317,  4.8571,
         7.0432,  5.7199,  7.3620,  4.3643,  7.1671,  4.2558,  6.4917,  8.5035,
        12.4828,  6.7572,  5.0384,  7.6601,  7.9021,  7.1964,  5.6799,  5.5391,
         7.0424,  5.2250,  6.7150,  7.4233, 10.0172,  6.9643,  4.3398,  9.1431,
         5.1289,  8.5614, 10.1236,  5.9420,  5.8000,  6.2074,  3.9669,  7.0433],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [34/80], Step [300/642], LR 1.0e-04, Loss: 12.5
Max Train Loss:  tensor([ 9.8085,  4.3995,  9.7910,  7.3076,  8.5610,  7.5636,  8.1711,  8.8624,
         8.0115,  7.2487,  6.8838,  4.3193,  4.1723,  7.9705,  8.2444,  7.4074,
         8.2584,  8.8281,  6.5799,  7.0004,  6.7918,  4.5406,  8.4538,  5.5376,
         9.3235,  4.9264,  7.7587,  5.2188,  4.8322,  6.7489,  8.1562,  7.7587,
         9.4448,  2.8514,  5.8233,  7.4417,  5.8266, 11.0177,  6.6540,  8.4735,
         6.3030,  9.5421,  4.8927,  7.8810,  6.4195,  9.7124,  6.4228,  4.8295,
         8.8404,  5.8196,  9.2915,  4.4719,  5.5326,  5.3258,  8.4540,  6.8003,
         8.6229,  8.1316,  7.1819,  7.5861,  8.9860,  6.8658,  5.4674,  4.8697,
         8.3975,  8.7642,  6.7770,  8.1682,  8.6498,  6.2551,  4.3235,  6.6882,
         5.1722,  6.7718,  9.3968,  5.5276,  5.0759,  4.4714,  3.9550,  7.6262],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [34/80], Step [400/642], LR 1.0e-04, Loss: 11.0
Max Train Loss:  tensor([ 8.4654,  5.6713,  6.2524,  6.1531,  6.0420,  5.9926,  8.0159,  6.0307,
         7.4214,  7.8600,  8.1613,  5.3887,  3.4411, 12.3936,  4.2958,  7.3483,
         7.9022,  7.3716,  5.9785,  5.7260,  9.5731,  4.5695,  8.4333,  7.0666,
         8.4425,  7.6339,  5.8217,  8.0015,  4.6541,  8.9008,  6.2721,  5.8412,
         9.5676,  6.9765,  7.1111,  7.2527,  8.7381,  7.1639,  7.8497, 10.5596,
         6.5183,  8.7635,  6.1243,  7.2963,  8.3149,  8.5082,  5.3695,  3.4001,
         6.7720,  5.8824,  6.2430,  3.5753,  6.3214,  7.0078,  6.4608,  9.2395,
         9.6599,  5.8018,  6.2603,  6.3510,  9.9780,  9.1900,  7.5887,  4.3129,
         1.9396,  5.8131,  7.5586,  5.2497,  3.9440,  6.2214,  5.0415,  8.6189,
         4.2822,  7.9513,  5.3452,  5.9184,  7.7411,  8.6231,  3.9847,  5.9794],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [34/80], Step [500/642], LR 1.0e-04, Loss: 12.4
Max Train Loss:  tensor([10.4076,  7.3005,  9.4435,  6.8907,  3.6747,  6.1400,  7.2863,  6.6427,
         7.8020,  5.5729,  9.6672,  6.4980,  3.5312,  7.3788,  6.2009,  7.3123,
         5.5792,  5.0320,  4.9409,  5.8877,  4.7249,  8.1912,  5.9909,  6.4861,
         8.7300,  8.1865, 13.0254,  8.0446,  6.9956,  9.2318,  6.5509,  6.7181,
         9.6793,  5.5646,  9.1766,  9.8001, 10.6390,  5.1975,  7.2224, 11.0315,
         6.5332, 10.8348,  6.3699,  6.5239,  6.2932,  8.3179,  6.3369,  4.5746,
         5.1916,  6.6130,  9.1885,  5.9177,  5.6890,  8.8675,  7.1403,  4.4711,
         9.5222,  7.7046,  5.1164,  5.7442, 10.1212,  7.1178,  7.7572,  6.4762,
         3.3291,  9.0797,  8.1354,  8.8063,  3.3282,  6.5655,  4.4675,  6.1748,
         6.2584,  7.3474,  4.0837,  5.8930,  6.6848,  6.0637,  4.1018,  6.1190],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [34/80], Step [600/642], LR 1.0e-04, Loss: 13.0
Max_Val Meta Model:  tensor([ 17.6659,  21.4173,  28.2726,  17.6454,   3.7216,   6.0780,   5.6549,
          6.8495,   5.3745,   6.7336,   7.0901,   6.2309,   4.3843,   8.0636,
          4.8483,   6.2883, 135.1849,   4.1834,   4.9840,   4.1512,   2.9823,
          4.7333,   3.9643,   4.2105,   9.6491,   7.2907,  10.1257,   5.4507,
          5.7621,   5.1649,   4.6655,   4.7274,   8.6896,   2.9888,   3.7461,
          4.7842,   3.4336,   5.0384,   4.8467,  16.0873,   7.3760,   9.8043,
          5.6889,   7.6314,   6.4071,   8.3313,   4.6591,   4.4816,   4.3960,
          5.6847,   4.7535,   3.7110,   7.0510,   4.2054,   5.9018,   4.4239,
          8.9904,   5.8598,   7.8238,   3.7293,   5.4172,  10.7367,   4.9862,
          3.5881,   2.0308,   4.6539,   6.3374,   3.0108,   5.1293,  10.6070,
          4.5076,   8.8164,   8.9676,   6.4543,   6.4891,   5.7428,   5.2725,
          4.0502,   4.1339,   6.1620], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 13.2070,  19.9949,  22.1047,  16.5412,   4.2404,   7.2539,   6.4246,
          7.6497,   6.0698,   7.6509,   8.1870,   6.9672,   5.2374,   8.6586,
          5.3541,   7.2473, 120.5456,   4.9796,   5.9005,   4.9565,   3.7352,
          5.6180,   4.5873,   4.9076,  10.4343,   7.8681,  11.7949,   6.8882,
          6.3929,   5.5775,   5.4158,   5.6172,   9.8589,   3.6547,   4.7920,
          5.8752,   4.5668,   5.2734,   5.7610,  15.6805,   8.0196,   9.5135,
          6.0061,   8.0238,   6.8580,   8.3140,   5.4481,   5.0697,   4.8787,
          6.5669,   5.3711,   4.4851,   7.8443,   4.0224,   6.8900,   5.0444,
          9.7533,   6.4575,   8.0011,   4.6007,   5.5612,  10.2688,   5.8087,
          4.2835,   2.5117,   5.5666,   7.3665,   3.7753,   5.4626,  11.3497,
          5.3588,   9.5935,   9.7509,   7.3461,   6.9687,   6.2410,   6.1888,
          4.7734,   4.9587,   7.1568], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 39.3685,  58.4390,  65.5757,  42.4388,  11.1292,  19.6472,  16.8792,
         19.9045,  15.9430,  19.5512,  21.4656,  18.0133,  14.0989,  22.6679,
         13.6680,  20.1666, 311.5628,  12.3201,  15.0518,  12.8772,   9.6767,
         14.5439,  12.0827,  12.7225,  28.1911,  20.6653,  31.1842,  17.4537,
         17.5345,  15.8164,  15.5907,  15.2787,  26.4332,  10.5492,  12.7155,
         15.8420,  12.5882,  13.9286,  16.1239,  42.8457,  20.7199,  25.4738,
         15.3764,  21.3154,  17.9041,  21.4580,  13.7438,  13.0523,  12.9826,
         16.6322,  13.8291,  12.6105,  20.6921,  10.3467,  17.4179,  13.1609,
         25.9338,  16.8976,  24.3599,  11.6628,  14.9204,  26.9052,  15.3757,
         11.3559,   6.3294,  14.2813,  19.3856,   9.6813,  13.9379,  29.7467,
         13.8937,  25.3265,  24.9330,  19.7883,  17.7137,  18.5605,  16.1470,
         13.4139,  12.7647,  18.8332], device='cuda:0')
Outer loop valEpocw Maximum [34/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 135.2
model_train val_loss valEpocw [34/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 120.5
model_train val_loss  valEpocw [34/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 311.6
Max_Val Meta Model:  tensor([31.2351, 28.6667, 33.1539, 33.7193, 32.9508, 29.8233, 35.1740, 33.1615,
        32.4106, 34.0975, 32.6745, 35.4706, 31.5514, 35.2180, 32.7369, 31.4743,
        33.4120, 33.2642, 33.5378, 35.9411, 32.5965, 33.0001, 31.6026, 33.9009,
        31.5133, 31.5174, 32.2097, 32.7393, 30.9393, 29.6350, 29.8079, 31.6912,
        31.6554, 28.9951, 31.7505, 31.2772, 30.6496, 32.2901, 30.1988, 32.7635,
        32.2391, 30.8969, 32.9080, 32.3256, 31.6951, 31.9750, 32.4646, 32.3713,
        31.6529, 33.1398, 32.7782, 30.4599, 32.1923, 32.6578, 32.9964, 32.0243,
        32.2472, 32.5440, 27.8304, 32.6362, 31.4217, 31.0249, 31.5579, 32.1514,
        32.8776, 33.0208, 32.1244, 32.0611, 32.8403, 32.6219, 31.8856, 31.4529,
        32.3817, 31.6991, 32.5770, 28.3660, 32.1676, 29.9840, 32.9398, 32.4778],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([18.4836,  7.4700, 15.9215,  7.3662,  4.0354, 12.4327, 31.1286, 16.1992,
        12.9148, 13.7889,  8.4008, 78.5118,  5.0973,  6.4300,  6.4516,  4.3215,
         5.1566,  7.0094,  4.2794,  8.5617,  2.6402,  4.0116,  3.4804,  4.8102,
         9.1694,  7.6729, 11.2697,  6.9135,  5.9747,  2.6238,  3.7183,  4.0341,
         4.7788,  2.4915,  2.9820,  3.6851,  3.5485,  2.8569,  3.3497,  2.6415,
         4.0994,  1.8581,  2.7385,  2.0448,  2.2072,  1.9359,  3.6932,  2.5104,
         2.9223,  4.8835,  3.6702,  3.1167,  4.8735,  2.0832,  5.0554,  3.8251,
         4.4158,  2.8151,  3.8436,  1.8550,  1.2046,  3.3174,  2.9816,  2.1309,
         1.5950,  3.8478,  5.3561,  5.2652,  1.4531,  4.1313,  3.7420,  2.9812,
         4.2563,  2.9331,  5.0649,  2.7620,  4.4740,  2.7551,  3.4052,  5.1547],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 55.8536,  21.9268,  46.4277,  18.7770,  10.4764,  34.8762,  81.1409,
         42.2520,  33.7241,  35.0714,  22.0271, 205.1233,  13.8090,  16.2173,
         16.6025,  11.9405,  13.2978,  17.5214,  10.8553,  21.8690,   6.8748,
         10.3675,   9.2135,  12.2958,  24.8379,  20.2786,  29.7201,  17.5960,
         16.5083,   7.4356,  10.6573,  10.9131,  12.8307,   7.1352,   7.9473,
          9.9595,   9.7692,   7.5091,   9.3972,   7.1553,  10.6465,   4.9677,
          7.0074,   5.3959,   5.7562,   4.9666,   9.3539,   6.5003,   7.8047,
         12.3109,   9.4239,   8.7435,  12.7757,   5.3673,  12.7899,   9.9746,
         11.6866,   7.3777,  11.7023,   4.7193,   3.2638,   8.8097,   8.0005,
          5.6014,   4.0082,   9.8384,  14.1148,  13.4799,   3.7197,  10.7955,
          9.8630,   7.9126,  11.0091,   7.8754,  13.1606,   8.2290,  11.6592,
          7.7855,   8.8318,  13.6023], device='cuda:0')
Outer loop valEpocw Maximum [34/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 35.9
model_train val_loss valEpocw [34/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 78.5
model_train val_loss  valEpocw [34/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 205.1
Max_Val Meta Model:  tensor([28.9010, 28.5298, 30.6828, 33.2848, 32.8857, 29.0737, 31.7466, 33.7717,
        32.9964, 32.5836, 33.6938, 31.3728, 31.5530, 33.2605, 31.9285, 31.3757,
        32.4206, 32.4143, 33.4724, 31.5066, 33.3880, 32.9634, 32.8469, 34.0769,
        31.2985, 32.5708, 32.0440, 31.3947, 30.9326, 29.5831, 29.7591, 31.6317,
        32.0155, 28.8842, 31.7490, 31.2779, 30.8404, 30.5266, 30.2107, 32.9918,
        31.5887, 32.1081, 33.6951, 32.8110, 32.4170, 33.8214, 33.7165, 32.5016,
        32.7737, 32.7490, 34.4100, 30.6203, 32.5824, 32.7777, 33.5859, 33.1253,
        33.4373, 32.5711, 27.8417, 32.6527, 33.8428, 31.0358, 31.6314, 32.0882,
        32.8375, 32.9266, 32.1560, 31.0388, 32.8685, 32.5340, 32.0696, 31.5302,
        32.3761, 31.8257, 31.7117, 28.4172, 32.1892, 29.9911, 33.0348, 32.4842],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 4.8267,  3.5703,  2.4679,  3.3598,  2.7583,  3.1034,  2.7543,  5.2726,
         3.4195,  2.7747,  7.5104,  5.6148,  4.0629,  6.2548,  2.0500,  6.0877,
         2.9559,  2.9786,  6.2214,  3.8910,  2.5509,  5.7303,  4.2776,  3.4740,
         3.6267,  3.5682,  9.7491,  9.5965,  5.0906,  4.0364,  4.5925,  6.5861,
         7.4139,  3.8833,  4.1885,  5.6861,  5.9177,  5.3193,  5.6534, 21.3273,
        12.7430, 24.8913, 27.0763, 27.1633, 21.4178, 26.6215,  7.6857,  7.9957,
        16.4036,  7.5593, 13.5644, 15.9673, 21.0296, 12.9362, 24.1203, 34.8115,
        40.3544, 10.8193,  7.6284,  6.7867, 46.3327,  7.2898,  9.4527,  6.3131,
         5.8656,  5.9734,  8.8373,  5.4271,  4.1757,  5.4835,  5.4013,  8.5968,
         5.9101, 17.4940,  3.4710,  8.2051,  7.6817,  4.2292,  5.1327,  7.3455],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 14.5345,  10.4938,   7.2164,   8.6023,   7.1725,   8.7571,   7.2871,
         13.4772,   8.9203,   7.1714,  19.4261,  14.7492,  10.9927,  16.1418,
          5.3108,  16.8331,   7.7334,   7.5320,  15.8031,  10.2451,   6.6067,
         14.8255,  11.2105,   8.9041,   9.8360,   9.3555,  25.7297,  24.9748,
         14.0479,  11.4581,  13.1805,  17.8574,  19.8745,  11.1632,  11.1586,
         15.3651,  16.2243,  14.3892,  15.8562,  57.8276,  33.6409,  66.6859,
         69.3288,  71.8258,  55.9272,  69.0107,  19.2481,  20.6628,  43.4065,
         19.1996,  34.2375,  44.8490,  55.2546,  33.3909,  61.0933,  90.9166,
        107.0253,  28.3871,  23.2604,  17.2705, 129.7458,  19.3705,  25.3400,
         16.6566,  14.8007,  15.3174,  23.2963,  13.9823,  10.6894,  14.3610,
         14.1542,  22.8389,  15.3092,  47.0883,   9.0842,  24.4624,  20.0369,
         11.9476,  13.2738,  19.3807], device='cuda:0')
Outer loop valEpocw Maximum [34/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 34.4
model_train val_loss valEpocw [34/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 46.3
model_train val_loss  valEpocw [34/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 129.7
Max_Val Meta Model:  tensor([31.7269, 27.7848, 30.0861, 32.5501, 32.1273, 28.4882, 31.1374, 30.0941,
        32.2870, 32.3860, 31.4712, 30.9075, 30.9269, 31.3425, 31.3816, 31.5891,
        30.6075, 32.0129, 31.2664, 30.7967, 31.5843, 31.9148, 31.4554, 31.5352,
        30.4366, 31.9449, 29.5107, 30.1742, 30.4141, 28.5405, 28.6796, 28.2523,
        30.1556, 27.6551, 30.3498, 30.5942, 31.4285, 32.1123, 29.9467, 32.2745,
        30.2756, 30.9269, 29.8818, 29.7490, 30.5807, 30.8701, 31.8117, 31.7438,
        32.1071, 29.3707, 30.5770, 29.4553, 31.9267, 30.1630, 32.0745, 30.9239,
        30.5208, 31.4761, 27.2828, 30.2320, 29.3331, 30.1615, 31.7159, 32.3811,
        30.8878, 30.2879, 31.4557, 33.7351, 31.7533, 31.4722, 32.2078, 31.3526,
        32.3075, 30.6153, 36.1881, 27.6508, 31.2334, 29.3336, 29.8572, 31.1484],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 8.8796,  3.5969,  7.7364,  4.8404,  4.1093,  6.0809,  5.5996,  7.5502,
         3.5555,  9.1227,  5.7627,  4.1589,  2.9511,  4.8884,  6.7727,  4.5420,
         2.3320,  2.9494,  3.9152,  3.5742,  2.3968,  3.7551,  4.0824,  4.8887,
         6.5152,  2.1445,  7.6032,  2.7821,  5.5735,  2.3925,  3.1115,  3.4794,
         2.8565,  2.2541,  2.1943,  2.8944,  1.9958,  2.3164,  2.2415,  2.7785,
         3.8354,  1.6613,  2.6332,  1.9954,  2.3182,  2.0040,  3.4671,  2.3658,
         2.8336,  3.8346,  3.6866,  2.8635,  4.5905,  2.2360,  4.8235,  3.0012,
         1.4623,  2.3288,  5.0533,  2.2164,  2.5821,  4.6779,  3.2501,  1.9403,
         1.5935,  3.4293,  5.1246,  2.2043,  1.4154,  3.6242,  3.6761,  4.7041,
         2.7248,  3.1154, 83.0555,  3.5980,  4.2337,  3.2969,  3.1747,  4.8642],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 26.0651,  10.6056,  23.1099,  12.4378,  10.7458,  17.1609,  14.6744,
         20.1425,   9.3701,  23.5028,  15.2769,  10.9838,   7.8933,  12.9717,
         17.5830,  12.5576,   6.1310,   7.3431,  10.1471,   9.5016,   6.2385,
          9.7579,  10.7083,  12.8423,  17.5993,   5.6466,  20.7447,   7.1573,
         15.2236,   6.8141,   9.0120,  10.1649,   7.7361,   6.5477,   5.9383,
          7.7771,   5.2747,   6.1352,   6.2544,   7.5766,  10.1728,   4.4107,
          6.9768,   5.4452,   6.0654,   5.2444,   8.8842,   6.0505,   7.4785,
         10.1815,   9.6735,   8.0576,  12.0936,   5.8727,  12.2398,   7.8673,
          3.9770,   6.1816,  15.4685,   5.7004,   7.1338,  12.4931,   8.6276,
          5.0274,   4.1015,   9.0613,  13.4754,   5.5887,   3.5983,   9.5268,
          9.3225,  12.5674,   7.0526,   8.4118, 219.6312,  10.6906,  11.0706,
          9.2337,   8.4071,  12.9412], device='cuda:0')
Outer loop valEpocw Maximum [34/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 36.2
model_train val_loss valEpocw [34/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 83.1
model_train val_loss  valEpocw [34/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 219.6
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [89.85392478 97.2137279  91.96403553 97.02489005 97.26733349 96.5997003
 96.99808726 94.74177946 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.13285657 96.65086926 94.18866729 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.38893288 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.93990083 97.84237521 92.98254164
 96.90915072 96.22689782 96.9627563  93.94013231 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 92.10901427 96.13796128 96.24273583 96.9067141
 90.9857336  97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.44230699 96.15623591 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [87.32471583 97.2137279  90.70795921 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.08023781 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.35847516 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.75593621 97.84237521 92.10657765
 96.90915072 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.10390955 96.13796128 96.24273583 96.9067141
 89.88560081 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [97.62783223 12.83648153 63.16251112 14.88207434  4.2713218  22.61595745
  8.63829052 34.85256952  5.61983334 25.12482444  1.82073367  1.11534444
  1.2962053  13.99820617  6.85229502 31.93841798 11.20239472  6.91142012
  0.90164369  1.84521025  2.79992259  0.48887073  1.08140055  5.27714219
 18.6388594  11.74562712 30.97439522 10.03885153  4.7133534   1.38755341
 11.64323769  0.89694297 37.74207531  1.30283782  6.04295472  5.84448833
 10.13056018 44.50922989 25.78581189 38.01506662  7.75254484 50.53967337
 32.21706272 29.76253134 20.21186947 36.14240052  2.69368544  2.17059638
  9.921914    1.92889475  2.80308003  1.38046522  1.22966314 19.27717701
  1.56440123  8.35021373 66.06479992 28.19901467 17.17587948 11.49557347
 66.93524903 17.0521434  25.62754592  7.30716613  6.33853195  5.40679955
  4.27514022 10.45666456  4.69310359 10.34902911  0.48280781 28.89112633
  5.77566212 24.21322225 26.31413291 13.57171404  1.15645102  2.39937555
  0.97293122  0.83562298]
Accuracy th:0.5 is [45.66830326 97.2137279  70.24646386 97.02489005 97.26733349 74.81146672
 75.09045942 75.01979752 75.97495157 96.33045406 76.29780339 98.52097319
 99.41399349 79.38987098 75.90063474 96.56680596 96.29512311 75.62407865
 98.65376884 98.30776915 79.03777975 76.73761285 98.38695922 75.86652209
 81.69856605 96.65086926 94.0778012  75.35483242 98.01293844 76.08825429
 97.30875598 98.57457877 96.36213009 98.02024829 85.119577   75.83362776
 75.72519828 87.45263825 97.11504489 73.04370073 78.07166092 92.05906361
 75.06731156 74.67745276 96.9627563  93.87434364 98.02877645 98.57336046
 93.08731619 85.04526017 85.61299204 98.55508583 98.99976852 75.21472692
 98.70615611 75.5607266  70.79470279 91.7654512  96.24273583 96.9067141
 89.79300934 97.17717864 92.13581706 75.66671946 98.42838172 76.15038803
 98.20786784 74.92720605 76.66085939 97.39038267 77.16036598 95.99054592
 76.55973977 95.45083515 75.01126936 80.65325715 85.96508327 76.18450068
 77.20788002 99.14718388]
Accuracy th:0.7 is [45.75358487 97.2137279  70.24646386 97.02489005 97.26733349 74.81146672
 75.09045942 75.3523958  75.97495157 96.46934126 76.29780339 98.52097319
 99.41399349 79.95151131 75.90063474 96.56680596 96.29512311 75.62407865
 98.65376884 98.30776915 79.50804693 76.73761285 98.38695922 76.5633947
 82.6110793  96.65086926 94.0778012  75.35483242 98.01293844 76.08825429
 97.30875598 98.57457877 96.36213009 98.02024829 85.38882324 75.83362776
 75.72519828 87.86686322 97.11504489 73.04370073 78.77706168 92.05906361
 75.06731156 74.67745276 96.9627563  93.87434364 98.02877645 98.57336046
 95.07681437 86.11615356 85.80670313 98.55508583 98.99976852 75.21472692
 98.70615611 75.5607266  70.79470279 92.19064095 96.24273583 96.9067141
 89.79300934 97.17717864 92.30272536 75.66793777 98.42838172 76.15038803
 98.20786784 74.92720605 76.66085939 97.55485435 77.16036598 95.99054592
 76.7595424  95.45083515 75.01126936 80.76655986 86.15879436 76.18450068
 77.20788002 99.14718388]
Avg Prec: is [55.94437803  3.18909461 11.11611924  3.35934953  2.28330088  3.69719044
  3.22894538  5.64423186  2.42058752  3.76625645  1.62885427  1.61703738
  0.58199807  5.12009767  2.64268545  3.07664055  3.61719854  2.62241878
  1.32849248  1.75799816  1.95017412  0.83842043  1.79443585  2.56117926
  5.16273455  3.48739573  6.42120255  3.36646257  2.17125088  1.98365576
  2.64976704  1.28765473  3.78522637  1.74214457  2.40152296  2.50794967
  2.95608754  2.49902129  2.89122272  7.52536014  2.34975966  8.25442838
  3.34934752  4.14381893  3.21801466  6.45290001  2.2175254   1.56295147
  2.04888261  1.63867851  1.89335984  1.64870491  1.06337336  3.07751492
  1.34045942  2.7420797  11.21820127  3.54244095  3.92524466  2.75552441
 10.75145649  2.20916363  3.79257181  2.92073194  1.5589913   2.49967487
  1.74992052  4.16071273  1.25389499  2.35954573  0.17660127  3.44128069
  1.89170694  4.48701024  3.86849048  3.07341937  0.81555965  1.90683138
  0.12756561  0.80595624]
mAP score regular 15.63, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [90.2508907  97.22450607 92.58788649 96.96290206 97.90716795 96.63651992
 96.80843112 94.88003588 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.30856815 96.52938685 94.3244388  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.47955752 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.76727209 97.82744101 93.14597504
 97.07750953 96.48703192 97.03764606 94.00802252 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.38201659 96.39235618 96.16314124 96.78102499
 91.60874007 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.47798789 95.7993871  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [89.41375788 97.22450607 91.12788699 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.33440466 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.40730498 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.85945636 97.82744101 92.43590702
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.98089045 96.39235618 96.16314124 96.78102499
 90.31566883 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [97.44720786 13.42230043 67.42752238 15.56458575  3.56858991 22.16203763
  9.69059005 34.78402983  6.68199016 28.72520441  1.97400726  1.09397265
  1.23914812 15.23045565  6.99078489 38.94516801 12.45342529  7.85186913
  0.80871432  1.90519338  2.6620588   0.49955834  1.07210761  5.71459652
 19.40555269 12.40392574 30.5909183   9.84020118  5.23111173  1.44442147
 13.3444434   0.8234563  41.23231388  1.24164873  5.25588957  5.31967065
 10.48925501 50.0794337  31.55867856 39.12686371  8.8518521  49.13581491
 33.07626502 29.89649493 19.94993469 35.56921314  2.53153325  1.95011031
 11.37767126  1.65960235  3.39343882  1.49892784  1.42131983 20.49192865
  1.65202603  8.52024256 59.24339253 28.79554195 15.56076843 12.38720113
 65.43544524 19.51741983 27.43782098  7.14787436  7.23844894  5.00751747
  4.52959814 10.92183776  4.26344719 11.32620334  0.51602361 32.21637515
  5.49346178 26.24455138 35.7973755  12.64588401  1.31161027  2.35769982
  0.18313545  0.76824687]
Accuracy th:0.5 is [45.7582779  97.22450607 68.01455017 96.96290206 97.90716795 73.20676682
 73.31888283 72.74584548 74.841169   96.40979645 74.91093006 98.5325261
 99.34972718 76.64748237 74.93335327 96.31262924 96.21047911 74.30052072
 98.78167277 98.34068316 76.93150958 75.55123701 98.31327703 74.74151033
 77.21553679 96.52938685 94.3393876  74.43755139 97.81747515 74.89847273
 97.52597354 98.67204823 96.39983058 98.18870369 85.26048285 74.54966739
 74.56212472 89.96188056 97.0276802  71.94608466 75.68079328 92.37362035
 73.66270523 73.4409647  97.03764606 94.02795426 98.18621222 98.77668984
 94.15252759 84.98891297 83.65099534 98.55993223 98.87385704 73.63779057
 98.6969629  74.37028178 68.94137579 92.95911503 96.16314124 96.78102499
 90.13379176 97.04761193 92.16184568 74.48488925 98.32075143 75.34693674
 98.13139996 73.69758577 75.61850661 97.46119541 75.96232902 96.07843137
 75.44908688 95.44559882 73.5904527  81.41614969 87.82669357 75.01058873
 76.03707302 99.15040985]
Accuracy th:0.7 is [45.9750355  97.22450607 68.01455017 96.96290206 97.90716795 73.20676682
 73.31888283 72.98004335 74.841169   96.41976231 74.91093006 98.5325261
 99.34972718 77.09345492 74.93335327 96.31262924 96.21047911 74.30052072
 98.78167277 98.34068316 77.30024666 75.55123701 98.31327703 75.0504522
 77.86082667 96.52938685 94.3393876  74.43755139 97.81747515 74.89847273
 97.52597354 98.67204823 96.39983058 98.18870369 85.60928819 74.54966739
 74.56212472 90.16368936 97.0276802  71.94608466 76.22891596 92.37362035
 73.66270523 73.4409647  97.03764606 94.02795426 98.18621222 98.77668984
 95.94389217 85.58935645 83.82041508 98.55993223 98.87385704 73.63779057
 98.6969629  74.37028178 68.94137579 93.27553131 96.16314124 96.78102499
 90.13379176 97.04761193 92.32379102 74.48488925 98.32075143 75.34693674
 98.13139996 73.69758577 75.61850661 97.52846501 75.96232902 96.07843137
 75.55123701 95.44559882 73.5904527  81.49836809 88.01355358 75.01058873
 76.03707302 99.15040985]
Avg Prec: is [54.00636835  3.77429485 14.8881413   4.594252    1.66601904  4.29485668
  9.80122972  8.75837972  6.85409769  5.15458644  2.26363887  5.34978657
  1.54949149  5.87083297  2.95231091  4.14675904 24.77564241  5.8235591
  1.5594468   3.36136481  3.71294596  1.42262419  1.65146208  5.23209771
  5.75118127 14.02212638  8.32694628  4.53702135  3.95759818  6.98516595
  2.2910074   0.87438016  3.03793174  1.16613486  1.74909484  2.43513581
  2.04495968  2.20177938  2.20809605  6.2364498   1.76207777  6.02628109
  2.23010187  2.73844713  2.35767394  4.85185097  1.77953831  1.04652003
  1.47004516  1.20320462  1.23853586  1.00414663  0.75477219  2.31501666
  0.92113085  1.88759603 10.242265    3.03735556  3.89785049  2.73203741
  7.9773146   2.02016274  3.37717703  2.58085134  1.34488856  1.86897563
  1.54341032  3.5357462   1.06005689  2.16036782  0.19064957  3.11513406
  1.53233189  3.9346269   3.19550389  2.32522015  0.55574486  1.44316167
  0.12363747  0.59758722]
mAP score regular 16.28, mAP score EMA 4.37
Train_data_mAP: current_mAP = 15.63, highest_mAP = 15.93
Val_data_mAP: current_mAP = 16.28, highest_mAP = 16.57
tensor([0.3358, 0.3400, 0.3389, 0.3875, 0.3836, 0.3546, 0.3792, 0.3755, 0.3823,
        0.3873, 0.3774, 0.3801, 0.3713, 0.3801, 0.3858, 0.3616, 0.3798, 0.3981,
        0.3870, 0.3785, 0.3822, 0.3858, 0.3784, 0.3818, 0.3690, 0.3812, 0.3645,
        0.3856, 0.3641, 0.3522, 0.3477, 0.3445, 0.3683, 0.3461, 0.3678, 0.3709,
        0.3799, 0.3802, 0.3578, 0.3664, 0.3761, 0.3783, 0.3771, 0.3668, 0.3831,
        0.3861, 0.3927, 0.3887, 0.3776, 0.3794, 0.3826, 0.3559, 0.3832, 0.3804,
        0.3936, 0.3826, 0.3694, 0.3754, 0.3277, 0.3871, 0.3589, 0.3757, 0.3752,
        0.3868, 0.3912, 0.3796, 0.3798, 0.3918, 0.3917, 0.3814, 0.3867, 0.3756,
        0.3900, 0.3712, 0.3878, 0.3356, 0.3831, 0.3554, 0.3733, 0.3755],
       device='cuda:0')
Max Train Loss:  tensor([11.2613,  7.8595,  7.9514,  5.4434,  5.5580,  6.6322,  7.0445,  7.9809,
         6.8572,  6.6196,  7.6411,  4.4356,  3.5859, 10.1490,  7.5697,  8.1364,
         6.5432,  6.0696,  5.8437,  5.7878,  5.7040,  4.7355,  7.1838,  7.2802,
         7.7196, 10.3125, 10.7725,  6.9052,  6.9535,  4.3005,  8.8553,  6.2047,
         8.8314,  8.2930,  6.6162,  6.7904,  7.7147,  7.0974,  6.9138,  7.3743,
         6.9894,  9.1083,  6.2335,  6.0960,  9.2043,  7.4492,  6.1710,  6.8584,
         6.3458,  6.8468,  4.6833,  4.6122,  5.7964,  4.7206,  7.0496,  5.5495,
         9.9428,  5.8534,  5.7931,  8.0315,  7.8649, 10.1000,  5.5995,  6.5253,
         3.1410,  7.0076,  6.9519,  4.0131,  6.4022,  6.5659,  5.3068,  7.9197,
         5.3014,  8.3477,  6.5046,  8.9911,  8.4145,  4.3088,  3.9903,  6.7752],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [35/80], Step [000/642], LR 1.0e-04, Loss: 11.3
Max Train Loss:  tensor([10.8847,  7.2735,  7.1829,  7.9801,  9.8724,  6.6773,  7.9259, 14.2306,
         7.0703,  5.4726,  6.5053,  5.1761, 14.7237,  9.3166,  9.1287,  8.0582,
        10.1954,  8.1408,  9.5306,  7.9304,  8.3422, 13.0715,  6.2562,  5.7716,
         9.4994, 11.6063,  9.5477,  9.9399,  6.4049,  7.3292,  7.4386,  7.9800,
         5.1097,  6.3779,  6.3827,  6.6148, 10.7258, 13.6494,  8.7483,  9.4180,
         9.8349, 10.9875,  9.5039,  9.1239, 11.5620, 11.7462, 14.9907, 13.2156,
         6.3413, 14.2247,  7.0918,  5.4970,  8.7392,  7.0045,  7.3127,  6.3380,
        10.0961, 14.8137,  4.0622, 11.7997, 11.1122,  7.9755,  8.2060,  4.5709,
        10.0685, 10.5633,  4.2435, 14.2573, 15.2496,  6.6714, 12.9573,  8.3632,
        11.0579,  8.5633, 10.3027,  7.7995,  9.0627,  5.0966, 15.1588,  5.0412],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [35/80], Step [100/642], LR 1.0e-04, Loss: 15.2
Max Train Loss:  tensor([10.5495,  4.1337,  6.5915,  6.9911,  8.8904,  7.5931,  7.3496,  6.5043,
         5.2421,  6.1476,  3.3219,  4.9895,  5.7230,  9.7205,  6.8164,  6.8670,
        12.6282,  7.8407, 10.7307,  6.9681, 10.6777,  5.8873,  7.4215,  5.9009,
         7.8967,  7.0523, 10.4001,  6.3196,  6.3880,  7.1970,  5.6737,  3.1672,
         7.2658,  6.6445,  5.2281,  4.4364,  6.3852,  7.5219,  5.6111, 10.3056,
         5.7366, 11.3488,  6.8299,  7.9598, 11.1251, 10.7618,  9.6408, 10.4343,
         9.7102,  9.4834,  8.1065,  6.9267,  7.5070,  9.6039,  8.5713, 10.1546,
        11.3214,  6.1506,  4.4799, 13.3863, 10.9616,  8.3261, 10.8062,  9.2386,
         9.4776,  8.0302,  8.3495,  7.7673,  9.0038,  7.9756,  4.2880,  7.6235,
         8.4235,  6.6940, 10.8901,  5.8000,  9.1920,  8.2168,  5.7168,  5.7434],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [35/80], Step [200/642], LR 1.0e-04, Loss: 13.4
Max Train Loss:  tensor([12.2266,  4.2164,  7.9186,  7.1946,  7.5462,  7.0146, 10.2943,  9.1950,
         6.1836,  8.1567,  3.9284,  5.5437,  5.5364, 10.9877,  6.0096,  7.4124,
        11.9951,  5.5636,  7.5672,  4.5938,  9.0185,  5.7614,  4.3317,  5.4340,
         8.5513,  7.5875,  8.9982,  8.5961,  6.3397,  5.9885,  7.6070,  5.0884,
        11.5876,  4.6602,  8.8755, 10.4579,  6.4501,  8.3711,  8.1324, 10.4167,
         6.7732,  9.0231,  7.8432,  6.2569,  8.8027,  8.1991,  8.3640,  6.9479,
         7.2821,  7.7015,  7.5517,  6.4120,  8.8335,  7.5154,  6.2363,  5.4883,
         9.7869,  8.6420,  7.4874, 10.0643,  8.3653,  6.7385,  6.0609,  5.9448,
         6.0543,  9.3045,  4.7759,  5.3235,  7.1023,  7.8065,  4.0847,  7.5914,
         7.3939,  8.6767,  9.4630,  8.1226,  8.5441,  4.0494,  5.5158,  4.6879],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [35/80], Step [300/642], LR 1.0e-04, Loss: 12.2
Max Train Loss:  tensor([13.1056,  6.4882,  8.3163,  6.0818, 11.1269,  6.9692,  6.3132,  7.7182,
         5.5209,  5.2782,  4.3290,  5.3891,  7.1780,  8.4395,  7.2644,  5.9509,
         8.2011,  5.5602,  5.7770,  6.2711,  4.9432,  4.7480,  6.7422,  9.4124,
         7.6609,  9.3257, 11.1145,  5.9200,  6.1146,  4.3232,  6.7637,  5.8444,
         8.3112,  5.1854,  6.8847,  7.8154,  6.9104,  5.4471,  5.8637, 10.1575,
         6.1080, 11.8790,  6.1892,  6.9841,  6.4781,  8.1083,  7.1612,  8.2362,
        10.5254,  7.5099,  5.9721,  6.0853,  7.4952,  5.9004,  6.7053,  4.8069,
        12.5064,  8.9985,  6.5968,  7.5694,  7.6753,  7.3339, 12.7469,  5.4514,
        10.9976, 10.3794,  9.5798,  8.7512,  7.2247,  8.2950,  4.2087,  9.1348,
         8.6147, 11.9963,  8.9297,  6.5741,  7.4867,  4.9372,  6.2889,  4.8429],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [35/80], Step [400/642], LR 1.0e-04, Loss: 13.1
Max Train Loss:  tensor([11.4277,  5.3131, 12.0060,  7.1933,  8.1280,  8.7600,  5.7795, 10.5208,
         8.4174,  6.6670,  4.3999,  7.6374,  5.5058,  5.7456,  3.8151,  8.5751,
         8.8303,  4.2956,  6.8208,  4.4777,  6.8844,  4.5948,  6.3387,  4.5050,
         8.2037,  7.3197,  7.3726, 10.1423,  5.3386,  3.1066,  5.6698,  5.8806,
         9.2886,  5.8671,  4.6826,  7.1379,  6.6971,  5.6189,  7.2256, 10.3525,
         6.6913,  9.8227,  7.1436,  7.8209,  5.0679,  7.7282,  7.3209,  3.5478,
         5.8019,  9.3581,  3.7197,  7.6230,  7.4075,  4.7652,  8.9795,  7.6323,
        10.9876,  7.4890,  4.6905,  6.1178,  9.5514,  7.7518,  8.8968,  6.0425,
         7.1704,  6.9120,  5.2849, 11.6288,  4.1518,  5.8083,  4.0578,  7.7017,
         7.4411, 11.3316,  6.9550,  5.0190,  7.3124,  5.4035,  5.4921,  6.9848],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [35/80], Step [500/642], LR 1.0e-04, Loss: 12.0
Max Train Loss:  tensor([12.6277,  6.0470,  9.7723,  5.5233,  6.5615,  8.6213,  9.1454,  7.5248,
         6.2109,  6.5760,  6.7036,  6.2810,  5.5559,  9.8395,  5.7651,  9.0840,
         6.6987,  7.2411,  3.6264,  6.6146,  6.2918,  6.6196,  8.7325,  7.8754,
         6.5155,  8.6953,  9.3745,  8.8912,  4.6836,  3.7432,  6.5286, 11.1173,
         6.3541,  7.2284,  4.7838,  4.4660,  5.9838,  6.9024,  4.6496,  9.3566,
         4.7002,  8.6520,  6.1845,  6.5085,  5.5493,  6.4400,  4.7952,  6.0821,
         3.4823,  7.6847,  3.7027,  5.4267,  6.6597,  5.1800,  6.2245,  7.5456,
         6.9920,  8.7770,  4.5698,  5.7345,  8.2191,  7.6898,  7.5484,  3.4259,
         6.6870,  7.6279,  3.2853,  7.6516,  4.9755,  6.0932,  4.1014,  5.5140,
         4.1737,  6.7687,  9.3634,  5.5364,  7.8951,  5.6220,  6.2110,  4.7097],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [35/80], Step [600/642], LR 1.0e-04, Loss: 12.6
Max_Val Meta Model:  tensor([ 20.1482,  20.8471,  31.1464,  18.4177,   4.3752,   9.2889,   5.8392,
          7.6524,   4.8565,   6.1864,   4.0120,   6.6426,   6.3440,   8.2223,
          5.0707,   6.1013, 131.6999,   3.8032,   3.6818,   3.5580,   3.7627,
          4.7639,   4.2181,   3.9453,  10.6957,   8.2095,   9.9918,   1.8209,
          6.2867,   5.4421,   4.3467,   3.0211,   5.4579,   3.4872,   4.2854,
          4.2771,   4.9165,   5.0208,   4.1202,  16.1755,   6.2772,   8.1061,
          5.2933,   8.0215,   6.9427,   8.9537,   4.0237,   4.6100,   3.5786,
          6.6759,   3.8687,   4.5342,   7.9109,   3.9304,   5.6942,   3.4091,
          9.8205,   5.7619,   7.4627,   4.8605,   6.7537,  10.9829,   5.1283,
          3.2332,   6.1808,   3.5966,   2.6529,   4.5694,   6.6109,   9.9037,
          4.1728,   9.3939,   8.7047,   6.4460,   6.1252,   5.1639,   7.4793,
          4.8571,   5.6261,   4.7877], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 15.9419,  19.4083,  23.2302,  17.0805,   5.0880,  13.1473,   6.8785,
         10.3515,   5.3383,   7.1849,   4.6691,   7.8318,   7.1260,   7.6140,
          5.5374,   6.5586, 124.0709,   4.4123,   4.2613,   4.1045,   4.1933,
          5.3278,   4.5703,   4.2958,  11.3826,   8.8870,  10.7460,   2.2661,
          6.8687,   5.9093,   4.6173,   3.5517,   5.8674,   3.9564,   5.1255,
          4.9086,   6.3694,   5.1794,   4.7044,  14.7838,   6.4214,   8.1482,
          5.5238,   8.3451,   7.1371,   7.9289,   4.4557,   4.9539,   3.8062,
          7.2647,   4.0885,   5.2096,   8.5633,   4.2434,   6.4546,   3.6260,
         10.0705,   5.9287,   7.5463,   5.3045,   5.9378,  11.1657,   5.2695,
          3.4937,   6.9085,   4.0971,   2.9366,   5.3624,   6.9994,  10.1254,
          4.7837,   9.8022,   9.1403,   7.0433,   6.3250,   5.5736,   8.2995,
          5.3853,   6.3708,   5.4561], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 47.4785,  57.0874,  68.5496,  44.0824,  13.2629,  37.0750,  18.1372,
         27.5650,  13.9636,  18.5495,  12.3702,  20.6024,  19.1911,  20.0316,
         14.3523,  18.1387, 326.6461,  11.0824,  11.0124,  10.8440,  10.9711,
         13.8082,  12.0785,  11.2517,  30.8463,  23.3127,  29.4847,   5.8768,
         18.8667,  16.7799,  13.2813,  10.3090,  15.9294,  11.4301,  13.9357,
         13.2327,  16.7669,  13.6212,  13.1493,  40.3509,  17.0735,  21.5410,
         14.6472,  22.7493,  18.6276,  20.5351,  11.3466,  12.7460,  10.0804,
         19.1494,  10.6855,  14.6380,  22.3474,  11.1551,  16.3990,   9.4765,
         27.2623,  15.7930,  23.0292,  13.7030,  16.5441,  29.7187,  14.0456,
          9.0321,  17.6589,  10.7923,   7.7319,  13.6875,  17.8679,  26.5448,
         12.3718,  26.0976,  23.4373,  18.9769,  16.3115,  16.6097,  21.6614,
         15.1539,  17.0642,  14.5307], device='cuda:0')
Outer loop valEpocw Maximum [35/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 131.7
model_train val_loss valEpocw [35/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 124.1
model_train val_loss  valEpocw [35/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 326.6
Max_Val Meta Model:  tensor([30.3818, 28.7091, 31.7883, 30.8057, 34.8276, 34.4337, 33.7224, 31.6648,
        31.8885, 34.2086, 28.4448, 34.4751, 31.4195, 33.6395, 35.8553, 34.7427,
        32.4510, 32.3941, 34.0087, 36.1701, 32.0203, 32.5575, 31.2006, 34.5955,
        31.2162, 32.2653, 29.2796, 31.1700, 30.7003, 29.3101, 29.3768, 29.1822,
        30.8496, 28.5263, 30.7221, 30.8615, 31.8689, 31.9766, 29.9334, 33.5243,
        30.9727, 29.6246, 31.3569, 31.0236, 31.4014, 31.9572, 31.0142, 32.0558,
        31.3968, 31.5257, 31.8628, 30.1323, 32.1356, 31.5865, 32.4257, 31.4950,
        31.4018, 30.3472, 33.4033, 31.8656, 30.4289, 35.5518, 31.8904, 30.0059,
        32.4120, 31.5924, 31.3333, 31.2433, 32.6380, 32.0244, 31.7302, 31.3925,
        35.2929, 31.3601, 30.9904, 27.9645, 31.9820, 29.6884, 31.4700, 31.7306],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([20.5792,  7.7056, 15.5608,  5.8376,  6.1896, 21.5208, 30.7454, 19.0691,
        13.2239, 13.0800,  6.4891, 52.5584,  6.4344,  4.0375,  6.3586,  4.1223,
         4.6390,  7.7247,  3.0715,  8.0476,  3.1263,  3.8076,  3.5838,  4.6152,
         9.5324,  8.4141, 10.3935,  5.4747,  6.5333,  3.0476,  3.2696,  2.3576,
         2.7541,  2.8625,  3.2623,  2.8998,  4.9519,  2.7979,  2.9226,  1.9823,
         2.6882,  1.9821,  2.5144,  2.5990,  2.8096,  2.1458,  2.8549,  2.5138,
         2.0733,  5.4986,  2.6028,  3.6681,  5.6337,  2.5273,  4.6757,  2.7237,
         4.0230,  2.8876,  3.1661,  2.4776,  2.2955,  3.9834,  2.0782,  0.8077,
         5.0909,  2.4022,  1.6242,  5.2967,  3.0409,  3.2747,  3.2892,  3.4616,
         3.3368,  2.8373,  4.5159,  2.1660,  6.3358,  3.2834,  4.6232,  3.7733],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 61.6668,  22.8466,  46.2231,  15.3488,  15.6919,  59.6358,  80.0981,
         50.7160,  34.5402,  33.3234,  18.1447, 137.9110,  17.3546,  10.3517,
         15.9733,  11.2322,  12.1918,  19.4904,   7.8911,  20.7663,   8.1902,
          9.8386,   9.4725,  11.7881,  25.7800,  22.0423,  29.3532,  14.2364,
         17.9838,   8.6307,   9.3697,   6.8064,   7.4633,   8.2235,   8.8772,
          7.8159,  12.9829,   7.3080,   8.1634,   5.3544,   7.1510,   5.3537,
          6.6609,   7.0587,   7.3171,   5.5067,   7.3634,   6.4825,   5.4985,
         14.4253,   6.7997,  10.2886,  14.6472,   6.6408,  11.8879,   7.1060,
         10.8561,   7.8273,   9.4505,   6.4041,   6.4117,  10.3442,   5.5588,
          2.1667,  12.9646,   6.3130,   4.2771,  13.7128,   7.7820,   8.5940,
          8.5882,   9.2361,   8.3921,   7.6288,  11.8752,   6.4396,  16.4783,
          9.2672,  12.4363,  10.0410], device='cuda:0')
Outer loop valEpocw Maximum [35/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 36.2
model_train val_loss valEpocw [35/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 52.6
model_train val_loss  valEpocw [35/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 137.9
Max_Val Meta Model:  tensor([29.2723, 29.4037, 29.2096, 30.7320, 33.1227, 32.8078, 31.3198, 33.1450,
        31.4499, 32.8413, 28.7468, 32.6768, 31.3475, 33.1839, 33.4080, 33.1968,
        33.1849, 32.7201, 30.2522, 33.8097, 32.8878, 33.3068, 33.0914, 33.0329,
        31.0493, 33.2284, 28.8326, 29.6343, 30.6184, 29.2192, 29.3062, 29.0516,
        30.8262, 28.4129, 30.6340, 30.7691, 29.9486, 30.9820, 29.8554, 33.4367,
        31.1748, 31.3150, 32.2127, 31.6100, 32.1519, 33.7663, 32.4553, 30.5056,
        32.6710, 32.4787, 33.2513, 30.2870, 33.7749, 31.5997, 32.7987, 32.7886,
        34.3728, 30.2495, 31.0615, 31.5493, 32.8537, 33.3812, 32.0054, 29.6171,
        32.4313, 31.4448, 31.1844, 30.2845, 32.5373, 31.9629, 31.6422, 31.5359,
        33.6034, 31.3930, 30.9685, 27.9358, 32.0351, 29.6215, 31.4420, 31.7072],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 8.0139,  3.5231,  1.5487,  3.1301,  2.3880,  3.0053,  2.3654,  4.7636,
         2.1285,  3.0914,  3.3524,  4.0091,  5.9759,  6.3764,  2.9484,  5.8555,
         3.0961,  2.1446,  3.5185,  3.1340,  2.7239,  4.8640,  3.2926,  2.4557,
         4.0704,  3.7849,  8.9986,  3.8698,  5.3890,  3.5828,  3.3546,  4.3309,
         4.9503,  3.0007,  4.3268,  3.5249,  5.4081,  5.2674,  4.0277, 18.8062,
        12.1749, 24.8270, 28.6865, 27.9179, 21.7610, 30.8738,  6.3274,  6.7200,
        16.8711,  8.5204, 13.4857, 16.1191, 20.9062, 11.4607, 24.8390, 35.2605,
        27.6012,  9.0675,  4.5016,  6.4895, 26.7456,  5.1399, 10.5970,  7.6544,
         8.6830,  5.0447,  5.3044,  7.6326,  5.9620,  5.5393,  4.4884,  9.8156,
         5.4828, 15.6542,  2.8438,  5.8819,  9.4257,  4.7129,  6.0946,  5.3743],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([24.0011, 10.4366,  4.5997,  8.2196,  6.1481,  8.3292,  6.2640, 12.4821,
         5.6173,  7.9922,  9.3574, 10.4523, 16.1126, 16.5196,  7.5715, 15.9476,
         8.0930,  5.4112,  9.3557,  8.1351,  7.0920, 12.5140,  8.5852,  6.3796,
        10.9863,  9.9661, 25.5822, 10.2599, 14.8158, 10.1418,  9.6043, 12.5153,
        13.4063,  8.6223, 11.7767,  9.4949, 14.7773, 13.9572, 11.2440, 50.7946,
        32.3534, 67.0965, 76.0308, 75.7617, 56.6590, 78.6779, 16.0584, 17.9128,
        44.2737, 22.1022, 34.8343, 45.2306, 54.3608, 30.1450, 63.9842, 92.0239,
        74.3298, 24.5866, 13.4568, 16.8161, 74.6617, 13.4522, 28.3295, 20.6191,
        22.1259, 13.2602, 13.9693, 19.8737, 15.2588, 14.5365, 11.7133, 26.2028,
        13.9792, 42.1533,  7.4760, 17.4773, 24.4971, 13.2927, 16.3779, 14.2726],
       device='cuda:0')
Outer loop valEpocw Maximum [35/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 34.4
model_train val_loss valEpocw [35/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 35.3
model_train val_loss  valEpocw [35/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 92.0
Max_Val Meta Model:  tensor([30.4715, 28.7322, 28.6009, 30.7450, 32.3000, 32.4174, 31.6819, 31.8545,
        31.4065, 32.3353, 28.8926, 32.7263, 31.7828, 31.7829, 32.4270, 30.7775,
        32.5649, 32.4689, 30.4213, 31.2150, 32.1857, 31.1245, 32.8646, 32.2295,
        31.3580, 31.4980, 29.0704, 29.9379, 31.0616, 29.4380, 29.3656, 29.1303,
        32.4103, 28.5020, 30.9778, 31.0843, 29.5797, 32.5864, 30.0434, 31.9642,
        31.2239, 31.2724, 31.5544, 30.9742, 30.7035, 30.3563, 31.6296, 30.6541,
        31.4833, 31.2243, 30.7951, 30.2471, 30.9074, 32.0193, 32.6462, 30.4137,
        30.8950, 32.3045, 30.4055, 32.4257, 31.0129, 32.3969, 32.7470, 31.9997,
        30.8343, 31.2399, 29.8267, 32.9114, 31.3174, 32.1569, 32.4551, 31.6193,
        32.1354, 31.5608, 35.9767, 28.1555, 32.1312, 30.0140, 31.9130, 32.7586],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 9.8927,  3.9266,  8.0252,  3.4158,  6.1114, 13.2609,  6.3147, 12.4027,
         3.8153,  7.8301,  2.8038,  6.7040,  4.3332,  4.8739,  6.6188,  4.0123,
         1.6929,  3.1047,  2.6792,  2.7391,  2.9515,  3.5316,  3.7565,  3.6627,
         7.4007,  3.3276,  7.3634,  0.9797,  5.6452,  2.4015,  2.6469,  2.0914,
         2.0131,  2.5075,  2.4634,  2.0782,  3.0294,  2.1591,  2.2262,  2.3990,
         2.5607,  2.0164,  2.4672,  2.6001,  2.5377,  2.4080,  2.9189,  2.4324,
         1.9393,  4.5029,  2.6025,  3.2813,  4.9832,  2.2545,  4.2572,  1.7850,
         2.2410,  2.2092,  4.7862,  2.8929,  3.3028,  4.3529,  2.3469,  1.2191,
         4.6160,  2.2215,  2.1226,  2.9338,  2.9044,  2.6266,  3.0604,  5.6985,
         2.2359,  3.1575, 86.8621,  2.8540,  5.7819,  3.6601,  4.2347,  3.4330],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 29.2051,  11.6502,  24.0988,   8.9568,  15.9136,  36.6921,  16.6054,
         32.9464,  10.1055,  20.3345,   7.8184,  17.5064,  11.5978,  12.9049,
         17.2502,  11.1446,   4.4603,   7.8074,   7.1353,   7.3082,   7.7395,
          9.3731,   9.7843,   9.6237,  19.9454,   8.8602,  20.9249,   2.5786,
         15.4270,   6.7949,   7.6128,   6.0678,   5.4136,   7.2338,   6.6708,
          5.5795,   8.3544,   5.6522,   6.2192,   6.5673,   6.7842,   5.3631,
          6.5267,   7.1048,   6.7397,   6.3808,   7.5160,   6.4734,   5.1573,
         11.9449,   6.9350,   9.2073,  13.2921,   5.9248,  10.8358,   4.7763,
          6.1310,   5.8686,  14.3118,   7.4393,   9.1411,  11.5383,   6.2440,
          3.2356,  12.1582,   5.8975,   5.7406,   7.5344,   7.6214,   6.9042,
          7.8424,  15.2418,   5.8039,   8.4913, 227.0750,   8.4599,  15.0483,
         10.2654,  11.2853,   9.0850], device='cuda:0')
Outer loop valEpocw Maximum [35/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 36.0
model_train val_loss valEpocw [35/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 86.9
model_train val_loss  valEpocw [35/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 227.1
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [88.45774296 97.2137279  92.20769727 97.02489005 97.26733349 94.57608947
 96.99808726 93.89505488 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.05975804 96.65086926 94.18013913 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.9910698  97.84237521 92.81197841
 96.90915072 96.22689782 96.9627563  92.80588687 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 92.08952133 96.13796128 96.24273583 96.9067141
 92.62070394 97.17717864 96.10506695 96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 96.33045406
 97.96420609 95.44961684 96.19522179 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [84.90759128 97.2137279  90.67140995 97.02489005 97.26733349 96.36700333
 96.99808726 94.67477248 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.83634459 97.84237521 92.25399301
 96.90915072 96.22689782 96.9627563  93.91576613 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.98596508 96.13796128 96.24273583 96.9067141
 91.51813453 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.15258099 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [97.35225946 12.93551378 64.41104283 12.43324911  8.16443016 24.81780325
  7.93441432 32.2234482   8.73460262 18.15726162  1.45652482  3.49151792
  0.64217714 16.2408964   4.14569643 12.27404164 11.3196968   5.36804686
  1.05641862  1.81991905  3.42988207  0.58491448  2.49133745  7.28354428
 18.4035669  10.0683988  31.18405941 12.65766987  4.46844842  2.77352132
 26.20544474  0.9531498  35.1309089   2.10185314 16.21369159 31.66047234
 11.50603036 42.01725649 16.96777952 39.72020176  8.10125658 47.16142365
 16.54370537 15.84475785 15.22960782 37.0161797   2.55387916  1.99699214
  6.0333891   1.78429878  2.10565775  1.07713555  1.12994721  9.48635243
  1.44648524  8.75570844 67.55795659 29.4941986  16.21828659 13.88017045
 65.90033852  9.14098478 27.54796943 14.98602659  8.52823085  9.87935875
 12.14837371 10.84636626  5.16686128 11.24448538  0.47284214 44.16971114
  9.17719935 26.77755217 27.06988903  8.31097818  1.43112513  2.65535898
  0.24822012  0.96800792]
Accuracy th:0.5 is [45.59154981 97.2137279  69.99671057 97.02489005 97.26733349 74.59582607
 74.92355113 74.95279054 75.89819812 96.29146818 76.07972612 98.52097319
 99.41399349 79.26804011 75.82144467 96.56680596 96.29512311 75.46204359
 98.65376884 98.30776915 79.04874453 76.57557778 98.38695922 75.80682497
 82.07746007 96.65086926 94.0778012  75.28782544 98.01293844 76.0212473
 97.30875598 98.57457877 96.36213009 98.02024829 84.98678135 75.60336741
 75.53636042 87.29547642 97.11504489 72.9547642  78.08749893 92.05906361
 74.92233282 74.53247402 96.9627563  93.87434364 98.02877645 98.57336046
 93.15066824 85.01967569 85.57522447 98.55508583 98.99976852 75.08193126
 98.70615611 75.42549433 70.46454112 91.69966253 96.24273583 96.9067141
 89.79300934 97.17717864 92.23328176 75.48762807 98.42838172 76.02490223
 98.20786784 74.73349496 76.54755668 97.35748833 77.02513371 95.99054592
 76.41232441 95.45083515 74.86629062 80.68005994 85.8578721  76.06632473
 77.07264775 99.14718388]
Accuracy th:0.7 is [45.66343003 97.2137279  69.99671057 97.02489005 97.26733349 74.59582607
 74.92355113 75.30488176 75.89819812 96.4608131  76.07972612 98.52097319
 99.41399349 79.81140581 75.82144467 96.56680596 96.29512311 75.46204359
 98.65376884 98.30776915 79.5324131  76.57557778 98.38695922 76.54755668
 83.06307184 96.65086926 94.0778012  75.28782544 98.01293844 76.0212473
 97.30875598 98.57457877 96.36213009 98.02024829 85.24993604 75.60336741
 75.53636042 87.70482816 97.11504489 72.9547642  78.77584337 92.05906361
 74.92233282 74.53247402 96.9627563  93.87434364 98.02877645 98.57336046
 95.1657509  86.06376628 85.76162571 98.55508583 98.99976852 75.08193126
 98.70615611 75.42549433 70.46454112 92.09195794 96.24273583 96.9067141
 89.79300934 97.17717864 92.38069712 75.49006469 98.42838172 76.02490223
 98.20786784 74.73349496 76.54755668 97.55729097 77.02757033 95.99054592
 76.62674675 95.45083515 74.86629062 80.78970773 86.04670996 76.06632473
 77.07264775 99.14718388]
Avg Prec: is [56.0143381   3.0953393  11.49949131  3.38632719  2.22216637  3.77353075
  3.31961533  5.64828256  2.4691117   3.97406721  1.66471938  1.78245902
  0.63852433  5.10466354  2.56427897  3.12952032  3.73679044  2.69221322
  1.33816104  1.70117179  1.87386504  0.8968523   1.89355287  2.36039584
  5.04717331  3.59890191  6.61295056  3.12607524  2.07188965  1.9095095
  2.58930776  1.34970094  3.74645771  1.60793825  2.44582024  2.58967888
  3.04574266  2.56540716  2.80599404  7.56870122  2.25354097  8.21414812
  3.42200933  4.07932682  3.15239876  6.39493038  2.09488379  1.50234776
  2.13439832  1.57189919  1.84692085  1.59533193  1.01665945  2.9989229
  1.40938112  2.79758451 11.40459012  3.64285834  3.90434103  2.78988381
 10.74486063  2.17388853  3.85046762  2.99821976  1.57690888  2.56730616
  1.79323046  4.10997006  1.26894283  2.42498101  0.17634067  3.46894323
  1.93788068  4.60376003  3.95882418  2.99627474  0.77932542  1.77028054
  0.13116213  0.76479382]
mAP score regular 15.76, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [89.40628348 97.22450607 92.76976356 96.96290206 97.90716795 94.07529212
 96.80843112 93.81617958 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.12419962 96.52938685 94.46146947 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.95164063 97.82744101 92.77225503
 97.07750953 96.48703192 97.03764606 92.21416648 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.07307472 96.39235618 96.16314124 96.78102499
 92.73488303 97.04761193 96.06099111 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.54433565
 98.03174129 95.44809029 95.90652017 97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [87.98863891 97.22450607 91.28484939 96.96290206 97.90716795 96.19054738
 96.80843112 94.72556494 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.97406383 97.82744101 92.57293769
 97.07750953 96.48703192 97.03764606 94.09522386 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.55143633 96.39235618 96.16314124 96.78102499
 92.25901288 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.76699803 97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [97.24354431 13.2322111  67.96729959 13.47170084  7.44032486 24.6257293
  9.08930534 32.99493837 12.18012867 19.75696865  1.46545449  3.89883534
  0.61270852 17.39644476  4.49191946 15.28586688 12.77901737  5.7194683
  0.98396138  1.77922253  3.43190784  0.61823681  2.80517736  7.53076186
 18.85699322 11.26796589 31.28000079 14.90479229  5.3733032   2.87417316
 27.39239452  0.83770447 38.1460096   2.22823451 15.19751255 36.59865609
 11.52915681 47.8816521  19.23327132 41.1013345   8.93176782 45.36048575
 17.84251577 17.08634837 14.42445145 37.67993364  2.43774954  1.79457433
  7.01296687  1.59852814  2.16032869  1.13564489  1.33736508 11.6309139
  1.41670305  9.33610654 61.44441786 31.40020574 14.31414626 15.1829893
 65.85923292  9.44321582 30.24229282 16.5042122   9.9202893   9.62716174
 14.31452727 12.0876095   4.8059944  11.81361496  0.39910218 48.27304384
  8.85868676 28.51925589 35.45920278  7.82495912  1.49105793  2.6174164
  0.22947028  0.96206297]
Accuracy th:0.5 is [45.77571817 97.22450607 67.83018163 96.96290206 97.90716795 73.00246655
 73.11458255 72.58140868 74.64185166 96.40979645 74.70164686 98.5325261
 99.34972718 76.56028104 74.73901886 96.31262924 96.21047911 74.09123751
 98.78167277 98.34068316 76.84679971 75.34195381 98.31327703 74.59451379
 77.32765279 96.52938685 94.3393876  74.24321698 97.81747515 74.70413833
 97.52597354 98.67204823 96.39983058 98.18870369 85.21812791 74.34536712
 74.36280738 89.81986696 97.0276802  71.75673319 75.53628821 92.37362035
 73.45840496 73.24663029 97.03764606 94.02795426 98.18621222 98.77668984
 94.33689613 84.8593567  83.53638787 98.55993223 98.87385704 73.44345616
 98.6969629  74.16598151 68.77693898 92.9018113  96.16314124 96.78102499
 90.13379176 97.04761193 92.20420061 74.29802925 98.32075143 75.15758527
 98.13139996 73.51321723 75.42417221 97.45870394 75.75304582 96.07843137
 75.26471834 95.44559882 73.39611829 81.27911902 87.83167651 74.80130553
 75.82778982 99.15040985]
Accuracy th:0.7 is [45.9675611  97.22450607 67.83018163 96.96290206 97.90716795 73.00246655
 73.11458255 72.82058948 74.64185166 96.41976231 74.70164686 98.5325261
 99.34972718 77.01621945 74.73901886 96.31262924 96.21047911 74.09123751
 98.78167277 98.34068316 77.18314772 75.34195381 98.31327703 74.92837033
 77.99038294 96.52938685 94.3393876  74.24321698 97.81747515 74.70413833
 97.52597354 98.67204823 96.39983058 98.18870369 85.56195032 74.34536712
 74.36280738 90.06153923 97.0276802  71.75673319 76.10683409 92.37362035
 73.45840496 73.24663029 97.03764606 94.02795426 98.18621222 98.77668984
 96.07095697 85.43986845 83.73570521 98.55993223 98.87385704 73.44345616
 98.6969629  74.16598151 68.77693898 93.2082617  96.16314124 96.78102499
 90.13379176 97.04761193 92.36365448 74.30301218 98.32075143 75.15758527
 98.13139996 73.51321723 75.42417221 97.52597354 75.75304582 96.07843137
 75.37434288 95.44559882 73.39611829 81.38874355 88.01355358 74.80130553
 75.82778982 99.15040985]
Avg Prec: is [54.25713889  3.7665874  14.87188254  4.59345199  1.69648339  4.29201094
  9.51555199  8.72323482  6.83636574  5.13228417  2.27779322  5.30052283
  1.54629209  5.81085539  2.94438646  4.03110459 24.26402789  5.72837417
  1.57290783  3.89765314  3.70876993  1.43650758  1.76553003  5.2067648
  5.71851007 13.9902062   8.29644827  4.65671497  3.87137319  7.05370514
  2.29465468  0.87467795  3.00780639  1.15712475  1.71591649  2.42069469
  2.04811658  2.12638816  2.14253021  6.22313828  1.76084037  6.05460384
  2.19866513  2.74150169  2.35534459  4.87301421  1.77088453  1.04663656
  1.44341681  1.2227899   1.26712419  1.01355997  0.75965767  2.31197117
  0.92448968  1.89546361 10.27002708  3.05140008  3.89086272  2.74962894
  7.97773006  2.02226275  3.30085575  2.5617222   1.3438488   1.85980953
  1.54565667  3.53492069  1.05500179  2.16497161  0.19015583  3.11525301
  1.53147532  3.92498905  3.19434239  2.33113435  0.56578848  1.42057119
  0.12050559  0.59984821]
mAP score regular 16.55, mAP score EMA 4.36
Train_data_mAP: current_mAP = 15.76, highest_mAP = 15.93
Val_data_mAP: current_mAP = 16.55, highest_mAP = 16.57
tensor([0.3362, 0.3375, 0.3352, 0.3804, 0.3844, 0.3620, 0.3789, 0.3760, 0.3792,
        0.3848, 0.3590, 0.3837, 0.3727, 0.3796, 0.3839, 0.3603, 0.3793, 0.3957,
        0.3763, 0.3762, 0.3804, 0.3773, 0.3826, 0.3811, 0.3705, 0.3764, 0.3530,
        0.3782, 0.3651, 0.3539, 0.3496, 0.3462, 0.3712, 0.3483, 0.3687, 0.3721,
        0.3637, 0.3836, 0.3584, 0.3653, 0.3774, 0.3775, 0.3784, 0.3672, 0.3773,
        0.3797, 0.3899, 0.3754, 0.3758, 0.3768, 0.3766, 0.3569, 0.3769, 0.3807,
        0.3930, 0.3745, 0.3660, 0.3767, 0.3354, 0.3883, 0.3601, 0.3774, 0.3755,
        0.3808, 0.3808, 0.3762, 0.3685, 0.3897, 0.3744, 0.3811, 0.3867, 0.3749,
        0.3889, 0.3725, 0.3882, 0.3370, 0.3861, 0.3559, 0.3737, 0.3778],
       device='cuda:0')
Max Train Loss:  tensor([10.4996,  4.3926, 10.2835,  7.9172,  7.5531,  8.1291,  7.3041, 10.5182,
         5.8222,  4.2105,  3.9401,  5.4670,  7.1986,  6.5352,  9.5211,  7.1899,
         8.5900,  6.6622,  4.5594,  4.6001,  9.2217,  4.6422,  5.9390,  6.2147,
         8.2693,  6.1909,  6.6382,  3.8913,  5.5878,  5.3126,  6.5812,  8.6249,
         7.2100,  5.1764,  5.9752,  5.4287,  6.4694,  7.4550,  5.2127,  8.8481,
         5.2647,  9.2136,  4.9242,  7.0070,  5.7600, 11.9357,  4.7937,  4.6290,
         6.9380,  7.5941,  5.5102,  5.0840,  8.9112,  7.6973,  6.4861,  6.5637,
        12.4824,  7.4067,  7.6418, 10.3920, 13.0243,  5.9888,  7.5340,  5.2153,
         7.8154,  6.5484,  4.8513, 10.2481,  4.6846,  4.5008,  4.1686,  7.0542,
         4.2629,  9.4863,  7.5439,  8.1806,  8.2141,  4.1267,  5.6141,  5.8303],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [36/80], Step [000/642], LR 1.0e-04, Loss: 13.0
Max Train Loss:  tensor([13.1543,  8.8523, 15.6619,  5.5517,  6.4636,  8.5805,  7.9523, 18.0462,
         8.1089, 11.1642, 11.3988,  8.6021,  9.3105, 14.1221, 12.0954,  8.2481,
        11.8026, 10.4318,  7.6820,  5.6081,  7.0008, 11.7696, 10.8715, 11.0042,
        10.8972,  6.4968,  8.0940, 11.1847, 11.2613,  6.7066,  8.0076,  4.0176,
         9.4359,  4.9332,  6.3454,  8.5019, 11.7721, 10.4191,  6.3998, 10.4501,
        14.9401,  9.1192,  5.5828,  6.0868,  6.3580, 11.5879, 11.5239,  5.6377,
        13.4841,  6.8091,  6.7415, 14.2292,  4.4208, 14.5399,  8.4274, 12.1133,
        11.2861,  7.6774,  6.7782,  7.3180, 12.8708,  9.5995, 13.9650,  8.4973,
         8.0354, 11.0885, 11.9502, 10.0839, 13.7271,  7.2030, 12.6096,  6.1359,
        10.1994,  7.5247, 12.0765,  5.2478,  3.1001,  3.9898,  9.4580,  9.8225],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [36/80], Step [100/642], LR 1.0e-04, Loss: 18.0
Max Train Loss:  tensor([11.1394,  4.4414,  7.8090,  5.6724,  7.2283,  5.7196,  7.6119,  7.5653,
         7.4435,  5.4335,  6.5051,  6.4976,  8.9326,  8.5451,  9.4804,  8.7906,
         7.6581,  6.5280,  4.3588,  6.0036,  6.7110,  5.6133,  8.7733,  6.0740,
         7.6269,  8.3739, 10.0105,  9.2544,  4.8996,  5.8860,  7.3367,  7.9066,
         8.4502,  5.8010,  6.4277,  7.6965,  6.2580,  6.4929, 12.4634, 10.9077,
         6.3593, 10.8298, 10.8288,  9.3192,  7.3939, 10.6285,  9.8057,  4.2825,
         7.1958,  6.6795,  5.8601,  7.0584,  5.4011, 11.4387,  7.9214,  5.7620,
         9.2142,  6.5735,  6.6436,  5.8683,  7.9934,  7.9085,  8.3043,  7.8434,
         6.6861,  7.6972,  5.6399,  6.8328,  7.0502,  5.8419,  3.9194, 10.3483,
         8.4508, 10.3676,  9.8226,  7.3144,  2.7036,  4.8393,  8.8136,  9.1731],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [36/80], Step [200/642], LR 1.0e-04, Loss: 12.5
Max Train Loss:  tensor([13.7227,  4.3367,  9.4001,  6.8501,  7.8762,  6.4923,  6.7969,  9.6468,
         8.5994,  7.6767,  7.2071,  6.4159,  8.1475,  7.7812,  9.0982,  6.0971,
         8.9184,  9.2157,  5.5962,  6.1368,  5.4037,  3.4982,  7.4577,  5.6697,
        10.4226, 10.7979,  8.2728,  6.6556,  7.3464,  4.1310,  7.1800,  5.1112,
         7.3292,  2.7575,  9.7758,  8.5682,  5.1434,  7.5448,  6.7662, 11.6266,
         7.0908,  9.6725,  5.9360,  8.7050,  7.6611,  6.9853,  9.6425,  5.1406,
         7.2311,  6.1162,  5.7341,  6.4355,  6.2357,  5.8683,  7.7692,  8.7792,
        12.9943,  8.6957,  8.4223,  7.4004, 12.6230,  4.8102,  9.0815,  7.8538,
         6.0914,  8.5964,  5.7251,  4.7517,  7.1626,  7.5221,  3.8391,  7.8768,
         5.9695,  8.8655,  3.8869,  6.3571,  3.5681,  3.9230,  8.7358,  3.5755],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [36/80], Step [300/642], LR 1.0e-04, Loss: 13.7
Max Train Loss:  tensor([ 8.3189,  5.1900,  8.5858,  8.3758,  6.1710,  6.4002,  6.5967,  7.3344,
        10.0801,  5.9525,  5.5924,  7.5721,  8.0946,  8.7849,  6.8051,  7.8612,
         6.9308,  8.7804,  9.4836,  7.8693,  6.3285,  4.6176,  5.7521,  6.4447,
         6.7611,  7.4806,  8.8669,  4.8552,  4.7336,  4.8212,  7.2139,  3.9652,
         6.1574,  3.9310,  6.3781,  5.8961,  8.5127,  7.3247,  7.9794, 10.9526,
         6.2434,  8.1859,  6.7077,  6.4016,  5.5758,  9.0190,  6.1721,  4.1834,
         4.3393,  5.3316,  7.8658,  8.3180,  6.9336,  9.0630,  8.8423,  4.5931,
         7.6864,  6.2807,  6.4501,  3.1662,  9.4856,  5.6324,  6.4111,  5.7702,
         5.3827,  7.3304,  4.8630,  4.8494,  5.0565,  5.0262,  4.8677,  7.5421,
         6.6709,  4.5880,  6.2296,  5.6070,  4.5958,  3.8333,  8.7210,  3.6336],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [36/80], Step [400/642], LR 1.0e-04, Loss: 11.0
Max Train Loss:  tensor([11.7095,  3.4951, 10.4164,  7.0103,  5.0541,  6.6264,  6.2430,  8.7406,
         4.5838,  8.7317,  7.1563,  7.5634,  8.2065,  6.0665,  7.7821,  7.5839,
         7.6293,  5.7923,  5.5848, 10.0393,  3.5438,  3.5446,  4.9053,  5.1873,
         9.5723,  7.0443, 10.9120,  7.4241,  7.2808,  7.1909,  6.3054,  6.5878,
         1.8829,  8.1600,  6.0465,  6.3815,  7.3837,  7.6121,  5.9072,  9.9485,
         5.1337,  7.3539,  6.0763,  7.4999,  7.5548,  8.2621,  6.5903,  3.8493,
         4.4692,  4.4109,  8.1979,  7.9125,  4.8714,  4.3809,  9.2493,  6.4039,
        10.4878,  9.3561,  6.0708,  5.9779,  9.2491,  7.1022,  5.6602,  5.4386,
         6.9220,  6.6629,  6.4512, 10.0111,  6.5456,  8.4478,  6.6729,  6.7964,
         6.4992,  5.5763,  9.5133,  7.4255,  2.6954,  6.7916,  8.8025,  3.4158],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [36/80], Step [500/642], LR 1.0e-04, Loss: 11.7
Max Train Loss:  tensor([10.6722,  7.0299,  9.6160,  5.4080,  7.8533,  6.7058,  7.9575,  8.3567,
         7.0955,  9.5247,  6.5857,  7.7807,  7.8217, 10.3874,  6.6640,  8.2148,
         9.9044,  6.5100,  6.7931,  3.9911,  4.9006,  3.7624,  6.2925,  5.4951,
         5.7672,  7.7450,  8.4308,  7.6131,  6.4700,  3.3138,  6.1345,  4.0714,
         5.0641,  7.4376,  3.9781,  5.4200,  8.0902,  6.7568,  6.0024, 10.4302,
         7.1654, 11.2339,  5.6857,  7.3679,  6.2700,  8.9123,  7.0552,  3.2044,
         3.8325,  4.6483,  6.7088,  5.8783,  5.0224,  7.5013,  7.5439,  4.1824,
         8.7490,  7.3100,  7.3949, 12.7923,  9.3107,  7.9707,  6.8035,  8.3272,
         6.4260,  6.8334,  4.9991,  6.8906,  6.1616,  8.4595,  4.8939,  7.9441,
         9.0296,  8.6704,  8.4436,  7.4166,  4.8772,  4.9312,  9.0754,  4.7739],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [36/80], Step [600/642], LR 1.0e-04, Loss: 12.8
Max_Val Meta Model:  tensor([ 22.0606,  24.1396,  28.4841,  17.7524,   3.9598,   3.9103,   5.6864,
          7.6755,   4.5998,   7.4017,   6.5062,   6.3949,   8.3807,   7.9216,
          5.2674,   5.5543, 131.6727,   4.2770,   4.1702,   2.9090,   4.2279,
          3.7933,   4.0142,   4.8528,  11.2227,   7.3276,   9.0137,   4.4982,
          5.9071,   5.5137,   5.0846,   3.1554,   5.0112,   3.2982,   3.1503,
          4.8045,   5.0005,   6.0654,   5.1692,  16.2649,   6.1827,   9.5663,
          5.6892,   7.7883,   7.0418,   8.5670,   3.9873,   4.2113,   3.8366,
          5.3565,   4.4167,   5.1947,   5.6219,   4.6063,   6.6498,   4.2018,
          8.0658,   5.8088,   8.2282,   4.7299,   7.0122,  11.0207,   4.7843,
          4.8745,   5.7403,   3.4511,   4.3043,   5.0714,   7.4237,  10.8841,
          4.1420,   8.8217,   8.9534,   4.8866,   6.6591,   6.4322,   2.8813,
          4.0199,   9.0923,   2.6939], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 15.0477,  22.6189,  21.4439,  16.9325,   4.1030,   4.1425,   6.4564,
          9.8453,   5.1449,   9.1173,   7.1907,   7.1641,   9.2309,   8.0331,
          5.0147,   6.6857, 113.3389,   5.2346,   4.6979,   3.2678,   4.7549,
          4.2728,   4.4264,   5.1055,  11.7947,   7.6103,  10.0599,   5.9005,
          6.5403,   5.7996,   5.6039,   3.6530,   5.6494,   3.8222,   3.8684,
          5.6401,   6.0799,   6.0829,   6.1001,  15.8606,   6.1755,   8.6603,
          5.8100,   7.7385,   7.2737,   7.9001,   4.4017,   4.2374,   4.0688,
          5.8746,   5.0155,   5.7601,   6.0052,   4.4411,   7.4142,   4.4782,
          9.0119,   6.2815,   8.2736,   5.6093,   5.7331,  10.8707,   4.9436,
          5.4341,   6.3817,   3.9296,   4.9588,   6.6022,   7.8976,  11.2849,
          4.6948,   9.0967,   9.4630,   5.3812,   6.8136,   6.6550,   3.3459,
          4.4219,   9.9549,   3.0738], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 44.7613,  67.0177,  63.9651,  44.5131,  10.6746,  11.4434,  17.0421,
         26.1824,  13.5677,  23.6915,  20.0280,  18.6686,  24.7700,  21.1592,
         13.0640,  18.5557, 298.8500,  13.2291,  12.4859,   8.6873,  12.5006,
         11.3241,  11.5700,  13.3973,  31.8319,  20.2168,  28.5005,  15.6014,
         17.9140,  16.3857,  16.0312,  10.5512,  15.2212,  10.9745,  10.4934,
         15.1557,  16.7183,  15.8575,  17.0224,  43.4240,  16.3654,  22.9391,
         15.3545,  21.0758,  19.2779,  20.8038,  11.2906,  11.2870,  10.8275,
         15.5900,  13.3193,  16.1379,  15.9350,  11.6661,  18.8642,  11.9591,
         24.6207,  16.6729,  24.6669,  14.4449,  15.9196,  28.8014,  13.1648,
         14.2705,  16.7577,  10.4457,  13.4558,  16.9427,  21.0931,  29.6095,
         12.1392,  24.2647,  24.3316,  14.4473,  17.5535,  19.7452,   8.6662,
         12.4245,  26.6404,   8.1353], device='cuda:0')
Outer loop valEpocw Maximum [36/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 131.7
model_train val_loss valEpocw [36/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 113.3
model_train val_loss  valEpocw [36/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 298.9
Max_Val Meta Model:  tensor([29.7139, 27.8837, 29.2700, 31.5216, 34.8991, 34.7873, 37.3322, 29.9491,
        31.7691, 33.1012, 30.5462, 35.9704, 31.7502, 32.0696, 30.4139, 30.9786,
        32.5939, 31.5715, 34.4633, 32.5324, 32.0346, 31.9036, 31.6989, 34.4449,
        31.4554, 31.9807, 29.8527, 31.2403, 30.8635, 29.4513, 29.6864, 29.3767,
        31.1506, 28.6908, 30.9412, 31.2195, 30.7475, 32.4911, 30.2089, 32.5323,
        31.2470, 31.5191, 31.6078, 31.4162, 31.0947, 31.3011, 31.1248, 31.1152,
        31.3760, 31.2237, 31.5612, 30.3667, 31.5675, 31.7824, 32.6384, 31.0074,
        30.6985, 31.9379, 28.2701, 32.0154, 30.7361, 34.5809, 32.4022, 32.2083,
        31.5502, 31.3002, 30.7487, 32.0844, 31.4114, 32.4147, 32.0454, 31.7175,
        33.7358, 31.4793, 31.6306, 28.2649, 32.1969, 29.8146, 31.9119, 31.8413],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([22.2156,  5.7180, 17.1694,  4.9296,  5.4516, 13.6390, 30.9575, 14.9085,
        12.8881, 15.9689,  7.7975, 69.9037,  7.9124,  4.8344,  6.7853,  4.0906,
         4.8243,  6.9857,  3.4426,  7.7118,  3.3450,  2.8298,  3.2131,  5.8759,
         9.7667,  7.6682, 10.9939,  6.6141,  6.0360,  2.3987,  3.3792,  2.2973,
         2.1128,  2.4993,  2.2555,  3.2635,  4.4084,  3.3914,  3.5395,  1.6084,
         2.3303,  1.9530,  2.6103,  2.4001,  2.7280,  2.1663,  2.6426,  1.8941,
         2.6046,  4.2818,  3.2936,  3.9605,  3.0778,  2.7172,  5.2773,  3.6946,
         3.2739,  2.7168,  4.0048,  1.9799,  2.2791,  3.3349,  1.8742,  2.5786,
         4.4387,  2.3654,  3.2491,  5.8504,  4.0205,  4.1966,  3.1321,  2.5469,
         3.8482,  2.1736,  7.8456,  2.9944,  2.0588,  2.3124,  7.5672,  1.8497],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 66.5185,  17.1081,  50.8689,  13.1002,  14.9523,  36.5357,  80.5189,
         40.8547,  34.0525,  41.3085,  21.7696, 183.9499,  21.3637,  12.6942,
         18.1021,  11.3096,  12.6951,  17.8584,   9.0407,  20.2935,   8.8474,
          7.5066,   8.4320,  15.1314,  26.4829,  20.3522,  31.1819,  17.5764,
         16.6344,   6.8042,   9.6925,   6.6459,   5.7016,   7.1968,   6.1262,
          8.7949,  12.1464,   8.8113,   9.9027,   4.3570,   6.1967,   5.1743,
          6.9230,   6.5092,   7.2481,   5.7012,   6.8937,   5.0668,   6.9691,
         11.3990,   8.7819,  11.1239,   8.1659,   7.1718,  13.4594,   9.8997,
          8.9742,   7.2495,  11.9758,   5.1123,   6.3903,   8.5701,   4.9937,
          6.7608,  11.6972,   6.3248,   8.8828,  15.0461,  10.8178,  10.9910,
          8.1614,   6.7814,   9.8080,   5.8371,  20.5838,   8.9145,   5.3016,
          6.5330,  20.3439,   4.9114], device='cuda:0')
Outer loop valEpocw Maximum [36/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 37.3
model_train val_loss valEpocw [36/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 69.9
model_train val_loss  valEpocw [36/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 183.9
Max_Val Meta Model:  tensor([29.2998, 27.9984, 29.0856, 31.3458, 27.3202, 32.8644, 31.7048, 31.0199,
        32.6609, 33.1375, 31.5809, 31.1586, 31.9865, 32.3457, 30.0602, 31.0764,
        33.3595, 31.6142, 31.8504, 32.4224, 32.9126, 32.1520, 32.1036, 32.4261,
        29.4595, 31.2011, 29.9619, 32.2800, 31.1349, 29.7223, 29.9322, 29.5854,
        31.8622, 28.8547, 31.2295, 31.4799, 31.0022, 31.1204, 30.4439, 32.7988,
        31.7815, 32.6237, 32.5987, 32.0206, 32.0203, 34.3213, 32.4006, 31.4976,
        32.6690, 32.9178, 31.9784, 30.7738, 32.1818, 32.1478, 33.4442, 32.3302,
        32.2953, 32.2211, 28.4780, 32.3213, 32.6838, 33.1999, 32.8004, 32.4235,
        31.7827, 31.5428, 31.0720, 32.1197, 31.7212, 32.6156, 32.5075, 32.0048,
        33.0559, 31.9157, 31.3985, 28.5483, 32.5575, 30.1319, 32.2625, 32.1769],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 5.9539,  3.6942,  3.4149,  3.3922,  2.2333,  0.2988,  2.3113,  4.8689,
         3.2470,  2.9449,  6.8894,  5.7703,  8.7014,  6.3119,  1.9153,  4.4624,
         3.2643,  3.5624,  4.0594,  2.4399,  3.1511,  4.1504,  3.6222,  2.9340,
         3.7584,  2.9512,  9.2824,  7.8251,  4.7760,  4.2325,  4.5055,  4.8050,
         3.9560,  3.6297,  3.6572,  5.4399,  6.7305,  5.3924,  5.2983, 18.4383,
        12.5021, 28.0424, 28.5634, 25.8460, 20.5935, 28.6389,  6.8420,  7.2288,
        16.3125,  7.4450, 14.9609, 15.9069, 21.5070, 11.0205, 23.6849, 36.3688,
        39.2475,  9.4824,  6.5456,  6.1614, 24.3190,  5.1729,  9.2322,  8.1380,
         8.5422,  4.3838,  6.4514,  9.5515,  7.1587,  5.8409,  4.7595,  9.8320,
         6.6642, 13.2718,  3.4714,  8.5743,  5.7364,  4.0797, 10.1852,  3.2201],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 17.7345,  11.0562,  10.1545,   9.0240,   6.2317,   0.8101,   6.1445,
         13.0848,   8.5611,   7.6448,  19.0361,  15.2691,  23.4634,  16.6325,
          5.1387,  12.3406,   8.5612,   9.1448,  10.8151,   6.4565,   8.2932,
         11.0535,   9.5335,   7.7574,  10.6746,   7.9940,  26.3403,  20.6515,
         13.1344,  12.0018,  12.9181,  13.9320,  10.6485,  10.4841,   9.9292,
         14.6518,  18.5551,  14.3841,  14.8211,  49.9413,  33.2041,  74.4594,
         75.7435,  70.2492,  54.7078,  74.7753,  17.6485,  19.3031,  43.2300,
         19.6544,  39.9137,  44.6734,  57.1828,  29.0986,  60.4483,  97.4662,
        107.7482,  25.3069,  19.5903,  15.9105,  68.0531,  13.5287,  24.5521,
         21.4112,  22.5746,  11.7344,  17.6196,  24.6215,  19.2311,  15.3197,
         12.3296,  26.2166,  17.1713,  35.6820,   9.1537,  25.5219,  14.7710,
         11.5068,  27.2664,   8.5421], device='cuda:0')
Outer loop valEpocw Maximum [36/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 34.3
model_train val_loss valEpocw [36/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 39.2
model_train val_loss  valEpocw [36/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 107.7
Max_Val Meta Model:  tensor([31.1898, 27.8906, 28.8107, 31.3509, 27.0373, 31.4226, 31.5459, 30.9151,
        31.5477, 31.9116, 31.5124, 31.1048, 31.9685, 31.9213, 30.0100, 31.0194,
        31.8193, 31.5018, 31.7080, 32.2445, 31.4200, 30.4965, 30.8902, 32.2024,
        29.5498, 31.0071, 29.8427, 32.2695, 31.1122, 29.5171, 29.6443, 29.2340,
        30.8016, 28.5014, 31.0918, 31.3547, 30.6874, 29.9715, 30.2755, 32.2136,
        31.3998, 31.4774, 31.2572, 31.3021, 31.1682, 31.3766, 31.0852, 31.2837,
        31.1367, 30.2334, 31.5881, 30.4143, 30.5555, 32.0563, 32.2468, 31.0002,
        32.3781, 31.7886, 28.3880, 30.8312, 31.0573, 31.4995, 31.8260, 31.9650,
        31.1688, 31.2521, 30.8734, 33.5316, 31.6744, 30.6910, 30.5750, 31.8393,
        32.0298, 31.5385, 35.9234, 28.3617, 32.2070, 30.0358, 32.3486, 31.2670],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 9.3276,  2.2785,  8.1948,  2.7110,  7.7216,  5.1189,  8.2054,  8.5054,
         3.4817, 11.1028,  5.8265,  5.0480,  6.9552,  5.2523, 12.5870,  4.2220,
         1.7903,  3.2796,  3.8411,  2.8583,  3.7117,  3.1935,  4.0910,  6.4788,
         7.5607,  2.5950,  7.5844,  2.0750,  6.1589,  2.6544,  3.0400,  2.5169,
         1.4223,  2.5954,  2.0675,  2.9356,  3.2037,  3.0193,  2.9461,  2.3110,
         2.7498,  1.6794,  3.1860,  2.8028,  2.9313,  1.9750,  3.0242,  2.5146,
         3.0985,  3.8030,  3.6271,  4.3717,  3.3154,  3.4497,  5.7073,  3.3784,
         1.7214,  2.3325,  5.3573,  2.8719,  2.5895,  4.8486,  2.6371,  3.0932,
         4.9009,  2.7361,  3.3557,  3.0481,  4.5839,  3.7238,  3.4716,  5.7690,
         3.3702,  2.8612, 50.7915,  4.6496,  2.3185,  3.3950,  8.1137,  2.1870],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 27.5326,   6.8251,  24.5547,   7.2125,  21.5868,  14.1142,  21.7369,
         22.9002,   9.3035,  29.2853,  16.1188,  13.3887,  18.7027,  13.9312,
         33.8095,  11.6939,   4.7694,   8.3835,  10.2594,   7.5905,   9.8935,
          8.6907,  10.8859,  17.1710,  21.4818,   7.0372,  21.5545,   5.4574,
         16.8823,   7.5322,   8.7493,   7.3357,   3.8975,   7.5422,   5.6035,
          7.9007,   8.8603,   8.2139,   8.2496,   6.3032,   7.2945,   4.4853,
          8.5188,   7.6537,   7.7966,   5.2361,   7.9315,   6.7013,   8.3357,
         10.3578,   9.6904,  12.2953,   9.0037,   9.1018,  14.6092,   9.0669,
          4.6649,   6.2802,  16.0641,   7.5315,   7.2227,  12.9241,   7.0632,
          8.1975,  13.1012,   7.3460,   9.1621,   7.7661,  12.2681,  10.0223,
          9.2262,  15.4259,   8.7893,   7.7022, 134.8849,  13.8441,   5.9824,
          9.5520,  21.6022,   5.8623], device='cuda:0')
Outer loop valEpocw Maximum [36/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 35.9
model_train val_loss valEpocw [36/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 50.8
model_train val_loss  valEpocw [36/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 134.9
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [89.51158002 97.2137279  91.74839488 97.02489005 97.26733349 96.5997003
 96.99808726 94.16795604 97.44398826 96.50467221 98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21813818 96.65086926 94.22643486 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.39989766 98.02024829 97.80217103 97.70470633
 96.94082674 97.15524908 97.11504489 93.06538663 97.84237521 92.57075328
 96.90915072 96.22689782 96.9627563  93.55636505 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.46452894 96.13796128 96.24273583 96.9067141
 92.52933078 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 96.46934126
 97.96420609 95.45083515 96.49127082 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [86.65951926 97.2137279  92.22109867 97.02489005 97.26733349 96.5997003
 96.99808726 94.69426542 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.07901951 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.78517562 97.84237521 92.61704901
 96.90915072 96.22689782 96.9627563  93.92794922 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 89.80153751 96.13796128 96.24273583 96.9067141
 92.15287338 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99420085
 97.96420609 95.45083515 96.34020053 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [97.70111239  3.31253809 63.99678323 11.19250737 12.50775611 28.44140687
  9.94110722 31.57203898  4.37254359 30.92456632  1.4162701   1.08937592
  0.64711317 16.48541298  8.94598943 26.71965258 14.74821588  3.82189859
  1.14214247  2.10072682  6.2774552   0.56133516  1.13764484 14.46358692
 19.40434769 11.05078913 31.36259902 19.19776126  4.12127595  1.41399234
 37.15980327  0.8492561  38.22784294  1.49563463 11.09414833 20.57572023
 10.9125347  30.28381122 28.07020448 39.40045271 14.02438306 48.47164443
 20.4862981  29.61422492 18.7183094  36.88428466  2.78224244  3.20842016
  5.77777793  2.10145331  1.17824847  1.35099182  1.27944318 11.6060955
  1.73225468  6.62444113 66.17761564 29.38746102 16.61650243 15.94529502
 66.36893536  6.35394124 26.84370343 14.72103793  9.27660814  7.79077821
  5.50658065 12.14887262  4.42085381  6.65319098  0.72376268 46.02055742
  9.50859009 27.52708783 34.13172998 17.92132216  1.58983451  3.17680236
  0.2686239   1.11211922]
Accuracy th:0.5 is [45.75114826 97.2137279  69.82736565 97.02489005 97.26733349 74.30221367
 74.56415005 74.72374849 75.45838866 96.31705267 75.76174754 98.52097319
 99.41399349 79.0304699  75.38407183 96.56680596 96.29512311 75.08558619
 98.65376884 98.30776915 78.65035757 76.21130347 98.38695922 75.491283
 81.84598141 96.65086926 94.0778012  74.91867789 98.01293844 75.60336741
 97.30875598 98.57457877 96.36213009 98.02024829 85.00018275 75.26102265
 75.26467757 87.31740598 97.11504489 72.50764489 77.720788   92.05906361
 74.55074865 74.23886161 96.9627563  93.87434364 98.02877645 98.57336046
 93.41869617 84.70291541 85.20364031 98.55508583 98.99976852 74.68110769
 98.70615611 75.10751575 70.30006944 91.74839488 96.24273583 96.9067141
 89.79300934 97.17717864 92.29176058 75.09167773 98.42838172 75.55585336
 98.20786784 74.35703756 76.15647958 97.3660165  76.60969043 95.99054592
 76.06510642 95.45083515 74.47034027 80.46685591 85.90051291 75.70936027
 76.68400726 99.14718388]
Accuracy th:0.7 is [45.74505671 97.2137279  69.82736565 97.02489005 97.26733349 74.30221367
 74.56415005 75.06487494 75.45838866 96.46324972 75.76174754 98.52097319
 99.41399349 79.58480038 75.38407183 96.56680596 96.29512311 75.08558619
 98.65376884 98.30776915 79.11696982 76.21130347 98.38695922 76.27952876
 82.82793826 96.65086926 94.0778012  74.91867789 98.01293844 75.60336741
 97.30875598 98.57457877 96.36213009 98.02024829 85.25359097 75.26102265
 75.26467757 87.71335632 97.11504489 72.50764489 78.38354796 92.05906361
 74.55074865 74.23886161 96.9627563  93.87434364 98.02877645 98.57336046
 95.37530001 85.82741438 85.40466125 98.55508583 98.99976852 74.68110769
 98.70615611 75.10751575 70.30006944 92.12607059 96.24273583 96.9067141
 89.79300934 97.17717864 92.46841535 75.09289604 98.42838172 75.55585336
 98.20786784 74.35703756 76.15647958 97.55607266 76.61090874 95.99054592
 76.28440199 95.45083515 74.47034027 80.57041215 86.12346341 75.70936027
 76.68400726 99.14718388]
Avg Prec: is [55.58493783  3.01294156 11.00212666  3.44184047  2.16173122  3.56187783
  3.33449134  5.52764515  2.51266463  3.82306072  1.63767347  1.66403166
  0.63367997  5.197424    2.65341576  3.07349935  3.67310413  2.82114151
  1.33390052  1.71879938  1.92652985  0.86424146  1.883825    2.48360117
  5.04371598  3.66882368  6.39741433  3.29911528  2.05962116  1.87936148
  2.62226764  1.29242399  3.66895375  1.62342834  2.39180687  2.3559485
  3.0655898   2.55187499  2.86308227  7.38434863  2.35392908  8.3953458
  3.43218213  4.11439826  3.29159509  6.50882085  2.07652458  1.52103352
  2.20673272  1.6307237   1.92038433  1.60244087  1.0797149   3.02493497
  1.3974568   2.59424404 11.19973388  3.68776007  3.99391779  2.81844793
 10.81434454  2.12361479  3.80271805  3.10674687  1.65894495  2.53953712
  1.78890874  4.16450742  1.23998034  2.40275466  0.19240725  3.35603403
  1.897001    4.58047588  3.860289    3.11279157  0.85359333  1.85396841
  0.15655649  0.73998736]
mAP score regular 16.68, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [90.1387747  97.22450607 92.11949074 96.96290206 97.90716795 96.63651992
 96.80843112 93.96068466 97.38894287 96.59416498 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43812442 96.52938685 94.40416573 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.45713431 98.18870369 98.00931809 97.89471062
 97.27931833 96.84829459 97.0276802  93.17587264 97.82744101 92.78969529
 97.07750953 96.48703192 97.03764606 93.56703291 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.36955926 96.39235618 96.16314124 96.78102499
 92.79218676 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.63901139
 98.03174129 95.44559882 96.35498418 97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [88.8282632  97.22450607 92.79467823 96.96290206 97.90716795 96.63651992
 96.80843112 94.75546254 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.41228791 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.86942223 97.82744101 93.0214017
 97.07750953 96.48703192 97.03764606 94.08525799 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 90.09143683 96.39235618 96.16314124 96.78102499
 92.54054862 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.11829484
 98.03174129 95.44559882 96.12576924 97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [97.51416369  3.4683614  67.74729404 11.97395063 11.81327876 28.13991384
 11.1424395  32.35144628  5.53360491 35.6270099   1.34871023  1.05368355
  0.62266887 17.33328073  9.22438372 35.30361169 16.23595057  3.98965391
  1.03846777  2.15834551  5.92118508  0.59231809  1.08673604 13.74321112
 19.0761992  12.29454228 31.00922454 24.94580742  4.8505968   1.47191574
 43.44796243  0.77766056 41.77000251  1.44820831  9.50771057 23.20699542
 11.87762316 35.08967604 32.23539049 41.5662095  16.36245345 48.25650074
 22.47450838 30.063446   19.91338681 37.84643736  2.71446868  3.10948129
  6.9377691   2.0106502   1.22483714  1.60705661  1.44398513 13.4727953
  1.85381779  7.86769061 59.53045833 29.53023658 13.87122547 17.26400827
 65.15728598  5.79383137 29.50349342 14.91604378  9.76480883  7.51651323
  5.10359338 12.82587806  3.98148807  6.89440697  0.53648158 50.34046903
  9.20542928 29.23319011 42.62200338 17.99522748  1.73988147  3.40692648
  0.22328599  0.99521549]
Accuracy th:0.5 is [45.7657523  97.22450607 67.66574482 96.96290206 97.90716795 72.82806388
 72.94017988 72.47676707 74.47741485 96.40979645 74.53222712 98.5325261
 99.34972718 76.53038344 74.56461619 96.31262924 96.21047911 73.91683484
 98.78167277 98.34068316 76.72720931 75.16755114 98.31327703 74.49485512
 77.24543439 96.52938685 94.3393876  74.06881431 97.81747515 74.52475272
 97.52597354 98.67204823 96.39983058 98.18870369 85.06116551 74.18093031
 74.18840471 89.77502055 97.0276802  71.61222812 75.42915514 92.37362035
 73.27901936 73.07222762 97.03764606 94.02795426 98.18621222 98.77668984
 94.50880733 84.8444079  83.4591524  98.55993223 98.87385704 73.27403643
 98.6969629  73.99157884 68.66233151 92.8644393  96.16314124 96.78102499
 90.13379176 97.04761193 92.31880808 74.14854125 98.32075143 74.9981314
 98.13139996 73.35376336 75.24976954 97.45122954 75.57366021 96.07843137
 75.102773   95.44559882 73.22669856 81.20935795 87.84662531 74.63188579
 75.64840422 99.15040985]
Accuracy th:0.7 is [45.97254404 97.22450607 67.66574482 96.96290206 97.90716795 72.82806388
 72.94017988 72.69103321 74.47741485 96.41976231 74.53222712 98.5325261
 99.34972718 76.96888158 74.56461619 96.31262924 96.21047911 73.91683484
 98.78167277 98.34068316 77.08348905 75.16755114 98.31327703 74.89847273
 77.90567307 96.52938685 94.3393876  74.06881431 97.81747515 74.52475272
 97.52597354 98.67204823 96.39983058 98.18870369 85.45980018 74.18093031
 74.18840471 90.02665869 97.0276802  71.61222812 75.99471809 92.37362035
 73.27901936 73.07222762 97.03764606 94.02795426 98.18621222 98.77668984
 96.18805591 85.43239405 83.66345267 98.55993223 98.87385704 73.27403643
 98.6969629  73.99157884 68.66233151 93.1708897  96.16314124 96.78102499
 90.13379176 97.04761193 92.44836435 74.14854125 98.32075143 74.9981314
 98.13139996 73.35376336 75.24976954 97.52597354 75.57366021 96.07843137
 75.21239754 95.44559882 73.22669856 81.29905075 88.03597678 74.63188579
 75.64840422 99.15040985]
Avg Prec: is [54.18906277  3.75565169 14.83751804  4.60628204  1.71101775  4.30128092
  9.3347167   8.70844491  6.81738085  5.11929738  2.26644695  5.30676854
  1.55066136  5.78099007  2.95201088  3.95166809 23.571413    5.65769325
  1.58208168  4.30173742  3.70987133  1.44290256  1.83162927  5.19182945
  5.71449978 13.9847809   8.3220657   4.73010566  3.87066905  6.73304002
  2.29843929  0.88587398  3.02140193  1.15903642  1.76394684  2.41971215
  2.04606552  2.12816632  2.21311517  6.16634962  1.76199115  6.1123557
  2.20710493  2.73945495  2.35196115  4.83309028  1.74055121  1.04638513
  1.46798722  1.2115284   1.24091     1.02386689  0.76091746  2.31058179
  0.9200787   1.89622316 10.23563004  3.03508178  3.90830017  2.70179257
  7.9794268   2.00719763  3.28396549  2.54790391  1.33776366  1.85683812
  1.53769431  3.53290758  1.05457229  2.1531081   0.19134088  3.11223672
  1.53432408  3.91476483  3.19099521  2.33568766  0.57402366  1.4218015
  0.12429307  0.59833785]
mAP score regular 17.56, mAP score EMA 4.35
Train_data_mAP: current_mAP = 16.68, highest_mAP = 16.68
Val_data_mAP: current_mAP = 17.56, highest_mAP = 17.56
tensor([0.3375, 0.3339, 0.3350, 0.3752, 0.3581, 0.3630, 0.3767, 0.3717, 0.3751,
        0.3789, 0.3615, 0.3776, 0.3711, 0.3781, 0.3725, 0.3608, 0.3752, 0.3902,
        0.3748, 0.3772, 0.3747, 0.3676, 0.3748, 0.3776, 0.3516, 0.3692, 0.3521,
        0.3792, 0.3645, 0.3531, 0.3483, 0.3438, 0.3644, 0.3445, 0.3683, 0.3710,
        0.3623, 0.3670, 0.3574, 0.3666, 0.3768, 0.3746, 0.3740, 0.3666, 0.3764,
        0.3789, 0.3821, 0.3752, 0.3714, 0.3684, 0.3748, 0.3558, 0.3685, 0.3794,
        0.3906, 0.3734, 0.3700, 0.3714, 0.3338, 0.3808, 0.3578, 0.3754, 0.3728,
        0.3786, 0.3744, 0.3734, 0.3666, 0.3864, 0.3732, 0.3699, 0.3725, 0.3743,
        0.3865, 0.3720, 0.3842, 0.3356, 0.3889, 0.3549, 0.3748, 0.3729],
       device='cuda:0')
Max Train Loss:  tensor([10.5637,  6.5729, 10.0005,  5.9955,  6.2103,  2.9011,  7.4970,  8.6793,
         7.4639,  7.3694,  5.9195,  7.7710,  7.7268,  8.4766,  5.9251,  6.1092,
         5.7253,  7.0148,  5.0523,  4.9186,  5.4013,  5.4291,  7.5539,  5.9185,
         8.6131,  7.6939,  9.7463,  7.2941,  5.6834,  7.4487,  6.2140,  5.4739,
         5.9737,  3.2589,  3.8075,  5.3335,  8.7714,  6.5505,  7.3844,  8.9116,
         6.3859,  9.0137,  6.2370,  6.4358,  4.7823,  8.3282,  6.7593,  3.1884,
         5.7988,  6.9758,  5.8180,  5.1247,  5.8137,  5.0669,  6.5515,  7.1724,
         7.8507,  6.4567,  7.0559,  6.8229, 10.9155,  7.5642,  7.0965,  7.0677,
         6.8092,  7.0445,  4.2544,  6.5518,  6.0433,  6.7809,  3.9448,  7.7086,
         5.7518,  5.4341,  5.9276,  5.3328,  5.2734,  4.8056,  9.0315,  4.8489],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [37/80], Step [000/642], LR 1.0e-04, Loss: 10.9
Max Train Loss:  tensor([10.0368,  5.8204, 15.5858, 10.2568,  6.4773,  8.8270,  5.1712, 15.3183,
         7.6149, 11.3684,  9.6710, 10.3627,  3.9878,  8.3228,  9.6930,  5.6333,
        10.6110,  6.8634,  5.9346,  8.7970,  9.9062,  5.8416,  6.1887, 10.4189,
         8.4819,  7.3062, 12.7035,  8.4717, 14.6360,  7.7189, 11.8578,  6.7092,
        12.7927,  6.5463, 13.7836,  7.1507,  9.1510,  6.6523,  9.8802, 11.4378,
        11.6113, 10.1529,  5.4739,  7.9393,  7.3939, 14.3595,  6.3415, 13.9828,
        14.1204, 12.4343, 12.4616,  9.3787, 14.6035,  6.5099,  6.9845,  8.7473,
        14.5421,  5.9667,  4.5462,  8.0406,  9.2399,  8.0796,  8.3847,  6.3605,
         5.6158,  9.6812,  4.9803, 13.4440,  6.5333,  9.7093, 13.6958,  9.2087,
        12.2965,  8.8863, 11.5543,  7.1351, 14.2785,  5.0593,  3.6295, 11.3994],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [37/80], Step [100/642], LR 1.0e-04, Loss: 15.6
Max Train Loss:  tensor([ 9.2745,  5.5309, 10.3963,  8.1618,  6.4317,  8.5139,  6.7922, 11.9218,
         7.0134, 11.5086,  4.5135,  8.7415,  4.6898, 11.0583,  3.3692,  6.1175,
        11.0427,  6.4095,  5.4080,  7.3631,  7.6761,  5.9053,  7.9400,  9.5014,
         6.9500,  8.2032,  9.0111,  9.7126,  7.0906,  3.1188,  5.6358,  7.5302,
         9.7807,  2.9304,  7.9539,  6.0448,  6.6004,  6.1082,  8.7038,  8.5399,
         7.7461,  7.8368,  5.9965,  7.8386,  5.4283,  7.7788, 10.8425,  9.7201,
         3.2979,  4.4098,  4.8635,  7.0270,  5.1777,  4.4951,  4.7949,  7.7480,
        11.4304,  8.0306,  7.3510,  8.0337,  9.7995,  9.1540,  8.7979,  7.7158,
         5.9953,  7.2864,  6.9020,  9.7219,  7.3253, 10.1466,  4.6654, 10.8463,
         5.6977,  7.9353,  8.5525,  4.2372,  5.8907,  4.9222,  3.3898,  6.8758],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [37/80], Step [200/642], LR 1.0e-04, Loss: 11.9
Max Train Loss:  tensor([12.5832,  6.1397, 11.1996,  8.1810,  5.1755, 10.9791,  7.2167,  8.5374,
         6.9359,  9.5537,  7.6323,  2.5007,  5.3473,  6.1625,  8.6846,  6.9903,
         6.9651,  9.0518,  6.1607,  5.6616,  5.7064,  6.2491,  5.6071,  4.9076,
         9.7019,  5.8909,  8.6809,  8.1052,  8.1469,  3.4416,  7.0434,  4.1010,
         7.1324,  3.8961,  7.9618,  7.6064,  5.2802,  6.7562,  3.8447,  9.6992,
         6.4090, 12.7374,  8.2210,  8.2648,  5.4619,  7.1618,  6.2993,  7.4089,
         8.5161,  6.3693,  5.4978,  9.1854,  6.8946,  6.1215,  5.4743,  7.8054,
         8.7435,  8.1377,  3.4410, 11.1037,  6.8166,  7.1281,  9.5663,  8.2809,
         8.1273,  6.4231,  8.4596,  6.6807,  7.6468,  4.7293,  5.2439,  8.1896,
         6.6260,  9.1546,  8.0646,  8.6834,  7.1777,  7.1511,  4.2080,  5.6060],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [37/80], Step [300/642], LR 1.0e-04, Loss: 12.7
Max Train Loss:  tensor([10.4562,  3.2956,  8.5130,  7.6003,  6.2602,  6.8492,  6.2768,  6.8607,
         4.5981,  7.4185,  5.9757,  5.9197,  5.5631, 11.6529,  7.8112,  5.9322,
         9.2849,  9.4207,  4.3897,  3.7912,  8.5439,  5.4923,  7.3470,  5.8456,
         7.3377,  6.0832,  8.0183,  9.3849,  5.8699,  5.1205,  6.7932,  4.2697,
         7.6482,  4.9612,  6.9036,  8.5965,  6.3236,  8.5230,  6.8339,  8.7032,
         6.7298,  8.7545,  9.5958,  7.7708,  7.6106,  6.0166,  8.3476,  6.6422,
         3.3627,  4.8940,  5.6295,  7.3931,  5.0249,  5.5676,  6.3306,  7.6878,
        11.6645,  6.9156,  7.3238,  5.2376,  6.6912,  6.7000,  8.5753,  7.5282,
         7.2505,  9.4120,  7.4539,  9.6722,  7.1430,  6.3278,  4.5167,  7.3646,
         7.4828,  6.3656,  4.4263,  4.9793,  6.4027,  5.8495,  3.2346,  5.5485],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [37/80], Step [400/642], LR 1.0e-04, Loss: 11.7
Max Train Loss:  tensor([12.0476,  6.3504,  9.5421,  6.0741,  6.3097,  7.7840,  7.3373,  6.8432,
         6.3884,  9.2091,  5.4853,  6.9586,  3.7200,  8.8095,  7.4434,  7.1142,
         7.7153,  6.1250,  6.4631,  9.5887,  2.9688,  6.4361,  4.6260,  4.8128,
         7.4880,  7.0472,  8.3726,  7.7554,  8.1276,  5.2397,  6.6250,  5.7136,
         5.8790,  4.9108,  6.3154,  6.0502, 10.1529,  6.9277,  6.1833, 11.8313,
         7.2898, 13.4879,  6.6360,  7.6568,  7.0579, 12.2388,  5.7872,  5.3898,
         2.4282,  5.3289,  5.6107,  9.5801,  6.0027,  9.0745,  5.0063,  7.2931,
        10.4973, 11.8784,  7.2553,  8.4029, 15.2575,  7.5455, 11.0848,  5.3633,
         5.9070, 11.4335,  4.5965,  7.5526,  8.9241,  5.3121,  5.8860,  8.4278,
         7.5013,  7.5900,  6.7480,  8.2004,  6.6983,  4.6231,  3.3869,  5.7284],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [37/80], Step [500/642], LR 1.0e-04, Loss: 15.3
Max Train Loss:  tensor([11.6655,  7.9981,  8.6533,  5.7188,  7.7561,  8.0197,  7.0458,  8.0508,
         6.3193,  7.2241,  7.7295,  5.3113,  3.8280,  9.4430,  8.2368,  6.4003,
         8.6190,  5.3235,  6.8199,  4.3399,  7.0269,  6.6892,  4.5533,  5.3737,
        10.4882, 10.4447,  9.6748,  8.9883,  7.0317,  5.4566,  8.9495,  4.3483,
         7.3728,  6.6613,  6.5392,  6.5899,  7.0838,  6.7308,  8.7597,  5.7624,
         6.7695,  7.6378,  5.7094,  8.1703,  6.6958,  9.2941,  6.5433,  7.9574,
         4.7509,  7.7360,  5.5730,  9.3024,  6.4386,  5.4451,  4.1384,  7.9003,
         9.0033,  7.9509,  7.6402,  8.0725,  9.9812,  6.8733,  6.2829,  4.9494,
         5.9728,  7.4825,  5.3298,  5.7972,  7.3675,  5.7398,  4.7772,  6.2701,
         7.1255,  6.8783,  6.6638,  7.1102,  7.2090,  4.7215,  4.4627,  5.8342],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [37/80], Step [600/642], LR 1.0e-04, Loss: 11.7
Max_Val Meta Model:  tensor([ 20.8580,  21.6610,  25.2598,  17.6045,   4.4122,   6.0909,   5.8845,
          7.9155,   4.0720,   6.6743,   5.4157,   4.5887,   4.5915,  10.7754,
          6.3075,   4.3958, 115.5111,   4.5618,   4.7009,   3.6397,   3.0811,
          4.6559,   3.8023,   4.1707,  10.4487,   7.4174,   9.9528,   5.1127,
          6.8400,   5.3090,   3.9980,   3.1555,   5.0208,   4.5255,   3.8338,
          4.9673,   5.9303,   5.6875,   4.0049,  16.4496,   6.5648,   8.9823,
          5.6107,   8.0666,   6.9515,   7.9871,   3.2149,   6.3127,   3.1881,
          5.2437,   4.2086,   7.2420,   6.4935,   4.5687,   4.1536,   4.3910,
         11.6140,   8.5559,   8.9793,   4.8543,   5.0143,   9.4127,   6.2181,
          4.4400,   5.4460,   4.2779,   4.7422,   5.5788,   8.0546,   9.4728,
          4.8315,   8.6452,   8.1833,   6.7449,   6.6256,   5.6587,   6.1063,
          4.8228,   3.5017,   5.8916], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([15.6466, 20.2010, 19.5416, 16.7338,  4.5072,  7.3358,  6.8826,  8.3016,
         4.6090,  7.9494,  6.1735,  5.0323,  5.3944, 12.4427,  6.4101,  5.3375,
        98.8821,  5.6247,  5.5265,  4.2778,  3.8491,  5.4675,  4.3160,  4.6312,
        10.7434,  8.4060, 10.9041,  6.3455,  7.7246,  5.8809,  4.5223,  3.8177,
         5.6171,  5.1945,  4.6419,  5.8346,  7.3469,  5.4324,  4.7946, 15.8561,
         6.7723,  8.6934,  5.9670,  8.4171,  7.2442,  7.5413,  3.8741,  7.2315,
         3.5046,  5.9342,  4.5343,  8.1362,  7.2150,  4.2984,  4.9346,  4.5353,
        14.4202,  9.4024,  9.2733,  6.0131,  4.9811,  8.9964,  6.5329,  5.2480,
         6.2866,  5.0109,  5.5959,  7.0600,  8.7765,  9.9603,  5.6196,  9.1540,
         8.8337,  7.6249,  7.1391,  6.2990,  7.0009,  5.3165,  4.2194,  6.7897],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 46.3537,  60.5024,  58.3412,  44.6052,  12.5880,  20.2114,  18.2709,
         22.3334,  12.2889,  20.9783,  17.0762,  13.3262,  14.5348,  32.9117,
         17.2090,  14.7929, 263.5617,  14.4150,  14.7459,  11.3402,  10.2728,
         14.8728,  11.5156,  12.2663,  30.5555,  22.7680,  30.9722,  16.7317,
         21.1942,  16.6561,  12.9832,  11.1035,  15.4141,  15.0773,  12.6021,
         15.7254,  20.2769,  14.8014,  13.4136,  43.2575,  17.9755,  23.2098,
         15.9546,  22.9598,  19.2446,  19.9014,  10.1381,  19.2746,   9.4367,
         16.1069,  12.0990,  22.8686,  19.5792,  11.3291,  12.6322,  12.1476,
         38.9720,  25.3190,  27.7790,  15.7912,  13.9195,  23.9621,  17.5237,
         13.8610,  16.7908,  13.4181,  15.2648,  18.2713,  23.5179,  26.9300,
         15.0881,  24.4577,  22.8536,  20.4949,  18.5841,  18.7682,  18.0031,
         14.9819,  11.2577,  18.2071], device='cuda:0')
Outer loop valEpocw Maximum [37/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 115.5
model_train val_loss valEpocw [37/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 98.9
model_train val_loss  valEpocw [37/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 263.6
Max_Val Meta Model:  tensor([30.6400, 27.6976, 28.9714, 32.9204, 33.4396, 34.7582, 35.2533, 30.4215,
        30.8621, 33.8662, 30.9799, 34.4123, 31.6708, 32.3155, 33.6817, 33.3159,
        32.4289, 31.3977, 31.2363, 35.8740, 31.6666, 31.4104, 31.3066, 31.4716,
        29.9426, 31.6167, 29.8587, 31.4509, 31.0642, 29.5535, 29.6731, 29.3709,
        30.6833, 28.7472, 31.1845, 31.3246, 30.7443, 31.3490, 30.2212, 31.2185,
        31.4482, 31.4282, 31.4860, 31.5862, 31.1311, 31.4536, 31.7962, 31.5066,
        31.1936, 30.7724, 31.5715, 30.5211, 31.0762, 31.8823, 32.4913, 31.0631,
        31.5667, 31.8974, 26.6035, 31.6385, 30.8065, 32.2993, 31.6382, 32.1307,
        31.3063, 31.3681, 30.8360, 31.5871, 31.6259, 31.5080, 31.0321, 32.0534,
        31.6658, 31.7163, 31.5954, 28.3659, 32.7301, 29.9716, 31.8379, 31.8631],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([20.8164,  5.8863, 16.2517,  5.5601,  4.4854, 12.8307, 30.8423, 16.0058,
        12.8660, 12.8353,  6.8651, 91.6549,  4.9696,  9.5067,  7.1926,  3.3841,
         5.4830,  7.2587,  3.2495,  8.3089,  2.3096,  3.2037,  2.9817,  4.8007,
         8.9038,  7.6582, 11.0720,  6.3769,  6.2814,  2.5462,  2.6554,  2.0673,
         2.1713,  3.3083,  2.5858,  3.0903,  5.0908,  2.7946,  2.5923,  1.7279,
         2.5054,  2.3412,  2.4373,  2.9790,  2.5629,  1.7872,  2.0532,  3.9122,
         1.6603,  3.7315,  2.3233,  5.2711,  3.7187,  2.0253,  2.7845,  2.9748,
         4.2859,  3.4877,  4.2035,  2.5150,  1.2339,  2.4923,  2.2476,  1.9207,
         3.7925,  2.5846,  3.2649,  6.0359,  4.4058,  2.9840,  3.3262,  2.5074,
         3.0993,  2.8934,  4.6650,  2.3564,  4.3044,  2.6250,  2.2959,  4.1277],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 61.9685,  17.7643,  48.5651,  14.6368,  12.1264,  34.4035,  80.4667,
         43.7578,  34.4936,  33.9752,  19.1293, 246.4093,  13.3987,  25.5640,
         19.2463,   9.0633,  14.5835,  18.7462,   8.7233,  21.6278,   6.1833,
          8.6859,   7.9400,  12.7974,  25.3412,  20.6599,  31.3364,  16.8532,
         17.2933,   7.2293,   7.6146,   6.0040,   5.9532,   9.5586,   6.9963,
          8.3186,  14.0389,   7.5731,   7.2590,   4.7539,   6.6584,   6.2330,
          6.5210,   8.0738,   6.8158,   4.6870,   5.3677,  10.4382,   4.4770,
         10.1129,   6.2010,  14.8252,  10.0792,   5.3445,   7.1231,   7.9854,
         11.5946,   9.3986,  13.0023,   6.6117,   3.4625,   6.5772,   6.0658,
          5.0565,  10.1017,   6.9327,   8.9319,  15.8580,  11.8307,   8.0450,
          8.9970,   6.6641,   8.1360,   7.7699,  12.2361,   7.0112,  11.0215,
          7.4190,   6.1453,  11.0801], device='cuda:0')
Outer loop valEpocw Maximum [37/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 35.9
model_train val_loss valEpocw [37/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 91.7
model_train val_loss  valEpocw [37/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 246.4
Max_Val Meta Model:  tensor([29.4051, 27.1716, 28.2454, 32.7536, 31.8068, 32.0413, 29.6797, 31.8608,
        30.9992, 30.7789, 30.0696, 31.7747, 31.1782, 31.1746, 31.2143, 32.3152,
        32.5849, 30.3090, 30.9926, 31.3390, 29.8339, 30.9302, 30.9772, 31.1371,
        29.2440, 29.8207, 29.3655, 29.6985, 30.6765, 29.0558, 29.1306, 28.8906,
        30.4079, 28.2094, 30.6734, 29.5131, 30.2265, 30.6522, 29.7205, 31.1387,
        32.3012, 32.4392, 32.8233, 31.9352, 31.7683, 33.7708, 32.6648, 31.1793,
        31.8973, 30.2868, 33.2981, 30.5082, 31.9672, 31.5951, 32.8032, 32.0000,
        32.6587, 31.6991, 25.8312, 31.1745, 32.1713, 32.2906, 31.4271, 31.6015,
        30.9122, 30.8546, 30.4675, 31.0378, 31.3194, 30.9612, 30.8032, 32.2081,
        31.2839, 31.4669, 31.0981, 28.0410, 32.3503, 29.5848, 31.3970, 31.4948],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 5.6852,  5.0543,  2.1463,  4.5758,  3.3098,  3.3711,  2.8818,  5.2999,
         2.7273,  4.0755,  6.3561,  3.3569,  5.4326,  7.1190,  2.5647,  3.8169,
         4.3483,  3.8765,  6.3762,  3.1466,  3.3892,  6.4539,  3.6777,  3.1448,
         4.0050,  3.1962,  9.0217,  9.4661,  7.0311,  4.6948,  4.7364,  5.5520,
         5.0563,  4.5012,  4.7298,  4.5269,  6.3630,  5.5730,  5.3379, 18.0003,
        16.9580, 29.6795, 26.2665, 26.1164, 20.7577, 29.3200,  6.6245,  9.1456,
        16.4512,  7.9930, 13.5396, 16.5830, 20.5422, 15.1910, 24.5319, 33.6646,
        30.5909, 19.1481, 10.3521,  6.2534, 31.8784,  6.5219, 11.8965,  8.7285,
         9.1630,  6.8269,  7.8290, 10.1355,  9.1353,  5.7133,  6.3721, 10.8172,
         7.0584, 15.7934,  4.9401,  9.1802,  9.3693,  5.7050,  5.1182,  7.8982],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([16.8483, 15.2590,  6.4401, 12.0486,  9.0447,  9.1681,  7.7813, 14.0538,
         7.2685, 10.9302, 17.7216,  8.9323, 14.6228, 19.2349,  6.8832, 10.3260,
        11.5660, 10.0675, 17.0975,  8.4009,  9.2124, 17.5113,  9.8292,  8.3973,
        11.3866,  8.7655, 25.5507, 25.4931, 19.3115, 13.3314, 13.5904, 16.1688,
        13.8347, 13.0350, 12.7840, 12.6086, 17.5682, 15.1575, 14.9496, 49.5182,
        45.0014, 79.0817, 69.4399, 70.9253, 55.2342, 75.8948, 17.1025, 24.3712,
        43.8723, 21.7175, 35.3419, 46.6667, 55.7745, 40.1229, 62.9160, 90.4178,
        83.3432, 51.6010, 32.0506, 16.4173, 89.2366, 17.2239, 32.0687, 23.0559,
        24.4755, 18.3346, 21.4089, 26.6546, 24.4919, 15.4143, 17.1118, 28.7823,
        18.5494, 42.4796, 12.9968, 27.3230, 24.0082, 16.0831, 13.6560, 21.1783],
       device='cuda:0')
Outer loop valEpocw Maximum [37/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 33.8
model_train val_loss valEpocw [37/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 33.7
model_train val_loss  valEpocw [37/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 90.4
Max_Val Meta Model:  tensor([27.6308, 27.0507, 28.3423, 31.5712, 31.7136, 32.3206, 29.7821, 31.9012,
        30.9274, 30.9569, 30.0146, 29.3657, 31.1891, 29.3286, 31.3064, 31.5149,
        31.1975, 30.2447, 30.9088, 31.2892, 29.6647, 30.8262, 28.9367, 31.1285,
        29.3640, 29.6679, 29.0796, 29.5245, 30.7447, 28.9705, 29.0339, 28.6616,
        31.4787, 28.1579, 30.8268, 29.5528, 28.8257, 30.5632, 29.6183, 31.6953,
        31.0486, 31.0523, 31.9514, 30.9533, 30.5689, 31.0711, 31.3234, 31.1372,
        30.4670, 30.0811, 29.7504, 30.1299, 31.5019, 30.1261, 30.3521, 29.7773,
        29.8710, 31.1134, 25.6460, 30.3701, 30.5385, 31.0271, 31.8503, 30.9164,
        32.4843, 31.7779, 30.3871, 33.4327, 31.3223, 30.8072, 31.0120, 30.7168,
        31.3726, 31.1075, 36.3773, 27.9074, 30.2201, 29.6765, 31.4191, 31.4594],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 9.7171,  3.2033,  9.5064,  4.4796,  7.4261,  7.0884,  6.8379,  8.0193,
         3.5445,  9.4678,  4.8664,  3.5977,  3.4015,  7.1891, 10.8960,  3.1445,
         3.5657,  4.5054,  4.1878,  4.0771,  2.8730,  4.1287,  4.7242,  6.1297,
         7.2227,  3.1496,  7.1882,  2.7380,  6.9130,  2.8837,  3.0821,  2.6836,
         1.8097,  3.9762,  2.8228,  2.4559,  3.9744,  2.7508,  2.7377,  4.0828,
         3.4423,  2.9553,  3.2766,  3.8004,  3.6206,  2.4783,  2.7143,  4.9332,
         2.4232,  4.0034,  3.5023,  6.5819,  4.8216,  2.9990,  3.5779,  3.3903,
         2.8048,  4.3673,  5.9877,  3.9277,  3.1212,  5.8401,  3.6674,  2.7940,
         4.9809,  3.4139,  4.0975,  3.6653,  5.7452,  3.1008,  4.4123,  4.6520,
         3.3600,  4.4342, 67.7708,  4.1437,  5.3454,  4.1007,  3.0375,  5.2998],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 31.0212,   9.6791,  28.6858,  11.8834,  20.3570,  19.2854,  18.4103,
         21.2757,   9.4628,  25.3588,  13.5706,   9.7725,   9.1267,  19.8420,
         29.2791,   8.5774,   9.5960,  11.6656,  11.2385,  10.9088,   7.7983,
         11.2106,  12.8877,  16.3835,  20.4849,   8.6554,  20.3801,   7.3461,
         18.9333,   8.1926,   8.8588,   7.8263,   4.9392,  11.5232,   7.5946,
          6.8454,  11.3824,   7.5335,   7.6738,  11.1203,   9.1176,   7.8841,
          8.6322,  10.3590,   9.6343,   6.5135,   7.1178,  13.1106,   6.5397,
         10.9007,   9.5548,  18.5191,  12.9663,   8.0563,   9.3096,   9.2739,
          7.7273,  11.8866,  18.5471,  10.4243,   8.6989,  15.6163,   9.8635,
          7.4748,  13.1852,   9.0393,  11.1834,   9.4961,  15.3558,   8.3784,
         11.7410,  12.5676,   8.8163,  11.9352, 179.6634,  12.3183,  14.2324,
         11.5270,   8.0742,  14.2936], device='cuda:0')
Outer loop valEpocw Maximum [37/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 36.4
model_train val_loss valEpocw [37/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 67.8
model_train val_loss  valEpocw [37/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 179.7
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [88.77815816 97.2137279  92.36364079 97.02489005 97.26733349 96.5997003
 96.99808726 94.73446961 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 94.80391321 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.12166031 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.37187656 98.02024829 97.80217103 97.70470633
 96.94082674 97.15890401 97.11504489 91.74108503 97.84237521 91.50838806
 96.90915072 96.24395414 96.9627563  93.14092177 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.5290993  95.31682119 96.24273583 96.9067141
 92.71451371 97.18083357 96.07704584 96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 96.18791194
 97.96420609 95.44596192 96.33654561 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [85.37907677 97.2137279  92.06028192 97.02489005 97.26733349 96.5997003
 96.99808726 94.73568792 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.11069553 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.17717864 97.11504489 92.75106297 97.84237521 92.88629524
 96.90915072 96.22689782 96.9627563  93.91820275 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 92.1979508  96.15623591 96.24273583 96.9067141
 92.07490162 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.20253165 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [97.76901007  7.7964446  65.37244037 11.65735708 12.40588716 21.46083668
  8.4755602  33.03574493  5.20762812 25.63478417  1.35809293  1.31694406
  0.71342526 17.57132498  8.55118742 25.01357259 12.41331013  3.64047223
  0.82560715  2.10738555  1.48002397  0.48449728  1.59584936 11.95878973
 19.47385476 10.08689397 30.16122929 15.19240485  4.48843557  1.75230632
  9.22296073  0.87763133 36.59425992  1.95472403  5.91696723 28.05092077
 11.27552992 35.52839822  7.79338491 34.48073522 15.86179519 48.89671172
 26.44986715 29.77471822 15.79598298 37.0954229   2.49363355  1.60367437
  7.20359681  1.9925214   4.09428094  1.5533878   1.31837974 17.6548669
  1.79213489  9.88795157 66.51751562 26.20952479 14.75519431 14.75590603
 67.29699527 32.94514389 26.35450838 14.22972953  6.26027553  6.74500161
  4.77437546 12.2673646   3.87954973  9.54251301  0.52861498 40.98105825
 14.73658168 24.10342002 29.87528437 15.26998871  1.46471659  2.34739897
  0.26516115  1.03690255]
Accuracy th:0.5 is [45.48433864 97.2137279  69.83345719 97.02489005 97.26733349 74.34485447
 74.66039644 74.76882592 75.574128   96.31461605 75.81657144 98.52097319
 99.41399349 79.37037804 75.47544499 96.56680596 96.29512311 75.21107199
 98.65376884 98.30776915 78.76000536 76.33678927 98.38695922 75.58265616
 82.14690367 96.65086926 94.0778012  75.01248766 98.01293844 75.78245879
 97.30875598 98.57457877 96.36213009 98.02024829 84.93683069 75.45717036
 75.23421986 87.17608216 97.11504489 72.60876451 77.78779498 92.05906361
 74.59826269 74.29124889 96.9627563  93.87434364 98.02877645 98.57336046
 93.5344355  84.85154908 85.43755559 98.55508583 98.99976852 74.77004422
 98.70615611 75.19401567 70.21844276 91.64362033 96.24273583 96.9067141
 89.79300934 97.17717864 92.41480976 75.15868471 98.42838172 75.71788843
 98.20786784 74.34119955 76.28683861 97.33434047 76.73273961 95.99054592
 76.17719082 95.45083515 74.57633313 80.44370804 85.8992946  75.73250813
 76.79487336 99.14718388]
Accuracy th:0.7 is [45.53916253 97.2137279  69.83345719 97.02489005 97.26733349 74.34485447
 74.66039644 75.07583972 75.574128   96.46446803 75.81657144 98.52097319
 99.41399349 79.89546911 75.47544499 96.56680596 96.29512311 75.21107199
 98.65376884 98.30776915 79.2253993  76.33678927 98.38695922 76.3404442
 83.18368441 96.65086926 94.0778012  75.01248766 98.01293844 75.78245879
 97.30875598 98.57457877 96.36213009 98.02024829 85.23287972 75.45717036
 75.23421986 87.59396206 97.11504489 72.60876451 78.51390699 92.05906361
 74.59826269 74.29124889 96.9627563  93.87434364 98.02877645 98.57336046
 95.53489845 85.83959747 85.63613991 98.55508583 98.99976852 74.77004422
 98.70615611 75.19401567 70.21844276 92.0578453  96.24273583 96.9067141
 89.79300934 97.17717864 92.60242931 75.16112133 98.42838172 75.71788843
 98.20786784 74.34119955 76.28683861 97.55485435 76.73273961 95.99054592
 76.37943008 95.45083515 74.57633313 80.54482767 86.10153385 75.73250813
 76.79487336 99.14718388]
Avg Prec: is [55.92305128  3.19457448 11.36079538  3.39421472  2.22521017  3.83401007
  3.41575179  5.58725063  2.4890084   4.01765143  1.56590924  1.61507641
  0.61611782  5.0967006   2.76071375  3.08187306  3.76022531  2.68352286
  1.44368451  1.79204073  1.99039163  0.83642195  1.96401856  2.46578211
  5.2683151   3.66179793  6.60333836  3.35423831  2.06080685  1.93605067
  2.66609407  1.34279789  3.63147041  1.59723217  2.3119324   2.34363417
  2.99976117  2.49914604  2.73227744  7.45570693  2.39508027  8.07188241
  3.43100558  4.06636641  3.34635393  6.44717984  2.15013153  1.50270432
  2.0763064   1.55487564  1.88689863  1.68762004  1.02848038  3.04600817
  1.34060321  2.69352402 11.3298379   3.77952809  3.94696907  2.76103131
 11.03359533  2.11972207  3.87843238  3.11350317  1.60044226  2.53144788
  1.78886549  4.21609922  1.25210963  2.37343007  0.18670903  3.31004631
  1.92828114  4.48938905  3.8585719   3.16075495  0.81107443  1.89804065
  0.13377602  0.83311879]
mAP score regular 16.14, mAP score EMA 3.78
starting validation
Accuracy th:0.5 is [90.03164163 97.22450607 92.45833022 96.96290206 97.90716795 96.63651992
 96.80843112 94.88252734 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 94.3020156  97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.22228866 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.44965991 98.18870369 98.00931809 97.89471062
 97.27931833 96.85078606 97.0276802  91.36457633 97.82744101 91.71587313
 97.07750953 96.46710018 97.03764606 92.99399557 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21204873
 98.6969629  97.58576874 90.49505444 95.30607669 96.16314124 96.78102499
 92.8943369  97.0650522  96.04853377 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.38488178
 98.03174129 95.43563296 96.10583751 97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [88.64140319 97.22450607 92.75232329 96.96290206 97.90716795 96.63651992
 96.80843112 94.87754441 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11174228 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.40167427 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.91805566 97.0276802  92.59037796 97.82744101 93.28798864
 97.07750953 96.48703192 97.03764606 94.12013853 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.43184593 96.47955752 96.16314124 96.78102499
 92.43092409 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 95.89157137 97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [97.49948211  7.72521947 68.4286701  11.66487638 11.74602818 20.17361694
  8.79273085 33.17446343  6.04564236 26.72782972  1.20907836  1.21301781
  0.58244321 20.03381316  8.65050437 35.18135134 13.0133523   3.79914865
  0.75036131  2.14824679  1.32403238  0.49874399  1.7682293  13.13222965
 19.68769868 10.55929614 29.98734417 18.52138669  5.02521692  1.85718224
 11.27561105  0.81731109 40.54723869  1.95737108  4.78522094 35.02908466
 11.99678188 41.90885939  9.62391627 34.95886059 17.56193868 49.08514792
 27.91443408 30.68302304 16.98867973 37.37193288  2.65571631  1.54565328
  9.30212433  2.12720538  5.16524887  1.95554206  1.63904045 19.51211686
  2.05321418 11.07950044 59.8922643  28.75185612 13.13291161 15.42822184
 66.27781761 37.15444115 29.19822009 14.76342393  7.52810681  6.72036527
  5.10817419 12.67452166  3.67736428 10.47991239  0.45490075 45.40190613
 14.40906248 26.03713951 37.72464689 15.66374346  1.69039008  2.52824113
  0.2387008   0.95165097]
Accuracy th:0.5 is [45.7657523  97.22450607 67.53618855 96.96290206 97.90716795 72.68854174
 72.82058948 72.3845828  74.33789272 96.40979645 74.39270499 98.5325261
 99.34972718 76.49301144 74.44004285 96.31262924 96.21047911 73.7773127
 98.78167277 98.34068316 76.65495677 75.03301193 98.31327703 74.44751725
 77.34509306 96.52938685 94.3393876  73.94424097 97.81747515 74.39519645
 97.52597354 98.67204823 96.39983058 98.18870369 85.01133617 74.05137404
 74.04888258 89.74014002 97.0276802  71.49762065 75.34195381 92.37362035
 73.13949722 72.93270548 97.03764606 94.02795426 98.18621222 98.77668984
 94.76044547 84.72980043 83.46413534 98.55993223 98.87385704 73.13451429
 98.6969629  73.85703964 68.54772405 92.83205023 96.16314124 96.78102499
 90.13379176 97.04761193 92.35867155 74.03642524 98.32075143 74.8635922
 98.13139996 73.21922416 75.13017914 97.45122954 75.44410394 96.07843137
 74.9757082  95.44559882 73.11209109 81.12464808 87.91887784 74.49236365
 75.50888208 99.15040985]
Accuracy th:0.7 is [45.9526123  97.22450607 67.53618855 96.96290206 97.90716795 72.68854174
 72.82058948 72.61379774 74.33789272 96.41976231 74.39270499 98.5325261
 99.34972718 76.94396691 74.44004285 96.31262924 96.21047911 73.7773127
 98.78167277 98.34068316 76.99628771 75.03301193 98.31327703 74.87604953
 78.03522934 96.52938685 94.3393876  73.94424097 97.81747515 74.39519645
 97.52597354 98.67204823 96.39983058 98.18870369 85.40498792 74.05137404
 74.04888258 89.99676109 97.0276802  71.49762065 75.90253382 92.37362035
 73.13949722 72.93270548 97.03764606 94.02795426 98.18621222 98.77668984
 96.38488178 85.37259885 83.65348681 98.55993223 98.87385704 73.13451429
 98.6969629  73.85703964 68.54772405 93.14348357 96.16314124 96.78102499
 90.13379176 97.04761193 92.50317662 74.04389964 98.32075143 74.8635922
 98.13139996 73.21922416 75.13017914 97.52597354 75.44410394 96.07843137
 75.0878242  95.44559882 73.11209109 81.23427262 88.12317811 74.49236365
 75.50888208 99.15040985]
Avg Prec: is [54.43163354  3.75708774 14.93671665  4.61477063  1.57499365  4.31254678
  9.15248101  8.70755324  6.84277855  5.12695664  2.27827245  5.32653885
  1.5553106   5.80062561  2.92604873  3.99417739 22.35727534  5.66413003
  1.59277518  4.54268776  3.68984125  1.50916297  1.8589985   5.07374087
  5.73854617 14.06825888  8.33807645  4.76178314  3.8925514   6.16346919
  2.30049985  0.87610653  3.07068658  1.11326399  1.70626892  2.40335174
  2.03706764  2.17356543  2.24302465  6.23523102  1.74012835  6.01972122
  2.23003615  2.73861677  2.36742609  4.84651486  1.78304395  1.04667125
  1.41015899  1.19326577  1.21808363  1.0040128   0.7450994   2.35899635
  0.86173427  1.860561   10.14006827  2.99521974  3.89958817  2.61861296
  7.9311563   2.00358234  3.24213106  2.57132426  1.36474072  1.85776304
  1.57167891  3.44621408  1.06236169  2.16585425  0.1859838   3.08780621
  1.56374687  3.88775373  3.51930493  2.32388859  0.59214436  1.48807603
  0.12725548  0.58525365]
mAP score regular 17.08, mAP score EMA 4.33
Train_data_mAP: current_mAP = 16.14, highest_mAP = 16.68
Val_data_mAP: current_mAP = 17.08, highest_mAP = 17.56
tensor([0.3112, 0.3309, 0.3331, 0.3764, 0.3651, 0.3679, 0.3702, 0.3781, 0.3755,
        0.3730, 0.3589, 0.3689, 0.3721, 0.3636, 0.3721, 0.3665, 0.3709, 0.3846,
        0.3734, 0.3747, 0.3677, 0.3688, 0.3654, 0.3747, 0.3529, 0.3645, 0.3533,
        0.3713, 0.3648, 0.3530, 0.3497, 0.3442, 0.3665, 0.3462, 0.3714, 0.3586,
        0.3508, 0.3661, 0.3570, 0.3681, 0.3773, 0.3754, 0.3801, 0.3673, 0.3766,
        0.3830, 0.3828, 0.3765, 0.3693, 0.3689, 0.3674, 0.3561, 0.3733, 0.3727,
        0.3838, 0.3667, 0.3615, 0.3673, 0.3231, 0.3762, 0.3582, 0.3738, 0.3712,
        0.3768, 0.3791, 0.3794, 0.3673, 0.3818, 0.3734, 0.3702, 0.3727, 0.3709,
        0.3843, 0.3722, 0.3856, 0.3364, 0.3752, 0.3551, 0.3754, 0.3708],
       device='cuda:0')
Max Train Loss:  tensor([10.0251,  7.8474,  7.3449,  7.4783,  4.5199,  6.2787,  8.6311,  7.4893,
         5.4049,  6.6814,  8.1495,  3.5447,  3.9395, 11.1492,  8.2078,  6.2567,
        10.1718,  7.8303,  7.0929,  7.1141,  5.8167,  8.2481,  5.2500,  7.2218,
         7.9132,  7.0022,  8.1480,  7.7128,  6.9154,  6.5517,  9.4234,  4.8084,
         5.6926,  5.2790,  5.3576,  5.7367,  6.7205,  7.4325,  6.1381, 10.0880,
         6.7542,  9.8350,  7.2031,  7.6186,  7.2666,  8.8598,  6.3295,  7.2374,
         5.2782,  4.6819,  7.4821,  9.3118,  6.2178,  6.9609,  6.2493,  6.7806,
        10.6673,  7.7578,  7.5676,  8.9377,  8.6171,  5.8131,  7.6166,  5.5115,
         7.3737,  8.1895,  6.6834, 10.2390,  6.9813,  4.3755,  4.8949,  7.1717,
         4.5763,  8.9702,  6.3018,  6.8041,  7.1403,  4.7441,  3.6015,  6.5203],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [38/80], Step [000/642], LR 1.0e-04, Loss: 11.1
Max Train Loss:  tensor([11.3688,  7.3920, 14.1677,  8.2584,  7.3686,  7.8345,  8.2740, 11.8895,
         8.9558, 10.1460,  6.4966, 14.1480, 12.4870,  5.8119,  8.3637,  6.3626,
        10.5194,  9.9852, 13.3994,  5.0563,  6.3848, 12.3477,  7.2990,  9.5356,
        10.7115,  9.4559,  9.1500,  8.7011,  9.0510,  5.9160,  5.2364,  4.8953,
        10.7994,  6.7613,  6.8855,  7.6975,  6.4763, 10.1442,  7.5485, 14.0223,
         9.0536, 12.6358,  9.4381, 11.4726,  5.5360, 12.3645,  8.9667,  9.0827,
        13.3469,  6.7243,  7.5233,  4.7857, 10.6967,  9.3593, 10.2306,  8.1109,
        11.4209,  9.2374,  7.9419,  4.4758, 14.9437, 10.1174, 12.5885, 13.4268,
        10.0307, 11.9576, 14.2141, 13.2894,  5.5578,  8.4862, 10.7532,  9.2631,
        13.4687,  8.7927, 11.7114,  7.2440,  7.2647,  6.5303, 10.3296,  4.7723],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [38/80], Step [100/642], LR 1.0e-04, Loss: 14.9
Max Train Loss:  tensor([10.4363,  5.0969,  9.1187, 11.4504,  5.3976,  9.9507,  6.1174,  7.9917,
         5.7143,  7.8640,  6.4040,  6.8194,  7.0050,  8.0738,  9.0510,  6.3437,
         8.8019,  8.5972,  6.0711,  4.5671,  5.0568,  4.3676,  6.8429,  8.4428,
         6.9247,  8.5687,  8.5985,  5.5371,  9.6758,  4.3056,  6.3716,  2.9515,
         8.4175,  5.1844,  9.6333,  8.6954,  6.7889,  7.2567,  7.2588, 10.5390,
         6.5572, 11.1790,  5.1260,  7.6451,  5.2243,  8.7480,  6.1093,  8.1583,
         7.7039,  5.1798,  5.0250,  3.3447,  5.3855,  6.1571,  7.7547,  7.8019,
        10.6415, 10.4729,  8.5103,  5.5653,  9.3366,  5.7423,  8.4876,  8.7676,
         9.7943,  9.6516,  6.6933, 12.8350,  3.7352,  6.7586, 10.2156,  7.7152,
         4.6120,  8.2261,  7.0241,  5.1391,  5.3376,  4.9671,  5.4745,  5.0527],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [38/80], Step [200/642], LR 1.0e-04, Loss: 12.8
Max Train Loss:  tensor([11.2113,  6.9458,  9.9924,  6.1214,  4.8856,  6.8906,  8.8353,  9.3846,
         5.8042,  9.8423,  5.6560,  9.4742,  5.5079,  6.3342,  6.2661, 11.1102,
         6.4194,  5.1123,  6.9982,  5.8366,  8.7073,  4.5176,  7.5222,  7.6628,
         8.6529,  1.9188,  6.4166,  7.1082,  6.5124,  5.9618,  4.8378,  4.3804,
         9.2377,  4.3438,  5.4343,  4.7223,  6.2431,  7.1528, 10.5718, 10.1041,
         7.8792, 10.9638,  9.7919,  8.0019,  9.5059,  8.6891,  8.8818,  5.6384,
         5.6101,  7.4025,  6.5176,  4.6982,  3.5990,  5.8660,  7.2556,  8.6286,
        11.0990,  8.5429,  5.5072,  8.4673, 10.1189,  8.0925,  7.7812,  8.5169,
         8.4902,  8.9554,  8.3665,  9.3812,  5.3411,  7.2972,  2.8839, 10.1864,
         5.9611, 10.2679, 10.8101,  8.0529,  6.0182,  5.2992,  6.2190,  5.2025],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [38/80], Step [300/642], LR 1.0e-04, Loss: 11.2
Max Train Loss:  tensor([10.2469,  6.9995, 10.2550,  8.2823,  9.4706,  7.5758, 10.3336,  7.2418,
        10.4701,  5.5518,  5.5756,  6.3288,  6.0718, 11.7529,  5.2202,  6.1555,
         7.3029,  4.4536,  6.9488,  5.7561,  7.2503,  6.3218,  4.4275, 10.3631,
         7.9569,  5.7421,  6.9892,  9.1080,  6.1060,  5.4368,  5.6702,  4.5235,
         5.8379,  7.7962,  7.1352,  4.6871,  7.1031,  6.4894,  6.2904, 11.6700,
         5.9727,  8.8425,  7.8690,  6.4165,  7.1926,  8.8815,  7.3532,  4.8173,
         6.1262,  6.4728,  5.7440,  4.5395,  7.2336,  5.6741,  7.4270,  4.7153,
         9.1411,  8.6924,  6.4037,  4.6349,  8.2755,  6.2893,  4.6304,  9.0235,
         3.4567,  6.1772,  6.3791,  6.9232,  5.3727,  6.7823,  2.8403,  7.7297,
         6.3728,  6.3149,  7.5338,  5.6065,  6.4444,  7.2097,  5.5967,  6.1261],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [38/80], Step [400/642], LR 1.0e-04, Loss: 11.8
Max Train Loss:  tensor([12.2942,  7.2527, 10.4015,  6.0711,  7.7902,  6.9885,  8.9320,  9.5362,
         6.7390,  6.8578,  6.3104,  6.5742,  6.1981,  8.9399,  8.3475,  5.6828,
         6.9966,  6.8240,  6.3145,  4.7433,  5.6069,  5.6683,  5.6547,  7.6408,
         4.9348,  6.6562,  8.1800,  5.6251,  6.6668,  5.2620,  6.5999,  4.5465,
         6.4952,  4.2377,  6.8094,  8.3059,  7.1275,  5.9142,  5.9080,  9.6570,
         4.4919,  8.4180,  5.0265,  8.0460,  6.0868,  6.9297,  7.4628,  4.5734,
         6.9107,  7.2690,  5.2940,  5.4897,  5.4178,  7.1702,  8.2970,  7.2333,
         8.7539,  6.1836,  5.4558,  5.9453,  9.4404,  8.9843,  6.9640,  7.9126,
         5.0351,  6.7368,  9.0437,  6.2070,  4.6547,  6.3998,  2.8837,  8.5723,
         4.6744,  6.9869,  6.5808,  4.5965,  7.1045,  5.4339,  6.7390,  6.0023],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [38/80], Step [500/642], LR 1.0e-04, Loss: 12.3
Max Train Loss:  tensor([11.0742,  3.5908, 12.2628,  6.5399,  8.4141,  8.3223,  7.7500,  9.1670,
         3.6917,  7.3036,  8.6114,  6.8332,  6.2238,  8.2754,  6.9093,  6.0870,
         5.5316,  6.2623,  6.2701,  5.9160,  9.4814,  4.6010,  3.6258,  5.1377,
         5.5118,  6.3255,  6.0462,  8.5866,  6.0001,  6.5896,  6.9694,  4.3638,
         6.5989,  7.4822,  6.0341,  7.3293,  7.4398,  5.9492,  5.4043,  8.2023,
         5.3133,  7.3930,  6.5464,  7.1665,  7.6164,  7.3535,  6.5488,  4.4220,
         5.7009,  5.7953,  8.4135,  7.3463,  5.3478,  6.4820,  7.2714,  5.1232,
         9.7130,  5.7563,  4.0738,  5.5577,  8.2061,  4.8310,  6.7762,  5.3948,
         4.5332,  6.1778,  6.7358, 10.4022,  5.5757,  7.2337,  3.9411,  4.7192,
         3.5133,  7.8486,  8.8902,  5.4346,  6.6741,  6.1067,  5.7062,  4.5691],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [38/80], Step [600/642], LR 1.0e-04, Loss: 12.3
Max_Val Meta Model:  tensor([ 16.7973,  23.4328,  27.1578,  17.2066,   3.1178,   6.6502,   5.6422,
          8.2418,   4.5558,   6.3554,   5.4745,   5.2980,   6.2280,   8.0753,
          7.1627,   3.9004, 131.3015,   3.0389,   5.3366,   4.7072,   3.5859,
          4.5283,   3.5633,   4.2689,  10.3597,   9.6575,   9.6572,   3.8015,
          4.9002,   4.6217,   3.9277,   3.0516,   5.2882,   2.9910,   4.2548,
          6.3136,   5.2684,   4.4799,   4.0595,  17.6569,   6.7394,   8.4930,
          5.5442,   7.8344,   7.5059,   9.2417,   3.4263,   3.8000,   2.9338,
          5.3051,   4.1522,   3.4639,   5.2615,   4.1528,   5.8656,   4.7654,
         10.6183,   6.3168,   7.4462,   4.1591,   5.7773,  10.5417,   4.5750,
          3.9865,   2.7717,   3.2008,   5.2735,   4.9377,   6.2152,  10.0885,
          2.8399,   7.2931,   8.8508,   7.5563,   6.3625,   5.9488,   5.4989,
          5.1109,   5.6346,   4.4743], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 13.3028,  22.2628,  19.9715,  16.1295,   3.4103,   8.3942,   6.8840,
         10.6538,   5.1373,   7.8413,   6.0075,   5.6868,   6.9414,   7.7675,
          7.0118,   4.5074, 113.2674,   3.7720,   5.9340,   5.2725,   4.3031,
          5.1031,   4.0267,   4.8009,  12.0810,  11.8801,  11.2047,   4.8243,
          5.4400,   5.1855,   4.4268,   3.5040,   5.7116,   3.4546,   4.9190,
          7.3630,   7.1687,   4.5886,   4.7108,  17.3865,   7.0893,   8.3946,
          4.9389,   8.0271,   7.4698,   8.0547,   3.4203,   3.8796,   3.1038,
          5.8471,   4.6961,   3.9181,   5.4704,   3.6017,   6.4137,   4.3317,
         10.6757,   6.2448,   7.6152,   4.5921,   5.1543,  10.2462,   4.8298,
          4.4390,   2.9385,   3.5779,   5.8742,   5.9136,   6.6321,  10.1969,
          3.2425,   7.4684,   9.0881,   7.9203,   6.5749,   6.0543,   6.1041,
          5.6485,   6.2872,   5.0316], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 42.7420,  67.2731,  59.9502,  42.8558,   9.3405,  22.8175,  18.5962,
         28.1803,  13.6796,  21.0249,  16.7391,  15.4159,  18.6557,  21.3603,
         18.8421,  12.2987, 305.4238,   9.8089,  15.8899,  14.0722,  11.7021,
         13.8356,  11.0187,  12.8138,  34.2328,  32.5953,  31.7174,  12.9941,
         14.9112,  14.6878,  12.6605,  10.1797,  15.5846,   9.9798,  13.2450,
         20.5306,  20.4380,  12.5334,  13.1940,  47.2374,  18.7880,  22.3630,
         12.9953,  21.8558,  19.8349,  21.0282,   8.9340,  10.3048,   8.4052,
         15.8486,  12.7833,  11.0025,  14.6545,   9.6637,  16.7096,  11.8112,
         29.5344,  17.0022,  23.5698,  12.2051,  14.3895,  27.4143,  13.0098,
         11.7817,   7.7508,   9.4300,  15.9935,  15.4892,  17.7603,  27.5441,
          8.6999,  20.1383,  23.6489,  21.2794,  17.0528,  17.9967,  16.2687,
         15.9071,  16.7486,  13.5677], device='cuda:0')
Outer loop valEpocw Maximum [38/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 131.3
model_train val_loss valEpocw [38/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 113.3
model_train val_loss  valEpocw [38/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 305.4
Max_Val Meta Model:  tensor([28.7800, 32.0991, 34.9443, 34.9616, 33.9585, 38.8614, 35.0971, 31.6900,
        31.9566, 34.9081, 30.7913, 39.3664, 31.8759, 34.1715, 36.0575, 31.6198,
        31.6080, 31.6894, 34.9445, 33.3511, 31.3266, 31.4310, 30.5527, 34.3444,
        30.2188, 29.9833, 30.1556, 30.8693, 30.9749, 29.5429, 29.5646, 29.4376,
        30.9029, 28.8792, 31.4866, 30.3637, 29.7400, 30.9969, 30.2267, 34.9092,
        31.5299, 31.1645, 31.9794, 31.5262, 31.1912, 31.9540, 30.8952, 31.3419,
        31.1501, 30.9152, 31.1113, 30.2543, 31.4987, 31.4803, 32.1857, 30.5624,
        30.9276, 31.5366, 35.4039, 31.3875, 30.9048, 34.9911, 35.3885, 32.0603,
        31.6815, 31.8313, 29.1849, 31.1482, 31.5802, 31.6499, 30.9265, 33.5318,
        33.3397, 31.8705, 34.4572, 28.4689, 31.4970, 30.1232, 31.8763, 31.6736],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([18.3161,  6.0040, 14.7677,  6.5178,  3.1165, 15.1071, 30.4037, 16.1948,
        13.3202, 15.1420,  7.0843, 63.4530,  6.2548,  5.2741, 10.0922,  3.0655,
         4.8918,  6.9712,  4.4591,  8.3625,  3.1230,  3.6110,  3.1056,  5.2478,
         9.1615,  8.8305,  9.9534,  5.8943,  5.3340,  2.5163,  2.6588,  2.2849,
         2.3703,  2.2945,  2.8102,  4.1783,  5.0369,  2.3390,  2.6581,  1.2920,
         3.3484,  1.4687,  2.1855,  2.5647,  3.0125,  1.8842,  2.1031,  1.7590,
         1.9523,  4.2010,  3.1976,  2.5883,  2.6529,  1.9215,  4.6363,  3.4240,
         3.7054,  2.4047,  3.3945,  2.4368,  1.8866,  3.3030,  2.4365,  2.1645,
         1.7081,  2.0530,  4.0229,  5.3983,  2.8784,  3.8525,  2.0744,  0.8949,
         3.3782,  3.0009,  5.0605,  2.4216,  4.3702,  3.4680,  4.4529,  3.4594],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 59.0587,  17.9825,  44.5904,  17.2080,   8.2318,  39.3614,  80.1819,
         42.8864,  35.1680,  39.0725,  19.5727, 170.8626,  16.7494,  13.9701,
         25.8240,   8.3828,  13.1551,  18.0780,  11.6752,  21.8159,   8.4342,
          9.7420,   8.4414,  13.5923,  25.8824,  24.2571,  28.0244,  15.8014,
         14.5958,   7.1169,   7.6086,   6.5933,   6.4503,   6.5601,   7.5144,
         11.5722,  14.2742,   6.3298,   7.4052,   3.4500,   8.8393,   3.8738,
          5.7257,   6.9242,   7.9786,   4.8509,   5.5122,   4.6602,   5.2477,
         11.2971,   8.6439,   7.2672,   7.0394,   5.1142,  11.9681,   9.3071,
         10.2138,   6.4993,  10.2508,   6.4402,   5.2321,   8.4816,   6.6806,
          5.6861,   4.4455,   5.3885,  11.3217,  14.2594,   7.6778,  10.3173,
          5.5805,   2.3847,   8.6895,   8.0159,  13.5875,   7.1598,  11.5860,
          9.7307,  11.9151,   9.2882], device='cuda:0')
Outer loop valEpocw Maximum [38/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 39.4
model_train val_loss valEpocw [38/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 63.5
model_train val_loss  valEpocw [38/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 170.9
Max_Val Meta Model:  tensor([27.0157, 28.7464, 28.7168, 31.9895, 32.9773, 32.7117, 32.7982, 31.2943,
        32.1662, 31.7338, 32.4990, 32.9833, 30.7416, 28.3463, 33.3974, 30.6088,
        32.9755, 32.0300, 32.2107, 32.7825, 32.7347, 33.2942, 31.5684, 32.5985,
        30.2210, 31.8659, 30.2443, 31.7483, 31.0483, 29.6492, 29.7283, 29.5380,
        30.8444, 29.0210, 31.6128, 30.4833, 29.9110, 31.1807, 30.3191, 32.8325,
        31.9114, 32.6621, 32.8978, 32.1390, 32.0460, 33.9145, 32.2377, 31.5729,
        32.3418, 32.0817, 32.6248, 30.5597, 32.0597, 31.7536, 32.2181, 31.8535,
        33.0554, 31.6958, 30.6765, 31.5083, 32.2240, 33.3868, 31.0322, 31.4881,
        31.9232, 31.9666, 29.1620, 31.2517, 31.7845, 31.7520, 31.0806, 32.5193,
        33.2694, 30.4215, 29.9470, 28.6087, 31.6565, 30.2242, 31.9838, 31.7547],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 4.1195,  3.3395,  1.9854,  2.8660,  2.3114,  2.5727,  1.9058,  4.7534,
         2.1008,  2.4841,  5.5544,  3.3593,  5.8786,  5.2567,  2.3003,  3.0046,
         2.6675,  1.3174,  5.6090,  5.6583,  3.1701,  4.9432,  3.2286,  3.1650,
         5.2836,  4.8911,  9.6781,  6.5942,  3.3279,  3.4968,  3.5870,  4.5080,
         4.5204,  3.3324,  4.5633,  5.0346,  5.1466,  5.0076,  4.1756, 13.2904,
        12.2454, 23.6932, 25.3137, 27.2391, 20.3353, 29.6909,  6.9694,  5.8865,
        16.3120,  7.1543, 14.5620, 15.7787, 22.1806, 14.2209, 23.4747, 33.8188,
        28.5235, 12.6301,  4.6847,  5.5995, 35.6423,  4.9031,  7.7894,  6.6326,
         6.5517,  4.4586,  6.9968,  8.8212,  5.3527,  5.4239,  3.3775,  4.6198,
         3.9778, 16.3340,  3.6443,  9.3517,  7.6524,  5.2251,  6.2765,  5.1554],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([13.2560,  9.9970,  6.0309,  7.5702,  6.1532,  6.8642,  5.0347, 12.5706,
         5.5358,  6.6042, 15.1040,  8.8451, 16.2323, 15.1502,  6.0643,  8.2186,
         7.0782,  3.4256, 14.9279, 14.8573,  8.4724, 13.1037,  8.7446,  8.3629,
        14.9266, 13.2815, 27.2268, 17.6174,  9.1146,  9.8865, 10.2471, 13.0108,
        12.3125,  9.5098, 12.1922, 13.9354, 14.5774, 13.5435, 11.6359, 35.7432,
        32.2794, 62.3368, 66.2968, 73.5138, 53.8670, 75.5186, 17.9771, 15.5608,
        43.3915, 18.9761, 38.7920, 44.3052, 58.8650, 37.8251, 61.2958, 91.7857,
        78.6316, 34.1032, 14.1368, 14.7857, 98.7818, 12.8020, 21.3499, 17.6445,
        17.0271, 11.6916, 19.7444, 23.3278, 14.2376, 14.5139,  9.0707, 12.3033,
        10.2488, 45.5020,  9.7777, 27.6514, 20.2754, 14.6545, 16.7856, 13.8466],
       device='cuda:0')
Outer loop valEpocw Maximum [38/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 33.9
model_train val_loss valEpocw [38/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 35.6
model_train val_loss  valEpocw [38/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 98.8
Max_Val Meta Model:  tensor([28.3572, 28.7168, 28.1170, 32.0639, 31.7935, 31.5333, 32.0152, 31.1411,
        32.0655, 31.8136, 30.1791, 31.5094, 30.7498, 28.0110, 31.3877, 30.6491,
        31.8967, 31.9661, 31.9948, 31.3073, 31.7842, 31.8312, 30.4656, 32.3328,
        30.2072, 31.5163, 30.0908, 31.7607, 31.0958, 29.4661, 29.4902, 29.2726,
        30.8884, 28.8027, 30.7105, 30.3844, 29.6142, 30.8393, 30.5046, 32.0101,
        31.5634, 31.1908, 30.1932, 31.3957, 31.1145, 30.7453, 32.2513, 31.3566,
        31.9663, 32.2377, 29.9447, 30.1541, 30.5073, 31.1172, 32.3315, 30.0839,
        31.3503, 30.4396, 30.3200, 31.5039, 30.9601, 31.6848, 31.4437, 32.5858,
        31.0158, 30.9822, 28.9956, 33.5449, 31.6743, 31.5692, 31.1577, 31.4200,
        31.8849, 30.1188, 35.7729, 28.4383, 31.4313, 30.1853, 31.9474, 30.8694],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 9.2314,  1.7006,  7.0400,  3.8541,  3.0761,  6.6811,  5.1167,  8.6053,
         3.4390,  9.5424,  3.8552,  4.3263,  3.7637,  6.0087, 17.6895,  1.8486,
         1.5878,  2.0814,  3.8490,  3.1153,  2.5940,  3.1396,  2.9391,  3.9825,
         7.0898,  4.0846,  7.7289,  1.6548,  4.8538,  1.7524,  1.8432,  1.8962,
         1.4112,  1.8614,  1.7973,  2.5071,  2.7331,  1.7743,  1.7464,  1.9078,
         2.8378,  1.2657,  2.0560,  2.3322,  2.6959,  1.5497,  1.9849,  1.6665,
         1.7043,  3.0833,  2.6480,  2.1874,  2.2320,  1.5609,  4.0409,  2.6402,
         2.1473,  1.8579,  4.2705,  2.3949,  2.5171,  3.5001,  2.4159,  2.0723,
         1.6523,  1.7562,  3.4480,  2.7354,  2.4941,  2.8042,  1.7784,  1.0553,
         1.6228,  3.6139, 79.9432,  3.0344,  3.7656,  3.5464,  3.8156,  2.9291],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 29.4734,   5.0932,  21.4976,  10.1677,   8.2733,  18.0594,  13.5858,
         22.7829,   9.0950,  25.3678,  10.7593,  11.5943,  10.3651,  17.3655,
         47.5710,   5.0575,   4.2510,   5.3894,  10.2614,   8.3065,   6.9696,
          8.4553,   8.0283,  10.5382,  20.0089,  11.1001,  21.7555,   4.4024,
         13.2616,   4.9559,   5.2746,   5.4851,   3.8493,   5.3213,   4.9156,
          6.9327,   7.7569,   4.8298,   4.8464,   5.1648,   7.4669,   3.3382,
          5.5556,   6.3095,   7.1453,   4.0717,   5.1102,   4.3957,   4.5368,
          8.1717,   7.2747,   6.1439,   6.0254,   4.1778,  10.4229,   7.2729,
          5.9054,   5.1132,  12.8917,   6.3310,   6.9470,   9.2988,   6.5601,
          5.4276,   4.3509,   4.6796,   9.7150,   7.1147,   6.6152,   7.5069,
          4.7326,   2.8508,   4.2439,  10.0712, 212.7202,   8.9652,   9.9867,
          9.9154,  10.1701,   7.9519], device='cuda:0')
Outer loop valEpocw Maximum [38/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 35.8
model_train val_loss valEpocw [38/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 79.9
model_train val_loss  valEpocw [38/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 212.7
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [90.23769204 97.2137279  92.34658447 97.02489005 97.26733349 96.59848199
 96.99808726 94.11313215 97.44398826 96.49979898 98.53193796 98.52097319
 99.41399349 95.31682119 97.1296646  96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 94.37872346 96.6009186  94.17039266 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36700333 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.98619656 97.84237521 93.1165556
 96.91280564 96.22689782 96.9627563  93.66235791 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31484753
 98.70615611 97.46591781 92.21744374 96.13674297 96.24273583 96.9067141
 92.42577454 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 94.59192749 96.20253165 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [87.59639868 97.2137279  91.65824003 97.02489005 97.26733349 96.5997003
 96.99808726 94.71497667 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.20108186 96.65086926 94.12166031 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.77421084 97.84237521 92.46476042
 96.90915072 96.22689782 96.9627563  93.87556194 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 92.18820433 96.13796128 96.24273583 96.9067141
 91.23305028 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.42890559 96.15623591 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [97.79826239  3.63302388 64.81376196 16.03840863  3.64827046 24.25152073
  8.02451203 33.88421685  6.6843232  30.76873777  1.4458246   1.7642772
  0.71106308 20.52033555  8.88409915 14.74001306 13.75819409  7.94815324
  0.91279125  1.16671064  1.59316077  0.5053996   1.01633896  3.87966373
 20.27463021 12.99568138 32.01293466 10.73732408  4.76855459  2.34041647
 27.47847746  0.85026614 37.48281155  1.43102439 16.70311917 28.04835924
 11.090325   34.62797529 21.74776286 40.6256777   7.63501569 50.86761604
 35.55077    20.54367308 20.25817411 36.72747501  3.18701981  2.19797594
  4.72991107  1.62226022  1.11831694  1.23051621  1.24927218 14.20070034
  1.69319656 10.16118107 68.317645   26.18168723 11.7224984  11.81457504
 65.57685158 13.59712982 28.82250314 15.04174457 11.84234402 10.06800343
  6.45066528 12.08676475  4.52605344 11.80694709  0.6297967  33.96106171
  5.84429493 24.99700241 26.05258151 15.53051218  1.50101293  2.45902263
  0.26591558  0.98597964]
Accuracy th:0.5 is [45.61104275 97.2137279  69.55690111 97.02489005 97.26733349 73.97083369
 74.34241786 74.50567123 75.2122903  96.27563017 75.49859285 98.52097319
 99.41399349 79.11696982 75.15990302 96.56680596 96.29512311 74.85654415
 98.65376884 98.30776915 78.63330125 75.93109246 98.38695922 75.32559301
 82.37594571 96.65086926 94.0778012  74.70669217 98.01293844 75.44011403
 97.30875598 98.57457877 96.36213009 98.02024829 84.67123939 75.07827634
 74.93817083 87.1115118  97.11504489 72.42967313 77.54291493 92.05906361
 74.39724175 74.00494633 96.9627563  93.87434364 98.02877645 98.57336046
 93.92916753 84.83205614 85.30110501 98.55508583 98.99976852 74.44231917
 98.70615611 74.89553003 70.09295696 91.46940218 96.24273583 96.9067141
 89.79300934 97.17717864 92.47694351 75.00761443 98.42838172 75.45595205
 98.20786784 74.17429125 75.92743753 97.33190385 76.37333853 95.99054592
 75.88723334 95.45083515 74.28515734 80.30482085 85.88467489 75.43402249
 76.4525286  99.14718388]
Accuracy th:0.7 is [45.69510605 97.2137279  69.55690111 97.02489005 97.26733349 73.97083369
 74.34241786 74.83339628 75.2122903  96.45715817 75.49859285 98.52097319
 99.41399349 79.6578989  75.15990302 96.56680596 96.29512311 74.85654415
 98.65376884 98.30776915 79.09138534 75.93109246 98.38695922 76.16135281
 83.33353638 96.65086926 94.0778012  74.70669217 98.01293844 75.44011403
 97.30875598 98.57457877 96.36213009 98.02024829 84.95266871 75.07827634
 74.93817083 87.5293917  97.11504489 72.42967313 78.27511848 92.05906361
 74.39724175 74.00494633 96.9627563  93.87434364 98.02877645 98.57336046
 95.76881373 85.83106931 85.51065411 98.55508583 98.99976852 74.44231917
 98.70615611 74.89553003 70.09295696 91.890937   96.24273583 96.9067141
 89.79300934 97.17717864 92.64628842 75.00883274 98.42838172 75.45595205
 98.20786784 74.17429125 75.92743753 97.55241773 76.37577515 95.99054592
 76.11505708 95.45083515 74.28515734 80.43152496 86.10762539 75.43402249
 76.4525286  99.14718388]
Avg Prec: is [55.77241217  3.12795762 11.39140597  3.37401143  2.26006753  3.88972837
  3.35482966  5.66913556  2.44210748  3.90582074  1.60443165  1.58874983
  0.59059924  5.049767    2.69210348  3.17161531  3.66484178  2.78256004
  1.29900548  1.78408579  2.00303699  0.85365678  1.719707    2.45002884
  5.09976018  3.68756048  6.62655821  3.1627938   2.0455405   1.88283791
  2.62542818  1.32100308  3.63104685  1.68052138  2.3665782   2.42418013
  3.15514212  2.49640445  2.70795486  7.241855    2.2896421   8.39566879
  3.37816201  4.13691333  3.29577214  6.44170016  2.06824594  1.54339233
  2.10775758  1.56669943  1.79255884  1.60652326  1.00344488  3.07495965
  1.34916594  2.70668947 11.3243158   3.64984942  3.92624774  2.88598291
 10.80396813  2.15454212  3.76166286  3.00416056  1.62927595  2.45895852
  1.81025832  4.1990901   1.25501881  2.40134702  0.23770974  3.35775013
  1.91039804  4.57443499  3.90920867  3.22591753  0.82722552  1.88304298
  0.13881176  0.76571055]
mAP score regular 16.18, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [90.3181603  97.22450607 92.91177716 96.96290206 97.90716795 96.63153699
 96.80843112 93.83611132 97.38894287 96.48204898 98.5250517  98.5325261
 99.34972718 95.11423375 96.49699778 96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 94.22727159 96.46710018 94.26962653 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.41976231 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  93.06126517 97.82744101 93.26805691
 97.08498393 96.48703192 97.03764606 93.55706705 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.19709993
 98.6969629  97.58576874 90.93106112 96.38737325 96.16314124 96.78102499
 92.68505369 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 94.2945412  95.90153723 97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [89.42621521 97.22450607 92.51563395 96.96290206 97.90716795 96.63651992
 96.80843112 94.66327827 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43314149 96.52938685 94.4290804  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.79716969 97.82744101 92.9018113
 97.07750953 96.48703192 97.03764606 94.07280066 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.63365473 96.39235618 96.16314124 96.78102499
 91.95754541 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44061589 95.77945537 97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [97.4868448   4.08348505 68.62106262 17.42720944  3.12867754 24.11071048
  8.90728257 34.2355982   7.82208302 34.33661088  1.45589602  1.82100644
  0.7001701  24.14447101  8.92772885 21.68735873 16.0266034   9.72553388
  0.851122    1.17856564  1.40200685  0.53059064  1.0664077   4.25289563
 20.74527007 14.13000262 31.50845819 10.71505616  5.85045216  2.52953981
 34.37129203  0.7882873  40.42715772  1.38577    15.68086713 36.38914777
 11.18425713 41.64155489 25.90007965 40.91580341  7.83763898 50.60571068
 35.05891285 20.52396957 20.60872842 36.64246366  3.31634686  2.19475738
  5.56508009  1.50767495  1.14600039  1.31868015  1.39382302 16.79106717
  1.67445799 10.76657274 61.33067678 27.30112095  9.9710571  12.86951208
 64.69717486 14.89388123 32.90808363 16.89726215 15.74833929  9.52880301
  7.96011466 12.95826372  4.20223931 13.3230384   0.53304177 37.18131838
  5.50608941 27.2551768  34.66271611 15.08987769  1.70296537  2.47616663
  0.20715927  0.87780466]
Accuracy th:0.5 is [45.68104243 97.22450607 67.35182002 96.96290206 97.90716795 72.45434387
 72.58639161 72.22512893 74.12362658 96.40979645 74.16349005 98.5325261
 99.34972718 76.36345517 74.21082791 96.31262924 96.21047911 73.5530807
 98.78167277 98.34068316 76.46809677 74.79881406 98.31327703 74.25567432
 77.37997359 96.52938685 94.3393876  73.71502604 97.81747515 74.16598151
 97.52597354 98.67204823 96.39983058 98.18870369 84.88676284 73.82714204
 73.81966764 89.67536189 97.0276802  71.29332038 75.1700426  92.37362035
 72.92024815 72.71843935 97.03764606 94.02795426 98.18621222 98.77668984
 95.02204948 84.6351247  83.32710467 98.55993223 98.87385704 72.90031642
 98.6969629  73.63779057 68.36335551 92.77474649 96.16314124 96.78102499
 90.13379176 97.04761193 92.38358622 73.82714204 98.32075143 74.63936019
 98.13139996 72.99000922 74.9009642  97.43628074 75.20492314 96.07843137
 74.76891646 95.44559882 72.90280788 81.04741261 87.90642051 74.25816578
 75.27468421 99.15040985]
Accuracy th:0.7 is [45.95510377 97.22450607 67.35182002 96.96290206 97.90716795 72.45434387
 72.58639161 72.44936094 74.12362658 96.41976231 74.16349005 98.5325261
 99.34972718 76.79946184 74.21082791 96.31262924 96.21047911 73.5530807
 98.78167277 98.34068316 76.84430824 74.79881406 98.31327703 74.72157859
 78.10499041 96.52938685 94.3393876  73.71502604 97.81747515 74.16598151
 97.52597354 98.67204823 96.39983058 98.18870369 85.23556818 73.82714204
 73.81966764 89.93198296 97.0276802  71.29332038 75.72813115 92.37362035
 72.92024815 72.71843935 97.03764606 94.02795426 98.18621222 98.77668984
 96.55928445 85.27543165 83.50898174 98.55993223 98.87385704 72.90031642
 98.6969629  73.63779057 68.36335551 93.09116277 96.16314124 96.78102499
 90.13379176 97.04761193 92.57044622 73.83461644 98.32075143 74.63936019
 98.13139996 72.99000922 74.9009642  97.52348207 75.20492314 96.07843137
 74.89847273 95.44559882 72.90280788 81.14956275 88.13065252 74.25816578
 75.27468421 99.15040985]
Avg Prec: is [54.4352142   3.75347453 14.91369735  4.57102016  1.56435277  4.32240467
  8.88953695  8.67711712  6.79108835  5.10951123  2.28708504  5.30527496
  1.55570665  5.79418335  2.97279034  3.98087454 19.98425714  5.49868957
  1.60371967  4.49181704  3.71237096  1.41240314  2.04434283  4.60411305
  5.74510422 14.31862895  8.38862599  5.01866939  3.96264288  5.84180177
  2.28926873  0.87003367  3.15403721  1.09130133  1.70906145  2.44013546
  2.09276001  2.16557886  2.26284052  6.27118441  1.7484035   6.05212105
  2.24241826  2.74058582  2.37841696  4.8760765   1.76657492  1.04315198
  1.41316734  1.18854672  1.21259485  0.98442885  0.74389772  2.35283396
  0.86743119  1.85054981 10.24569974  3.00666876  4.20805733  2.58671447
  7.98242845  2.02770615  3.24685195  2.56693709  1.35533032  1.86969341
  1.57142316  3.49246793  1.0599609   2.15483459  0.1873797   3.11739385
  1.55707078  3.92851482  3.51333052  2.3353718   0.59215947  1.47759181
  0.128148    0.57204926]
mAP score regular 17.19, mAP score EMA 4.30
Train_data_mAP: current_mAP = 16.18, highest_mAP = 16.68
Val_data_mAP: current_mAP = 17.19, highest_mAP = 17.56
tensor([0.3122, 0.3339, 0.3283, 0.3788, 0.3721, 0.3699, 0.3761, 0.3781, 0.3786,
        0.3766, 0.3587, 0.3735, 0.3631, 0.3471, 0.3726, 0.3656, 0.3733, 0.3855,
        0.3753, 0.3753, 0.3721, 0.3717, 0.3657, 0.3781, 0.3545, 0.3685, 0.3555,
        0.3752, 0.3659, 0.3539, 0.3503, 0.3461, 0.3665, 0.3506, 0.3648, 0.3619,
        0.3530, 0.3686, 0.3595, 0.3692, 0.3801, 0.3791, 0.3676, 0.3703, 0.3776,
        0.3813, 0.3894, 0.3793, 0.3759, 0.3784, 0.3643, 0.3561, 0.3706, 0.3738,
        0.3878, 0.3636, 0.3630, 0.3635, 0.3311, 0.3782, 0.3620, 0.3762, 0.3679,
        0.3843, 0.3806, 0.3760, 0.3551, 0.3779, 0.3769, 0.3740, 0.3744, 0.3706,
        0.3851, 0.3594, 0.3851, 0.3383, 0.3790, 0.3573, 0.3746, 0.3684],
       device='cuda:0')
Max Train Loss:  tensor([10.5381,  5.2465, 10.4462,  9.1788, 10.5615,  6.5737,  6.9246,  8.3646,
         5.7346,  6.0134,  7.9577,  7.2678,  5.4515,  6.5279,  8.8502,  9.7147,
         6.1099,  4.7316,  9.2242,  7.4354,  6.8866,  5.6694,  5.0130,  7.4043,
         9.5372,  7.8072,  6.1088,  5.7210,  4.9111,  5.5351,  4.5455,  5.8145,
         6.2869,  5.4664,  7.3045,  7.5647,  6.2444,  6.2321,  5.9514,  8.9547,
         6.6343,  8.3844,  6.3489,  8.4080,  6.1102,  8.6104,  5.1390,  5.2924,
         5.4593,  4.8118,  5.1729,  5.5326,  5.5098,  5.3905,  6.7132,  7.7515,
         8.8090,  6.6091,  6.8145,  7.3210,  8.7547,  7.3740,  6.8425,  6.7861,
         5.6019,  6.5844,  6.5328,  7.3862,  5.6784,  6.9657,  3.9366,  6.4837,
         5.5619,  9.2355,  5.2293,  6.0365,  5.6151,  4.5552,  5.6769,  4.5169],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [39/80], Step [000/642], LR 1.0e-04, Loss: 10.6
Max Train Loss:  tensor([11.3005,  4.4980, 11.2049,  8.0210,  7.0188,  6.4431,  9.8489,  9.8365,
         8.1924,  8.2679,  4.8860,  6.4719,  8.2750,  9.6645,  8.5102, 15.9214,
        10.3738, 11.0229,  7.2538, 12.0007,  7.7381, 11.6055,  8.1094,  5.3428,
         9.4088,  9.0186,  6.4107,  5.9805,  6.0677,  8.4274,  8.5172,  9.9663,
        14.6950,  6.6013,  6.2019,  7.0661,  9.1728,  9.0930,  9.6248, 14.4618,
        13.3813, 12.5461,  5.1506,  5.3420,  6.0935, 12.0322,  7.3809, 13.0079,
        11.5188,  9.9456,  5.6726,  4.6749, 14.8843,  5.0281,  6.7057,  6.5275,
        11.0913, 10.6803,  6.3758,  6.5513, 10.6320,  8.3078,  7.7234, 11.3316,
        13.4942, 13.2009,  8.9548, 14.0662, 14.4099,  9.1097, 14.7765, 13.1215,
         8.0945,  9.7005, 10.8831,  6.0011,  5.5660,  6.6614,  5.5797, 14.5955],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [39/80], Step [100/642], LR 1.0e-04, Loss: 15.9
Max Train Loss:  tensor([ 8.5851,  2.5888, 10.5502,  8.8497,  5.9190,  8.1132,  7.0510,  7.8388,
         5.3578,  7.1782,  7.6238,  6.0817,  7.7826,  9.9431, 10.8782,  8.1942,
        10.2022,  7.8959,  7.6390,  4.6625,  6.3671,  5.4507,  9.2210,  5.1243,
         3.1440,  8.0594,  6.8187,  6.7768,  4.8156,  5.8544,  7.8934,  4.6714,
         7.1618,  3.9165,  7.0254,  6.3281,  5.2118,  8.6107,  7.4022,  9.3413,
         5.6773,  7.0804,  4.2892,  4.9084,  6.5821,  8.1712,  7.0868,  5.5366,
         7.3782,  3.8348,  9.5080,  8.3597,  5.1854,  6.0315,  7.0396,  5.7649,
         7.6440,  8.6662,  5.4343,  5.6782,  9.6795,  6.3718,  6.2788, 10.1814,
         5.7059,  6.8072,  9.0255,  7.6667,  5.1498,  6.8154,  4.9732,  7.3704,
         5.7060,  6.9895,  8.5365,  7.3112,  4.0037,  5.1143,  5.1617,  4.7795],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [39/80], Step [200/642], LR 1.0e-04, Loss: 10.9
Max Train Loss:  tensor([ 9.1684,  7.3105,  9.0960,  4.9816,  5.4792,  7.1055,  5.4392,  8.3279,
         6.9038,  8.5604,  6.0153,  6.5287,  7.9620,  9.9581,  6.3211,  8.8140,
         7.6143,  4.6168,  7.8674,  3.9367,  3.3730,  4.4965,  6.7866,  7.0778,
         8.7450,  8.0475,  9.2101,  8.1689,  5.8554,  4.0241,  6.3022,  7.9409,
         7.7552,  8.9483,  5.6927,  5.4033,  6.9033,  6.8364,  6.4431,  8.5835,
         7.9530,  8.6889,  9.4164,  6.8493,  9.6876,  9.3142,  6.6189,  6.1662,
         7.0840,  4.9565,  4.4966,  6.2650,  6.7907,  7.5556,  7.4903,  9.0437,
         8.6890,  9.7813,  6.8301,  4.9086, 10.1445,  8.2651,  6.6616,  8.6297,
         5.6158,  6.4042,  9.4817,  7.5041,  6.5905,  8.0720,  6.4394,  8.9388,
         7.1114,  8.1208, 10.0443,  4.2135,  4.0941,  5.7447,  5.2659,  5.7935],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [39/80], Step [300/642], LR 1.0e-04, Loss: 10.1
Max Train Loss:  tensor([10.5693,  8.4292,  8.8591,  7.2133,  4.7474,  5.6178,  6.5163,  5.8017,
         8.4932,  5.2601,  5.4357,  5.2847,  7.9452, 10.1786,  7.3186,  7.2337,
         8.3405,  5.1004,  6.8700, 10.2019,  4.6349,  4.5741,  7.6723,  8.3162,
         6.8501, 11.0850,  8.1665,  6.5512,  5.7970,  8.8409,  8.6903,  4.8325,
         7.7714,  7.2322,  4.6131,  6.5708,  6.0630,  5.9442,  6.3413,  7.1019,
         5.7302,  7.8541,  6.1628,  5.4412,  5.0730, 10.3235,  7.2171,  6.0904,
         4.9014,  5.0316,  5.1901,  4.3683,  6.1537,  9.4378,  5.6568,  5.4372,
        13.0591,  6.9882,  7.6159,  7.6277, 10.8497,  5.6297,  7.9547,  8.1002,
         5.7006,  9.3257,  3.1815, 10.4859,  6.5898,  7.6316,  5.1480,  5.8053,
         6.6450,  7.3948,  9.2820,  8.1679,  3.4390,  6.0946,  5.3399,  4.9449],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [39/80], Step [400/642], LR 1.0e-04, Loss: 13.1
Max Train Loss:  tensor([10.6820,  5.5161, 10.4504,  7.5871,  6.5578,  6.4578,  4.3323,  9.7415,
         6.8070,  5.9285,  5.6081,  4.3484,  7.3684,  9.0915,  7.4915,  6.3150,
         7.8566,  4.7404,  7.9213,  4.9067,  9.3841,  4.5824,  4.4602,  7.7226,
        10.5180,  9.3504, 12.9709,  8.1428,  7.4532,  5.1533,  6.3734,  7.5664,
         7.6992,  3.8093,  4.1023,  4.4768,  6.0666,  9.8229,  7.0290,  7.3927,
         8.1955,  9.1202,  6.9768,  6.6218,  7.6676,  7.4124,  5.3664,  5.7792,
         4.2289,  7.0114,  5.3968,  5.1830,  6.9672,  5.6611,  6.6819, 11.7655,
         8.0207,  6.5617,  5.7431,  7.2530,  8.9413,  7.4527,  7.0218,  5.8694,
         8.0031,  6.2362,  5.4709,  9.1716,  6.8357,  7.1044,  5.7700,  6.7649,
         6.3034,  5.8137,  6.1317,  6.9738,  3.4192,  7.3485,  5.9835,  5.7074],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [39/80], Step [500/642], LR 1.0e-04, Loss: 13.0
Max Train Loss:  tensor([ 9.3591,  6.1734, 11.4723,  6.4969,  4.7590,  8.1396,  5.6407,  6.9767,
         5.9085,  7.4971,  7.8648,  5.1390,  7.3935,  9.3364,  5.6942,  7.9787,
         9.8630,  6.8171,  9.6308,  6.4367,  5.3356,  4.6094,  4.4938,  8.6584,
        10.9658, 11.9701,  8.6257,  5.1588,  5.6161,  8.4956,  6.6511,  5.8668,
         6.9860,  6.7575,  5.9897,  6.2079,  6.7175,  7.2710,  6.9058,  6.9063,
         4.2513,  6.0284,  5.3255,  5.8049,  5.2446,  7.0155,  4.2499,  5.0113,
         4.8690,  3.9265,  5.2900,  4.3571,  6.0635,  4.8838,  5.7867,  4.4187,
         9.0222,  7.4625,  7.9185,  4.0535,  7.5299,  6.0257,  9.1146,  6.4326,
         5.0424,  5.7501,  3.1740, 10.7340,  6.0593,  4.1069,  5.1502,  7.1387,
         3.4788,  7.8060,  9.3959,  7.7392,  3.4278,  4.5953,  5.3435,  4.9445],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [39/80], Step [600/642], LR 1.0e-04, Loss: 12.0
Max_Val Meta Model:  tensor([ 16.3908,  20.0725,  32.3067,  17.4728,   3.9785,   6.2490,   5.3682,
          7.5619,   5.2390,   6.0227,   5.4133,   6.0223,   7.9090,   7.1652,
          5.1266,   5.3307, 138.6741,   4.1894,   2.4850,   4.1427,   3.4843,
          4.6020,   3.5786,   4.0682,  10.1653,   7.0725,   9.4897,   2.5194,
          5.9585,   4.9940,   4.5122,   3.9808,   6.7990,   2.9413,   3.4303,
          3.8595,   5.3556,   5.0835,   4.4557,  16.0277,   5.8621,   8.6649,
          5.4142,   7.3086,   7.4558,   8.5110,   3.1392,   5.8240,   4.2121,
          4.7289,   3.1100,   4.3407,   6.7306,   3.4937,   4.5277,   3.3934,
          9.2611,   5.5984,   8.1219,   4.0980,   5.5561,  10.0932,   5.7583,
          4.0707,   5.0450,   4.1609,   2.2758,   4.4845,   7.2335,   9.6534,
          5.1441,   9.2046,   8.5958,   4.9840,   6.3330,   5.5426,   3.4101,
          5.3351,   5.3350,   4.9365], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 14.3039,  19.0243,  23.6769,  16.2678,   5.2450,   8.2208,   6.4669,
          9.3021,   6.1579,   8.0415,   6.3287,   6.9108,   9.1627,   7.5800,
          6.0938,   6.8410, 118.5875,   5.4031,   3.2143,   5.0344,   4.3900,
          5.5928,   4.3925,   4.9425,  10.0550,   8.2842,  10.0904,   3.8661,
          6.8726,   5.4954,   5.1504,   4.8726,   7.6086,   3.6886,   4.2371,
          4.4111,   7.2270,   5.7235,   5.2763,  15.8762,   6.0968,   8.1779,
          5.9017,   7.4672,   8.1039,   8.0672,   3.8279,   6.7674,   4.8137,
          5.4806,   3.4354,   5.2806,   7.5178,   3.8778,   5.4442,   4.2366,
          9.8473,   6.0668,   8.2088,   4.9248,   5.3064,   9.7666,   6.3386,
          4.8834,   6.1226,   4.9260,   2.8440,   5.6689,   8.1533,  10.3408,
          6.1772,   9.8309,   9.2726,   5.8152,   6.7866,   6.0665,   4.2364,
          6.1786,   6.3860,   5.9197], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 45.8155,  56.9712,  72.1149,  42.9468,  14.0946,  22.2230,  17.1930,
         24.6002,  16.2660,  21.3524,  17.6457,  18.5039,  25.2335,  21.8401,
         16.3569,  18.7094, 317.7097,  14.0164,   8.5638,  13.4156,  11.7970,
         15.0468,  12.0128,  13.0705,  28.3621,  22.4826,  28.3826,  10.3052,
         18.7827,  15.5283,  14.7037,  14.0769,  20.7600,  10.5193,  11.6160,
         12.1896,  20.4745,  15.5284,  14.6757,  43.0002,  16.0390,  21.5693,
         16.0564,  20.1651,  21.4619,  21.1596,   9.8294,  17.8441,  12.8044,
         14.4820,   9.4289,  14.8282,  20.2850,  10.3746,  14.0394,  11.6519,
         27.1258,  16.6896,  24.7908,  13.0209,  14.6603,  25.9579,  17.2289,
         12.7075,  16.0871,  13.1011,   8.0079,  15.0030,  21.6342,  27.6510,
         16.4977,  26.5276,  24.0776,  16.1820,  17.6222,  17.9297,  11.1778,
         17.2923,  17.0493,  16.0667], device='cuda:0')
Outer loop valEpocw Maximum [39/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 138.7
model_train val_loss valEpocw [39/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 118.6
model_train val_loss  valEpocw [39/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 317.7
Max_Val Meta Model:  tensor([28.5837, 27.6879, 29.5982, 31.6300, 28.3774, 31.2099, 36.5925, 31.2065,
        31.7849, 32.5377, 32.7479, 36.7027, 31.0787, 30.9525, 32.3909, 31.8980,
        33.0224, 32.1470, 33.2579, 35.0711, 31.3964, 31.4414, 30.3193, 30.4207,
        30.1035, 31.2815, 29.7730, 30.8427, 29.7646, 29.5080, 29.6298, 29.4443,
        30.9062, 28.7523, 30.5248, 30.2462, 29.8016, 30.8120, 30.3285, 31.3536,
        31.4599, 31.0770, 30.6212, 31.3744, 31.1189, 31.1732, 30.9553, 31.6817,
        31.4488, 31.1770, 30.3973, 30.3134, 30.9760, 31.2051, 32.0201, 30.0358,
        32.3981, 30.8322, 27.6506, 31.2732, 30.7465, 30.1495, 30.3329, 32.2050,
        31.4564, 31.3784, 29.5439, 30.3521, 31.7204, 31.4744, 31.3927, 31.9887,
        32.3658, 28.7073, 32.1771, 28.4028, 31.7555, 30.1989, 31.9706, 31.3355],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([18.2394,  6.5004, 14.6772,  6.2745, 17.0420, 14.4585, 31.9385, 15.6031,
        12.9276, 16.0726,  6.9227, 76.1104,  7.5720,  4.1082,  5.8948,  3.8910,
         4.4814,  6.9276,  2.0888,  7.6568,  2.8165,  3.6280,  2.8743,  4.4178,
         9.0261,  7.2640, 10.2660,  5.8600,  5.6270,  2.3592,  3.1358,  3.0906,
         3.4340,  2.2175,  2.4754,  2.3869,  5.0051,  3.0436,  3.0084,  1.7251,
         1.9985,  1.7018,  2.6967,  2.2171,  3.4283,  2.2033,  2.1381,  3.9371,
         2.8272,  3.5809,  1.9183,  3.3862,  4.1889,  2.0836,  3.4481,  3.4699,
         3.4061,  2.8338,  3.7122,  2.3155,  1.3760,  3.0205,  2.8963,  2.1770,
         4.1035,  2.6774,  1.5663,  5.1628,  4.0865,  3.4270,  4.0842,  2.9774,
         3.6593,  2.4951,  5.7706,  2.5175,  2.6105,  3.5934,  4.2221,  3.8509],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 58.0564,  19.5780,  44.3617,  16.5544,  47.0555,  39.3768,  83.4526,
         41.9196,  34.1172,  42.4688,  18.8721, 203.5945,  20.8085,  11.6378,
         15.7572,  10.5283,  11.8710,  17.8840,   5.4411,  20.0314,   7.5511,
          9.7471,   7.8339,  11.9178,  25.4035,  19.7319,  28.8227,  15.5514,
         15.8501,   6.6470,   8.9552,   8.9076,   9.3579,   6.3414,   6.7903,
          6.5867,  14.1566,   8.2929,   8.3385,   4.6926,   5.2502,   4.5070,
          7.3458,   5.9922,   9.0617,   5.7867,   5.5719,  10.3655,   7.5215,
          9.4896,   5.2575,   9.4789,  11.3125,   5.5638,   8.8904,   9.5417,
          9.4168,   7.7792,  11.2602,   6.1121,   3.7836,   8.1568,   7.9684,
          5.6634,  10.7964,   7.1256,   4.4018,  13.9762,  10.8469,   9.1700,
         10.8518,   7.9714,   9.5181,   7.1833,  15.0255,   7.4038,   6.8043,
         10.0243,  11.2213,  10.4224], device='cuda:0')
Outer loop valEpocw Maximum [39/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 36.7
model_train val_loss valEpocw [39/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 76.1
model_train val_loss  valEpocw [39/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 203.6
Max_Val Meta Model:  tensor([27.0731, 27.8405, 29.1271, 31.6168, 28.6653, 31.0305, 30.5198, 32.3922,
        31.3350, 32.8039, 32.7321, 31.0302, 31.2006, 31.2377, 32.0466, 32.1843,
        31.2859, 30.5047, 32.0134, 32.3256, 31.1236, 31.7831, 31.8272, 31.3482,
        30.2246, 31.9579, 28.3233, 31.3339, 29.5299, 29.8011, 30.0083, 29.8414,
        31.1991, 29.1571, 30.7745, 30.9410, 30.2006, 31.3983, 30.6302, 32.0952,
        31.9822, 32.7914, 31.7246, 32.3412, 32.1410, 33.2126, 32.8188, 31.9930,
        32.6573, 32.0367, 32.1400, 30.7812, 31.9053, 31.6648, 33.0047, 31.5495,
        32.2310, 31.1753, 27.9757, 30.8522, 32.6361, 30.5843, 30.7645, 31.3309,
        31.9886, 31.8206, 29.8843, 30.6955, 31.9229, 31.8480, 31.2548, 32.4761,
        32.6915, 29.2663, 32.4663, 28.7135, 32.1747, 30.3557, 32.0714, 31.6166],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 4.9709,  3.8460,  1.6237,  3.2460,  0.3084,  2.4237,  2.4244,  4.9466,
         3.6074,  1.9531,  5.9085,  5.2485,  8.4377,  5.9958,  3.1348,  4.1629,
         1.6039,  2.7667,  2.3923,  3.7685,  3.4823,  5.1623,  3.2621,  2.8003,
         3.0018,  4.2878,  8.4803,  4.7273,  5.1538,  3.6609,  4.3231,  5.9086,
         5.6608,  3.8870,  3.7282,  3.8794,  5.8938,  5.7886,  4.9717, 16.9886,
        13.0836, 24.6522, 27.3375, 26.0205, 22.7929, 28.0488,  6.8125,  8.3234,
        16.4481,  6.9064, 12.0033, 16.0309, 20.7790, 10.6397, 24.7348, 37.6268,
        29.2415,  8.7056,  6.7576,  5.9577, 32.3106,  5.4156,  9.6959,  7.3069,
         7.8843,  6.6579,  4.8084,  8.8520,  7.2029,  5.6963,  6.1375,  9.9748,
         5.7194, 13.3046,  3.5193,  7.1016,  6.1052,  5.7299,  6.4327,  5.9524],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 16.0123,  11.5752,   4.8586,   8.5986,   0.8495,   6.5862,   6.5147,
         12.9065,   9.6034,   5.1460,  16.0713,  14.1276,  23.2624,  16.8670,
          8.3984,  11.2388,   4.3405,   7.3551,   6.3154,   9.9482,   9.4432,
         13.8458,   8.8439,   7.4902,   8.4700,  11.6089,  24.5247,  12.6396,
         14.6348,  10.3121,  12.3026,  16.9651,  15.4200,  11.0694,  10.2375,
         10.6064,  16.5942,  15.6453,  13.7622,  46.1031,  34.4461,  65.0489,
         74.4210,  70.0173,  60.1955,  72.6828,  17.4167,  21.9245,  43.4485,
         18.1097,  32.4206,  44.8767,  55.6935,  28.4213,  63.7056, 103.2011,
         80.4986,  23.9177,  20.4697,  15.8210,  89.2947,  14.5995,  26.7378,
         19.3462,  20.6308,  17.6454,  13.5263,  23.9372,  19.1762,  15.2090,
         16.5228,  26.6080,  14.8625,  38.2260,   9.1176,  20.9147,  15.8861,
         16.0436,  17.1918,  16.1095], device='cuda:0')
Outer loop valEpocw Maximum [39/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 33.2
model_train val_loss valEpocw [39/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 37.6
model_train val_loss  valEpocw [39/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 103.2
Max_Val Meta Model:  tensor([28.9645, 27.7170, 28.7577, 31.5965, 28.2591, 30.9696, 30.5077, 30.7250,
        31.0618, 31.5157, 30.6123, 30.8877, 31.2826, 30.9499, 31.9691, 31.3224,
        31.1892, 30.4848, 31.8386, 30.9412, 31.0038, 31.5994, 30.6505, 31.1011,
        30.2225, 31.7534, 28.2752, 31.4050, 29.6440, 29.6784, 29.7685, 29.5744,
        31.1428, 28.9037, 30.4151, 30.9713, 29.7460, 30.4190, 30.4552, 32.3755,
        30.6686, 30.3002, 31.1716, 31.6404, 31.3229, 30.6447, 30.9931, 31.9621,
        31.2593, 31.8094, 30.6415, 30.4954, 30.5168, 31.5609, 31.9442, 30.2084,
        30.3878, 31.0709, 28.1449, 31.1850, 31.3382, 30.3615, 31.7553, 31.9766,
        31.2386, 30.2071, 29.7883, 32.9727, 29.9812, 31.6703, 31.7613, 30.0352,
        31.0391, 28.8543, 36.4586, 29.0105, 30.3580, 30.4473, 32.2581, 31.8338],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 9.1313,  2.8658,  7.9053,  4.1206, 16.7941,  7.3662,  6.3079,  8.0500,
         4.0442, 12.4513,  3.9538,  4.0799,  5.8216,  5.0216,  6.5355,  3.2974,
         1.3651,  2.9194,  1.9340,  3.2555,  2.5597,  3.4131,  2.9827,  4.1994,
         7.0200,  2.6598,  6.9091,  1.6625,  5.3481,  1.9853,  2.4669,  2.7746,
         1.9974,  1.9684,  1.8924,  1.6529,  3.0238,  2.2145,  2.0547,  1.8837,
         1.7603,  1.6770,  2.6143,  1.9560,  3.1282,  1.5920,  1.7800,  3.5152,
         2.6001,  2.7885,  1.9535,  3.0604,  3.8217,  1.9173,  3.1922,  2.3183,
         1.6128,  2.1813,  4.5396,  2.0377,  2.5393,  4.0070,  3.1125,  2.0785,
         3.6909,  2.2857,  1.5042,  2.4886,  3.6417,  2.8290,  3.7647,  4.3727,
         2.1814,  2.6710, 61.5928,  3.4494,  2.3108,  3.8380,  3.8956,  3.6129],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 28.9283,   8.6448,  23.9709,  10.8926,  46.3796,  20.0627,  16.8547,
         21.4449,  10.8301,  33.2611,  11.0542,  11.0299,  15.9900,  14.2482,
         17.5484,   8.9918,   3.6906,   7.7043,   5.1256,   8.7631,   6.9278,
          9.1848,   8.1571,  11.2728,  19.7583,   7.2351,  20.0614,   4.4165,
         15.1372,   5.5973,   7.0518,   8.0156,   5.4655,   5.6370,   5.2528,
          4.5156,   8.5980,   6.0997,   5.7045,   5.0918,   4.7242,   4.5288,
          7.0981,   5.2988,   8.2719,   4.2345,   4.6568,   9.2351,   6.9610,
          7.3652,   5.3705,   8.5735,  10.4488,   5.1289,   8.2394,   6.3742,
          4.4978,   5.9913,  13.7865,   5.3883,   6.9730,  10.8365,   8.4620,
          5.4671,   9.7886,   6.2444,   4.2222,   6.6233,  10.0097,   7.5767,
          9.9537,  12.0138,   5.7883,   7.6984, 162.0723,  10.1606,   6.1831,
         10.6982,  10.3312,   9.7595], device='cuda:0')
Outer loop valEpocw Maximum [39/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 36.5
model_train val_loss valEpocw [39/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 61.6
model_train val_loss  valEpocw [39/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 162.1
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [90.32175534 97.2137279  92.15165507 97.02489005 94.8721385  96.56680596
 96.99808726 94.65284292 97.44398826 96.43157369 98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.25201935 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.40598921 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 93.21036537 97.84237521 92.98863318
 96.90915072 96.27197524 96.9627563  93.75007614 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 92.26252117 96.13796128 96.24273583 96.9067141
 92.68161938 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 96.41695398
 97.96420609 95.45083515 96.41695398 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [87.74137742 97.2137279  90.88826891 97.02489005 96.6557425  96.5997003
 96.99808726 94.73812454 97.44398826 96.4742145  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.80345025 97.84237521 92.73644327
 96.90915072 96.22689782 96.9627563  93.88530841 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.89337362 96.13796128 96.24273583 96.9067141
 91.24888829 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 96.00394732
 97.96420609 95.45083515 96.28781326 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [97.76604737 12.0121023  64.04612067 12.86994477 10.34329733 22.3116124
  7.31941087 33.46727193  4.62295006 31.3213682   1.42848703  1.04897647
  0.72400334 16.89631894  5.44132383 22.3778175  13.8762243   5.98347315
  1.232238    2.46759946  1.61413509  0.50845603  2.01301171  9.15603839
 21.14313966 11.22723121 32.72384455 13.31949362  4.40616445  1.92947796
 29.85353684  0.87469343 37.72207693  1.34369387  7.04576374 28.05550857
 12.19124658  8.99305292 20.52033126 42.38304913 18.26538199 50.72969072
 21.61625332 32.2158748   8.15646687 35.92303792  2.42733593  1.62816703
  5.42651508  1.9645695   5.79046594  1.18223504  1.15452693  9.9432423
  1.61000242  4.0794815  67.90434197 27.85674142 12.1838842   9.05599929
 67.79725271 25.17420838 26.57460003 14.17572305  3.91681581  9.28123294
  7.56424193 12.04820302  4.18607942 18.79349697  0.67100484 45.95549753
  7.54976881 24.34765207 34.41792184 14.83249877  1.4209157   2.33657549
  0.25362679  1.04386814]
Accuracy th:0.5 is [45.6877962  97.2137279  69.61416162 97.02489005 97.26733349 74.00372802
 74.35338263 74.52029093 75.25005787 96.32070759 75.51199425 98.52097319
 99.41399349 79.17179372 75.1562481  96.56680596 96.29512311 74.91136804
 98.65376884 98.30776915 78.5419281  75.95180371 98.38695922 75.35361411
 82.62204408 96.65086926 94.0778012  74.69816401 98.01293844 75.42427602
 97.30875598 98.57457877 96.36213009 98.02024829 84.8271829  75.08193126
 74.92476944 87.21872297 97.11504489 72.36510276 77.59042897 92.05906361
 74.34972771 73.9306295  96.9627563  93.87434364 98.02877645 98.57336046
 93.88652672 84.66880277 85.22435156 98.55508583 98.99976852 74.40698822
 98.70615611 74.82608643 70.10635835 91.48158526 96.24273583 96.9067141
 89.79300934 97.17717864 92.54760541 74.98324825 98.42838172 75.35483242
 98.20786784 74.12434059 75.94571216 97.32703062 76.37455684 95.99054592
 75.90550797 95.45083515 74.2644461  80.38522922 86.11859017 75.47179006
 76.42938073 99.14718388]
Accuracy th:0.7 is [45.77795105 97.2137279  69.61416162 97.02489005 97.26733349 74.00372802
 74.35338263 74.88578356 75.25005787 96.46568633 75.51199425 98.52097319
 99.41399349 79.7273425  75.1562481  96.56680596 96.29512311 74.91136804
 98.65376884 98.30776915 79.03777975 75.95180371 98.38695922 76.19424715
 83.69415577 96.65086926 94.0778012  74.69816401 98.01293844 75.42427602
 97.30875598 98.57457877 96.36213009 98.02024829 85.12079531 75.08193126
 74.92476944 87.66218735 97.11504489 72.36510276 78.27268186 92.05906361
 74.34972771 73.9306295  96.9627563  93.87434364 98.02877645 98.57336046
 95.70667999 85.72020321 85.42171757 98.55508583 98.99976852 74.40698822
 98.70615611 74.82730474 70.10635835 91.94454259 96.24273583 96.9067141
 89.79300934 97.17717864 92.68649261 74.99177642 98.42838172 75.35483242
 98.20786784 74.12434059 75.94571216 97.55485435 76.37577515 95.99054592
 76.15647958 95.45083515 74.2644461  80.50096856 86.34641391 75.47179006
 76.42938073 99.14718388]
Avg Prec: is [55.83548043  3.06603175 11.11524079  3.275845    2.25787676  3.77746481
  3.26231149  5.53191366  2.48794977  3.88671922  1.56578638  1.59960652
  0.63849613  5.32807093  2.63395038  3.11834348  3.67222063  2.76043171
  1.33378029  1.76900567  1.98067759  0.90236094  1.81548885  2.40709543
  5.01970125  3.64738614  6.4700638   3.29599695  1.985847    2.01002781
  2.59817238  1.29988384  3.76163683  1.64587084  2.3799923   2.3913222
  2.97070928  2.54200719  2.77836878  7.36117841  2.19012392  8.16826991
  3.39833903  4.06217879  3.22924233  6.5891071   2.09893083  1.49912021
  2.09110461  1.70038729  1.86211806  1.66520788  1.08182809  3.08484228
  1.41006081  2.7190401  11.18771071  3.7071826   4.25056119  2.80106557
 10.82431885  2.15757705  3.76423692  2.95441935  1.59527367  2.52967539
  1.80048965  4.32555894  1.1942645   2.41332765  0.16448037  3.31181013
  1.93137217  4.71096833  3.93547785  3.23944282  0.90463875  1.90062035
  0.13834752  0.75152341]
mAP score regular 16.20, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [90.41034457 97.22450607 92.68007076 96.96290206 94.89249321 96.53436978
 96.80843112 94.5561452  97.38894287 96.42474525 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.48638413 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.48952338 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  93.27054837 97.82744101 92.91177716
 97.07750953 96.46710018 97.03764606 93.71652092 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.24498592 96.39235618 96.16314124 96.78102499
 92.94914916 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.47208312
 98.03174129 95.44559882 96.26778284 97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [89.36891148 97.22450607 91.64112913 96.96290206 96.85826046 96.63651992
 96.80843112 94.89249321 97.38894287 96.46710018 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.33440466 96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  92.9092857  97.82744101 93.0662481
 97.07750953 96.48703192 97.03764606 94.06034332 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.5464534  96.39235618 96.16314124 96.78102499
 91.90273314 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.14071804
 98.03174129 95.44559882 96.03358497 97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [97.52624547 12.77282442 66.9415     13.46574956  8.01008878 20.95416971
  7.89068231 33.2252854   5.7418044  38.42272778  1.42540959  1.00357025
  0.68711454 18.20416054  5.81393662 28.92766746 17.16915289  7.34335942
  1.21839881  2.94853433  1.502617    0.53583108  2.68876195  9.73999675
 20.99452173 12.25426259 31.53276611 13.66611345  5.04272844  1.96239983
 38.80923358  0.81276048 42.64327446  1.29560929  6.45075969 33.12355534
 12.65906123 15.51521771 26.08146485 43.35650749 21.70648496 49.72285669
 23.51705402 30.91592643  8.15039872 36.18151311  2.35835187  1.42119499
  6.4325147   1.86002975  7.05072428  1.26180133  1.28042062 11.56538485
  1.6713551   4.01508283 61.27398929 28.09356157 10.76899458 10.15542453
 66.39714853 28.51981988 29.38347821 15.3949105   4.92565541  8.77428535
  9.87834151 12.88161109  4.5852806  21.84640679  0.67701732 50.00888405
  8.10427196 26.45797974 42.82073022 14.01841058  1.73794126  2.31269208
  0.22372197  0.98842454]
Accuracy th:0.5 is [45.7358547  97.22450607 67.20482348 96.96290206 97.90716795 72.2874156
 72.42444627 72.08809826 73.95669831 96.40979645 73.99157884 98.5325261
 99.34972718 76.34352343 74.05386551 96.31262924 96.21047911 73.38615243
 98.78167277 98.34068316 76.3534893  74.63188579 98.31327703 74.18591325
 77.39990532 96.52938685 94.3393876  73.54809777 97.81747515 73.99905324
 97.52597354 98.67204823 96.39983058 98.18870369 84.84689937 73.66021377
 73.66270523 89.65543015 97.0276802  71.13635797 75.03301193 92.37362035
 72.76328575 72.55649401 97.03764606 94.02795426 98.18621222 98.77668984
 95.10675935 84.51802576 83.27478387 98.55993223 98.87385704 72.73837108
 98.6969629  73.47584523 68.2362907  92.69252809 96.16314124 96.78102499
 90.13379176 97.04761193 92.54304009 73.68263697 98.32075143 74.49734659
 98.13139996 72.83304681 74.73901886 97.43129781 75.03301193 96.07843137
 74.62441139 95.44559882 72.74086255 80.96021128 87.94130104 74.09622044
 75.102773   99.15040985]
Accuracy th:0.7 is [45.9600867  97.22450607 67.20482348 96.96290206 97.90716795 72.2874156
 72.42444627 72.3173132  73.95669831 96.41976231 73.99157884 98.5325261
 99.34972718 76.76208984 74.05386551 96.31262924 96.21047911 73.38615243
 98.78167277 98.34068316 76.73717518 74.63188579 98.31327703 74.68669806
 78.14485388 96.52938685 94.3393876  73.54809777 97.81747515 73.99905324
 97.52597354 98.67204823 96.39983058 98.18870369 85.20567058 73.66021377
 73.66270523 89.89211949 97.0276802  71.13635797 75.60854075 92.37362035
 72.76328575 72.55649401 97.03764606 94.02795426 98.18621222 98.77668984
 96.60163939 85.21314498 83.43922067 98.55993223 98.87385704 72.73837108
 98.6969629  73.47584523 68.2362907  93.0064529  96.16314124 96.78102499
 90.13379176 97.04761193 92.68754516 73.6876199  98.32075143 74.49734659
 98.13139996 72.83304681 74.73901886 97.52099061 75.03301193 96.07843137
 74.75396766 95.44559882 72.74086255 81.05986995 88.17549892 74.09622044
 75.102773   99.15040985]
Avg Prec: is [54.55577605  3.76060255 14.90990479  4.58235395  1.57918648  4.34005076
  8.65752118  8.67891355  6.83881185  5.11258997  2.3014346   5.36248247
  1.56056314  5.80395751  2.92302586  3.95052136 18.69886111  5.46291799
  1.59985935  4.36727148  3.74111992  1.41513236  2.14266633  4.29838369
  5.76827087 14.22685849  8.37047993  5.31663177  3.99641017  5.47932116
  2.30002618  0.87858255  3.07166982  1.11038817  1.70129309  2.41200992
  2.03601809  2.16081751  2.24414196  6.34666605  1.75073475  6.05999448
  2.22602699  2.75326441  2.37751212  4.87670354  1.78638331  1.04825352
  1.40098816  1.1870724   1.21215369  0.99361675  0.73505438  2.36117265
  0.87004091  1.8608636  10.27048937  2.95839457  4.17246873  2.52810501
  8.01698009  1.98302998  3.36025605  2.56183246  1.36803829  1.84684969
  1.62402662  3.46863316  1.06081523  2.15887915  0.18799117  3.07481882
  1.57377596  3.93061231  3.52598922  2.34220075  0.59159867  1.50871605
  0.12706661  0.58192299]
mAP score regular 17.25, mAP score EMA 4.28
Train_data_mAP: current_mAP = 16.20, highest_mAP = 16.68
Val_data_mAP: current_mAP = 17.25, highest_mAP = 17.56
tensor([0.3135, 0.3319, 0.3320, 0.3781, 0.3628, 0.3679, 0.3734, 0.3765, 0.3746,
        0.3752, 0.3586, 0.3708, 0.3641, 0.3548, 0.3735, 0.3672, 0.3697, 0.3776,
        0.3782, 0.3728, 0.3694, 0.3723, 0.3652, 0.3733, 0.3554, 0.3689, 0.3453,
        0.3754, 0.3533, 0.3553, 0.3512, 0.3474, 0.3658, 0.3508, 0.3603, 0.3668,
        0.3534, 0.3651, 0.3607, 0.3705, 0.3688, 0.3719, 0.3691, 0.3707, 0.3788,
        0.3803, 0.3842, 0.3809, 0.3740, 0.3811, 0.3644, 0.3572, 0.3678, 0.3745,
        0.3880, 0.3648, 0.3576, 0.3646, 0.3295, 0.3782, 0.3637, 0.3704, 0.3673,
        0.3841, 0.3790, 0.3668, 0.3567, 0.3722, 0.3635, 0.3744, 0.3754, 0.3649,
        0.3809, 0.3483, 0.3882, 0.3397, 0.3730, 0.3582, 0.3758, 0.3706],
       device='cuda:0')
Max Train Loss:  tensor([ 8.7256,  5.7037,  7.2422,  6.4166,  4.6091,  6.1981,  6.4389,  8.0679,
         6.7018,  7.2339, 10.5634,  5.3312,  7.4640, 11.3780,  4.9972,  7.1411,
         9.9231,  6.3942,  4.6313,  4.1263,  5.9475,  6.3781,  5.6132,  5.4122,
         9.2120,  6.3725,  8.8891,  7.6959,  4.8544,  5.5213,  5.8010,  4.0853,
         8.9104,  4.3101,  4.9557,  5.1952,  7.8507,  8.4319,  7.0352,  9.4761,
         4.6395,  7.3907,  6.1338,  5.9816,  8.2581,  7.2677,  5.5341,  5.1100,
         8.2955,  5.7608,  4.8054,  5.4670,  7.7242,  7.5016,  5.4824,  6.6490,
         7.6830,  6.2930,  8.6576,  8.5554, 12.1950,  6.7524,  6.4199,  4.6056,
         5.0917,  6.9112,  2.3792,  6.3189,  6.2611,  4.6019,  5.2341,  6.6450,
         5.8271,  8.7444,  6.5653,  4.9018,  4.5930,  7.7350,  6.1995,  5.8406],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [40/80], Step [000/642], LR 1.0e-04, Loss: 12.2
Max Train Loss:  tensor([10.6961,  7.1066,  9.6727,  7.3460,  7.2652, 13.1264,  7.7814,  8.3007,
         7.1402, 13.6785, 14.1081, 14.5835,  2.9905, 10.1205,  6.9587,  8.2781,
        11.9676,  9.7321, 14.5908,  6.2758,  7.1046, 11.7482,  6.6518,  9.2182,
        13.3587,  6.6475, 10.1075, 11.0220,  4.8645,  6.6344,  9.9670,  9.1864,
        10.3089,  8.1553,  6.9877,  8.2928,  9.8303,  5.1289,  9.3526, 11.3284,
         6.0881, 12.1231,  5.4678,  8.2566,  6.8702, 12.8190, 16.2789,  4.8031,
         8.1370,  9.4813,  4.2797,  5.7891,  4.4097, 13.0662, 11.2910,  6.3462,
        12.3100,  9.6140,  7.8325,  8.9164,  8.6388,  5.3630,  5.5400, 10.9786,
         7.4479,  6.7481, 12.9264, 11.1689, 10.6919,  6.3705,  6.3970,  5.8324,
        12.6192,  4.8032, 14.2154,  3.7861, 14.4607, 12.1230,  5.4845,  9.9077],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [40/80], Step [100/642], LR 1.0e-04, Loss: 16.3
Max Train Loss:  tensor([11.7666,  4.5168,  8.3370,  9.0619, 10.0754,  7.6404,  9.4722,  9.7842,
         7.6654,  7.0641,  5.2120,  5.3703,  2.6244,  8.1222,  5.4862,  5.5911,
         4.9872,  8.2509,  6.1259,  4.9393,  6.2995,  4.4002,  4.6959,  8.3031,
         8.4338,  8.1756, 11.2209,  8.9136,  7.4937,  2.9307,  7.0235,  9.0772,
         6.8502,  3.6757,  5.3032,  4.3839,  5.9991,  6.3470,  7.2485,  8.8666,
         7.3419,  9.8857,  9.7487,  7.4476,  4.4380,  9.3834,  5.7335,  4.1601,
         4.3066,  6.9751,  4.6695,  6.1206,  5.7177,  8.8446,  6.2047,  7.8709,
         8.4874,  8.9930,  6.2191,  7.7972,  7.4753,  7.6646,  2.4986,  8.5579,
         6.8863,  6.8451,  5.3969,  8.9041,  8.7081,  5.0886,  5.8680,  9.8475,
         8.3651,  9.2881,  7.6472, 10.9321,  6.8770,  8.0312,  4.9790,  9.8848],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [40/80], Step [200/642], LR 1.0e-04, Loss: 11.8
Max Train Loss:  tensor([ 8.6420,  8.5331,  8.4609,  4.8066,  6.4388,  6.5015,  7.6721,  8.5700,
         6.3744,  6.6985,  6.0567,  6.6682,  2.7344,  9.1727,  6.9289,  7.7203,
         5.2025,  5.4945,  7.4914,  4.0840,  6.0118,  4.4606,  7.6550,  6.7864,
         8.4333,  9.3852,  9.1801,  5.4589,  5.6298,  6.2158,  7.2761,  8.5499,
         8.6524,  6.1818,  5.2106,  5.3585,  7.2357,  7.2581, 12.5144,  9.7494,
         7.5880,  9.0501,  6.7237,  9.4540,  5.8574,  6.9020,  5.8095,  6.3940,
         6.7521,  7.8324,  4.4917,  5.3558,  5.6717,  6.9915,  4.1661,  6.5417,
        10.4054,  7.4073,  5.8758,  5.6187,  9.7675,  7.7798,  7.5511,  6.0664,
         7.5202,  6.9204,  6.5929,  8.5075,  4.6021,  6.0932,  5.9550,  7.7083,
         6.1344,  7.7873,  7.0423,  8.0229,  6.1587,  9.2364,  5.0836, 10.4941],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [40/80], Step [300/642], LR 1.0e-04, Loss: 12.5
Max Train Loss:  tensor([ 9.1241,  7.0698,  8.7024,  6.7420,  7.1302,  9.9322,  7.6047, 11.1152,
         7.0438,  9.6504,  5.2742,  6.8306,  4.7047, 11.1701,  7.9916,  7.3032,
         9.9881,  4.4739,  7.7199,  8.4693,  6.0537,  5.4665,  4.3519,  5.8873,
         7.9880,  5.5924,  7.4754,  3.7577,  6.5545,  6.1968,  4.4872,  7.3715,
         5.9911,  3.8916,  4.7749,  5.0023,  7.1124,  5.7958,  6.7949,  8.0891,
         4.7887,  6.8071,  6.5919,  7.5933,  7.2649,  6.4239,  5.9009,  5.1885,
         5.2304,  7.7039,  5.4920,  5.5021,  6.7468,  8.0505,  7.7437,  8.7983,
         8.2145,  6.0986,  5.5532, 11.6669,  8.5166,  5.3750,  7.0923,  7.8125,
         5.0800,  6.0381,  7.2429,  5.3143,  4.7288,  5.3478,  5.9714,  7.7250,
         7.4630,  8.0583,  9.0864,  4.8743,  6.8826,  7.6247,  5.6675,  3.4093],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [40/80], Step [400/642], LR 1.0e-04, Loss: 11.7
Max Train Loss:  tensor([ 9.1756,  4.2494, 10.3175,  6.5475,  5.7197,  5.9483,  7.2868,  5.5638,
         7.6810,  6.6666,  7.3975,  4.6263,  3.6132,  6.1578,  3.4621,  7.0839,
         8.0016,  6.3917,  7.4834,  6.3060,  4.3581,  5.3056,  7.0667,  6.4781,
         5.4699,  5.7040,  6.8964,  7.3850,  5.2506,  5.9869,  5.6271,  5.0511,
         7.2416,  4.9084,  6.6000,  7.1214,  8.0976,  6.0396,  6.3828,  8.7612,
         3.6804,  9.4294,  7.6148,  5.7815,  6.7248,  7.7830,  3.8478,  4.2767,
         7.3256,  7.7532,  7.5554,  6.7267,  5.6281,  7.7091,  4.1193,  7.3546,
         9.9031,  8.0943,  6.0180,  5.6346,  9.4610, 10.1804,  8.4137,  9.0414,
         5.3666,  5.7829,  8.5962,  8.3933,  5.3273,  6.5897,  5.9789,  6.1853,
         6.1700,  6.8152,  3.9419,  6.2785,  6.0977,  7.1065,  5.1029,  4.2950],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [40/80], Step [500/642], LR 1.0e-04, Loss: 10.3
Max Train Loss:  tensor([ 5.8523,  5.2544, 10.4109,  7.5829,  4.5732,  5.5326,  5.8225,  6.7303,
         4.0807,  6.0763,  7.5858,  6.2202,  2.8482,  5.1101,  6.6788,  7.3093,
         7.4881,  6.3694,  8.4869,  5.1635,  3.7798,  4.6005,  5.5658,  5.0567,
         5.8712,  5.3148,  9.6400,  6.7120,  7.5546,  5.0926,  4.5801,  5.5121,
         9.7104,  4.2093,  8.3679,  7.7720,  8.4683,  5.3874,  8.5245,  9.9558,
         5.1055, 10.0852,  8.1748,  7.6016,  5.9139,  7.1165,  5.8556,  5.9401,
         7.2376,  7.7238,  6.6067,  8.7285,  4.9463,  6.5588,  4.1874,  6.4072,
        13.8468,  9.2958,  6.5730,  7.7281, 11.8700,  6.2931,  7.5052,  5.9295,
         4.2710,  6.5604,  6.1763,  6.5851,  4.2055,  6.2134,  6.1103,  6.7046,
         8.9659,  8.6000,  7.3497,  7.1705,  6.8553,  7.4766,  5.2380,  4.3084],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [40/80], Step [600/642], LR 1.0e-04, Loss: 13.8
Max_Val Meta Model:  tensor([ 16.5277,  22.9425,  26.0156,  17.1030,   4.0517,   6.3523,   5.9502,
          7.4802,   4.3134,   5.9811,   5.2334,   4.7950,   3.6052,   7.4671,
          4.8257,   4.9560, 138.0442,   2.9602,   4.7004,   3.2714,   3.7469,
          4.5552,   3.4457,   3.7651,  10.0304,   7.3194,  11.1421,   4.2368,
          5.9354,   5.6047,   4.0210,   4.3385,   3.4612,   3.3385,   2.5829,
          3.4002,   4.9793,   4.8865,   5.5246,  16.9789,   6.1251,   8.5674,
          5.5200,   7.0361,   5.8855,   8.5494,   3.8344,   4.1202,   3.1910,
          3.9328,   3.3998,   4.5400,   5.6007,   4.2837,   3.2066,   3.6202,
         10.6541,   5.5970,   7.8107,   3.5655,   5.6304,   9.5480,   3.1419,
          4.2215,   2.3937,   4.3860,   5.5727,   4.9142,   5.3540,  10.6784,
          6.0383,   7.5851,  10.0666,   5.0277,   6.9925,   6.0057,   5.5458,
          6.5373,   5.1475,   2.0154], device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 13.9359,  21.9700,  18.9473,  16.2418,   4.1882,   7.8637,   6.9742,
         10.8080,   4.8676,   7.9014,   5.7936,   5.2151,   4.1215,   7.9991,
          4.9309,   5.2912, 128.6606,   3.7934,   5.1272,   3.5025,   3.9473,
          5.0303,   3.7325,   3.8432,  10.5416,   8.6877,  11.1597,   5.2988,
          6.5138,   6.5113,   4.2303,   4.8983,   3.8580,   3.7297,   3.0852,
          4.0819,   6.3546,   4.7984,   6.5582,  16.6586,   6.2808,   8.8386,
          5.6028,   7.1017,   4.9106,   8.5146,   4.0019,   4.5612,   3.0737,
          4.1135,   3.2593,   5.1176,   5.8989,   3.8498,   3.5707,   3.5576,
         10.8368,   5.7176,   8.2605,   3.8900,   5.1072,   8.6424,   3.0617,
          4.7133,   2.6497,   4.9249,   6.0784,   6.2437,   5.7325,  11.1448,
          6.7083,   7.7934,  10.6556,   5.6383,   6.9234,   6.1020,   6.0776,
          7.0723,   5.7528,   2.2456], device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 44.4551,  66.1948,  57.0761,  42.9620,  11.5447,  21.3718,  18.6786,
         28.7029,  12.9950,  21.0606,  16.1573,  14.0654,  11.3204,  22.5439,
         13.2006,  14.4090, 348.0284,  10.0467,  13.5569,   9.3961,  10.6852,
         13.5096,  10.2208,  10.2951,  29.6596,  23.5507,  32.3157,  14.1155,
         18.4389,  18.3252,  12.0466,  14.0993,  10.5459,  10.6335,   8.5628,
         11.1274,  17.9827,  13.1414,  18.1808,  44.9607,  17.0309,  23.7632,
         15.1802,  19.1601,  12.9618,  22.3870,  10.4156,  11.9754,   8.2178,
         10.7935,   8.9438,  14.3258,  16.0379,  10.2801,   9.2038,   9.7511,
         30.3079,  15.6813,  25.0713,  10.2848,  14.0438,  23.3318,   8.3361,
         12.2695,   6.9922,  13.4250,  17.0414,  16.7758,  15.7703,  29.7683,
         17.8712,  21.3576,  27.9783,  16.1882,  17.8348,  17.9622,  16.2942,
         19.7457,  15.3085,   6.0589], device='cuda:0')
Outer loop valEpocw Maximum [40/80], Step [000/314], LR 1.0e-04, Meta Learning Max Validation Loss: 138.0
model_train val_loss valEpocw [40/80], Step [000/314], LR 1.0e-04, Main Model Weighted Validation Loss: 128.7
model_train val_loss  valEpocw [40/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 348.0
Max_Val Meta Model:  tensor([26.8925, 27.4585, 30.6726, 32.8476, 29.4594, 31.4086, 33.7151, 31.6160,
        31.2193, 33.0091, 32.0709, 32.5585, 30.6460, 33.7020, 32.3224, 33.4391,
        32.0180, 31.3656, 32.6289, 33.9266, 30.9487, 31.3733, 30.0993, 34.2333,
        29.9404, 31.1223, 28.9929, 30.8365, 29.8729, 29.4742, 29.4357, 29.4157,
        30.1128, 28.7082, 29.9171, 30.4810, 29.5431, 30.4144, 30.2795, 32.2733,
        30.3398, 29.1132, 30.7348, 31.2531, 30.7065, 30.9950, 31.5750, 31.4593,
        31.0684, 31.1888, 30.3337, 30.2298, 30.5818, 31.1902, 31.7774, 29.9996,
        30.4412, 30.8933, 27.3787, 31.0645, 31.0033, 30.2646, 30.9248, 32.1201,
        30.9517, 30.5114, 29.8173, 30.1900, 30.3194, 31.5352, 31.3500, 31.2026,
        32.3860, 29.3282, 31.8430, 28.4078, 30.8616, 30.2057, 31.8297, 30.9620],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([17.6918,  5.6295, 16.7688,  6.5118,  5.6686, 13.6220, 29.5796, 17.4538,
        12.4157, 15.2381,  7.4238, 68.7239,  4.4609,  5.7532,  6.2324,  4.2961,
         4.3695,  6.5667,  4.2003,  8.3143,  3.3224,  4.0310,  3.1292,  4.7354,
         8.9919,  7.9345, 10.0553,  6.1643,  6.1772,  3.9314,  3.1310,  3.8189,
         1.5341,  2.8904,  2.0615,  2.8603,  4.9182,  2.9962,  4.1106,  1.3949,
         2.7363,  2.1207,  3.0903,  2.0901,  0.6692,  1.6996,  3.0428,  2.7068,
         2.0595,  3.1940,  2.3302,  3.9984,  3.5678,  2.5913,  2.7420,  3.2925,
         4.0103,  3.1605,  4.4596,  2.3784,  1.9485,  2.9194,  1.1304,  2.3853,
         1.9583,  3.7437,  4.8191,  5.8589,  2.1997,  4.6620,  5.4178,  2.2414,
         6.2829,  2.6778,  6.0458,  2.8658,  4.8099,  5.2717,  4.5453,  1.6714],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 59.0109,  17.0476,  49.7041,  16.9149,  15.7455,  37.1230,  77.3927,
         46.5712,  33.1013,  40.2681,  20.2499, 185.0379,  12.2353,  15.7873,
         16.6012,  11.3764,  11.7307,  17.3383,  11.0233,  21.7776,   8.9705,
         10.8066,   8.5488,  12.4775,  25.2684,  21.5039,  29.0973,  16.3797,
         17.4707,  11.0630,   8.9354,  10.9832,   4.1909,   8.2392,   5.7138,
          7.7875,  13.9221,   8.2195,  11.3654,   3.7464,   7.4333,   5.8057,
          8.3686,   5.6363,   1.7668,   4.4479,   7.9210,   7.1035,   5.5052,
          8.3880,   6.3857,  11.1843,   9.6921,   6.9028,   7.0605,   9.0270,
         11.2191,   8.6504,  13.6139,   6.2773,   5.3497,   7.9322,   3.0868,
          6.2045,   5.1592,  10.2172,  13.5119,  15.9115,   6.0407,  12.4401,
         14.3945,   6.1168,  16.4249,   7.6873,  15.8201,   8.4180,  12.9252,
         14.6854,  12.0850,   4.5026], device='cuda:0')
Outer loop valEpocw Maximum [40/80], Step [100/314], LR 1.0e-04, Meta Learning Max Validation Loss: 34.2
model_train val_loss valEpocw [40/80], Step [100/314], LR 1.0e-04, Main Model Weighted Validation Loss: 68.7
model_train val_loss  valEpocw [40/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 185.0
Max_Val Meta Model:  tensor([26.3160, 27.3193, 30.5313, 32.7995, 29.3935, 31.0597, 31.0028, 32.6207,
        31.3423, 32.2788, 32.1682, 30.4758, 30.1685, 31.4137, 30.9928, 32.8452,
        31.1535, 31.8131, 32.5429, 30.5850, 32.2171, 32.3148, 31.9847, 32.3468,
        29.7854, 31.4647, 28.8057, 31.1583, 29.8313, 29.5097, 29.5270, 29.5390,
        31.5460, 28.7644, 29.9216, 30.9474, 29.7218, 30.6144, 30.3247, 32.7539,
        30.7710, 31.7121, 31.8072, 32.2214, 31.9114, 34.0952, 32.1906, 31.5570,
        32.3864, 31.5938, 31.8123, 30.6463, 31.2365, 31.8250, 32.5060, 32.5154,
        32.4250, 30.9602, 27.4723, 31.0532, 32.9583, 30.5276, 30.9371, 31.0786,
        31.1266, 30.6395, 29.8967, 30.2913, 30.3324, 31.5955, 31.1828, 31.4596,
        32.5308, 29.9365, 31.8510, 28.5463, 31.0332, 30.1927, 31.7985, 31.0046],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([ 5.2113,  3.3961,  2.3175,  3.1231,  1.9355,  2.7819,  1.7660,  4.8706,
         2.5305,  1.8726,  5.4031,  2.8739,  3.3010,  6.1173,  2.1939,  4.0650,
         2.0561,  1.7718,  4.9743,  2.4955,  2.6885,  4.9496,  3.2009,  2.5350,
         4.2076,  2.9204,  8.9156,  6.7322,  5.4247,  3.9815,  3.7586,  6.0323,
         3.2230,  2.8951,  2.8273,  3.3909,  5.9274,  5.7320,  5.5091, 16.9819,
        12.4096, 24.3016, 27.2811, 26.7964, 19.0449, 27.8787,  7.0725,  6.4404,
        15.8925,  5.9207, 12.0474, 16.2069, 20.7876, 12.6397, 25.5444, 35.4956,
        29.7122,  8.7192,  6.7305,  4.2989, 33.5829,  7.8719,  5.5093,  7.5203,
         5.7604,  5.5152,  7.8435,  9.2552,  5.0788,  5.4292,  6.9708,  7.0196,
         8.3561, 12.9297,  3.5072,  8.5079,  7.8671,  6.7802,  6.0758,  2.3663],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([17.5054, 10.2787,  6.8233,  8.1305,  5.3686,  7.5883,  4.7347, 12.7414,
         6.7548,  4.9507, 14.7225,  7.7949,  9.2130, 16.9467,  5.9094, 10.8476,
         5.5906,  4.6822, 13.0344,  6.7262,  7.2121, 13.1639,  8.6502,  6.7322,
        11.8381,  7.9090, 25.7610, 17.9662, 15.3600, 11.1962, 10.6996, 17.3083,
         8.7375,  8.2366,  7.8389,  9.1605, 16.7180, 15.6535, 15.2179, 45.6200,
        33.7395, 66.3827, 73.8129, 72.0612, 50.2410, 71.7191, 18.2919, 16.8966,
        42.1563, 15.4603, 32.5837, 45.3020, 56.2032, 33.6823, 65.8412, 97.1557,
        82.9405, 23.8763, 20.5372, 11.3555, 92.4960, 21.3616, 15.0589, 19.8690,
        15.1473, 15.0104, 21.9980, 25.1123, 13.9738, 14.4689, 18.6481, 19.1053,
        21.8205, 37.0572,  9.1465, 24.9895, 21.0912, 18.9244, 16.1920,  6.3704],
       device='cuda:0')
Outer loop valEpocw Maximum [40/80], Step [200/314], LR 1.0e-04, Meta Learning Max Validation Loss: 34.1
model_train val_loss valEpocw [40/80], Step [200/314], LR 1.0e-04, Main Model Weighted Validation Loss: 35.5
model_train val_loss  valEpocw [40/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 97.2
Max_Val Meta Model:  tensor([27.0095, 27.5310, 30.2308, 31.5732, 29.6527, 31.2562, 31.5129, 30.1753,
        31.4105, 31.2807, 32.2353, 30.6501, 30.2525, 31.3838, 31.3902, 30.9322,
        31.2288, 32.0887, 30.0808, 30.7908, 31.4109, 32.3030, 31.2081, 32.2894,
        30.3098, 31.5290, 29.1412, 31.5635, 30.1902, 29.7670, 29.7372, 29.6836,
        30.9920, 28.9975, 29.9169, 29.5749, 29.8664, 31.2210, 30.2921, 32.5996,
        30.6939, 30.7709, 31.0688, 30.2670, 31.1777, 30.5304, 30.3650, 31.8592,
        30.0302, 31.6063, 32.2940, 30.5147, 30.0115, 30.3627, 32.2059, 30.2918,
        30.6313, 31.4973, 27.6043, 30.4452, 31.5906, 30.6278, 31.5997, 31.5604,
        30.5968, 30.7937, 30.1313, 33.1943, 30.6691, 31.8087, 31.7266, 31.3870,
        31.6282, 29.5989, 36.6829, 28.7167, 31.1710, 30.5380, 32.1671, 30.7580],
       device='cuda:0', grad_fn=<NegBackward0>)
Max_Val Main Model Weighted Val Loss:  tensor([10.4332,  2.3333,  9.0525,  4.0594,  7.0724,  7.5704,  9.9708, 10.6388,
         3.2591, 12.6393,  4.7855,  4.2875,  2.4205,  6.0367,  7.3516,  3.7929,
         1.5667,  2.6840,  4.3781,  3.6593,  3.8277,  4.2440,  3.5283,  4.5650,
         6.8053,  3.4032,  7.7015,  2.7780,  6.1077,  3.2786,  2.8797,  3.8639,
         1.0736,  2.7842,  1.7694,  2.3859,  3.3500,  2.9633,  3.0589,  2.2003,
         2.6807,  1.9965,  3.3830,  2.1862,  0.7047,  1.6388,  3.2683,  2.7102,
         2.3527,  2.6525,  2.7520,  4.0489,  3.6038,  2.8338,  2.8784,  2.7777,
         2.9782,  2.6193,  5.6947,  2.6957,  2.6040,  4.7747,  1.7564,  2.5851,
         2.1976,  3.7992,  5.0709,  3.6589,  2.3798,  3.9561,  5.5265,  2.9393,
         5.4642,  3.1593, 54.9696,  4.0546,  5.0096,  6.1162,  4.6506,  1.8404],
       device='cuda:0')
Max_Val Main Model Unweighted Val Loss:  tensor([ 34.7011,   7.0634,  26.8524,  10.7326,  19.6449,  20.6509,  26.6415,
         28.7786,   8.7207,  33.8643,  13.0509,  11.6500,   6.7408,  16.7898,
         19.8219,  10.3379,   4.2635,   7.0668,  11.8503,   9.8886,  10.3549,
         11.3249,   9.5913,  12.1564,  19.1026,   9.2615,  22.2969,   7.3786,
         17.2533,   9.2288,   8.2169,  11.1146,   2.9450,   7.9351,   4.9656,
          6.6731,   9.4757,   8.0998,   8.5023,   5.9088,   7.2732,   5.3725,
          9.1534,   6.0419,   1.8600,   4.3408,   8.6819,   7.0947,   6.4052,
          6.9558,   7.4180,  11.3237,   9.9488,   7.6866,   7.3889,   7.6122,
          8.3272,   7.1632,  17.3999,   7.1998,   7.1353,  12.9742,   4.7814,
          6.8183,   5.8577,  10.3703,  14.1917,   9.7493,   6.5250,  10.5555,
         14.6357,   8.0222,  14.3984,   9.0709, 145.6614,  11.8952,  13.4464,
         17.0102,  12.3462,   4.9991], device='cuda:0')
Outer loop valEpocw Maximum [40/80], Step [300/314], LR 1.0e-04, Meta Learning Max Validation Loss: 36.7
model_train val_loss valEpocw [40/80], Step [300/314], LR 1.0e-04, Main Model Weighted Validation Loss: 55.0
model_train val_loss  valEpocw [40/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 145.7
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [89.86854449 97.2137279  92.19307757 97.02489005 97.26733349 96.5997003
 96.99808726 94.18744898 97.44398826 96.52172854 98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.13506171 96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.1576857  93.12873868 97.84237521 93.0337106
 96.90915072 96.22689782 96.96153799 93.90358305 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 91.66433157 96.13796128 96.24273583 96.9067141
 92.51592939 97.14915754 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.49614405 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [88.98527065 97.2137279  91.99936648 97.02489005 97.26733349 96.5997003
 96.99808726 94.74908931 97.44398826 96.4754328  98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.38695922 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.30875598 98.57457877 96.36213009 98.02024829 97.80217103 97.70470633
 96.94082674 97.14550261 97.11504489 92.85461922 97.84237521 92.77177422
 96.90915072 96.22689782 96.9627563  93.88896334 98.02877645 98.57336046
 97.99588212 98.51853657 98.36746628 98.55508583 98.99976852 97.31728415
 98.70615611 97.46591781 92.26130286 96.13796128 96.24273583 96.9067141
 91.91164825 97.17717864 96.1123768  96.98468586 98.42838172 97.34408694
 98.20786784 95.95277835 98.67326178 97.55972759 99.81603538 95.99054592
 97.96420609 95.45083515 96.34872869 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [97.54705349  4.63129878 65.19929389 12.3245516  24.31818367 25.02619813
 11.33682788 33.94579478  6.8510848  30.54497869  1.71338388  2.09382818
  0.91655512 16.82482638  6.60875677 24.43811092  9.14138545  5.45194442
  0.9539469   2.33055491  9.72307353  0.49838753  1.27549753 13.89378514
 19.8419221  11.97444813 30.56529083 14.68461678  4.62744448  2.82812669
 41.51373526  0.89334459 33.00223888  2.71682139  8.93176579 11.80204605
 11.22648498 31.41268478 35.27672635 41.84462085 14.11812995 51.43790131
 24.42908198 30.21002493 26.60277644 37.12513969  3.90251158  1.78035722
 12.42413717  2.63763877  6.47298252  1.32397195  1.22869165 18.77174045
  1.79092981  9.15082777 67.21577663 28.43678734 17.49460304 13.4097277
 65.59097909 19.46125428 30.1593513  13.69246306  7.84964923  5.61720539
  6.13411433 11.46538267  5.28678172  5.1407501   0.36208862 32.41334011
  5.63610175 26.84227485 35.09448921 15.57344559  1.48907395  2.64070406
  0.19972475  0.91194778]
Accuracy th:0.5 is [45.55012731 97.2137279  69.20237327 97.02489005 97.26733349 73.57731996
 73.83194649 74.22424191 74.76517099 96.29755973 74.98081164 98.52097319
 99.41399349 79.06823747 74.64455842 96.56680596 96.29512311 74.39967837
 98.65376884 98.30776915 78.3335973  75.43767742 98.38695922 74.95522715
 82.61595253 96.65086926 94.0778012  74.15723493 98.01293844 74.92720605
 97.30875598 98.57457877 96.36213009 98.02024829 84.38737345 74.57755144
 74.40333329 86.71921638 97.11504489 71.90945529 77.2882884  92.05906361
 73.85265774 73.42624968 96.9627563  93.87434364 98.02877645 98.57336046
 94.29587846 84.45803536 85.20607692 98.55508583 98.99976852 73.97814354
 98.70615611 74.36800234 69.69944323 91.30127557 96.24273583 96.9067141
 89.79300934 97.17717864 92.75715452 74.48252336 98.42838172 74.91136804
 98.20786784 73.58341151 75.4218394  97.29900951 75.87748687 95.99054592
 75.52052241 95.45083515 73.80636201 79.86135646 85.76649895 75.01370597
 75.90307136 99.14718388]
Accuracy th:0.7 is [45.57449349 97.2137279  69.20237327 97.02489005 97.26733349 73.57731996
 73.83194649 74.57267821 74.76517099 96.47055957 74.98081164 98.52097319
 99.41399349 79.57139898 74.64455842 96.56680596 96.29512311 74.39967837
 98.65376884 98.30776915 78.73563919 75.43767742 98.38695922 75.86895871
 83.70755717 96.65086926 94.0778012  74.15723493 98.01293844 74.92720605
 97.30875598 98.57457877 96.36213009 98.02024829 84.66392953 74.57755144
 74.40333329 87.16024415 97.11504489 71.90945529 77.97663284 92.05906361
 73.85265774 73.42624968 96.9627563  93.87434364 98.02877645 98.57336046
 95.97836284 85.50456257 85.40466125 98.55508583 98.99976852 73.97814354
 98.70615611 74.36800234 69.69944323 91.72768363 96.24273583 96.9067141
 89.79300934 97.17717864 92.91066142 74.4873966  98.42838172 74.91136804
 98.20786784 73.58341151 75.4218394  97.54998112 75.88479673 95.99054592
 75.81657144 95.45083515 73.80636201 79.97587749 85.97117482 75.01370597
 75.90307136 99.14718388]
Avg Prec: is [55.92035844  3.07371928 11.31137205  3.34970318  2.279289    3.83503245
  3.2955548   5.59050195  2.54717117  3.86157192  1.68501187  1.55362692
  0.64808589  5.14425724  2.6358355   3.09185186  3.67717775  2.69609915
  1.40860675  1.750861    2.02216583  0.87879395  1.86767395  2.48936111
  5.19365609  3.58218524  6.59139413  3.30588812  2.01942472  1.8099127
  2.64278734  1.33644378  3.75205785  1.68360941  2.42532221  2.41858368
  2.94795952  2.52038006  2.75734334  7.44364958  2.31302254  8.2968021
  3.36794979  4.1047298   3.20767162  6.48460888  2.09445483  1.55169467
  2.2024226   1.5915282   1.90932018  1.66950729  0.9948202   2.97265667
  1.38013988  2.71617276 11.21151344  3.62539515  4.05250064  2.7395057
 10.9134514   2.11296427  3.77765289  2.98450088  1.58699547  2.49950863
  1.71049364  4.2865997   1.27134678  2.35055596  0.17694521  3.34758582
  1.89974209  4.63778365  3.88563591  3.08444153  0.8001983   1.85701528
  0.17952804  0.72143607]
mAP score regular 17.15, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [89.70775095 97.22450607 92.45334729 96.96290206 97.90716795 96.63651992
 96.80843112 93.92580412 97.38894287 96.45962578 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.4066572  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.00525699 93.2531081  97.82744101 93.23566784
 97.07750953 96.48703192 96.98034233 94.02546279 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21204873
 98.6969629  97.58576874 90.79901338 96.39235618 96.16314124 96.78102499
 92.65017316 96.91805566 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 96.39983058 97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [90.19607843 97.22450607 92.88437103 96.96290206 97.90716795 96.63651992
 96.80843112 94.82771508 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.22109276 99.15040985 98.31327703 97.88474475
 95.43563296 96.52938685 94.3393876  96.79099086 97.81747515 98.11395969
 97.52597354 98.67204823 96.39983058 98.18870369 98.00931809 97.89471062
 97.27931833 96.78102499 97.0276802  93.00146997 97.82744101 93.1634153
 97.07750953 96.48703192 97.03764606 94.04539452 98.18621222 98.77668984
 97.96198022 98.5848469  98.33071729 98.55993223 98.87385704 97.21703167
 98.6969629  97.58576874 91.40693126 96.39235618 96.16314124 96.78102499
 92.31631662 97.04761193 96.07095697 96.93051299 98.32075143 97.40638314
 98.13139996 95.7769639  98.72436904 97.53593941 99.81563146 96.07843137
 98.03174129 95.44559882 96.18307298 97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [97.43499569  5.1589466  68.88082778 13.88658761 27.27308307 23.96632505
 12.14736445 33.96171586  9.24884828 33.96640382  1.61432426  1.96057349
  0.76143533 18.01721529  7.74480015 33.18404102 13.18623925  5.82679094
  0.89933227  2.74180208  9.74476987  0.5255653   1.44708749 14.52827365
 19.81966261 12.23919643 29.74871243 17.17240873  5.05882767  2.92163105
 47.99711244  0.81872313 36.4137273   2.88831939  7.99635177 12.63300309
 11.0523589  34.94215348 38.70241368 43.06472877 15.60692028 51.03312775
 28.19020899 30.47473499 26.57622004 36.66732828  4.02212881  1.57902875
 14.46605464  2.8099116   8.74818379  1.45808613  1.46612439 20.08843008
  1.85091945 10.67149675 60.0910593  29.87558307 16.65892635 15.82878687
 64.84909328 20.98586166 33.43296268 14.42466284 10.57735469  6.1766122
  7.40378741 12.22426953  4.98594434  5.04462915  0.33363183 35.97433098
  5.4474993  29.09640857 43.78830815 15.95817194  1.42288166  2.96636375
  0.17793628  0.78444925]
Accuracy th:0.5 is [45.77073523 97.22450607 67.00550614 96.96290206 97.90716795 72.06816653
 72.20021427 71.92366146 73.7324663  96.40979645 73.75738097 98.5325261
 99.34972718 76.21396716 73.83461644 96.31262924 96.21047911 73.15195456
 98.78167277 98.34068316 76.1965269  74.40267085 98.31327703 74.03144231
 77.43478586 96.52938685 94.3393876  73.33881456 97.81747515 73.76485537
 97.52597354 98.67204823 96.39983058 98.18870369 84.7846127  73.43099883
 73.4484391  89.57321175 97.0276802  70.9320577  74.85113486 92.37362035
 72.54403667 72.35219374 97.03764606 94.02795426 98.18621222 98.77668984
 95.45058176 84.40341829 83.15519346 98.55993223 98.87385704 72.50915614
 98.6969629  73.27652789 68.05192217 92.63771582 96.16314124 96.78102499
 90.13379176 97.04761193 92.56048035 73.48331963 98.32075143 74.29304632
 98.13139996 72.61878068 74.50980392 97.42631487 74.79881406 96.07843137
 74.40017939 95.44559882 72.52161347 80.88048434 88.01106211 73.86202257
 74.86857513 99.15040985]
Accuracy th:0.7 is [45.9301891  97.22450607 67.00550614 96.96290206 97.90716795 72.06816653
 72.20021427 72.1752996  73.7324663  96.41976231 73.75738097 98.5325261
 99.34972718 76.64997384 73.83461644 96.31262924 96.21047911 73.15195456
 98.78167277 98.34068316 76.57522984 74.40267085 98.31327703 74.59451379
 78.20714054 96.52938685 94.3393876  73.33881456 97.81747515 73.76485537
 97.52597354 98.67204823 96.39983058 98.18870369 85.15085831 73.43099883
 73.4484391  89.81737549 97.0276802  70.9320577  75.42915514 92.37362035
 72.54403667 72.35219374 97.03764606 94.02795426 98.18621222 98.77668984
 96.84580312 85.11597778 83.30219    98.55993223 98.87385704 72.50915614
 98.6969629  73.27652789 68.05192217 92.9840297  96.16314124 96.78102499
 90.13379176 97.04761193 92.70498542 73.49577696 98.32075143 74.29304632
 98.13139996 72.61878068 74.50980392 97.52099061 74.79881406 96.07843137
 74.54219299 95.44559882 72.52161347 80.94526248 88.26270025 73.86202257
 74.86857513 99.15040985]
Avg Prec: is [54.6121614   3.75699774 14.90986907  4.5693965   1.57115048  4.35052659
  8.41521507  8.61722389  6.8763437   5.0850317   2.31832039  5.38969403
  1.58051503  5.79921583  3.00100618  3.98269176 16.72085566  5.36634586
  1.60374666  4.21617939  3.77022285  1.4313403   2.30680891  3.9691405
  5.77562868 14.17208663  8.35928637  5.54766014  4.00597977  5.22097545
  2.305281    0.87746547  3.08512026  1.10901034  1.71351756  2.411277
  2.02583465  2.16539582  2.25333027  6.38735956  1.75584759  6.07203234
  2.23676146  2.75037923  2.36885311  4.85191355  1.78904747  1.04598552
  1.396205    1.17708753  1.20915278  0.99653556  0.74720913  2.36344864
  0.86821259  1.84290176 10.35403836  2.97899631  4.22470916  2.51789256
  8.00574505  2.00353975  3.40788761  2.56608243  1.37057397  1.84561845
  1.65495698  3.47069283  1.0700828   2.16060158  0.18876602  3.07422514
  1.56608767  3.94352313  3.53139267  2.33735478  0.59046375  1.53541299
  0.12746991  0.58400443]
mAP score regular 18.20, mAP score EMA 4.25
Train_data_mAP: current_mAP = 17.15, highest_mAP = 17.15
Val_data_mAP: current_mAP = 18.20, highest_mAP = 18.20
tensor([0.2985, 0.3307, 0.3392, 0.3780, 0.3606, 0.3674, 0.3735, 0.3707, 0.3749,
        0.3735, 0.3675, 0.3690, 0.3592, 0.3616, 0.3717, 0.3675, 0.3672, 0.3786,
        0.3703, 0.3714, 0.3692, 0.3756, 0.3674, 0.3763, 0.3564, 0.3686, 0.3461,
        0.3756, 0.3539, 0.3558, 0.3515, 0.3487, 0.3646, 0.3521, 0.3568, 0.3575,
        0.3550, 0.3679, 0.3604, 0.3724, 0.3688, 0.3734, 0.3702, 0.3633, 0.3793,
        0.3806, 0.3760, 0.3821, 0.3676, 0.3836, 0.3718, 0.3578, 0.3638, 0.3690,
        0.3901, 0.3658, 0.3586, 0.3662, 0.3274, 0.3744, 0.3644, 0.3687, 0.3670,
        0.3828, 0.3753, 0.3678, 0.3576, 0.3714, 0.3644, 0.3757, 0.3750, 0.3672,
        0.3835, 0.3492, 0.3872, 0.3411, 0.3748, 0.3591, 0.3755, 0.3686],
       device='cuda:0')
