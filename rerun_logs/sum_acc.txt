creating models for obj12...
done

loading annotations into memory...
Done (t=3.11s)
creating index...
index created!
loading annotations into memory...
Done (t=6.29s)
creating index...
index created!
len(val_dataset)):  40137
len(train_dataset)):  82081
FORWARD SUMMING
tensor([0.3431, 0.6094, 0.3249, 0.7614, 0.5848, 0.5332, 0.7668, 0.7281, 0.0290,
        0.7713, 0.3124, 0.9457, 0.8132, 0.7442, 0.1783, 0.0607, 0.9596, 0.8791,
        0.9785, 0.0833, 0.7050, 0.3128, 0.2590, 0.7470, 0.3550, 0.7317, 0.3077,
        0.8358, 0.4874, 0.3471, 0.4226, 0.4663, 0.1316, 0.7303, 0.1155, 0.4864,
        0.2390, 0.5420, 0.5508, 0.5085, 0.9609, 0.6458, 0.5176, 0.2935, 0.7735,
        0.7454, 0.5686, 0.7332, 0.5257, 0.4073, 0.7465, 0.9724, 0.8096, 0.5624,
        0.4209, 0.5420, 0.6318, 0.6418, 0.7027, 0.3896, 0.5737, 0.4306, 0.6403,
        0.4401, 0.9854, 0.2919, 0.6103, 0.3267, 0.3626, 0.8391, 0.4455, 0.1889,
        0.8032, 0.7343, 0.4788, 0.5523, 0.9184, 0.3305, 0.8959, 0.6047],
       device='cuda:0')
Sum Train Loss:  tensor([31.4933, 51.5768, 29.4213, 69.8334, 52.6468, 50.2885, 65.6151, 67.6431,
         3.6750, 67.1160, 35.4377, 68.4249, 67.9129, 81.9214, 14.4024,  6.1056,
        94.4950, 73.0508, 90.0828,  7.5059, 68.0204, 29.4385, 18.7222, 60.4027,
        37.6431, 55.7877, 20.2535, 78.3501, 49.4616, 29.1268, 34.4213, 46.4597,
        11.0244, 65.7285,  9.6114, 60.7010, 22.7410, 53.1233, 50.0069, 57.8143,
        99.6998, 53.7649, 62.3252, 29.5560, 70.7653, 54.6277, 54.2090, 48.1753,
        44.2937, 37.7779, 82.3788, 89.7056, 66.7746, 51.4850, 30.5072, 65.2439,
        56.0632, 62.2262, 57.7456, 40.5269, 54.4751, 44.2697, 49.3180, 42.5507,
        82.3238, 33.3590, 45.2720, 33.2981, 38.2488, 85.4471, 43.0193, 15.4579,
        66.7610, 62.3982, 47.4952, 43.7070, 97.0832, 30.7636, 77.9449, 50.3186],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [0/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 4088.9
Sum Train Loss:  tensor([28.5809,  7.8479, 12.2849,  8.5586,  9.8227,  9.5334,  8.3180, 12.5913,
         0.3216, 14.1469,  3.4685,  9.3421,  0.3897, 15.6678,  1.8463,  0.8206,
        17.0353, 15.3351,  5.5742,  0.5764, 18.4163,  1.8097,  5.1770,  4.6544,
         8.8424, 15.7380, 13.1300, 14.9134, 15.2124,  2.2887,  6.8087,  2.5641,
         4.1389,  7.6166,  3.4311, 18.7838,  1.6205,  6.0087,  7.6551, 20.6702,
         9.2269, 18.6813, 10.1448,  4.7725, 18.9040, 35.6958,  5.4472,  7.4890,
         7.2352,  6.0271,  7.7989,  5.7423,  8.6183, 11.4099,  5.5831,  7.2034,
        20.9630, 16.9876, 12.6510,  5.3732, 18.9246,  3.0772, 11.0148,  4.5408,
         9.5346,  5.3967,  5.9922,  8.2007,  3.8536, 14.3424,  0.1912,  6.4357,
         7.0962, 16.1079,  6.6297, 14.8323,  9.6154,  2.2154,  5.5456,  7.0252],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [0/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 748.1
Sum Train Loss:  tensor([26.7513,  8.8034, 13.3355, 13.2058,  6.7263, 11.6346, 13.2330, 19.7062,
         0.4988, 25.3801,  5.4700,  5.5390,  0.5037, 20.9102,  2.6016,  1.3341,
        19.7884,  6.5928, 15.3365,  1.9558, 13.1507,  0.3308,  1.2537,  5.2095,
         9.2490, 12.7022, 11.3382, 10.4634, 13.9475,  2.2308,  5.2197,  0.9443,
         3.2519, 10.6806,  1.6105, 10.2216,  3.4067,  5.4521, 15.2778, 13.6033,
        14.4120, 15.8924, 10.3614,  7.3768, 20.1130, 25.7531,  3.8903,  4.4042,
         5.0761,  0.7555,  3.9234,  7.1483,  8.0629,  3.1317,  0.6567,  7.0877,
        25.6973, 11.8649, 10.1928,  8.1232, 19.5162, 11.0503,  6.0544,  6.6940,
         8.7210,  7.2894,  3.1411,  5.7643,  6.8119, 11.0280,  0.0901,  5.6440,
        13.8532, 16.5812,  6.4543,  5.7999,  0.8969,  6.0548,  0.1456,  5.5054],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [0/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 709.9
Sum Train Loss:  tensor([28.5412,  5.9314, 10.9415,  2.2220,  7.9078,  6.2770, 16.3682, 10.9780,
         0.3718, 15.9869,  4.6215,  9.2474,  3.9595, 12.5989,  2.6769,  1.3085,
        26.6924, 10.0869,  1.5554,  0.2356,  7.1480,  5.9310,  1.8540,  8.6931,
        15.1260,  4.7474,  7.7571, 26.9017, 16.4411,  2.9435,  4.6320,  3.6612,
         2.1665,  4.2118,  2.0833,  3.2200,  2.7796,  8.2941, 10.8064, 16.9966,
         9.1666, 23.7109, 11.6970,  5.5986, 13.1510, 14.3944, 11.6890,  7.7699,
        10.5309,  6.6258,  4.7292,  8.9230,  5.8469,  8.0930,  2.0191,  3.5877,
        22.5919,  8.1070, 12.9128,  6.1171, 26.9667,  6.6431, 11.6903,  2.6436,
         5.9320,  4.4564,  3.6732,  7.1246,  1.7883,  6.4958,  0.0757,  3.8827,
        13.3412, 13.0637, 13.5969,  5.5834,  1.1073,  3.3114,  0.1513,  7.6014],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [0/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 671.3
Sum Train Loss:  tensor([30.7629,  8.6624, 12.8822, 11.7168, 11.1969,  4.1205, 13.1987, 11.5756,
         0.3717, 10.4950,  5.7593, 13.2312,  0.7960, 18.4469,  2.3945,  0.6160,
        13.8549,  7.7875, 12.4837,  0.7120,  9.7034,  1.5609,  2.5569,  4.5722,
         4.4521,  7.3331,  7.5548, 23.8004,  4.4164,  3.2335,  4.4372,  2.9285,
         2.4420,  2.7897,  0.9106,  6.7867,  4.7445,  5.3392,  6.0328, 17.1591,
        19.8141, 27.2120,  7.3077,  6.3436,  9.2813, 21.8587,  6.5135,  5.3775,
         4.6772,  2.3115,  7.6512, 21.8906,  4.2790,  4.5846,  3.5004,  1.4816,
        25.3993, 11.2302, 11.7808,  6.2299, 17.1274,  6.3164, 12.8614,  2.8806,
         7.4494,  3.4932,  4.5881,  7.2278,  0.6337,  5.4869,  3.5541,  3.9706,
         4.5367, 11.7905,  8.0409,  8.5316,  9.0211,  7.0016,  0.1476,  0.7430],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [0/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 647.9
Sum Train Loss:  tensor([27.7016, 12.7744,  9.4211,  4.2289,  4.8611,  4.0516, 18.6914, 12.4533,
         0.5153, 10.8575,  5.5003, 13.0217,  0.5820, 13.1672,  1.6480,  1.3636,
        10.4359,  5.1890,  4.2079,  1.4746,  3.0179,  2.6940,  2.2128,  8.7761,
         7.7538, 12.8741,  9.6607, 15.6054,  5.1801,  3.5356,  4.3963,  4.6241,
         1.6691,  7.8632,  1.7918,  8.1329,  4.6740,  7.0117,  8.1499, 15.2652,
        11.3010, 17.5796,  6.1757,  5.7541, 15.9316, 20.2843,  6.3103,  0.9087,
         3.2620,  0.7161,  3.8782, 12.2869,  6.2333,  6.5741,  1.9717, 11.2710,
        24.8502, 10.5862, 14.5267,  4.9910, 13.8488,  3.8117, 19.8023,  6.7454,
        12.4789,  7.1800, 12.2191,  5.1580,  1.6050, 13.5945,  2.6283,  4.4579,
         6.4621, 13.5256, 10.6991,  7.9714,  4.9657,  1.7340,  0.2413,  0.8714],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [0/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 632.4
Sum Train Loss:  tensor([26.7268, 16.0502,  7.8364, 12.3582,  9.0324, 11.5252,  9.2271, 16.6281,
         0.7686, 11.4956,  4.7791,  4.2915,  4.3918, 17.6352,  5.3407,  0.6350,
        17.7958, 10.4952,  1.8436,  0.4382,  7.8599,  5.8899,  2.0615,  5.3947,
        11.9869, 18.6133,  8.8830,  5.6834,  4.3482,  6.0222,  8.2060,  6.6436,
         0.8957,  8.4875,  0.2349,  0.8555,  4.8216,  3.4882,  4.7692, 13.6823,
         6.8145, 26.1389,  6.9494,  2.9221,  6.5471, 11.5142,  4.0870,  2.9003,
         2.1749,  2.7933,  5.6270,  8.9821,  3.6797,  7.1269,  3.4655, 12.0060,
        17.4685, 10.3096,  8.6808,  2.4206, 15.5096,  5.0744,  5.0437,  7.6994,
         6.0635,  2.7629,  5.6317,  5.2675,  4.0640, 12.1295,  2.4880,  1.3461,
        14.8293, 17.7511,  5.4432,  9.9018,  4.5876,  0.8411,  4.4812,  0.6755],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [0/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 602.3
Sum_Val Meta Model:  tensor([2.6973e+01, 4.6887e+01, 2.8578e+01, 4.1602e+01, 2.6048e+00, 6.3093e+00,
        5.3664e+00, 1.7440e+01, 2.8000e-01, 1.1862e+01, 1.9348e+00, 9.5736e+00,
        3.7101e+00, 1.2603e+01, 1.4531e+00, 4.6194e-01, 4.0611e+02, 2.6157e+00,
        2.0322e+00, 1.8840e-01, 1.6447e+00, 2.8977e-01, 4.6321e-01, 1.9468e+00,
        1.2373e+01, 1.1194e+01, 9.4257e+00, 4.3789e+00, 4.7487e+00, 3.0277e+00,
        1.3805e+00, 8.2248e-01, 9.6766e-01, 1.9310e+00, 2.8120e-01, 1.2813e+00,
        1.2329e+00, 2.6935e+00, 2.0374e+00, 2.8810e+01, 1.1975e+01, 1.6031e+01,
        5.2732e+00, 6.2484e+00, 1.4481e+01, 1.9801e+01, 1.3954e+00, 3.7136e+00,
        1.0179e+00, 1.8670e+00, 9.1461e-01, 1.9802e+00, 7.8300e+00, 2.0048e+00,
        4.8478e-01, 2.0679e+00, 1.8540e+01, 6.6831e+00, 1.5545e+01, 1.9567e+00,
        9.9233e+00, 1.1407e+01, 5.1714e+00, 3.0808e+00, 2.3446e+00, 1.0039e+00,
        1.3461e+00, 2.8634e+00, 2.8170e+00, 1.7464e+01, 1.1117e-01, 4.0805e+00,
        1.2140e+01, 8.9642e+00, 7.0096e+00, 5.9144e+00, 9.8383e-01, 2.1163e+00,
        1.9092e-01, 6.9131e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([2.4795e+01, 4.2921e+01, 2.6460e+01, 3.4783e+01, 7.3514e-01, 6.9088e+00,
        4.8846e+00, 1.7029e+01, 2.3325e-01, 1.0578e+01, 2.0065e+00, 9.4793e+00,
        3.9118e+00, 1.2078e+01, 1.2052e+00, 5.3298e-01, 3.7844e+02, 3.2150e+00,
        1.7774e+00, 1.5354e-01, 2.0583e+00, 3.0369e-01, 2.4078e-01, 2.1105e+00,
        1.1562e+01, 1.0717e+01, 9.0297e+00, 7.0335e+00, 4.8589e+00, 2.9914e+00,
        8.2946e-01, 3.2183e-01, 7.8437e-01, 4.7984e-01, 2.4025e-01, 9.6009e-01,
        1.2829e+00, 1.8699e+00, 1.1720e+00, 2.8783e+01, 1.1245e+01, 1.6010e+01,
        5.3009e+00, 6.2076e+00, 1.4512e+01, 1.8030e+01, 1.2032e+00, 3.6371e+00,
        9.8075e-01, 1.7921e+00, 6.7355e-01, 1.3901e+00, 7.4860e+00, 1.3204e+00,
        4.5402e-01, 1.7161e+00, 1.8729e+01, 6.2083e+00, 1.4397e+01, 1.5186e+00,
        9.1741e+00, 8.4771e+00, 4.6173e+00, 2.7703e+00, 1.4417e+00, 9.1044e-01,
        1.0020e+00, 3.2109e+00, 3.5985e+00, 1.9390e+01, 6.8722e-02, 4.0952e+00,
        1.3394e+01, 8.7091e+00, 6.5024e+00, 5.8837e+00, 7.9967e-01, 1.9080e+00,
        1.6632e-01, 5.3042e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([7.2264e+01, 7.0428e+01, 8.1453e+01, 4.5684e+01, 1.2571e+00, 1.2958e+01,
        6.3702e+00, 2.3389e+01, 8.0444e+00, 1.3715e+01, 6.4231e+00, 1.0024e+01,
        4.8104e+00, 1.6229e+01, 6.7595e+00, 8.7793e+00, 3.9438e+02, 3.6572e+00,
        1.8165e+00, 1.8425e+00, 2.9195e+00, 9.7096e-01, 9.2953e-01, 2.8254e+00,
        3.2572e+01, 1.4647e+01, 2.9347e+01, 8.4153e+00, 9.9695e+00, 8.6190e+00,
        1.9627e+00, 6.9014e-01, 5.9624e+00, 6.5707e-01, 2.0809e+00, 1.9740e+00,
        5.3671e+00, 3.4499e+00, 2.1280e+00, 5.6608e+01, 1.1702e+01, 2.4793e+01,
        1.0240e+01, 2.1151e+01, 1.8760e+01, 2.4188e+01, 2.1160e+00, 4.9604e+00,
        1.8654e+00, 4.4004e+00, 9.0230e-01, 1.4296e+00, 9.2470e+00, 2.3476e+00,
        1.0786e+00, 3.1661e+00, 2.9646e+01, 9.6731e+00, 2.0487e+01, 3.8983e+00,
        1.5992e+01, 1.9686e+01, 7.2107e+00, 6.2945e+00, 1.4630e+00, 3.1186e+00,
        1.6419e+00, 9.8293e+00, 9.9228e+00, 2.3107e+01, 1.5426e-01, 2.1680e+01,
        1.6677e+01, 1.1861e+01, 1.3580e+01, 1.0653e+01, 8.7071e-01, 5.7726e+00,
        1.8564e-01, 8.7721e-01], device='cuda:0')
Outer loop valEpocw Maximum [0/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 958.9
model_train val_loss valEpocw [0/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 899.2
model_train val_loss valEpocw [0/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1369.0
Sum_Val Meta Model:  tensor([44.4188,  9.6897, 49.6883,  6.1863,  0.4354, 15.4944, 23.5337, 23.3022,
        16.6916, 14.6973,  3.1187, 43.6853,  1.8260,  2.4088,  7.2737,  8.7285,
         5.9126,  6.6463,  1.7973, 17.8615,  0.1671,  0.0593,  0.1482,  1.2826,
        12.2251,  6.0687, 20.4849,  2.7703,  2.9540,  0.1300,  0.1884,  0.1374,
         0.1378,  0.1938,  0.0581,  0.1234,  1.3159,  0.2043,  0.1590,  2.1464,
         0.4898,  2.6509,  0.2188,  0.4618,  0.6616,  2.7726,  0.5180,  0.3825,
         0.1523,  2.0972,  0.1446,  0.1695,  0.1116,  0.1315,  0.0999,  0.8345,
        10.6012,  2.4718,  4.5896,  0.5601,  2.9542,  0.6305,  1.9859,  1.2460,
         0.5438,  0.4877,  0.6819,  3.5791,  0.3593,  2.1718,  0.1909,  0.7212,
         2.7673,  3.8000,  1.7370,  0.4679,  0.2840,  0.1980,  0.1415,  0.2547],
       device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([4.6376e+01, 1.1109e+01, 4.1836e+01, 5.4158e+00, 7.3043e-01, 1.7987e+01,
        3.3518e+01, 2.0953e+01, 1.2708e+01, 1.3523e+01, 3.0182e+00, 4.3524e+01,
        1.5656e+00, 3.6465e+00, 6.9927e+00, 2.4473e+00, 5.4846e+00, 6.4143e+00,
        6.0421e-01, 1.2438e+01, 2.4168e-01, 4.4133e-02, 2.3288e-01, 9.0886e-01,
        1.1394e+01, 6.4454e+00, 2.1425e+01, 3.2010e+00, 2.7856e+00, 2.8815e-01,
        3.2791e-01, 8.4393e-02, 5.6801e-01, 1.4276e-01, 2.9226e-01, 9.4615e-02,
        1.6617e+00, 6.1192e-01, 2.7379e-01, 2.1580e+00, 3.9145e-01, 2.5430e+00,
        1.9794e-01, 6.6781e-01, 5.4449e-01, 2.2535e+00, 2.7808e-01, 2.3025e-01,
        9.0871e-02, 2.8812e+00, 3.1375e-02, 5.9205e-02, 4.4366e-02, 2.1216e-01,
        3.9319e-02, 5.7778e-01, 8.7725e+00, 1.5713e+00, 4.1622e+00, 2.4599e-01,
        4.1156e+00, 1.3796e+00, 1.4573e+00, 1.0658e+00, 3.0032e-01, 4.2914e-01,
        4.2506e-01, 3.9646e+00, 1.5476e-01, 1.7314e+00, 4.0027e-02, 9.5418e-01,
        1.9079e+00, 2.3195e+00, 2.0094e+00, 4.2996e-01, 8.7461e-02, 1.1830e-01,
        1.3017e-02, 1.5092e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([8.6108e+01, 2.0438e+01, 6.2481e+01, 8.6827e+00, 3.2945e+00, 4.1942e+01,
        9.7872e+01, 6.0473e+01, 3.3922e+01, 3.4977e+01, 1.7603e+01, 2.3357e+02,
        8.5010e+00, 1.1838e+01, 1.5593e+01, 5.4547e+00, 1.1623e+01, 1.9113e+01,
        2.1541e+00, 2.2692e+01, 2.7852e+00, 1.0199e+00, 1.9514e+00, 5.6091e+00,
        2.8236e+01, 2.2698e+01, 3.5803e+01, 1.7288e+01, 1.3453e+01, 1.9966e+00,
        1.8860e+00, 8.8806e-01, 2.5165e+00, 1.5757e+00, 1.7760e+00, 1.2713e+00,
        8.1034e+00, 4.9105e+00, 2.4431e+00, 4.0673e+00, 9.4736e-01, 6.0914e+00,
        1.6372e+00, 1.6391e+00, 1.0157e+00, 4.3704e+00, 1.3262e+00, 8.4423e-01,
        7.1427e-01, 8.0332e+00, 4.2364e-01, 6.8317e-01, 5.0679e-01, 2.2127e+00,
        4.7616e-01, 3.6235e+00, 1.4205e+01, 4.1633e+00, 9.6993e+00, 1.6814e+00,
        6.6913e+00, 2.2306e+00, 2.9389e+00, 2.9372e+00, 1.1153e+00, 1.4116e+00,
        1.1410e+00, 1.7018e+01, 4.1095e-01, 4.2174e+00, 8.5432e-02, 1.8585e+00,
        3.9914e+00, 4.6911e+00, 7.3639e+00, 1.2753e+00, 4.6210e-01, 1.0852e+00,
        7.9954e-02, 3.6064e-01], device='cuda:0')
Outer loop valEpocw Maximum [0/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 409.7
model_train val_loss valEpocw [0/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 392.3
model_train val_loss valEpocw [0/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1084.3
Sum_Val Meta Model:  tensor([5.3505e+01, 1.8811e+00, 7.2795e+00, 1.1316e+00, 3.3895e-01, 8.3726e-01,
        4.1740e-01, 4.0739e+00, 8.0984e-01, 9.1200e-01, 2.5949e-01, 3.1699e-01,
        1.9360e-01, 5.9857e+00, 9.2771e-01, 1.0183e+00, 2.4284e+00, 8.3812e-01,
        3.1989e-01, 7.8296e-01, 4.4437e-01, 1.7697e-01, 1.5472e+00, 3.3892e-01,
        2.6631e+00, 8.4376e-01, 1.2114e+01, 2.4680e+00, 4.2626e-01, 6.5040e-01,
        2.1006e+00, 4.7322e-01, 3.6874e+00, 8.9610e-02, 7.1683e-01, 1.1115e+00,
        8.3753e+00, 1.5764e+00, 9.6376e-02, 1.7840e+01, 1.7413e+01, 4.7510e+01,
        1.7369e+01, 5.1446e+01, 4.1416e+01, 4.9774e+01, 7.6279e+00, 6.6817e+00,
        8.5751e+00, 1.1279e+01, 3.5109e+00, 3.7112e+00, 5.0871e+00, 1.7701e+00,
        4.4604e+00, 3.1690e+01, 1.1360e+02, 2.9485e+00, 3.5233e+00, 3.6182e-01,
        7.4154e+01, 7.4682e-01, 5.3145e+00, 3.2856e+00, 2.2017e+00, 2.6973e-01,
        2.5979e+00, 2.2280e+00, 1.8280e+00, 6.6211e-01, 1.8616e-01, 4.6672e+00,
        2.5337e+00, 1.5401e+01, 5.5569e-01, 4.6042e+00, 8.5276e-01, 1.9807e-01,
        8.1389e-02, 3.2434e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([3.3395e+01, 7.9080e-01, 5.3093e+00, 5.4981e-01, 4.7382e-02, 3.9847e-01,
        1.3750e-01, 3.4374e+00, 2.0128e-01, 3.4795e-01, 1.0576e-01, 5.2415e-02,
        6.0738e-02, 5.2312e+00, 6.5235e-01, 2.8286e+00, 2.2883e+00, 2.7773e-01,
        6.6305e-02, 1.9521e-01, 1.2779e-01, 3.2532e-02, 2.2376e-02, 4.0733e-02,
        2.2124e+00, 6.7367e-01, 1.2414e+01, 2.6886e+00, 3.8911e-01, 2.0344e-01,
        1.0417e+00, 4.5970e-01, 3.3908e+00, 5.0329e-03, 3.1626e-01, 2.2753e-01,
        7.6652e+00, 1.8861e+00, 4.1188e-02, 1.9551e+01, 1.4175e+01, 4.0335e+01,
        1.2385e+01, 4.9684e+01, 3.3376e+01, 4.8791e+01, 3.6604e+00, 4.5850e+00,
        8.2436e+00, 6.3342e+00, 3.7605e+00, 3.4504e+00, 5.1008e+00, 2.8142e+00,
        3.8972e+00, 2.4772e+01, 9.2167e+01, 3.9825e+00, 3.6909e+00, 5.7346e-01,
        9.0908e+01, 1.3555e+00, 5.8718e+00, 3.4079e+00, 2.0421e+00, 9.5013e-01,
        2.5565e+00, 2.3522e+00, 1.8931e+00, 1.6036e+00, 9.4266e-02, 3.6880e+00,
        3.0786e+00, 1.4469e+01, 4.4360e-01, 4.0645e+00, 9.4297e-01, 2.7173e-01,
        1.3655e-02, 4.0136e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.4120e+01, 1.5146e+00, 8.7049e+00, 9.9960e-01, 2.3340e-01, 1.1378e+00,
        7.5083e-01, 1.1645e+01, 5.2782e-01, 1.0731e+00, 8.4801e-01, 3.7280e-01,
        4.7922e-01, 1.7306e+01, 1.6615e+00, 8.2789e+00, 5.4467e+00, 9.0341e-01,
        3.6908e-01, 4.9385e-01, 9.9656e-01, 5.4340e-01, 1.1226e-01, 3.6240e-01,
        6.5226e+00, 2.1275e+00, 2.5532e+01, 1.1335e+01, 2.0467e+00, 7.0270e-01,
        3.5206e+00, 5.0417e+00, 9.1033e+00, 1.1272e-01, 1.0330e+00, 1.0289e+00,
        1.6843e+01, 1.6876e+01, 7.1880e-01, 3.5188e+01, 2.9347e+01, 7.1133e+01,
        7.9269e+01, 8.5649e+01, 5.6229e+01, 7.5757e+01, 1.0090e+01, 9.7313e+00,
        3.8266e+01, 1.1225e+01, 2.9503e+01, 4.6961e+01, 7.8725e+01, 3.6154e+01,
        8.2626e+01, 1.1308e+02, 1.5043e+02, 1.6661e+01, 9.9208e+00, 5.9992e+00,
        1.1635e+02, 2.3358e+00, 1.3624e+01, 1.3217e+01, 1.2056e+01, 5.1021e+00,
        8.5238e+00, 1.4001e+01, 5.7635e+00, 4.2668e+00, 2.6618e-01, 7.3917e+00,
        7.3750e+00, 3.7884e+01, 2.0480e+00, 1.4703e+01, 1.1229e+01, 3.3824e+00,
        1.9899e-01, 1.0469e+00], device='cuda:0')
Outer loop valEpocw Maximum [0/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 694.4
model_train val_loss valEpocw [0/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 616.0
model_train val_loss valEpocw [0/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1584.1
Sum_Val Meta Model:  tensor([6.5238e+01, 7.3396e-01, 1.8546e+01, 3.9849e-01, 1.2126e-01, 4.1023e+00,
        7.4006e-01, 4.9318e+00, 4.5886e-01, 5.7134e+00, 4.3607e-01, 5.2093e-01,
        4.4044e-02, 4.3376e+00, 5.0014e+00, 4.8632e-01, 7.0226e-01, 1.9267e-01,
        9.0815e-02, 1.8913e-01, 6.8293e-02, 2.0634e-02, 4.3051e-02, 7.8208e-02,
        7.6795e+00, 4.2865e-01, 1.8293e+01, 3.6137e-01, 2.2140e+00, 7.1054e-02,
        1.2048e-01, 5.7229e-02, 9.5698e-01, 4.0223e-01, 6.2976e-01, 3.6995e-01,
        6.7887e-02, 4.5400e-01, 6.2341e-01, 1.3448e+01, 4.6379e+00, 1.1769e+01,
        8.0886e-01, 3.6304e+00, 2.3217e+00, 5.3965e+00, 1.2005e-01, 1.4374e-01,
        1.4033e-01, 1.4343e-01, 7.9183e-02, 9.2343e-02, 9.3084e-02, 1.3315e+00,
        1.0480e-01, 4.3878e-01, 1.6469e+01, 1.4738e+00, 7.6824e+00, 2.7772e-01,
        2.0683e+01, 4.1287e-01, 2.7713e+00, 2.7327e+00, 1.9622e+00, 3.7054e-01,
        2.5135e+00, 2.7942e+01, 3.3227e-01, 9.7056e-01, 5.1900e-02, 7.7417e-01,
        1.5945e+00, 5.2842e+00, 1.2178e+02, 7.0417e+00, 4.8859e-02, 8.7699e-01,
        1.9739e-02, 3.9175e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.0069e+01, 3.0825e+00, 1.8424e+01, 1.4576e+00, 4.7412e-01, 4.1876e+00,
        1.9500e+00, 5.0774e+00, 1.7650e+00, 5.9106e+00, 4.7312e-01, 6.0440e-01,
        8.9856e-02, 4.6277e+00, 5.5025e+00, 1.1210e+00, 1.1655e+00, 7.6618e-01,
        1.1041e-01, 4.2511e-01, 1.4700e-01, 1.1648e-02, 7.8366e-02, 2.7679e-01,
        8.0129e+00, 9.7320e-01, 1.9573e+01, 1.0077e+00, 2.2505e+00, 1.9782e-01,
        4.6423e-01, 1.6391e-01, 3.8456e-01, 2.0200e-01, 2.3948e-01, 7.2025e-02,
        8.6415e-01, 3.6003e-01, 2.7530e-01, 4.6469e+00, 9.2691e-01, 3.7876e+00,
        2.9434e-01, 1.1768e+00, 7.9621e-01, 2.4289e+00, 2.0269e-01, 1.4015e-01,
        6.7245e-02, 1.6876e-01, 1.5131e-02, 2.4257e-02, 4.3291e-02, 3.4602e-01,
        6.3726e-02, 4.2577e-01, 6.8193e+00, 1.0829e+00, 6.6256e+00, 2.0756e-01,
        7.5096e+00, 2.0751e+00, 2.1644e+00, 1.8878e+00, 6.7959e-01, 4.9178e-01,
        9.1137e-01, 4.0758e+00, 3.3327e-01, 1.3345e+00, 3.0953e-02, 1.7778e+00,
        1.3852e+00, 2.9963e+00, 1.1993e+02, 2.5721e+00, 2.4987e-02, 6.2583e-01,
        3.6812e-03, 1.8495e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([8.5093e+01, 6.6787e+00, 3.1605e+01, 2.9503e+00, 2.9838e+00, 1.4476e+01,
        1.3997e+01, 2.0358e+01, 5.0572e+00, 2.3969e+01, 5.7258e+00, 6.5519e+00,
        1.4104e+00, 1.7187e+01, 1.5871e+01, 3.7031e+00, 3.3157e+00, 3.3438e+00,
        9.0454e-01, 1.3012e+00, 2.3579e+00, 5.4945e-01, 9.5190e-01, 3.8989e+00,
        2.5242e+01, 3.5722e+00, 3.4150e+01, 4.9917e+00, 1.6040e+01, 1.0431e+00,
        2.7834e+00, 1.5070e+00, 1.1251e+00, 1.4288e+00, 7.2904e-01, 5.1261e-01,
        4.5889e+00, 1.4080e+00, 1.2857e+00, 7.6558e+00, 1.4584e+00, 6.7210e+00,
        1.7872e+00, 2.2666e+00, 1.4316e+00, 4.3142e+00, 1.5586e+00, 7.9452e-01,
        7.0501e-01, 6.6436e-01, 4.0166e-01, 5.6075e-01, 4.9901e-01, 1.4361e+00,
        5.0205e-01, 1.9486e+00, 1.0287e+01, 3.3973e+00, 1.4304e+01, 2.1826e+00,
        1.0508e+01, 3.7138e+00, 4.3277e+00, 4.7702e+00, 1.9138e+00, 2.2135e+00,
        2.0299e+00, 7.9202e+00, 9.8214e-01, 3.2795e+00, 1.2410e-01, 3.6922e+00,
        2.6020e+00, 6.5586e+00, 3.7625e+02, 4.9705e+00, 6.0543e-01, 4.6848e+00,
        1.3077e-01, 4.3836e-01], device='cuda:0')
Outer loop valEpocw Maximum [0/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 415.1
model_train val_loss valEpocw [0/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 324.1
model_train val_loss valEpocw [0/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 881.2
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [69.21455635 97.2137279  89.67605171 97.0736224  97.34165032 96.75808043
 97.06509424 94.6918288  97.44155164 96.42791876 98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.6488956  98.30776915 98.15060733 99.18616976 98.88890243 97.8119175
 95.21935649 96.65086926 94.0778012  96.75442551 98.01293844 98.15913549
 97.33434047 98.57579708 96.35360193 98.00441028 97.80826257 97.78755132
 96.94082674 97.44155164 97.21738283 92.72547849 97.84359352 92.09683118
 96.91036903 96.23177106 96.9627563  93.87799856 98.02877645 98.57336046
 97.99588212 98.51853657 98.40888878 98.55021259 98.99976852 97.47200936
 98.70615611 97.46591781 89.19360144 96.09653878 96.24273583 96.9067141
 90.2181991  97.14793923 96.07217261 96.99443233 98.43447326 97.34408694
 98.20664953 95.95277835 98.67326178 97.57191067 99.81603538 96.08191908
 97.96664271 95.44718022 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [61.25534533 97.2137279  89.56884054 97.03707314 97.27220672 96.66670728
 97.0041788  94.73568792 97.44398826 96.47299619 98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15060733 99.18616976 98.79265603 97.80948088
 95.21935649 96.65086926 94.0778012  96.75077058 98.01293844 98.15913549
 97.31241091 98.57457877 96.36943994 98.02024829 97.80217103 97.72176265
 96.94082674 97.37088973 97.15281247 92.72913342 97.84237521 92.06028192
 96.91036903 96.22689782 96.9627563  93.87434364 98.02877645 98.57336046
 97.99588212 98.51853657 98.37477614 98.55508583 98.99976852 97.55485435
 98.70615611 97.46591781 89.1046649  96.13674297 96.24273583 96.9067141
 89.9964669  97.19301665 96.11725003 96.98468586 98.42716341 97.34408694
 98.20786784 95.95277835 98.67326178 97.56094589 99.81603538 96.00882056
 97.96420609 95.44961684 96.15136268 96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [79.20197374 11.2439843  34.51631133 22.04870825 28.45280853 31.43832497
 27.72409143 21.81824237 10.47821903 18.79739606  3.4316272   4.98257249
  1.93600882 11.86564894  5.92206744 10.53290433  7.31152281  8.11685741
 13.27750729  8.89509098 15.90722596  9.37506862 59.62199795 19.68288664
 12.47112705  8.40154679 17.87050571 12.37685596  4.89378297 10.21731861
 28.18316032 12.61982248 25.21603545 19.14974378 23.20025001 33.09094807
 10.72438834 39.21027557 31.68804064 24.29551464 10.60519512 30.41404882
 21.34666261 20.0469623  15.64595272 23.83794297  9.91121785  7.0924184
 15.31943706  9.92565355 26.19889103 12.64399682  5.09437752 39.12964053
  5.23097109 10.407998   33.68995123 20.99082553 12.98590422 10.30452876
 40.70963029 28.35453267 24.61571901 15.529755   15.08983133 12.65738182
 13.48362134  9.48322741 14.01335531 19.73706623  1.49267849 33.62598398
 18.20002888 19.91967483 11.15828773 11.22796974  1.98585835  4.81156034
  0.56458606  4.25398638]
Accuracy th:0.5 is [45.25651491 97.2137279  73.57122842 97.02489005 97.26733349 78.70883639
 79.01463189 77.69033028 79.87475786 96.42791876 80.29507438 98.52097319
 99.41399349 80.69346134 79.74683544 96.56680596 96.29512311 79.56287082
 98.65376884 98.30776915 80.92615831 80.82991192 98.38695922 79.75049037
 80.7019895  96.65086926 94.0778012  79.19615989 98.01293844 80.15131395
 97.30875598 98.57457877 96.36213009 98.02024829 86.14904789 79.76267346
 79.48124414 90.13413579 97.11504489 76.67060586 80.02948307 92.05906361
 78.99635726 78.60406184 96.9627563  93.87434364 98.02877645 98.57336046
 84.86007724 87.77549006 87.05912452 98.55508583 98.99976852 79.20956129
 98.70615611 79.51657509 73.89773516 93.36874551 96.24273583 96.9067141
 89.79300934 97.17717864 90.60074804 79.54459619 98.42838172 79.99415212
 98.20786784 78.59066045 80.69955288 97.55972759 81.26972137 95.99054592
 80.30238423 95.45083515 78.84285036 82.65372011 85.62639344 80.16715196
 81.3294185  99.14718388]
Accuracy th:0.7 is [45.38687394 97.2137279  73.57122842 97.02489005 97.26733349 78.70883639
 79.01463189 77.7756119  79.87475786 96.47299619 80.29507438 98.52097319
 99.41399349 81.13570741 79.74683544 96.56680596 96.29512311 79.56287082
 98.65376884 98.30776915 81.31723541 80.82991192 98.38695922 79.75292699
 81.19418623 96.65086926 94.0778012  79.19615989 98.01293844 80.15131395
 97.30875598 98.57457877 96.36213009 98.02024829 86.37930824 79.76267346
 79.48124414 90.39607217 97.11504489 76.67060586 80.5070601  92.05906361
 78.99635726 78.60406184 96.9627563  93.87434364 98.02877645 98.57336046
 86.49504757 88.54058796 87.25892716 98.55508583 98.99976852 79.20956129
 98.70615611 79.51657509 73.89773516 93.73789306 96.24273583 96.9067141
 89.79300934 97.17717864 90.79080421 79.54459619 98.42838172 79.99415212
 98.20786784 78.59066045 80.69955288 97.55972759 81.26972137 95.99054592
 80.30238423 95.45083515 78.84285036 82.74509326 85.72507645 80.16715196
 81.3294185  99.14718388]
Avg Prec: is [55.85481653  3.0511403  11.22081058  3.43267616  2.19035821  3.82349243
  3.23108741  5.50181378  2.4581233   3.82286177  1.65492382  1.61880859
  0.62008883  5.07912686  2.73892057  3.2274257   3.71223241  2.75555484
  1.37509225  1.72737257  1.93023098  0.85680062  1.74416412  2.47871839
  5.05725162  3.52319109  6.48445524  3.41765512  2.04209028  1.87180196
  2.62168998  1.32140194  3.65664086  1.6774494   2.36364345  2.42714302
  3.02026141  2.58204527  2.83808428  7.32636245  2.22789755  8.1797186
  3.31439733  3.93479797  3.18929157  6.43297829  2.11355424  1.47483511
  2.16323058  1.6004503   1.87938368  1.58967142  1.1194441   3.00826196
  1.2182347   2.71725419 11.14452183  3.71990876  3.87635957  2.85343387
 10.6749297   2.20450683  3.82340384  2.94822297  1.52923277  2.54351486
  1.70774064  4.15154947  1.25880631  2.46930115  0.20853799  3.45624956
  1.88141605  4.54479235  4.0169048   3.10894417  0.76628001  1.86147747
  0.17868513  0.72063743]
mAP score regular 17.77, mAP score EMA 3.75
starting validation
Accuracy th:0.5 is [72.36714254 97.22450607 89.90208536 97.07501806 98.01180955 96.74614446
 97.01273139 94.71809054 97.38146847 96.28522311 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.70194584 98.34068316 98.22358422 99.15040985 99.10805491 97.91464235
 95.43314149 96.52938685 94.3393876  96.76607619 97.81747515 98.11395969
 97.65802128 98.6670653  96.37989885 98.16877196 98.04918155 98.03174129
 97.27931833 97.3565538  97.2593866  92.74484889 97.82993248 92.43839849
 97.04013753 96.48204898 97.03764606 94.06781772 98.18621222 98.77419837
 97.95948875 98.5848469  98.39798689 98.54498343 98.87385704 97.30672447
 98.6969629  97.58576874 89.25181254 96.40730498 96.16314124 96.78102499
 90.63457658 97.14477913 96.17061564 96.94297033 98.34815756 97.40638314
 98.13139996 95.7769639  98.72436904 97.53095647 99.81563146 96.35747565
 98.03423275 95.45307322 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [67.08523308 97.22450607 89.70027655 96.98283379 97.91713382 96.76109326
 96.84829459 94.87256148 97.38894287 96.41976231 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.76921544 98.34068316 98.22109276 99.15040985 99.07566584 97.88474475
 95.43563296 96.52938685 94.3393876  96.79597379 97.81747515 98.11395969
 97.55088821 98.67204823 96.40979645 98.18870369 98.01430102 98.02426689
 97.27931833 97.26436953 97.14477913 92.74484889 97.82744101 92.39106062
 97.07750953 96.48703192 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 98.5848469  98.37556369 98.55993223 98.87385704 97.60071754
 98.6969629  97.58576874 89.06495254 96.39733911 96.16314124 96.78102499
 90.57228991 97.03764606 96.08092284 96.93051299 98.32324289 97.40638314
 98.13139996 95.7769639  98.72436904 97.53344794 99.81563146 96.13573511
 98.03174129 95.44310736 95.7545407  97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [82.11572239 12.56334203 38.22304341 31.47055116 34.63321513 36.06364856
 35.22663274 21.7414481  14.26391333 20.72596795  4.5083722   7.31231954
  2.97686215 13.99331268  6.71718639 14.52932974  9.59618728 10.49350635
 13.69002002 11.01626809 22.61236276 16.55903121 74.35651304 25.35457196
 14.11570758 10.18563204 19.07784362 13.68840001  6.29678646 12.2041407
 41.41576782 15.78378698 31.46527152 23.85023311 31.30067293 41.84339504
 12.40998694 50.76059238 42.53249515 26.10748348 13.65151004 32.36124905
 24.24974294 20.96534151 16.62182779 25.26378289 11.17224877  6.17049572
 18.52306878 13.60887191 32.90128075 12.55894005  8.04597221 46.85230165
  6.5366165  11.78412528 36.78535209 25.65015599 13.98542021 13.06516306
 43.9602616  37.49614847 33.68793275 22.1201429  26.03244784 13.23361832
 23.04763138 11.18713649 14.4870105  22.28459192  3.76720036 42.98579073
 20.0037941  24.39485301 18.83812255 14.45875125  2.1073493   5.6797362
  0.78285576  3.98163016]
Accuracy th:0.5 is [45.32227122 97.22450607 72.52908787 96.96290206 97.90716795 78.21461494
 78.35164561 77.05109998 79.71447791 96.41976231 79.97857339 98.5325261
 99.34972718 79.20123577 79.79171338 96.31262924 96.21047911 79.29840297
 98.78167277 98.34068316 80.23021153 80.62386327 98.31327703 79.38311284
 79.27847124 96.52938685 94.3393876  79.23113337 97.81747515 79.93123552
 97.52597354 98.67204823 96.39983058 98.18870369 86.57348581 79.52761791
 79.43543364 91.81802327 97.0276802  76.36594663 79.43045071 92.37362035
 78.64065575 78.26942721 97.03764606 94.02795426 98.18621222 98.77668984
 85.13341804 87.85409971 85.74881032 98.55993223 98.87385704 78.70045096
 98.6969629  79.29342004 72.83304681 94.32693026 96.16314124 96.78102499
 90.13379176 97.04761193 90.36051524 79.1912699  98.32075143 80.04086005
 98.13139996 78.37157735 80.58649127 97.53593941 81.11966515 96.07843137
 80.27256646 95.44559882 78.28437601 83.77806014 87.3234173  79.95365872
 81.19440915 99.15040985]
Accuracy th:0.7 is [45.49916536 97.22450607 72.52908787 96.96290206 97.90716795 78.21461494
 78.35164561 77.07103172 79.71447791 96.41976231 79.97857339 98.5325261
 99.34972718 79.57495578 79.79171338 96.31262924 96.21047911 79.29840297
 98.78167277 98.34068316 80.531679   80.62386327 98.31327703 79.38560431
 79.64471684 96.52938685 94.3393876  79.23113337 97.81747515 79.93123552
 97.52597354 98.67204823 96.39983058 98.18870369 86.86000448 79.52761791
 79.43543364 91.99740887 97.0276802  76.36594663 79.61980218 92.37362035
 78.64065575 78.26942721 97.03764606 94.02795426 98.18621222 98.77668984
 86.73044821 88.16553305 85.90328126 98.55993223 98.87385704 78.70045096
 98.6969629  79.29342004 72.83304681 94.57358547 96.16314124 96.78102499
 90.13379176 97.04761193 90.54737524 79.1912699  98.32075143 80.04086005
 98.13139996 78.37157735 80.58649127 97.53593941 81.11966515 96.07843137
 80.27256646 95.44559882 78.28437601 83.87024441 87.44549917 79.95365872
 81.19440915 99.15040985]
Avg Prec: is [53.19870144  3.66210423 14.95310887  4.54441494  1.4627455   4.6752847
 14.79713408  8.75716054  8.72501679  5.76357646  3.43926115  4.88053063
  2.68172377  5.72064266  3.02355715  3.87875428 13.5175289   6.56467588
  1.60164556  2.92941187  3.56633297  1.5794454   1.21869234  5.20105654
  5.50264968  7.405902    7.72225466  4.75501323  3.89667726  4.04189652
  2.00542154  0.83093402  2.88037226  1.13934221  1.67920831  2.09845492
  1.89622993  2.15678916  2.19140075  6.28000429  1.69287744  6.02706544
  2.14454398  2.6785238   2.33838324  4.84829084  1.61434791  1.0153002
  1.42975348  1.15518485  1.19388201  0.96756534  0.72892955  2.24891362
  0.84148786  1.82262712  9.93604651  2.94261906  3.83310756  2.68624993
  7.77520646  2.12443143  3.2488208   2.53559356  1.35895523  1.94137862
  1.53207254  3.52446644  1.10830543  2.26975715  0.20044733  3.36067706
  1.66323568  3.97104063  3.1943194   2.29434835  0.55990583  1.44875041
  0.12038715  0.61652716]
mAP score regular 21.61, mAP score EMA 4.17
Train_data_mAP: current_mAP = 17.77, highest_mAP = 17.77
Val_data_mAP: current_mAP = 21.61, highest_mAP = 21.61
tensor([0.5334, 0.4553, 0.5729, 0.4879, 0.1639, 0.2851, 0.1441, 0.2468, 0.3394,
        0.2548, 0.0825, 0.0929, 0.0624, 0.2626, 0.3457, 0.2980, 0.3418, 0.2297,
        0.1216, 0.3252, 0.0607, 0.0214, 0.0824, 0.0706, 0.2967, 0.2749, 0.5410,
        0.2020, 0.1386, 0.1860, 0.1659, 0.1091, 0.2964, 0.1076, 0.2285, 0.1108,
        0.1895, 0.2417, 0.1581, 0.5770, 0.6667, 0.5681, 0.2102, 0.5419, 0.6555,
        0.6151, 0.1297, 0.1852, 0.1018, 0.2651, 0.0373, 0.0434, 0.0865, 0.1963,
        0.1287, 0.2147, 0.6476, 0.2927, 0.4170, 0.0879, 0.7246, 0.5580, 0.4564,
        0.3100, 0.2330, 0.2036, 0.3315, 0.3357, 0.4383, 0.4733, 0.2432, 0.4907,
        0.6080, 0.4378, 0.4741, 0.5068, 0.0553, 0.2336, 0.0305, 0.3818],
       device='cuda:0')
Sum Train Loss:  tensor([3.6195e+01, 4.7766e+00, 1.7885e+01, 9.1783e+00, 1.3787e+00, 2.4751e+00,
        1.1972e+00, 5.6949e+00, 3.0017e+00, 3.2546e+00, 7.5177e-01, 5.2705e-01,
        3.1434e-01, 6.3111e+00, 6.4408e+00, 3.3223e+00, 8.9424e+00, 4.7836e+00,
        4.9199e-01, 2.4507e+00, 4.5203e-01, 8.2966e-02, 3.2622e-01, 7.3487e-01,
        8.9683e+00, 5.2220e+00, 1.8530e+01, 2.9365e+00, 1.9425e+00, 3.5132e+00,
        1.8991e+00, 6.8944e-01, 5.1813e+00, 5.3322e-01, 2.5402e+00, 1.1190e+00,
        3.9136e+00, 1.2880e+00, 2.6556e+00, 1.3928e+01, 1.6440e+01, 2.1159e+01,
        1.1629e+00, 8.5286e+00, 3.5306e+00, 1.6947e+01, 2.9755e-01, 1.6361e+00,
        1.3041e+00, 2.8286e+00, 5.4488e-01, 1.3643e-01, 1.0909e-01, 3.1926e+00,
        1.1645e+00, 1.7549e+00, 2.8863e+01, 5.2973e+00, 6.4511e+00, 1.3777e+00,
        2.6848e+01, 8.9532e+00, 7.3086e+00, 7.4700e+00, 3.0635e+00, 3.2815e+00,
        5.2371e+00, 6.8341e+00, 2.1174e+00, 5.6707e+00, 1.4038e+00, 1.0925e+01,
        8.0249e+00, 1.0774e+01, 9.6213e+00, 9.7338e+00, 5.3145e-02, 2.2136e+00,
        5.4584e-03, 4.4901e+00], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [1/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 452.6
Sum Train Loss:  tensor([3.7929e+01, 2.1389e+00, 2.3954e+01, 5.8082e+00, 1.6291e+00, 4.2779e+00,
        1.2210e+00, 5.8041e+00, 5.7315e+00, 4.1882e+00, 4.6829e-01, 7.9407e-01,
        3.2329e-01, 5.5632e+00, 2.5938e+00, 2.6641e+00, 4.7879e+00, 3.4805e+00,
        1.2507e+00, 2.2747e+00, 8.5912e-01, 1.8809e-02, 8.5798e-01, 2.1241e+00,
        7.9380e+00, 4.5806e+00, 1.9086e+01, 6.0008e+00, 1.3785e+00, 2.3727e+00,
        2.7240e+00, 2.4901e-01, 8.7320e+00, 1.0286e+00, 1.0831e+00, 6.4712e-01,
        3.5631e+00, 7.0434e-01, 4.0330e+00, 1.5109e+01, 5.3959e+00, 1.3814e+01,
        3.5263e+00, 9.9062e+00, 1.2769e+01, 1.4168e+01, 5.8780e-01, 1.5568e+00,
        1.4566e+00, 2.4422e+00, 3.3510e-01, 3.3204e-01, 6.8665e-02, 2.2509e+00,
        1.5596e+00, 5.3964e+00, 2.0328e+01, 2.4273e+00, 9.4336e+00, 1.6001e+00,
        1.9993e+01, 3.3016e+00, 6.0087e+00, 4.0060e+00, 2.9272e+00, 1.4533e+00,
        3.5626e+00, 6.3607e+00, 1.4643e+00, 5.3563e+00, 2.7542e-02, 6.6705e+00,
        5.6878e+00, 6.6307e+00, 9.1998e+00, 4.2393e+00, 2.7816e-01, 2.1564e+00,
        5.3822e-03, 4.6450e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [1/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 399.1
Sum Train Loss:  tensor([3.3518e+01, 3.3155e+00, 1.6425e+01, 5.6454e+00, 1.3424e+00, 1.4184e+00,
        1.7273e+00, 6.1665e+00, 5.2996e+00, 2.6298e+00, 4.8086e-01, 2.0155e+00,
        4.4241e-02, 4.7977e+00, 7.2955e+00, 5.1936e+00, 4.9683e+00, 1.6430e+00,
        1.2763e+00, 3.1872e+00, 5.4514e-01, 2.9028e-02, 4.4731e-01, 5.9597e-01,
        6.6369e+00, 3.2329e+00, 1.6221e+01, 3.0441e+00, 1.7134e+00, 1.9799e+00,
        3.1421e+00, 6.4871e-01, 3.7896e+00, 7.3815e-01, 1.4252e+00, 7.7849e-01,
        2.5412e+00, 1.9548e+00, 2.5782e+00, 1.5258e+01, 4.0146e+00, 1.2830e+01,
        2.1129e+00, 6.8109e+00, 9.7823e+00, 1.5792e+01, 6.9074e-01, 1.8106e+00,
        7.7951e-01, 1.6089e+00, 2.9804e-01, 1.1077e-01, 7.3157e-01, 1.7828e+00,
        2.3581e-01, 2.1702e+00, 2.2501e+01, 3.3008e+00, 8.5624e+00, 1.1522e+00,
        1.9358e+01, 8.1730e+00, 7.2810e+00, 3.9104e+00, 3.1095e+00, 1.7221e+00,
        3.8278e+00, 7.0911e+00, 2.8682e+00, 3.9030e+00, 5.8845e-02, 5.3364e+00,
        6.3555e+00, 9.1768e+00, 9.8079e+00, 1.2625e+01, 5.1439e-01, 3.6431e+00,
        4.3612e-03, 6.2169e+00], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [1/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 387.8
Sum Train Loss:  tensor([4.0621e+01, 8.1826e+00, 1.9921e+01, 6.7015e+00, 1.7450e+00, 3.9476e+00,
        3.9659e+00, 5.3340e+00, 6.4728e+00, 2.8919e+00, 4.2498e-01, 9.0834e-01,
        4.7652e-02, 8.0510e+00, 5.1865e+00, 4.5584e+00, 9.0280e+00, 1.3688e+00,
        1.1738e+00, 2.6874e+00, 3.2621e-01, 1.0710e-01, 3.4989e-01, 8.5883e-01,
        6.2036e+00, 3.8933e+00, 2.5932e+01, 4.4833e+00, 1.8183e+00, 3.0719e-01,
        1.8769e+00, 9.6312e-01, 4.6173e+00, 1.2905e+00, 1.3613e+00, 7.4432e-01,
        3.7036e+00, 4.9988e+00, 1.3619e+00, 1.2489e+01, 4.9098e+00, 1.2526e+01,
        1.3639e+00, 4.9676e+00, 7.3622e+00, 1.0689e+01, 1.4940e+00, 3.1838e-01,
        2.9464e-01, 2.5044e+00, 4.7842e-01, 4.9094e-01, 9.5668e-02, 2.2073e+00,
        6.2665e-01, 3.1582e+00, 1.8101e+01, 3.1994e+00, 8.1291e+00, 1.0427e+00,
        1.4640e+01, 3.7744e+00, 3.2247e+00, 1.5734e+00, 7.7550e-01, 1.5802e+00,
        1.2546e+00, 9.6087e+00, 2.2994e+00, 3.2800e+00, 3.7306e-02, 8.4251e+00,
        2.0754e+00, 5.5494e+00, 6.4199e+00, 1.5339e+01, 8.6733e-02, 2.3863e+00,
        3.3247e-03, 5.6044e+00], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [1/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 382.8
Sum Train Loss:  tensor([29.5385,  5.5815, 17.1487,  5.9765,  1.8770,  4.4972,  1.1979,  2.1019,
         5.2173,  4.1461,  1.1659,  0.7142,  0.0514,  3.0075,  7.9033,  8.2791,
         4.4270,  2.1837,  0.8779,  3.6338,  0.1631,  0.1180,  0.7008,  0.4015,
         5.6232,  6.3241,  7.9850,  2.9369,  3.6286,  2.5722,  1.0206,  0.4913,
         6.1980,  0.9290,  2.0044,  0.6634,  2.5376,  2.5619,  0.7847, 15.9317,
         9.0048, 15.6405,  1.8611,  9.1823, 10.1485,  8.5405,  0.8923,  1.6507,
         1.3912,  3.0304,  0.2764,  0.4911,  0.8542,  2.1767,  0.1568,  1.1274,
        26.0134,  3.7605,  5.3833,  2.0456, 24.6316,  5.9170,  4.1566,  6.1120,
         0.9254,  3.3143,  3.4768,  7.3446,  0.4592,  2.4035,  1.4700,  6.9047,
         7.0820,  8.7732,  4.3452,  5.7764,  0.2867,  2.6774,  0.1899,  3.6638],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [1/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 376.6
Sum Train Loss:  tensor([3.2242e+01, 2.3493e+00, 2.4468e+01, 1.8032e+00, 1.0867e+00, 3.8266e+00,
        2.6169e+00, 2.7568e+00, 1.8836e+00, 3.4808e+00, 2.1761e+00, 1.4782e-01,
        2.5088e-01, 3.7918e+00, 5.5759e+00, 3.8254e+00, 9.4974e+00, 3.7309e+00,
        6.0200e-01, 6.8884e-01, 1.4238e-01, 3.1874e-02, 3.2899e-01, 6.1749e-01,
        6.1398e+00, 2.4872e+00, 1.0523e+01, 2.2098e+00, 1.8223e+00, 2.0529e+00,
        1.5202e+00, 6.9221e-01, 3.3519e+00, 1.1949e+00, 2.3999e+00, 1.2845e+00,
        2.4406e+00, 2.5563e+00, 1.3129e+00, 1.7848e+01, 6.5739e+00, 1.5779e+01,
        2.2556e+00, 6.5328e+00, 8.9777e+00, 1.6542e+01, 1.2756e+00, 3.7712e-01,
        6.7088e-01, 6.2561e-01, 7.9708e-02, 6.0776e-02, 9.9408e-01, 8.9960e-01,
        1.0642e+00, 1.0035e+00, 2.8018e+01, 4.4520e+00, 5.9653e+00, 1.2427e+00,
        2.2566e+01, 8.4585e+00, 5.5323e+00, 5.3495e+00, 2.2912e+00, 3.4637e+00,
        3.7188e+00, 7.0211e+00, 4.1025e+00, 4.5202e+00, 7.2583e-02, 1.0824e+01,
        4.1687e+00, 7.4805e+00, 8.6815e+00, 4.8800e+00, 6.4075e-01, 2.7773e+00,
        4.3906e-03, 4.5153e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [1/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 378.2
Sum Train Loss:  tensor([2.9323e+01, 7.6926e+00, 1.8064e+01, 8.4276e+00, 1.0686e+00, 4.7476e+00,
        1.5002e+00, 4.9123e+00, 2.3491e+00, 7.3136e+00, 7.8074e-01, 4.9354e-01,
        3.2207e-01, 3.9853e+00, 2.5427e+00, 2.7047e+00, 6.1856e+00, 3.0201e+00,
        8.6703e-01, 1.1954e+00, 2.9597e-01, 1.2336e-01, 8.1507e-02, 1.0767e+00,
        9.2665e+00, 4.2829e+00, 1.1534e+01, 3.1838e+00, 1.3122e+00, 2.5754e+00,
        1.3396e+00, 1.1028e+00, 3.0358e+00, 5.6689e-01, 2.9246e+00, 5.6179e-01,
        2.5829e+00, 1.7466e+00, 1.3357e+00, 2.1804e+01, 4.0433e+00, 1.1939e+01,
        3.7295e+00, 1.0194e+01, 8.8883e+00, 1.1637e+01, 1.6729e+00, 3.3917e-01,
        1.0425e+00, 6.6497e-01, 6.4618e-02, 9.2556e-02, 4.1599e-01, 2.0775e+00,
        9.8495e-01, 3.4718e+00, 1.5194e+01, 3.9319e+00, 3.3381e+00, 8.1859e-01,
        1.3424e+01, 1.0815e+01, 6.3906e+00, 6.3440e+00, 1.1750e+00, 8.8535e-01,
        4.5801e+00, 6.6232e+00, 1.8135e+00, 3.6517e+00, 4.6707e-02, 1.1540e+01,
        6.3993e+00, 6.5317e+00, 7.2461e+00, 2.6638e+00, 4.5665e-01, 1.3467e+00,
        6.7798e-03, 2.4893e+00], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [1/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 357.2
Sum_Val Meta Model:  tensor([3.7711e+01, 3.4436e+01, 5.3490e+01, 2.3134e+01, 5.1293e-01, 3.8531e+00,
        8.9456e-01, 4.8827e+00, 2.6040e+00, 3.8006e+00, 4.3632e-01, 1.1001e+00,
        3.0799e-01, 4.4710e+00, 2.3980e+00, 1.7710e+00, 1.5314e+02, 6.4741e-01,
        1.1549e-01, 8.2343e-01, 1.0477e-01, 2.2947e-02, 1.2320e-01, 1.6185e-01,
        9.1686e+00, 5.1270e+00, 1.4039e+01, 7.6381e-01, 1.3161e+00, 1.5288e+00,
        5.7581e-01, 2.6486e-01, 2.6643e+00, 2.9929e-01, 6.5433e-01, 6.1236e-01,
        5.2168e-01, 1.3187e+00, 5.0028e-01, 2.8875e+01, 7.8980e+00, 1.3856e+01,
        2.0595e+00, 1.1133e+01, 1.2577e+01, 1.4981e+01, 1.8589e-01, 9.9523e-01,
        1.9202e-01, 1.1298e+00, 4.3168e-02, 7.2381e-02, 8.0328e-01, 8.1359e-01,
        1.7361e-01, 4.3875e-01, 1.7521e+01, 2.2871e+00, 8.9794e+00, 3.3592e-01,
        1.0822e+01, 1.4343e+01, 3.0948e+00, 1.6673e+00, 4.4420e-01, 3.8701e-01,
        7.9909e-01, 3.6079e+00, 3.1966e+00, 8.1013e+00, 7.1133e-02, 7.7118e+00,
        7.8701e+00, 5.1341e+00, 6.3812e+00, 4.7954e+00, 6.4441e-02, 1.4355e+00,
        3.4975e-03, 4.7160e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([3.6371e+01, 2.8957e+01, 4.6979e+01, 1.7995e+01, 2.0321e-01, 3.7956e+00,
        9.6560e-01, 5.2382e+00, 2.5840e+00, 3.3341e+00, 4.4076e-01, 9.2195e-01,
        3.3279e-01, 4.3611e+00, 1.8607e+00, 2.8371e+00, 1.3788e+02, 9.3597e-01,
        7.3009e-02, 6.9944e-01, 1.1619e-01, 1.8170e-02, 3.6533e-02, 1.2527e-01,
        8.1459e+00, 4.6602e+00, 1.5280e+01, 1.0997e+00, 1.4142e+00, 1.4982e+00,
        2.5516e-01, 1.1665e-01, 2.0916e+00, 1.5725e-01, 6.9376e-01, 4.8256e-01,
        7.5397e-01, 6.6332e-01, 3.2402e-01, 2.7598e+01, 7.9368e+00, 1.3992e+01,
        2.0112e+00, 9.5293e+00, 1.0869e+01, 1.2923e+01, 1.3293e-01, 9.0974e-01,
        1.4187e-01, 9.1327e-01, 1.7630e-02, 3.9499e-02, 8.5034e-01, 6.3503e-01,
        1.3517e-01, 3.7329e-01, 1.9524e+01, 2.4018e+00, 8.8923e+00, 5.3529e-01,
        1.1902e+01, 7.3873e+00, 2.9482e+00, 1.9850e+00, 4.0470e-01, 5.6190e-01,
        8.8135e-01, 4.5831e+00, 3.4236e+00, 7.6803e+00, 9.0181e-02, 8.7069e+00,
        7.2619e+00, 4.9091e+00, 5.9430e+00, 4.9342e+00, 6.6938e-02, 1.3866e+00,
        4.4892e-03, 5.9225e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.8189e+01, 6.3598e+01, 8.1996e+01, 3.6880e+01, 1.2397e+00, 1.3312e+01,
        6.7026e+00, 2.1221e+01, 7.6136e+00, 1.3087e+01, 5.3441e+00, 9.9234e+00,
        5.3343e+00, 1.6607e+01, 5.3832e+00, 9.5219e+00, 4.0343e+02, 4.0741e+00,
        6.0042e-01, 2.1510e+00, 1.9147e+00, 8.4890e-01, 4.4337e-01, 1.7752e+00,
        2.7459e+01, 1.6954e+01, 2.8244e+01, 5.4447e+00, 1.0202e+01, 8.0564e+00,
        1.5383e+00, 1.0693e+00, 7.0566e+00, 1.4616e+00, 3.0367e+00, 4.3563e+00,
        3.9779e+00, 2.7439e+00, 2.0498e+00, 4.7832e+01, 1.1905e+01, 2.4630e+01,
        9.5666e+00, 1.7585e+01, 1.6580e+01, 2.1009e+01, 1.0247e+00, 4.9122e+00,
        1.3941e+00, 3.4445e+00, 4.7247e-01, 9.1115e-01, 9.8327e+00, 3.2354e+00,
        1.0506e+00, 1.7386e+00, 3.0148e+01, 8.2065e+00, 2.1326e+01, 6.0881e+00,
        1.6426e+01, 1.3238e+01, 6.4599e+00, 6.4037e+00, 1.7371e+00, 2.7592e+00,
        2.6590e+00, 1.3653e+01, 7.8114e+00, 1.6226e+01, 3.7083e-01, 1.7742e+01,
        1.1944e+01, 1.1213e+01, 1.2536e+01, 9.7361e+00, 1.2098e+00, 5.9359e+00,
        1.4695e-01, 1.5511e+00], device='cuda:0')
Outer loop valEpocw Maximum [1/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 576.1
model_train val_loss valEpocw [1/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 530.7
model_train val_loss valEpocw [1/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1307.5
Sum_Val Meta Model:  tensor([3.5022e+01, 6.8508e+00, 4.1428e+01, 3.9544e+00, 1.7270e-01, 1.0234e+01,
        1.9670e+01, 1.4848e+01, 1.1180e+01, 7.6131e+00, 1.1607e+00, 1.9609e+01,
        5.0728e-01, 1.3898e+00, 5.3346e+00, 5.8483e+00, 3.5889e+00, 3.3787e+00,
        8.4519e-01, 1.1034e+01, 4.5900e-02, 6.2645e-03, 8.7825e-02, 3.2388e-01,
        6.0246e+00, 5.0697e+00, 1.6335e+01, 2.3765e+00, 1.8967e+00, 9.3000e-02,
        1.1693e-01, 4.7766e-02, 2.1616e-01, 5.0983e-02, 1.0724e-01, 3.8846e-02,
        8.7969e-01, 1.7177e-01, 8.8405e-02, 1.9621e+00, 4.6201e-01, 2.7880e+00,
        1.5109e-01, 5.6036e-01, 7.2372e-01, 2.6694e+00, 2.4534e-01, 1.5372e-01,
        5.7162e-02, 1.3299e+00, 1.9153e-02, 2.6390e-02, 3.1088e-02, 7.6013e-02,
        8.2488e-02, 7.3141e-01, 9.9728e+00, 1.8033e+00, 3.3155e+00, 3.9455e-01,
        3.2755e+00, 4.0240e-01, 1.5845e+00, 8.8590e-01, 2.5437e-01, 2.7958e-01,
        4.2181e-01, 2.4974e+00, 3.0671e-01, 1.8565e+00, 5.5469e-02, 4.8830e-01,
        2.5353e+00, 3.1637e+00, 1.8832e+00, 5.6934e-01, 3.4075e-02, 3.9349e-01,
        5.5488e-03, 1.5171e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([3.3261e+01, 7.1933e+00, 3.1047e+01, 5.1126e+00, 4.7203e-01, 1.0768e+01,
        1.9526e+01, 1.2097e+01, 9.9063e+00, 6.7460e+00, 1.0985e+00, 1.4813e+01,
        4.5859e-01, 1.9457e+00, 4.9100e+00, 1.4279e+00, 3.4436e+00, 3.7323e+00,
        1.3052e-01, 7.3480e+00, 1.0330e-01, 1.8752e-02, 9.2445e-02, 2.3955e-01,
        6.2542e+00, 5.4893e+00, 1.4749e+01, 2.7623e+00, 1.7000e+00, 1.6304e-01,
        1.4176e-01, 6.4435e-02, 3.4414e-01, 1.1913e-01, 1.5350e-01, 5.1091e-02,
        7.3656e-01, 5.1691e-01, 7.5059e-02, 2.0215e+00, 3.2375e-01, 1.8796e+00,
        1.7721e-01, 7.4017e-01, 8.6838e-01, 2.0147e+00, 7.1132e-02, 1.1516e-01,
        6.2952e-02, 1.8699e+00, 8.1759e-03, 1.8147e-02, 2.3420e-02, 3.2821e-01,
        5.1507e-02, 5.2326e-01, 7.4379e+00, 9.6976e-01, 3.0910e+00, 1.1664e-01,
        3.7856e+00, 6.1084e-01, 1.0626e+00, 3.7090e-01, 1.4126e-01, 1.1613e-01,
        2.8385e-01, 2.5059e+00, 3.5477e-01, 1.4090e+00, 1.0331e-02, 7.8634e-01,
        1.4178e+00, 1.8469e+00, 2.7243e+00, 7.9866e-01, 3.1161e-02, 2.1889e-01,
        8.4207e-04, 1.1800e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([7.3634e+01, 1.9367e+01, 5.3948e+01, 1.2033e+01, 4.1884e+00, 4.1074e+01,
        8.5480e+01, 5.1541e+01, 3.2536e+01, 3.4040e+01, 1.6803e+01, 1.8757e+02,
        8.0908e+00, 1.0860e+01, 1.4635e+01, 4.2854e+00, 1.0837e+01, 1.7563e+01,
        9.4624e-01, 1.8723e+01, 2.1101e+00, 1.1686e+00, 1.2369e+00, 4.0313e+00,
        2.7896e+01, 2.2723e+01, 3.0954e+01, 1.6458e+01, 1.2554e+01, 1.1718e+00,
        1.1113e+00, 8.4559e-01, 1.5725e+00, 3.2866e+00, 9.6818e-01, 1.1432e+00,
        5.0158e+00, 3.0765e+00, 7.4650e-01, 4.6245e+00, 7.0940e-01, 4.0672e+00,
        1.2869e+00, 1.7872e+00, 1.6639e+00, 3.7918e+00, 6.0192e-01, 8.7214e-01,
        7.3391e-01, 7.7139e+00, 2.6070e-01, 5.7381e-01, 3.4576e-01, 2.5021e+00,
        4.6539e-01, 3.3952e+00, 1.2731e+01, 3.8559e+00, 9.0184e+00, 1.4205e+00,
        5.7421e+00, 1.6861e+00, 2.4512e+00, 1.4561e+00, 8.6368e-01, 6.9275e-01,
        1.1892e+00, 1.5405e+01, 1.0428e+00, 3.9259e+00, 5.8063e-02, 2.4441e+00,
        3.0117e+00, 4.2466e+00, 8.9288e+00, 1.5151e+00, 5.2432e-01, 7.2676e-01,
        2.9292e-02, 4.3802e-01], device='cuda:0')
Outer loop valEpocw Maximum [1/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 286.2
model_train val_loss valEpocw [1/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 250.5
model_train val_loss valEpocw [1/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 955.0
Sum_Val Meta Model:  tensor([3.8985e+01, 1.3356e+00, 6.1935e+00, 9.1468e-01, 1.7248e-01, 5.4443e-01,
        1.8766e-01, 2.2456e+00, 7.1881e-01, 5.5133e-01, 9.9914e-02, 1.0091e-01,
        2.5198e-02, 3.3563e+00, 7.9432e-01, 8.3719e-01, 1.6554e+00, 4.7352e-01,
        1.4964e-01, 5.7382e-01, 7.8119e-02, 4.7503e-02, 6.6663e-01, 1.4365e-01,
        1.6251e+00, 4.9078e-01, 9.1217e+00, 9.8924e-01, 2.0766e-01, 4.1331e-01,
        1.0291e+00, 5.2004e-01, 3.0266e+00, 3.1861e-02, 8.6660e-01, 2.8232e-01,
        3.6313e+00, 2.7189e+00, 8.1535e-02, 1.4891e+01, 1.7696e+01, 4.5523e+01,
        2.0145e+01, 4.7478e+01, 3.8232e+01, 4.9880e+01, 2.6878e+00, 3.0304e+00,
        8.2364e+00, 6.6233e+00, 1.8179e+00, 1.8354e+00, 6.0060e+00, 3.2672e+00,
        1.0173e+01, 2.3979e+01, 1.0540e+02, 2.8903e+00, 3.0424e+00, 1.9767e-01,
        7.8301e+01, 5.8703e-01, 4.9237e+00, 3.1876e+00, 2.2040e+00, 1.8824e-01,
        2.0665e+00, 2.0340e+00, 1.8740e+00, 6.8670e-01, 6.0374e-02, 2.7278e+00,
        2.3767e+00, 1.3697e+01, 7.3046e-01, 7.8078e+00, 5.9941e-01, 3.3319e-01,
        6.2679e-03, 1.9115e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.9946e+01, 4.3028e-01, 3.4039e+00, 6.0450e-01, 2.5453e-02, 9.1519e-02,
        2.3661e-02, 2.1941e+00, 8.3891e-02, 9.6459e-02, 2.0241e-02, 1.9360e-02,
        7.5299e-03, 2.8673e+00, 1.5779e-01, 1.1902e+00, 9.8176e-01, 1.3087e-01,
        1.7395e-02, 1.1396e-01, 1.6682e-02, 6.8805e-03, 4.7330e-03, 1.3563e-02,
        1.0916e+00, 2.8406e-01, 9.2790e+00, 9.7951e-01, 1.6341e-01, 4.2332e-02,
        2.3694e-01, 5.2501e-01, 1.8826e+00, 3.7775e-03, 3.9358e-01, 1.9063e-01,
        3.1823e+00, 3.2373e+00, 3.7107e-02, 1.9796e+01, 1.6072e+01, 4.0605e+01,
        1.8893e+01, 4.3945e+01, 2.7777e+01, 4.5136e+01, 1.2715e+00, 1.8381e+00,
        7.1988e+00, 3.6196e+00, 1.3805e+00, 1.9828e+00, 7.8200e+00, 8.0172e+00,
        8.9516e+00, 2.6723e+01, 6.3336e+01, 3.4565e+00, 3.7362e+00, 3.8803e-01,
        7.1636e+01, 1.0238e+00, 5.1603e+00, 3.1572e+00, 1.9388e+00, 5.9479e-01,
        2.1983e+00, 2.2215e+00, 2.0934e+00, 3.4213e+00, 4.8981e-02, 2.9135e+00,
        3.8265e+00, 1.4836e+01, 8.0530e-01, 7.1329e+00, 5.3239e-01, 5.1711e-01,
        3.3381e-03, 3.3623e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.7428e+01, 1.3215e+00, 6.6254e+00, 1.6937e+00, 2.5486e-01, 4.6096e-01,
        2.9761e-01, 1.4264e+01, 3.2403e-01, 5.6239e-01, 3.6678e-01, 3.1938e-01,
        1.8565e-01, 1.6594e+01, 5.8071e-01, 5.3211e+00, 3.3981e+00, 7.2186e-01,
        1.8693e-01, 4.4663e-01, 4.8337e-01, 3.4579e-01, 5.0948e-02, 2.5348e-01,
        6.0319e+00, 1.7710e+00, 2.6377e+01, 9.3369e+00, 1.9877e+00, 3.1488e-01,
        1.7836e+00, 6.0978e+00, 7.0822e+00, 1.4472e-01, 2.1756e+00, 3.1901e+00,
        1.6516e+01, 1.7569e+01, 4.3326e-01, 4.4508e+01, 3.0699e+01, 7.0962e+01,
        7.3997e+01, 8.4308e+01, 4.7561e+01, 6.9902e+01, 8.9610e+00, 9.9263e+00,
        4.1236e+01, 1.0900e+01, 2.5856e+01, 4.5472e+01, 8.4263e+01, 5.2414e+01,
        7.6568e+01, 1.2342e+02, 1.1301e+02, 1.6151e+01, 1.2508e+01, 5.6394e+00,
        9.2520e+01, 3.0145e+00, 1.3578e+01, 1.4886e+01, 1.3908e+01, 3.9834e+00,
        1.0230e+01, 1.5086e+01, 7.4805e+00, 1.0778e+01, 3.3489e-01, 9.6549e+00,
        9.3326e+00, 4.2123e+01, 3.0560e+00, 1.4102e+01, 1.0370e+01, 2.2803e+00,
        1.3170e-01, 1.3235e+00], device='cuda:0')
Outer loop valEpocw Maximum [1/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 623.5
model_train val_loss valEpocw [1/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 530.3
model_train val_loss valEpocw [1/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1529.7
Sum_Val Meta Model:  tensor([5.0141e+01, 4.1690e-01, 1.4160e+01, 2.7193e-01, 6.1547e-02, 2.0404e+00,
        4.0254e-01, 2.4693e+00, 2.9394e-01, 2.9742e+00, 2.2321e-01, 2.6948e-01,
        5.4714e-03, 2.4740e+00, 3.4036e+00, 3.1824e-01, 4.0296e-01, 1.1112e-01,
        3.2831e-02, 1.0867e-01, 1.7375e-02, 4.3968e-03, 2.2650e-02, 2.7655e-02,
        3.4013e+00, 2.2500e-01, 1.2037e+01, 1.4109e-01, 9.9480e-01, 3.2088e-02,
        5.3596e-02, 9.0782e-03, 6.9221e-01, 2.9891e-01, 7.0983e-01, 2.2410e-01,
        3.3665e-02, 3.1334e-01, 3.8478e-01, 9.3458e+00, 4.9185e+00, 1.0813e+01,
        7.9911e-01, 3.1089e+00, 2.1185e+00, 5.2359e+00, 4.0012e-02, 4.8509e-02,
        7.7381e-02, 6.7650e-02, 1.7774e-02, 1.2944e-02, 1.6288e-02, 5.6924e-01,
        3.6863e-02, 2.0511e-01, 1.3760e+01, 9.0764e-01, 5.6938e+00, 9.8943e-02,
        1.8837e+01, 2.3580e-01, 2.1894e+00, 1.6303e+00, 7.6379e-01, 2.2818e-01,
        1.4682e+00, 1.1181e+01, 2.2311e-01, 6.6109e-01, 1.1266e-02, 4.0440e-01,
        1.3113e+00, 4.3324e+00, 1.4435e+02, 9.5673e+00, 2.4778e-02, 3.7677e+00,
        1.3110e-03, 1.8878e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([3.2593e+01, 1.6431e+00, 1.5606e+01, 1.1517e+00, 2.8208e-01, 2.1673e+00,
        1.5963e+00, 2.5868e+00, 8.7415e-01, 3.4874e+00, 2.0697e-01, 3.7923e-01,
        2.9993e-02, 2.7754e+00, 3.9455e+00, 5.4703e-01, 4.5720e-01, 3.7554e-01,
        2.4103e-02, 2.1424e-01, 3.9512e-02, 4.9820e-03, 2.9074e-02, 1.2260e-01,
        3.4369e+00, 6.1171e-01, 1.1702e+01, 2.5347e-01, 1.0376e+00, 2.7740e-02,
        9.3886e-02, 3.8752e-02, 1.6625e-01, 9.8945e-02, 5.0742e-02, 2.1360e-02,
        1.5524e-01, 9.0886e-02, 4.4333e-02, 3.5848e+00, 4.9641e-01, 2.3215e+00,
        1.5504e-01, 8.4227e-01, 1.0528e+00, 2.5817e+00, 4.3133e-02, 5.6325e-02,
        2.6459e-02, 4.8244e-02, 5.2782e-03, 7.6967e-03, 7.2078e-03, 1.9723e-01,
        1.9798e-02, 8.3567e-02, 5.6957e+00, 3.3267e-01, 5.0535e+00, 1.1006e-01,
        6.5828e+00, 9.6354e-01, 1.4366e+00, 6.4397e-01, 2.8437e-01, 1.4353e-01,
        6.0047e-01, 2.2784e+00, 1.0565e+00, 2.4708e+00, 2.2983e-02, 2.3016e+00,
        2.1069e+00, 2.6019e+00, 9.1877e+01, 3.0906e+00, 3.2979e-02, 2.0697e+00,
        1.7216e-03, 1.3493e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.9075e+01, 6.2265e+00, 3.2337e+01, 3.8774e+00, 3.1652e+00, 1.3630e+01,
        2.4231e+01, 1.9569e+01, 3.9673e+00, 2.5193e+01, 5.0118e+00, 7.8174e+00,
        9.1916e-01, 1.7870e+01, 1.7048e+01, 2.7651e+00, 1.9673e+00, 2.5634e+00,
        3.3056e-01, 1.0976e+00, 1.4435e+00, 3.7546e-01, 6.3026e-01, 2.9224e+00,
        2.4845e+01, 4.3100e+00, 2.9716e+01, 2.6115e+00, 1.5906e+01, 3.2769e-01,
        1.0565e+00, 1.0418e+00, 6.7162e-01, 1.7086e+00, 2.9314e-01, 3.3175e-01,
        1.9783e+00, 6.2810e-01, 4.0001e-01, 7.2794e+00, 7.9576e-01, 4.0481e+00,
        9.0926e-01, 1.7552e+00, 1.8953e+00, 4.5764e+00, 6.8990e-01, 6.6305e-01,
        4.5126e-01, 3.4676e-01, 2.5068e-01, 3.4270e-01, 1.6880e-01, 1.4917e+00,
        3.3262e-01, 6.7793e-01, 9.4167e+00, 1.4521e+00, 1.3877e+01, 1.9640e+00,
        9.2012e+00, 3.0268e+00, 3.2949e+00, 2.4128e+00, 1.4863e+00, 9.9071e-01,
        2.0660e+00, 8.3408e+00, 3.7061e+00, 7.2884e+00, 1.8867e-01, 7.9873e+00,
        4.1678e+00, 6.3915e+00, 2.5863e+02, 4.3214e+00, 6.3530e-01, 4.4967e+00,
        7.4605e-02, 4.7393e-01], device='cuda:0')
Outer loop valEpocw Maximum [1/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 359.5
model_train val_loss valEpocw [1/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 232.4
model_train val_loss valEpocw [1/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 732.4
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [74.85776245 97.21494621 90.288861   97.31850245 97.63404442 96.97737601
 97.3806362  94.7186316  97.48784737 96.50954545 98.53193796 98.52219149
 99.41399349 95.3180395  97.26977011 96.5582778  96.29512311 97.48053752
 98.65376884 98.30655085 98.18106505 99.18129652 99.21540917 97.90207234
 95.21448325 96.65086926 94.09607583 96.75564382 98.01293844 98.16279041
 97.57312898 98.51488164 96.43888354 98.0909102  98.06045248 97.97273425
 96.94326336 97.61455148 97.73272743 92.73400665 97.84237521 92.30638028
 96.90793241 96.25613723 96.91646057 93.87190702 98.02877645 98.57457877
 97.9654244  98.51975488 98.43569157 98.54168443 98.99976852 97.50855862
 98.70615611 97.46591781 89.78326287 96.2098415  96.2378626  96.90793241
 90.89557876 97.53901634 96.168419   96.99199571 98.47102253 97.34408694
 98.2297974  95.95277835 98.66595193 97.61576979 99.81603538 96.03562335
 97.98491734 95.46301824 96.25613723 96.90915072 99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [73.02786272 97.2137279  89.96600919 97.23687577 97.4086573  96.91280564
 97.27586165 94.75396255 97.44642487 96.48517927 98.53193796 98.52097319
 99.41399349 95.31682119 97.26977011 96.56680596 96.29512311 97.48053752
 98.65376884 98.30776915 98.15669887 99.18616976 99.17155005 97.82897382
 95.21935649 96.65086926 94.0778012  96.75198889 98.01293844 98.15913549
 97.48419244 98.57336046 96.41086244 98.06410741 97.9240019  98.02999476
 96.94082674 97.39038267 97.6035867  92.72913342 97.84237521 92.09195794
 96.91402395 96.23055275 96.96641123 93.94135062 98.02877645 98.57336046
 97.9934455  98.51853657 98.38208599 98.55508583 98.99976852 97.7838964
 98.70615611 97.46591781 89.51767157 96.1550176  96.24395414 96.9067141
 90.48500871 97.52317832 96.20131334 96.98590417 98.44543804 97.34408694
 98.22614247 95.95277835 98.6769167  97.60845994 99.81603538 96.2939048
 97.97029763 95.45205346 96.1964401  96.91767888 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [86.51687379 14.84032182 44.58234472 37.48783976 45.65599305 42.89887306
 41.75573725 29.32184269 22.15491943 27.26419549  6.23297491  9.83219225
  3.95869422 14.61794727  9.84832751 19.47122469 10.96124395 12.4822561
 18.99457738 14.81566014 23.32920784 15.45033303 73.94649395 38.79215041
 15.74173454 11.33346774 23.54653018 16.14643574  6.83048614 18.64069465
 41.87018041 17.28353452 37.38264023 30.74419262 41.95425939 48.20483273
 19.71680987 50.69013343 54.18857089 28.4167355  14.82528455 36.68872966
 28.21772928 25.86371519 21.09531544 32.58331641 13.29106054 11.19036792
 20.189276   15.7953385  35.19687315 15.3336039   7.26482397 49.71079371
  6.35630648 13.49128613 41.75447971 30.80512231 17.96544732 18.62693828
 49.5389132  47.57441226 34.76159869 21.62866382 23.538915   18.29615304
 21.81832951 12.14511729 20.41566958 30.20479052  3.36566246 44.09109919
 26.19420941 25.83994491 22.75446217 16.70059211  2.80534895  8.95039875
  0.93043707  5.57710106]
Accuracy th:0.5 is [45.48799357 97.2137279  73.61752415 97.02489005 97.26733349 78.65523081
 78.95371645 77.64037963 79.79922272 96.43766523 80.2560885  98.52097319
 99.41399349 80.74097538 79.77120162 96.56680596 96.29512311 79.5311948
 98.65376884 98.30776915 80.88229919 80.73244722 98.38695922 79.56530744
 80.85305978 96.65086926 94.0778012  79.11087828 98.01293844 80.12451115
 97.30875598 98.57457877 96.36213009 98.02024829 86.27209707 79.79434948
 79.42520193 90.13413579 97.11504489 76.58288764 80.01608168 92.05906361
 78.98173755 78.50172391 96.9627563  93.87434364 98.02877645 98.57336046
 85.75431586 88.02402505 87.14318783 98.55508583 98.99976852 79.16326555
 98.70615611 79.48002583 73.96108722 93.29199206 96.24273583 96.9067141
 89.79300934 97.17717864 90.64095223 79.5177934  98.42838172 80.02826476
 98.20786784 78.60284353 80.57772201 97.55972759 81.22342564 95.99054592
 80.28045467 95.45083515 78.70152654 82.67808628 85.56669631 80.11598299
 81.26362983 99.14718388]
Accuracy th:0.7 is [45.53185268 97.2137279  73.61752415 97.02489005 97.26733349 78.65523081
 78.95371645 77.70007675 79.79922272 96.4742145  80.2560885  98.52097319
 99.41399349 81.15763697 79.77120162 96.56680596 96.29512311 79.5311948
 98.65376884 98.30776915 81.27824953 80.73244722 98.38695922 79.56530744
 81.2867777  96.65086926 94.0778012  79.11087828 98.01293844 80.12451115
 97.30875598 98.57457877 96.36213009 98.02024829 86.48895603 79.79434948
 79.42520193 90.39119894 97.11504489 76.58288764 80.49731363 92.05906361
 78.98173755 78.50172391 96.9627563  93.87434364 98.02877645 98.57336046
 87.45629317 88.7842497  87.31740598 98.55508583 98.99976852 79.16326555
 98.70615611 79.48002583 73.96108722 93.66844946 96.24273583 96.9067141
 89.79300934 97.17717864 90.84075486 79.5177934  98.42838172 80.02826476
 98.20786784 78.60284353 80.57772201 97.55972759 81.22342564 95.99054592
 80.28045467 95.45083515 78.70152654 82.75483973 85.64588638 80.11598299
 81.26362983 99.14718388]
Avg Prec: is [56.06840037  3.19759101 11.16600416  3.25167665  2.24734132  3.74575737
  3.36457088  5.56129145  2.53348068  3.69823073  1.51395724  1.61675749
  0.63599269  4.95638427  2.61122082  3.12289527  3.58452214  2.61812629
  1.34298968  1.77445608  1.97464631  0.87321207  1.82970532  2.34536697
  5.20867936  3.59969258  6.60367374  3.35785356  2.13039982  1.88752046
  2.53403959  1.32668782  3.82189349  1.67655689  2.30626193  2.39676728
  3.08909645  2.61745719  3.00222066  7.36603564  2.2217868   8.27095863
  3.36393525  4.00850574  3.21633991  6.37425133  2.01601995  1.4857511
  2.12988036  1.56320122  1.82806305  1.5659422   1.07383554  3.05104362
  1.39701051  2.6797656  11.27496903  3.80697041  3.90516068  2.87290917
 10.75935272  2.26323139  3.8461959   2.95393683  1.64355231  2.49048981
  1.81642265  4.19864443  1.25437329  2.28629734  0.17409646  3.42300835
  1.84425847  4.66713619  3.95759265  3.06295551  0.91301304  1.91799695
  0.13014469  0.74592721]
mAP score regular 25.27, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [77.92809627 97.2220146  90.73672671 97.56085407 98.29583676 96.90061539
 97.53095647 94.80280041 97.49856741 96.51194658 98.5250517  98.5325261
 99.34972718 95.11672522 97.2070658  96.31262924 96.21047911 97.50604181
 98.77668984 98.34068316 98.34815756 99.17283305 99.40204799 98.04170715
 95.40573536 96.52938685 94.3618108  96.80593966 97.81747515 98.12641702
 98.05167302 98.54996637 96.55928445 98.35812343 98.36310636 98.05665595
 97.27931833 97.50105887 97.95699728 92.72990009 97.82744101 92.78969529
 96.97785086 96.38488178 96.83334579 93.68413185 98.18870369 98.7567581
 97.80252635 98.60477863 98.45279916 98.49266263 98.87385704 97.35406234
 98.6969629  97.58576874 89.68034482 96.54433565 96.13573511 96.78102499
 91.45676059 97.79256048 96.32259511 96.99279966 98.42539303 97.40638314
 98.24849889 95.7769639  98.62720183 97.54839674 99.81563146 95.93890924
 98.06413035 95.47798789 95.96631537 97.01273139 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [76.82935944 97.22450607 90.39041284 97.39890874 98.10399382 97.04013753
 97.39143434 94.90245908 97.40389167 96.43720258 98.5250517  98.5325261
 99.34972718 95.11423375 97.2070658  96.31262924 96.21047911 97.50604181
 98.78167277 98.34068316 98.25846476 99.14293545 99.40453945 97.92959115
 95.43563296 96.52938685 94.3543364  96.79348232 97.81747515 98.11146822
 97.89720208 98.64713357 96.59167352 98.24351596 98.28088796 98.33071729
 97.27931833 97.304233   97.75518848 92.74484889 97.82744101 92.54553155
 97.09744126 96.50696365 97.042629   94.12013853 98.18621222 98.7717069
 97.93457408 98.58982983 98.40047836 98.55993223 98.87385704 97.87477888
 98.6969629  97.58576874 89.72768269 96.40979645 96.16563271 96.78102499
 91.33717019 97.76266288 96.21546204 96.94047886 98.34815756 97.40638314
 98.19866956 95.7769639  98.71689464 97.69539328 99.81563146 96.52689538
 98.04170715 95.44559882 95.85419937 97.01023993 99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [88.92068478 15.53992198 50.3564121  50.0459716  50.17330022 45.61439135
 53.40265733 31.1839371  29.47863675 29.90641883  9.78134849 15.52369821
  4.14056275 16.40865474 12.11760537 24.80223554 14.00051024 14.60584308
 18.56983495 17.20443274 32.69344155 24.56291235 85.14836371 47.73085106
 15.98680377 14.17225845 23.34509069 20.95360278  8.59086493 21.26014748
 54.93135416 23.19833047 44.76905262 37.57872937 49.95288923 56.95798626
 23.05926082 59.56117187 68.68125928 30.9693886  19.06319121 40.20431307
 30.74201614 27.03081065 23.69297212 34.22901177 13.88584042  8.54448422
 22.84216068 20.46499132 41.53267074 16.1256283   9.1975944  59.16661013
  7.93130367 16.1753441  43.48100343 35.43771315 18.6553192  22.94888915
 53.33557279 60.17414043 40.66735202 29.75515673 34.81523281 21.4636612
 32.11904899 14.17936112 24.10200957 37.29524625  8.1118152  49.65095373
 30.47851398 29.90680611 32.804321   20.9145661   3.01967264 10.60293179
  0.97184405  5.07713198]
Accuracy th:0.5 is [45.34469442 97.22450607 72.41946334 96.96290206 97.90716795 78.10000747
 78.23703814 76.94396691 79.60485338 96.41976231 79.86396592 98.5325261
 99.34972718 79.13396617 79.68208885 96.31262924 96.21047911 79.1837955
 98.78167277 98.34068316 80.16543339 80.50427287 98.31327703 79.26850537
 79.22615043 96.52938685 94.3393876  79.13645763 97.81747515 79.81164512
 97.52597354 98.67204823 96.39983058 98.18870369 86.72546528 79.41301044
 79.32082617 91.84293794 97.0276802  76.2712709  79.33079204 92.37362035
 78.52106535 78.14983681 97.03764606 94.02795426 98.18621222 98.77668984
 86.10509007 87.90392904 85.73635299 98.55993223 98.87385704 78.58584349
 98.6969629  79.17382963 72.75331988 94.32693026 96.16314124 96.78102499
 90.13379176 97.04761193 90.48758004 79.08164536 98.32075143 79.92126965
 98.13139996 78.26195281 80.48184966 97.53593941 81.00007474 96.07843137
 80.16792486 95.44559882 78.16478561 83.80795774 87.37075516 79.84403418
 81.07481875 99.15040985]
Accuracy th:0.7 is [45.52657149 97.22450607 72.41946334 96.96290206 97.90716795 78.10000747
 78.23703814 76.96389865 79.60485338 96.41976231 79.86396592 98.5325261
 99.34972718 79.51516058 79.68208885 96.31262924 96.21047911 79.1837955
 98.78167277 98.34068316 80.46191793 80.50427287 98.31327703 79.27099684
 79.59737898 96.52938685 94.3393876  79.13645763 97.81747515 79.81164512
 97.52597354 98.67204823 96.39983058 98.18870369 87.01198395 79.41301044
 79.32082617 92.01983208 97.0276802  76.2712709  79.54505818 92.37362035
 78.52106535 78.14983681 97.03764606 94.02795426 98.18621222 98.77668984
 87.73450931 88.19044772 85.90577273 98.55993223 98.87385704 78.58584349
 98.6969629  79.17382963 72.75331988 94.5785684  96.16314124 96.78102499
 90.13379176 97.04761193 90.66447418 79.08164536 98.32075143 79.92126965
 98.13139996 78.26195281 80.48184966 97.53593941 81.00007474 96.07843137
 80.16792486 95.44559882 78.16478561 83.88270175 87.48287117 79.84403418
 81.07481875 99.15040985]
Avg Prec: is [53.26418978  3.68827985 14.94224891  4.53438794  1.46335559  4.62682593
 14.70448897  8.76764871  8.65290283  5.69264617  3.41970538  4.86398427
  2.55717865  5.74521612  3.00544438  3.72847653 14.25982203  6.54435618
  1.57606409  2.76031246  3.56870021  1.58265304  1.2211687   5.18580062
  5.49890504  7.51926467  7.76005219  4.71988238  3.91118284  4.12234671
  2.02405335  0.83498893  2.89082544  1.14054302  1.58803735  2.0893135
  1.9092691   2.18403355  2.15050944  6.21943024  1.67880115  6.02179533
  2.1536639   2.68467335  2.33880563  4.85736866  1.62214068  1.01980673
  1.40429286  1.14912374  1.18612152  0.97416513  0.7541258   2.24475231
  0.83868513  1.81826841  9.9102024   2.94772901  3.87784696  2.69367417
  7.80378405  2.11624168  3.27125945  2.52905964  1.35793342  1.94255719
  1.53160235  3.53117198  1.09575813  2.27655565  0.20071434  3.34665117
  1.6521989   3.95652573  3.17751186  2.30185459  0.56529622  1.42063296
  0.12059974  0.61601008]
mAP score regular 29.83, mAP score EMA 4.17
Train_data_mAP: current_mAP = 25.27, highest_mAP = 25.27
Val_data_mAP: current_mAP = 29.83, highest_mAP = 29.83
tensor([0.4194, 0.2531, 0.4738, 0.2766, 0.0834, 0.1526, 0.0613, 0.1256, 0.2130,
        0.1313, 0.0389, 0.0451, 0.0300, 0.1464, 0.2278, 0.1911, 0.2243, 0.1359,
        0.0701, 0.1915, 0.0248, 0.0119, 0.0430, 0.0394, 0.1300, 0.1373, 0.4003,
        0.0913, 0.0590, 0.0811, 0.0852, 0.0347, 0.2097, 0.0271, 0.1277, 0.0470,
        0.0743, 0.1280, 0.0832, 0.4792, 0.6194, 0.5916, 0.1989, 0.5091, 0.6569,
        0.6189, 0.0598, 0.0828, 0.0550, 0.1367, 0.0194, 0.0214, 0.0399, 0.1096,
        0.0564, 0.1168, 0.5887, 0.2057, 0.3355, 0.0508, 0.7329, 0.3127, 0.3930,
        0.2032, 0.1322, 0.1277, 0.2120, 0.1936, 0.3638, 0.3886, 0.1185, 0.2914,
        0.5929, 0.3951, 0.5194, 0.7126, 0.0770, 0.4210, 0.0240, 0.2601],
       device='cuda:0')
Sum Train Loss:  tensor([2.1413e+01, 4.5106e+00, 1.4691e+01, 5.5291e+00, 7.7942e-01, 1.4640e+00,
        1.0768e+00, 2.6791e+00, 4.4727e+00, 2.3112e+00, 4.0707e-01, 2.5528e-01,
        2.3124e-02, 2.6987e+00, 3.0293e+00, 7.7241e-01, 2.0456e+00, 1.7719e+00,
        2.4616e-01, 7.7135e-01, 1.8982e-01, 1.3027e-02, 1.4437e-01, 4.5531e-01,
        2.3844e+00, 3.4668e+00, 9.5641e+00, 2.3666e+00, 5.0582e-01, 2.8666e-01,
        1.2380e+00, 8.6081e-02, 2.9672e+00, 2.7393e-01, 1.6839e+00, 2.6759e-01,
        9.0276e-01, 2.1197e+00, 6.9949e-01, 1.0420e+01, 3.2520e+00, 1.2793e+01,
        8.9410e-01, 7.8783e+00, 4.1176e+00, 8.8663e+00, 5.5547e-01, 5.4289e-01,
        7.6677e-01, 7.3461e-01, 1.6948e-01, 5.5420e-02, 1.1663e-01, 1.2270e+00,
        9.6432e-01, 3.4514e+00, 1.7846e+01, 2.1822e+00, 7.6806e+00, 6.6242e-01,
        2.1029e+01, 1.5002e+00, 4.6347e+00, 8.1488e-01, 3.2558e-01, 1.8720e+00,
        6.6798e-01, 3.8197e+00, 8.9503e-01, 2.7592e+00, 2.3364e-02, 2.8110e+00,
        5.5267e+00, 5.1629e+00, 1.1581e+01, 6.5080e+00, 9.7251e-02, 2.1172e+00,
        2.5118e-03, 1.2917e+00], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [2/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 259.2
Sum Train Loss:  tensor([2.5134e+01, 4.4953e+00, 1.4797e+01, 4.5855e+00, 1.4493e+00, 1.7422e+00,
        3.5454e-01, 2.9483e+00, 1.8780e+00, 1.6416e+00, 3.9902e-01, 1.9197e-01,
        1.1884e-01, 4.6273e+00, 1.5123e+00, 4.3379e+00, 3.9740e+00, 1.2132e+00,
        4.6238e-01, 2.3221e+00, 1.8137e-01, 1.0941e-02, 1.1295e-01, 5.2807e-01,
        3.1475e+00, 1.9298e+00, 7.6378e+00, 1.9857e+00, 4.5032e-01, 1.7254e+00,
        8.1908e-01, 2.2337e-01, 1.4036e+00, 2.0740e-01, 1.6449e+00, 4.3941e-01,
        8.6082e-01, 2.0751e+00, 3.9336e-01, 1.2348e+01, 1.6899e+00, 1.2934e+01,
        3.8885e+00, 1.2426e+01, 9.4255e+00, 1.0236e+01, 4.3873e-01, 8.2325e-01,
        1.1519e+00, 1.6221e+00, 1.5087e-01, 4.3830e-01, 4.1959e-01, 6.4919e-01,
        5.2918e-01, 2.2088e+00, 1.8690e+01, 8.8397e-01, 5.3914e+00, 5.0032e-01,
        2.1532e+01, 3.6714e+00, 3.5541e+00, 2.4485e+00, 4.7494e-01, 1.5508e+00,
        1.7124e+00, 2.0096e+00, 4.2740e+00, 1.3759e+00, 2.0049e-02, 3.0892e+00,
        4.0493e+00, 5.6830e+00, 4.8943e+00, 7.6295e+00, 6.5642e-01, 5.3332e+00,
        1.9502e-03, 1.2821e+00], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [2/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 276.1
Sum Train Loss:  tensor([2.4960e+01, 2.9008e+00, 1.5425e+01, 4.9491e+00, 9.1431e-01, 1.4404e+00,
        9.5459e-01, 2.8229e+00, 3.9406e+00, 1.5359e+00, 1.7328e-01, 6.7908e-02,
        2.2783e-02, 2.9072e+00, 2.1614e+00, 3.3326e+00, 5.7253e+00, 5.3019e-01,
        3.6603e-01, 2.9513e-01, 2.3593e-01, 1.3989e-01, 2.9145e-01, 3.1426e-01,
        3.2705e+00, 2.2802e+00, 1.0339e+01, 1.2950e+00, 8.5738e-01, 7.4182e-01,
        7.8241e-01, 4.3947e-01, 3.6262e+00, 2.3636e-01, 2.1351e+00, 6.3556e-01,
        1.1450e+00, 4.3050e-01, 4.5826e-01, 1.2535e+01, 5.4258e+00, 1.7370e+01,
        2.3697e+00, 6.3269e+00, 1.9928e+00, 1.2103e+01, 3.8218e-01, 4.9911e-01,
        5.1860e-01, 7.5273e-01, 1.9102e-02, 7.4771e-02, 4.2883e-02, 7.0021e-01,
        4.4857e-01, 1.5108e+00, 2.3652e+01, 2.1527e+00, 7.3533e+00, 6.0440e-01,
        2.3101e+01, 7.3836e+00, 4.3568e+00, 2.2469e+00, 6.3739e-01, 1.3619e+00,
        1.8207e+00, 2.6785e+00, 3.7994e+00, 2.4900e+00, 4.1921e-02, 2.0041e+00,
        8.9416e+00, 5.5624e+00, 6.3510e+00, 6.1569e+00, 6.3536e-01, 2.2884e+00,
        1.0362e-02, 3.4482e+00], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [2/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 287.2
Sum Train Loss:  tensor([2.6155e+01, 4.4712e+00, 1.4793e+01, 6.3077e+00, 6.1094e-01, 1.7896e+00,
        3.8323e-01, 2.9985e+00, 2.8377e+00, 1.5868e+00, 5.1043e-01, 2.0405e-01,
        1.5952e-01, 3.5091e+00, 2.3484e+00, 7.9296e-01, 5.8593e+00, 1.7561e+00,
        1.4188e-01, 1.9237e+00, 3.9910e-01, 1.1626e-01, 2.5528e-01, 4.8587e-01,
        2.6579e+00, 1.2717e+00, 7.5630e+00, 4.5328e-01, 1.1375e+00, 8.2997e-01,
        1.0036e+00, 3.9450e-01, 2.9420e+00, 1.2186e-01, 2.5262e+00, 2.4955e-01,
        7.2211e-01, 1.1896e+00, 7.1786e-01, 1.3498e+01, 9.9854e+00, 1.2830e+01,
        4.0431e+00, 8.2434e+00, 7.1102e+00, 1.8134e+01, 4.9162e-01, 6.3079e-01,
        1.0699e-01, 7.0614e-01, 1.4612e-01, 1.8145e-01, 5.3401e-02, 6.2767e-01,
        9.7247e-01, 1.2805e+00, 1.9023e+01, 3.8184e+00, 8.0471e+00, 7.4205e-01,
        1.7954e+01, 1.7760e+00, 3.7952e+00, 2.7314e+00, 8.0845e-01, 7.2815e-01,
        1.3263e+00, 4.1275e+00, 5.6101e+00, 3.4684e+00, 2.7970e-02, 2.8008e+00,
        6.2806e+00, 1.2302e+01, 9.3478e+00, 1.2370e+01, 7.1384e-02, 5.2181e+00,
        4.6020e-03, 1.1823e+00], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [2/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 306.8
Sum Train Loss:  tensor([2.5428e+01, 2.7341e+00, 1.6766e+01, 2.8624e+00, 1.6212e+00, 2.5139e+00,
        1.2205e+00, 2.0413e+00, 1.3084e+00, 8.8739e-01, 8.9826e-01, 1.0375e-01,
        3.0756e-01, 4.4785e+00, 3.3641e+00, 7.4799e-01, 4.6019e+00, 2.1680e+00,
        4.2230e-01, 1.3203e+00, 3.2524e-01, 4.3182e-02, 1.8120e-01, 3.4718e-01,
        3.1666e+00, 3.5192e+00, 1.1307e+01, 7.0914e-01, 3.0670e-01, 1.0270e+00,
        7.9542e-01, 5.0506e-01, 2.9036e+00, 1.5613e-01, 7.5597e-01, 1.4918e-01,
        1.5704e+00, 7.5129e-01, 6.5506e-01, 1.7307e+01, 2.5306e+00, 2.1569e+01,
        1.0970e+00, 6.3541e+00, 6.9337e+00, 1.3112e+01, 4.3710e-01, 4.5417e-01,
        6.9306e-01, 2.1540e-01, 3.5846e-02, 3.8452e-02, 3.5878e-01, 6.5914e-01,
        4.0135e-01, 1.0507e+00, 1.8242e+01, 2.1564e+00, 6.0089e+00, 5.2807e-01,
        2.8878e+01, 3.3362e+00, 2.1724e+00, 1.4883e+00, 5.3500e-01, 1.2105e+00,
        8.9064e-01, 1.7018e+00, 1.5240e+00, 3.6816e+00, 1.6572e-02, 3.7163e+00,
        1.7528e+00, 6.1673e+00, 1.4433e+01, 4.6973e+00, 6.8810e-02, 3.6959e+00,
        1.0633e-01, 1.8262e+00], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [2/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 287.1
Sum Train Loss:  tensor([2.5096e+01, 5.0643e+00, 1.5186e+01, 2.5759e+00, 3.3158e-01, 1.4966e+00,
        1.1701e+00, 2.4219e+00, 4.5579e+00, 3.4317e+00, 9.6506e-02, 2.6851e-01,
        3.0047e-02, 3.5083e+00, 1.7837e+00, 5.2551e+00, 1.7066e+00, 2.7685e-01,
        1.0252e+00, 8.4785e-01, 6.5478e-02, 9.1237e-02, 2.1663e-01, 1.2184e-01,
        2.4640e+00, 1.1274e+00, 1.2381e+01, 1.7584e+00, 8.2322e-01, 2.8175e-01,
        7.6394e-01, 2.6581e-01, 2.1609e+00, 1.1037e-01, 8.0023e-01, 2.7184e-01,
        1.7128e+00, 1.3731e+00, 7.2595e-01, 1.8164e+01, 5.4772e+00, 1.8028e+01,
        2.0827e+00, 1.3980e+01, 1.3667e+01, 9.4176e+00, 4.6700e-01, 1.0371e+00,
        5.2762e-01, 2.5437e+00, 1.9835e-01, 1.9598e-01, 2.9878e-01, 1.3325e+00,
        2.9266e-01, 2.8983e+00, 2.2921e+01, 2.6092e+00, 5.4206e+00, 7.4802e-01,
        2.5235e+01, 3.4032e+00, 5.9186e+00, 3.9008e+00, 1.0619e+00, 3.2942e+00,
        2.0798e+00, 2.4209e+00, 2.3453e+00, 3.5590e+00, 1.0667e-02, 3.9077e+00,
        1.1926e+01, 6.6758e+00, 1.5580e+01, 9.4250e+00, 1.0047e-01, 3.1283e+00,
        3.1896e-03, 1.9208e+00], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [2/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 331.9
Sum Train Loss:  tensor([2.7796e+01, 3.3568e+00, 1.9125e+01, 4.9730e+00, 7.1686e-01, 1.9638e+00,
        5.9620e-01, 2.3293e+00, 1.6802e+00, 2.1492e+00, 3.3774e-01, 3.0660e-01,
        2.0869e-02, 3.6337e+00, 3.8900e+00, 2.8549e+00, 8.0040e+00, 7.4122e-01,
        3.3480e-01, 1.7217e+00, 1.9561e-01, 1.2208e-02, 1.9367e-02, 3.2666e-01,
        3.2139e+00, 1.5169e+00, 6.0373e+00, 1.0449e+00, 6.3119e-01, 3.5340e-01,
        6.7465e-01, 2.8140e-01, 9.0752e-01, 5.2178e-01, 1.2217e+00, 7.2898e-01,
        1.6161e+00, 5.8252e-01, 4.0258e-01, 1.1791e+01, 3.5026e+00, 7.6789e+00,
        9.3166e-01, 6.7385e+00, 6.7373e+00, 1.1168e+01, 3.1948e-01, 7.9512e-01,
        5.0190e-01, 5.5555e-01, 1.5183e-01, 5.6140e-02, 4.5668e-02, 4.5643e-01,
        2.0812e-01, 1.2702e+00, 2.3622e+01, 2.7141e+00, 7.6138e+00, 6.3408e-01,
        2.0357e+01, 3.1279e+00, 3.9560e+00, 4.0467e+00, 1.3087e+00, 1.2520e+00,
        3.4279e+00, 5.5171e+00, 3.6864e+00, 5.4373e+00, 8.2973e-01, 4.6843e+00,
        2.7420e+00, 4.6780e+00, 5.5680e+00, 1.3991e+01, 3.1901e-01, 7.9342e+00,
        3.2930e-03, 2.3426e+00], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [2/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 289.5
Sum_Val Meta Model:  tensor([2.7027e+01, 1.9707e+01, 4.5743e+01, 1.3494e+01, 2.1424e-01, 1.9478e+00,
        3.5878e-01, 2.4171e+00, 1.4574e+00, 1.8717e+00, 2.2264e-01, 5.4143e-01,
        1.7115e-01, 2.3005e+00, 1.3669e+00, 1.2680e+00, 1.0790e+02, 4.3792e-01,
        1.0975e-01, 5.1889e-01, 4.2476e-02, 6.2252e-03, 3.9118e-02, 4.5865e-02,
        3.8635e+00, 2.7196e+00, 1.0905e+01, 2.5574e-01, 5.3311e-01, 8.3296e-01,
        3.5429e-01, 5.9065e-02, 1.5723e+00, 5.4195e-02, 3.4961e-01, 1.4155e-01,
        5.0717e-01, 7.2626e-01, 2.2710e-01, 2.4440e+01, 6.8122e+00, 1.3599e+01,
        2.0475e+00, 8.0668e+00, 9.9717e+00, 1.4694e+01, 1.4016e-01, 4.2617e-01,
        8.0553e-02, 6.8764e-01, 1.4423e-02, 2.3822e-02, 2.7763e-01, 3.1698e-01,
        6.9114e-02, 3.0334e-01, 1.5435e+01, 1.7739e+00, 6.9059e+00, 2.8658e-01,
        9.2181e+00, 7.2218e+00, 2.3164e+00, 9.6316e-01, 1.4929e-01, 1.9659e-01,
        2.3933e-01, 1.8167e+00, 3.0104e+00, 8.4055e+00, 3.0065e-02, 4.2032e+00,
        8.7517e+00, 4.8191e+00, 6.4957e+00, 5.7774e+00, 6.3605e-02, 2.5751e+00,
        3.3715e-03, 2.3852e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([2.4033e+01, 1.6052e+01, 4.4794e+01, 9.6873e+00, 6.9625e-02, 1.7678e+00,
        3.8085e-01, 2.6777e+00, 1.4899e+00, 1.6066e+00, 2.2345e-01, 4.7740e-01,
        1.8704e-01, 2.2628e+00, 1.3254e+00, 2.3133e+00, 9.1197e+01, 5.7766e-01,
        4.9581e-02, 4.2443e-01, 4.4158e-02, 4.0518e-03, 9.7246e-03, 2.9645e-02,
        3.3873e+00, 2.5655e+00, 1.1349e+01, 4.8054e-01, 5.6141e-01, 8.3236e-01,
        1.5508e-01, 2.5700e-02, 1.4318e+00, 3.0049e-02, 3.3981e-01, 1.0350e-01,
        9.0886e-01, 2.7041e-01, 1.6320e-01, 2.1939e+01, 6.8966e+00, 1.3866e+01,
        1.9000e+00, 6.7072e+00, 8.3538e+00, 1.2238e+01, 1.4516e-01, 4.4902e-01,
        7.8838e-02, 5.3350e-01, 6.7157e-03, 1.6832e-02, 2.9513e-01, 2.0269e-01,
        7.6976e-02, 3.3806e-01, 1.6422e+01, 2.1914e+00, 6.4513e+00, 5.2181e-01,
        1.1286e+01, 3.5027e+00, 2.3101e+00, 1.2935e+00, 1.8677e-01, 5.2373e-01,
        3.3550e-01, 2.9351e+00, 3.0035e+00, 7.3481e+00, 4.3079e-02, 4.9063e+00,
        7.9992e+00, 5.6857e+00, 6.3006e+00, 6.2726e+00, 8.7923e-02, 2.7085e+00,
        5.8262e-03, 4.6164e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.7311e+01, 6.3416e+01, 9.4536e+01, 3.5028e+01, 8.3451e-01, 1.1588e+01,
        6.2100e+00, 2.1317e+01, 6.9950e+00, 1.2232e+01, 5.7469e+00, 1.0593e+01,
        6.2439e+00, 1.5454e+01, 5.8193e+00, 1.2105e+01, 4.0665e+02, 4.2494e+00,
        7.0751e-01, 2.2161e+00, 1.7825e+00, 3.3932e-01, 2.2621e-01, 7.5232e-01,
        2.6065e+01, 1.8691e+01, 2.8351e+01, 5.2622e+00, 9.5200e+00, 1.0261e+01,
        1.8197e+00, 7.4064e-01, 6.8260e+00, 1.1077e+00, 2.6611e+00, 2.2031e+00,
        1.2236e+01, 2.1130e+00, 1.9609e+00, 4.5784e+01, 1.1134e+01, 2.3437e+01,
        9.5502e+00, 1.3174e+01, 1.2718e+01, 1.9774e+01, 2.4274e+00, 5.4223e+00,
        1.4342e+00, 3.9013e+00, 3.4547e-01, 7.8786e-01, 7.3890e+00, 1.8499e+00,
        1.3644e+00, 2.8932e+00, 2.7897e+01, 1.0652e+01, 1.9230e+01, 1.0271e+01,
        1.5399e+01, 1.1200e+01, 5.8782e+00, 6.3653e+00, 1.4128e+00, 4.1012e+00,
        1.5828e+00, 1.5162e+01, 8.2560e+00, 1.8908e+01, 3.6344e-01, 1.6839e+01,
        1.3491e+01, 1.4391e+01, 1.2131e+01, 8.8024e+00, 1.1421e+00, 6.4330e+00,
        2.4234e-01, 1.7747e+00], device='cuda:0')
Outer loop valEpocw Maximum [2/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 425.2
model_train val_loss valEpocw [2/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 391.1
model_train val_loss valEpocw [2/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1307.5
Sum_Val Meta Model:  tensor([2.8188e+01, 3.6997e+00, 3.3448e+01, 2.5561e+00, 1.2769e-01, 5.6342e+00,
        8.9952e+00, 7.3992e+00, 8.2086e+00, 3.9416e+00, 6.9411e-01, 1.2718e+01,
        2.9529e-01, 8.5108e-01, 3.5739e+00, 4.3149e+00, 2.7378e+00, 2.2595e+00,
        8.0882e-01, 7.3123e+00, 1.9367e-02, 3.3719e-03, 5.3149e-02, 2.1566e-01,
        3.2298e+00, 2.9657e+00, 1.1478e+01, 1.4911e+00, 1.0015e+00, 4.0103e-02,
        6.0202e-02, 1.8711e-02, 1.6397e-01, 1.6626e-02, 5.7419e-02, 1.9099e-02,
        4.5936e-01, 8.3428e-02, 4.4295e-02, 1.4925e+00, 3.9090e-01, 2.7148e+00,
        1.3408e-01, 4.3285e-01, 5.7552e-01, 2.8028e+00, 1.2591e-01, 8.1180e-02,
        3.0450e-02, 7.4393e-01, 8.7741e-03, 1.3556e-02, 1.3197e-02, 4.5901e-02,
        3.6713e-02, 4.2040e-01, 8.0272e+00, 1.4419e+00, 2.7986e+00, 3.1471e-01,
        2.9056e+00, 2.6449e-01, 1.3172e+00, 6.5658e-01, 1.5880e-01, 2.1743e-01,
        2.9317e-01, 1.8136e+00, 2.6037e-01, 1.3821e+00, 2.2955e-02, 3.0537e-01,
        2.6521e+00, 2.5414e+00, 2.4402e+00, 6.8927e-01, 3.8708e-02, 6.3537e-01,
        3.6001e-03, 1.0472e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([2.4683e+01, 3.9780e+00, 2.6388e+01, 4.0012e+00, 2.1457e-01, 5.2189e+00,
        9.5017e+00, 6.1889e+00, 8.7900e+00, 3.5642e+00, 6.1702e-01, 8.7954e+00,
        2.9842e-01, 1.1666e+00, 3.6714e+00, 8.0369e-01, 2.5500e+00, 2.4080e+00,
        1.1409e-01, 4.2993e+00, 4.9568e-02, 4.4100e-03, 2.5479e-02, 1.5002e-01,
        3.1239e+00, 2.9970e+00, 1.0376e+01, 1.5880e+00, 8.8925e-01, 9.3002e-02,
        9.4795e-02, 1.8277e-02, 1.2503e-01, 3.6912e-02, 8.4113e-02, 1.7317e-02,
        6.7432e-01, 2.2903e-01, 3.4008e-02, 1.5661e+00, 4.3859e-01, 3.1596e+00,
        1.8676e-01, 9.8091e-01, 1.0001e+00, 2.1519e+00, 1.0793e-01, 9.8957e-02,
        4.6796e-02, 8.9299e-01, 5.9922e-03, 1.4616e-02, 3.3566e-02, 1.3829e-01,
        3.8804e-02, 3.4022e-01, 6.7533e+00, 8.5498e-01, 2.3176e+00, 1.0848e-01,
        4.8588e+00, 4.4552e-01, 6.9967e-01, 2.2445e-01, 6.3969e-02, 1.0838e-01,
        1.0847e-01, 1.8129e+00, 2.3455e-01, 1.3475e+00, 8.0763e-03, 6.1812e-01,
        2.2121e+00, 1.7584e+00, 3.3502e+00, 1.5842e+00, 4.4589e-02, 4.5376e-01,
        1.3553e-03, 8.0247e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.8312e+01, 1.8509e+01, 5.5567e+01, 1.5281e+01, 3.0428e+00, 3.5152e+01,
        9.2497e+01, 4.7585e+01, 3.8893e+01, 3.2360e+01, 1.6328e+01, 1.7221e+02,
        8.6463e+00, 1.0131e+01, 1.4935e+01, 3.6279e+00, 1.1503e+01, 1.6731e+01,
        1.2191e+00, 1.6845e+01, 1.8872e+00, 3.3876e-01, 5.1779e-01, 3.7446e+00,
        2.6158e+01, 2.1662e+01, 3.1481e+01, 1.5849e+01, 1.2878e+01, 1.2649e+00,
        1.2232e+00, 4.9696e-01, 7.0775e-01, 1.8069e+00, 8.3651e-01, 5.3883e-01,
        9.9206e+00, 2.0688e+00, 4.9167e-01, 4.5690e+00, 1.0535e+00, 6.8628e+00,
        1.3103e+00, 2.6310e+00, 1.8928e+00, 3.9340e+00, 1.6820e+00, 1.2304e+00,
        7.7980e-01, 6.6932e+00, 2.7523e-01, 6.3639e-01, 8.1376e-01, 1.3974e+00,
        6.7359e-01, 3.3017e+00, 1.2994e+01, 4.2167e+00, 7.8503e+00, 1.9049e+00,
        7.3278e+00, 2.2114e+00, 1.8575e+00, 1.2049e+00, 5.8934e-01, 8.9723e-01,
        6.2224e-01, 1.5638e+01, 7.9506e-01, 4.4604e+00, 8.3929e-02, 3.1215e+00,
        4.7050e+00, 4.9157e+00, 9.0881e+00, 2.1873e+00, 5.1344e-01, 9.4442e-01,
        5.0607e-02, 4.4727e-01], device='cuda:0')
Outer loop valEpocw Maximum [2/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 200.5
model_train val_loss valEpocw [2/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 179.1
model_train val_loss valEpocw [2/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 941.6
Sum_Val Meta Model:  tensor([3.0171e+01, 8.0996e-01, 5.3123e+00, 6.1279e-01, 1.2502e-01, 3.6302e-01,
        1.0375e-01, 1.4404e+00, 5.1397e-01, 3.4112e-01, 6.4537e-02, 6.6679e-02,
        2.1094e-02, 2.4029e+00, 5.4383e-01, 5.5821e-01, 1.1750e+00, 3.2132e-01,
        9.9988e-02, 3.4297e-01, 3.8682e-02, 3.6031e-02, 4.1709e-01, 1.0474e-01,
        8.8396e-01, 2.9068e-01, 6.2256e+00, 7.2863e-01, 1.0647e-01, 2.3788e-01,
        4.8242e-01, 3.4168e-01, 2.3258e+00, 1.1777e-02, 5.5527e-01, 1.8956e-01,
        1.5247e+00, 1.7558e+00, 4.3126e-02, 1.1356e+01, 1.6707e+01, 4.4448e+01,
        1.8226e+01, 4.3680e+01, 3.7109e+01, 4.8028e+01, 1.4164e+00, 1.6094e+00,
        5.4426e+00, 3.3510e+00, 1.3112e+00, 1.3794e+00, 3.9555e+00, 2.4484e+00,
        5.3352e+00, 1.4565e+01, 9.7476e+01, 2.4768e+00, 2.6497e+00, 1.2742e-01,
        7.7256e+01, 3.6073e-01, 4.1779e+00, 2.4371e+00, 1.5929e+00, 1.4648e-01,
        1.5426e+00, 1.5090e+00, 1.7376e+00, 6.4727e-01, 3.4273e-02, 1.8278e+00,
        2.4326e+00, 1.2120e+01, 7.1929e-01, 9.3501e+00, 1.0088e+00, 5.4857e-01,
        5.7336e-03, 1.3879e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.4787e+01, 1.4271e-01, 1.9254e+00, 2.0067e-01, 7.1676e-03, 2.0513e-02,
        6.0547e-03, 1.5562e+00, 2.3549e-02, 2.7340e-02, 8.8342e-03, 4.9996e-03,
        4.0938e-03, 2.3411e+00, 6.8448e-02, 5.1468e-01, 5.7651e-01, 5.8704e-02,
        1.3804e-02, 5.3752e-02, 5.4754e-03, 2.0892e-03, 1.2653e-03, 1.8911e-03,
        6.5146e-01, 1.3886e-01, 6.5619e+00, 6.5663e-01, 5.9662e-02, 2.5370e-02,
        1.2751e-01, 2.8170e-01, 1.7926e+00, 1.8399e-03, 2.7599e-01, 9.7256e-02,
        1.3713e+00, 1.9823e+00, 1.9899e-02, 1.4473e+01, 1.5313e+01, 3.8882e+01,
        1.6862e+01, 4.0467e+01, 2.6299e+01, 4.3097e+01, 8.6143e-01, 1.0162e+00,
        4.6122e+00, 2.0887e+00, 8.9397e-01, 1.4832e+00, 3.9353e+00, 2.8779e+00,
        4.5080e+00, 1.3172e+01, 5.7796e+01, 3.2446e+00, 2.6812e+00, 2.7481e-01,
        6.5535e+01, 4.4922e-01, 3.8598e+00, 2.3480e+00, 1.3751e+00, 6.1769e-01,
        1.4416e+00, 1.9245e+00, 1.6109e+00, 9.2348e-01, 2.2261e-02, 1.4346e+00,
        3.0514e+00, 1.2057e+01, 7.9683e-01, 8.1420e+00, 8.7347e-01, 8.8357e-01,
        6.0017e-03, 2.4424e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.2820e+01, 7.0191e-01, 4.5329e+00, 8.9006e-01, 1.0393e-01, 1.6254e-01,
        1.2018e-01, 1.5451e+01, 1.1821e-01, 2.4926e-01, 2.3553e-01, 1.1487e-01,
        1.3958e-01, 1.8936e+01, 3.3440e-01, 3.3708e+00, 2.7264e+00, 4.3292e-01,
        2.1402e-01, 3.1780e-01, 2.6890e-01, 1.1562e-01, 2.0835e-02, 4.7307e-02,
        5.8455e+00, 1.3651e+00, 2.6663e+01, 9.5665e+00, 1.1924e+00, 3.2343e-01,
        1.4914e+00, 6.4981e+00, 8.0628e+00, 8.6676e-02, 2.3758e+00, 2.1328e+00,
        1.5722e+01, 1.6245e+01, 2.8109e-01, 4.1333e+01, 3.0860e+01, 7.0200e+01,
        6.6803e+01, 8.3596e+01, 4.3383e+01, 6.6681e+01, 1.1362e+01, 1.0024e+01,
        4.2494e+01, 1.2045e+01, 2.5028e+01, 4.5058e+01, 6.9863e+01, 2.4515e+01,
        6.9484e+01, 9.6183e+01, 1.1147e+02, 1.8040e+01, 1.0412e+01, 5.1797e+00,
        8.5086e+01, 2.1789e+00, 1.1294e+01, 1.3821e+01, 1.3090e+01, 5.3519e+00,
        8.7561e+00, 1.6660e+01, 6.4012e+00, 3.3652e+00, 2.4140e-01, 7.6750e+00,
        7.1252e+00, 3.9432e+01, 2.4484e+00, 1.1788e+01, 1.0573e+01, 2.1509e+00,
        2.1693e-01, 1.3904e+00], device='cuda:0')
Outer loop valEpocw Maximum [2/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 544.4
model_train val_loss valEpocw [2/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 438.8
model_train val_loss valEpocw [2/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1393.3
Sum_Val Meta Model:  tensor([4.2875e+01, 3.7848e-01, 1.0495e+01, 2.1281e-01, 5.8534e-02, 1.5214e+00,
        3.2502e-01, 1.6803e+00, 3.1922e-01, 1.9194e+00, 1.9509e-01, 2.1190e-01,
        7.3291e-03, 1.9676e+00, 2.5889e+00, 2.7650e-01, 3.8486e-01, 1.1590e-01,
        3.6098e-02, 9.8933e-02, 1.5066e-02, 6.4173e-03, 2.2307e-02, 2.9565e-02,
        2.5066e+00, 2.0743e-01, 8.4708e+00, 1.5328e-01, 7.1952e-01, 3.0378e-02,
        5.6063e-02, 8.8385e-03, 7.9813e-01, 1.0071e-01, 4.7307e-01, 2.5960e-01,
        3.1164e-02, 2.0671e-01, 3.8681e-01, 7.2885e+00, 6.1155e+00, 1.2906e+01,
        1.3282e+00, 3.7861e+00, 3.2569e+00, 6.3765e+00, 4.2074e-02, 4.8882e-02,
        9.7667e-02, 7.0581e-02, 2.4019e-02, 1.7633e-02, 1.9888e-02, 6.0429e-01,
        4.0762e-02, 1.8663e-01, 1.3580e+01, 8.6185e-01, 4.7868e+00, 9.5741e-02,
        2.3169e+01, 2.2413e-01, 2.2952e+00, 1.5755e+00, 6.5973e-01, 2.6802e-01,
        1.2858e+00, 1.0525e+01, 3.0839e-01, 8.0176e-01, 1.3756e-02, 4.0480e-01,
        1.8024e+00, 3.9769e+00, 1.5267e+02, 1.2158e+01, 9.9608e-02, 6.4984e+00,
        3.1221e-03, 2.1018e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([2.0276e+01, 8.0596e-01, 9.5551e+00, 9.4852e-01, 1.1901e-01, 1.2165e+00,
        7.2542e-01, 1.7876e+00, 2.6425e-01, 1.9747e+00, 1.5471e-01, 3.4385e-01,
        2.2554e-02, 1.8865e+00, 3.0932e+00, 4.5693e-01, 3.0957e-01, 2.6899e-01,
        2.2892e-02, 1.2966e-01, 2.0927e-02, 1.8538e-03, 2.4157e-02, 7.2120e-02,
        2.4653e+00, 5.5054e-01, 7.6085e+00, 1.8603e-01, 7.6685e-01, 2.0258e-02,
        6.7552e-02, 1.7738e-02, 5.7264e-02, 1.4979e-02, 2.8127e-02, 9.0805e-03,
        2.9139e-01, 4.5106e-02, 2.5545e-02, 2.0548e+00, 5.6171e-01, 3.0381e+00,
        1.8732e-01, 1.2317e+00, 1.0321e+00, 1.5331e+00, 7.7838e-02, 4.0675e-02,
        2.0917e-02, 6.4383e-02, 5.5363e-03, 7.1620e-03, 1.4036e-02, 7.8397e-02,
        2.3240e-02, 1.1526e-01, 4.5411e+00, 3.2863e-01, 3.9632e+00, 9.5699e-02,
        6.8566e+00, 5.0968e-01, 9.8512e-01, 3.9487e-01, 1.5561e-01, 1.6661e-01,
        2.5351e-01, 1.7916e+00, 4.0677e-01, 7.0987e-01, 1.0703e-02, 7.9466e-01,
        8.9195e-01, 2.4169e+00, 7.8310e+01, 4.6137e+00, 7.1668e-02, 2.7482e+00,
        2.8216e-03, 8.0037e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.2082e+01, 4.3481e+00, 2.4295e+01, 4.5764e+00, 1.7658e+00, 9.9858e+00,
        1.5000e+01, 1.8999e+01, 1.3983e+00, 2.0630e+01, 4.5955e+00, 8.2501e+00,
        8.1889e-01, 1.5627e+01, 1.6889e+01, 3.4127e+00, 1.6722e+00, 2.2608e+00,
        3.8313e-01, 9.0968e-01, 1.0380e+00, 1.3023e-01, 6.2929e-01, 2.0383e+00,
        2.2947e+01, 5.5541e+00, 2.5752e+01, 2.6594e+00, 1.5685e+01, 3.2814e-01,
        1.0297e+00, 5.8837e-01, 2.6829e-01, 6.5862e-01, 2.5376e-01, 1.5716e-01,
        5.1837e+00, 4.5161e-01, 2.7436e-01, 5.5388e+00, 9.6013e-01, 5.5605e+00,
        1.0061e+00, 2.8110e+00, 1.8339e+00, 2.7995e+00, 1.6846e+00, 6.6198e-01,
        4.3550e-01, 6.7984e-01, 3.1935e-01, 3.2920e-01, 3.5865e-01, 7.7834e-01,
        5.1091e-01, 1.2478e+00, 8.2941e+00, 1.7283e+00, 1.2649e+01, 2.3248e+00,
        9.5700e+00, 2.6126e+00, 2.4918e+00, 1.8834e+00, 1.1118e+00, 1.4002e+00,
        1.1659e+00, 8.8732e+00, 1.5683e+00, 2.5002e+00, 1.2069e-01, 4.2585e+00,
        1.7677e+00, 7.0323e+00, 1.9897e+02, 5.5289e+00, 6.8825e-01, 4.3499e+00,
        9.8529e-02, 4.0506e-01], device='cuda:0')
Outer loop valEpocw Maximum [2/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 360.6
model_train val_loss valEpocw [2/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 177.8
model_train val_loss valEpocw [2/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 602.4
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [78.59431537 97.21007298 90.74572678 97.50977693 97.81800904 97.05900269
 97.53170649 94.76980056 97.48662906 96.52903839 98.53193796 98.55630414
 99.41399349 95.31682119 97.26977011 96.58751721 96.29634142 97.47322767
 98.66473362 98.31020577 98.22370585 99.18616976 99.22637395 97.99710043
 95.1791523  96.65086926 94.08145612 96.75564382 98.01293844 98.16522703
 97.80095272 98.50635348 96.50832714 98.14938902 98.20786784 98.26756497
 96.96762954 97.93983991 98.00928351 92.80345025 97.83750198 92.51105615
 96.91036903 96.17694716 96.9067141  94.10338568 98.01902998 98.579452
 97.98979057 98.5368112  98.50026194 98.56239568 98.99976852 97.93862161
 98.70615611 97.45738965 90.03301617 96.34629208 96.24273583 96.87138315
 91.68747944 97.81800904 96.37553149 97.02245343 98.50757179 97.35383341
 98.28340298 95.95399666 98.69884626 97.76684007 99.81603538 96.5582778
 98.05923417 95.52149706 96.44253847 97.0175802  99.18007822 98.1603538
 99.84405648 99.14718388]
Accuracy th:0.7 is [76.59994396 97.21494621 90.02570631 97.41840377 97.60115008 96.86894653
 97.30631937 94.79416674 97.44886149 96.49370744 98.53193796 98.52706473
 99.41399349 95.31682119 97.26977011 96.56924258 96.29512311 97.48053752
 98.65376884 98.30776915 98.17131858 99.18616976 99.18129652 97.87161462
 95.21935649 96.65086926 94.08511105 96.75198889 98.01293844 98.15791718
 97.73150912 98.56970553 96.42182722 98.07629049 98.01659337 98.08603696
 96.96519292 97.90207234 97.86186815 92.73035173 97.84481183 92.43308439
 96.93108027 96.28050341 96.97737601 94.0229773  98.03121307 98.57336046
 97.99588212 98.53193796 98.42960003 98.55508583 98.99976852 97.82166397
 98.70615611 97.46591781 89.53472789 96.21836966 96.24029922 96.91158733
 91.03933919 97.79364286 96.26222877 97.00296049 98.46127606 97.34408694
 98.24929034 95.95277835 98.68057163 97.65719229 99.81603538 96.48396097
 97.98126241 95.51175059 96.35360193 96.95422814 99.18007822 98.1603538
 99.84405648 99.14718388]
Avg Prec: is [89.87916707 17.79423308 50.53062497 44.67882667 52.86977951 47.90239163
 50.70240642 33.18906595 26.57955516 32.01648842  7.84521226 20.77974656
  4.12462221 15.42342296 13.89384137 25.18442581 12.88828874 14.42133994
 22.04620951 18.56886948 28.58083095 20.62989858 77.61396037 47.13466301
 17.47153041 12.96639351 24.8463515  19.78444569  7.56469059 22.16862116
 51.01297039 19.29601449 42.54315125 36.83066938 50.55068702 58.77469318
 23.66467447 58.82349785 63.14327068 32.9729303  18.33230625 42.70065699
 34.19829695 31.19872935 26.77335926 38.8977298  16.62223012 14.55234535
 22.47785546 20.08492952 40.10163426 20.58813965  7.95855181 55.72991208
  7.46176401 17.33803891 45.48843381 37.46856049 19.94606052 23.56621539
 56.95176479 57.40677037 40.9685507  26.3724609  31.06618885 23.70285982
 29.54250412 14.45065544 28.07616058 39.41400477  4.8652276  52.3908625
 34.15137159 30.24898837 30.89129307 27.70757923  3.59100914 12.69975088
  2.19820295  7.86217249]
Accuracy th:0.5 is [45.30524726 97.2137279  73.57731996 97.02489005 97.26733349 78.69787161
 79.04265299 77.6927669  79.89059587 96.4474117  80.28898283 98.52097319
 99.41399349 80.70077119 79.83089875 96.56680596 96.29512311 79.58845531
 98.65376884 98.30776915 80.90910198 80.83113023 98.38695922 79.69566648
 80.74219369 96.65086926 94.0778012  79.26560349 98.01293844 80.14034917
 97.30875598 98.57457877 96.36213009 98.02024829 86.78622336 79.80287765
 79.47271598 90.56419878 97.11504489 76.55973977 80.07943373 92.05906361
 79.06823747 78.65157588 96.9627563  93.87434364 98.02877645 98.57336046
 86.94825843 87.89853925 86.96896968 98.55508583 98.99976852 79.29606121
 98.70615611 79.61282148 73.97448861 93.44306234 96.24273583 96.9067141
 89.79300934 97.17717864 90.55201569 79.53850465 98.42838172 79.95882116
 98.20786784 78.64548434 80.70077119 97.55972759 81.29530586 95.99054592
 80.37670106 95.45083515 78.80508278 82.99606486 85.99310437 80.25365188
 81.34038328 99.14718388]
Accuracy th:0.7 is [45.32717681 97.2137279  73.57731996 97.02489005 97.26733349 78.69787161
 79.04265299 77.78048513 79.89059587 96.47177788 80.28898283 98.52097319
 99.41399349 81.09062999 79.83089875 96.56680596 96.29512311 79.58845531
 98.65376884 98.30776915 81.32454527 80.83113023 98.38695922 79.70175802
 81.16129189 96.65086926 94.0778012  79.26560349 98.01293844 80.14034917
 97.30875598 98.57457877 96.36213009 98.02024829 87.01282879 79.80287765
 79.47271598 90.82126192 97.11504489 76.55973977 80.56919385 92.05906361
 79.06823747 78.65157588 96.9627563  93.87434364 98.02877645 98.57336046
 88.84029191 88.5466795  87.11516673 98.55508583 98.99976852 79.29606121
 98.70615611 79.61282148 73.97448861 93.83779437 96.24273583 96.9067141
 89.79300934 97.17717864 90.75181833 79.53850465 98.42838172 79.95882116
 98.20786784 78.64548434 80.70077119 97.55972759 81.29530586 95.99054592
 80.37670106 95.45083515 78.80508278 83.07403662 86.10275216 80.25365188
 81.34038328 99.14718388]
Avg Prec: is [55.70103188  3.00483572 11.04625723  3.39389946  2.25179657  3.59052203
  3.26626647  5.5639482   2.49206769  3.70503939  1.61093883  1.66309456
  0.59440914  5.03669019  2.63124988  3.05879464  3.6355403   2.63514424
  1.34766193  1.77909446  2.03446527  0.86265766  1.7906235   2.47210661
  5.13827047  3.65142082  6.54666307  3.34350239  2.06727129  1.96070075
  2.50274858  1.27340915  3.66598356  1.69764228  2.37335573  2.41271155
  3.05583038  2.63081116  2.80092738  7.54041302  2.36605856  8.28528242
  3.33494479  4.08613242  3.26433615  6.43645589  2.0558343   1.52635367
  2.14836719  1.53521524  1.7947795   1.53204618  1.08460562  2.96048088
  1.41348076  2.72676193 11.27861376  3.75321951  3.96318107  2.80158118
 11.02783781  2.2185712   3.85168364  2.94420035  1.62391013  2.55874879
  1.80811141  4.21096994  1.24490801  2.43579036  0.2033509   3.43142948
  1.98864418  4.63923587  4.01100507  3.1397644   0.85792104  1.83661859
  0.14751467  0.72471357]
mAP score regular 30.17, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [81.19939208 97.20955727 91.22007126 97.72279941 98.47522236 97.19460847
 97.70535914 94.73303934 97.46866981 96.51942098 98.5250517  98.5997957
 99.34972718 95.11423375 97.21204873 96.30017191 96.21546204 97.47863567
 98.78914717 98.29832823 98.39798689 99.13546105 99.48675785 98.17624636
 95.38580362 96.52938685 94.23972893 96.83583726 97.81747515 98.14884022
 98.31825996 98.57488103 96.64648579 98.38303809 98.52006876 98.61225303
 97.17218527 97.90218502 98.26593916 92.93669183 97.84488128 92.67259636
 97.06754366 96.14570097 96.92553006 94.13259586 98.14634876 98.73931784
 97.95450582 98.63965917 98.59730423 98.54996637 98.87385704 98.07409622
 98.6969629  97.58327728 90.08396243 96.68884072 96.12576924 96.78351646
 92.09457608 98.02426689 96.60662232 97.06006926 98.5250517  97.41884047
 98.33570023 95.77945537 98.73184344 97.85235568 99.81563146 96.62157112
 98.14884022 95.54276603 96.26529138 97.11488153 99.24757705 98.19617809
 99.82559733 99.15040985]
Accuracy th:0.7 is [80.5765254  97.22699753 90.41781897 97.66051274 98.35064903 96.94297033
 97.43628074 94.94232255 97.39890874 96.45215138 98.5250517  98.55993223
 99.34972718 95.11423375 97.2070658  96.32757805 96.21047911 97.50604181
 98.78416424 98.34317463 98.36559783 99.15040985 99.40453945 98.02426689
 95.43563296 96.52938685 94.4066572  96.79597379 97.81747515 98.12143409
 98.14385729 98.65460797 96.50945512 98.25597329 98.36808929 98.4876797
 97.30921594 97.87228742 98.04419862 92.75232329 97.82744101 92.97157236
 97.0949498  96.49699778 97.03764606 94.27710093 98.17873782 98.7642325
 97.96198022 98.63467623 98.4503077  98.55993223 98.87385704 98.03423275
 98.6969629  97.58576874 89.70276802 96.55928445 96.16314124 96.83334579
 91.7034158  98.05914742 96.41228791 96.98781673 98.40296983 97.40638314
 98.21860129 95.7769639  98.7268605  97.71283355 99.81563146 96.86822632
 98.05416449 95.50041109 96.17809004 97.0351546  99.24757705 98.19617809
 99.82559733 99.15040985]
Avg Prec: is [91.76776203 17.30109816 55.49498143 56.2409215  57.70720794 50.07489568
 59.98041013 33.6710657  33.25450162 34.14314837 12.37831268 26.91870937
  4.71102491 17.46520702 16.93486744 30.25423028 15.12589377 16.50429356
 23.88559699 19.39675443 39.44171461 30.52384068 86.04068422 54.9085444
 17.88609218 14.40545549 24.95710194 25.30283509 10.08169366 25.35939844
 62.1372759  24.52195402 47.36451239 43.11027391 57.20573341 66.19046225
 25.40840418 65.65335549 75.35495381 35.70459035 21.21044898 45.00450271
 34.37141969 30.20504181 28.22943924 39.53054114 15.39756178 11.56684465
 26.01509739 25.60699605 47.38707725 20.06020468 10.10394899 64.15582177
 10.14872073 19.07303426 46.26275644 39.19658077 20.81960139 29.17229247
 58.5484763  68.70918261 47.43877612 34.30057896 42.08761676 23.73217388
 38.4405996  15.92877915 29.69292476 46.11201218  8.63955913 57.74482146
 36.46685111 31.99388743 42.36348076 30.91572494  3.09528111 16.0213461
  2.64660866  8.06398463]
Accuracy th:0.5 is [45.34718589 97.22450607 72.30485587 96.96290206 97.90716795 77.9704512
 78.10748187 76.81441064 79.46533124 96.41976231 79.72444378 98.5325261
 99.34972718 79.06918803 79.54256671 96.31262924 96.21047911 79.04427336
 98.78167277 98.34068316 80.08321499 80.36973366 98.31327703 79.13396617
 79.1464235  96.52938685 94.3393876  79.0268331  97.81747515 79.67710591
 97.52597354 98.67204823 96.39983058 98.18870369 86.96215462 79.28345417
 79.18130403 91.82051474 97.0276802  76.15168049 79.1987443  92.37362035
 78.39150908 78.01529761 97.03764606 94.02795426 98.18621222 98.77668984
 87.28604529 87.88648878 85.71392979 98.55993223 98.87385704 78.44632135
 98.6969629  79.0492563  72.65366121 94.32194733 96.16314124 96.78102499
 90.13379176 97.04761193 90.56730697 78.95707203 98.32075143 79.79669632
 98.13139996 78.12741361 80.34731046 97.53593941 80.86055261 96.07843137
 80.03338565 95.44559882 78.05017814 83.78055161 87.45546503 79.70451205
 80.94027954 99.15040985]
Accuracy th:0.7 is [45.53902883 97.22450607 72.30485587 96.96290206 97.90716795 77.9704512
 78.10748187 76.83932531 79.46533124 96.41976231 79.72444378 98.5325261
 99.34972718 79.45287391 79.54256671 96.31262924 96.21047911 79.04427336
 98.78167277 98.34068316 80.39215686 80.36973366 98.31327703 79.13645763
 79.52512644 96.52938685 94.3393876  79.0268331  97.81747515 79.67710591
 97.52597354 98.67204823 96.39983058 98.18870369 87.20880983 79.28345417
 79.18130403 92.01734061 97.0276802  76.15168049 79.45038244 92.37362035
 78.39150908 78.01529761 97.03764606 94.02795426 98.18621222 98.77668984
 89.11478187 88.19293918 85.88085806 98.55993223 98.87385704 78.44632135
 98.6969629  79.0492563  72.65366121 94.57358547 96.16314124 96.78102499
 90.13379176 97.04761193 90.72676084 78.95707203 98.32075143 79.79669632
 98.13139996 78.12741361 80.34731046 97.53593941 80.86055261 96.07843137
 80.03338565 95.44559882 78.05017814 83.87024441 87.5775469  79.70451205
 80.94027954 99.15040985]
Avg Prec: is [53.27665578  3.68796605 14.93869361  4.53929061  1.46303092  4.60647146
 14.65992049  8.76409765  8.61808821  5.68048679  3.43301439  5.00838005
  2.52906473  5.72832206  3.00323996  3.73580194 14.60434808  6.53642557
  1.57430499  2.76932785  3.5433815   1.57979555  1.23419233  5.15625802
  5.50751558  7.59142451  7.74736702  4.68130684  3.88942759  4.19974558
  2.03623055  0.8339264   2.87779098  1.13976419  1.65950152  2.10237367
  1.913962    2.11931562  2.18964367  6.27587912  1.69212719  6.02298196
  2.14857238  2.68828497  2.34040433  4.84987781  1.63270481  1.02217385
  1.40147339  1.16388625  1.18739053  0.97455364  0.73245099  2.25451882
  0.84329416  1.81636932  9.93592043  2.95333003  3.82798088  2.69992078
  7.80679274  2.10630371  3.09752685  2.52402199  1.33422648  1.94034112
  1.53197557  3.51672713  1.10200435  2.26827318  0.20099095  3.34446489
  1.6518615   3.95786227  3.18804144  2.30115693  0.55483191  1.43420464
  0.11917701  0.61151581]
mAP score regular 34.14, mAP score EMA 4.18
Train_data_mAP: current_mAP = 30.17, highest_mAP = 30.17
Val_data_mAP: current_mAP = 34.14, highest_mAP = 34.14
tensor([0.3436, 0.1768, 0.3823, 0.1888, 0.0634, 0.1135, 0.0441, 0.0873, 0.1782,
        0.0907, 0.0302, 0.0375, 0.0245, 0.1116, 0.1792, 0.1285, 0.1763, 0.1125,
        0.0538, 0.1363, 0.0180, 0.0124, 0.0349, 0.0322, 0.0969, 0.0924, 0.2860,
        0.0650, 0.0436, 0.0575, 0.0610, 0.0268, 0.1804, 0.0122, 0.0829, 0.0393,
        0.0527, 0.0892, 0.0730, 0.3636, 0.5677, 0.5593, 0.2051, 0.4632, 0.6493,
        0.5932, 0.0437, 0.0589, 0.0447, 0.0922, 0.0150, 0.0202, 0.0358, 0.0829,
        0.0415, 0.0853, 0.5358, 0.1714, 0.2856, 0.0379, 0.7324, 0.1897, 0.3579,
        0.1640, 0.1032, 0.1040, 0.1654, 0.1475, 0.3253, 0.3238, 0.0850, 0.1850,
        0.5739, 0.3251, 0.5558, 0.8285, 0.1781, 0.6160, 0.0315, 0.1785],
       device='cuda:0')
Sum Train Loss:  tensor([2.3050e+01, 2.3912e+00, 8.9323e+00, 2.8271e+00, 4.7305e-01, 1.1623e+00,
        2.2588e-01, 1.3620e+00, 6.4754e-01, 5.4113e-01, 1.9982e-01, 9.3526e-02,
        2.0154e-02, 3.5227e+00, 3.2741e+00, 2.2691e+00, 3.1865e+00, 3.0929e+00,
        1.8631e-01, 1.4479e+00, 2.1293e-01, 7.7367e-02, 6.9787e-02, 2.8383e-01,
        2.9939e+00, 1.7907e+00, 5.6921e+00, 1.0498e+00, 5.0693e-01, 6.0073e-01,
        5.6135e-01, 2.7010e-01, 2.1464e+00, 7.9618e-02, 5.2450e-01, 3.8371e-01,
        8.4362e-01, 6.4390e-01, 6.2830e-01, 1.0908e+01, 2.1738e+00, 1.0193e+01,
        7.7742e-01, 6.6179e+00, 9.5161e+00, 1.3554e+01, 2.2463e-01, 3.4611e-01,
        5.1753e-01, 4.9436e-01, 4.4237e-02, 1.0930e-01, 4.5645e-01, 2.6737e-01,
        1.7855e-01, 4.8577e-01, 1.9326e+01, 2.8951e+00, 5.7751e+00, 4.6610e-01,
        1.2987e+01, 8.7023e-01, 3.4561e+00, 2.3559e+00, 2.1858e-01, 5.6869e-01,
        1.8217e+00, 2.4394e+00, 1.6548e+00, 1.1876e+00, 1.9847e-02, 2.1886e+00,
        3.5886e+00, 4.8516e+00, 6.9583e+00, 1.2250e+01, 1.6861e+00, 2.9214e+00,
        6.6024e-03, 7.4493e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [3/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 226.4
Sum Train Loss:  tensor([2.0745e+01, 1.6291e+00, 7.4832e+00, 1.5925e+00, 6.6711e-01, 1.1588e+00,
        4.7811e-01, 2.0869e+00, 1.7909e+00, 1.3410e+00, 3.3035e-01, 2.6310e-01,
        2.5870e-02, 1.4428e+00, 3.6998e+00, 1.8556e+00, 2.5816e+00, 3.1951e-01,
        1.7979e-01, 1.2662e+00, 1.5661e-01, 1.1222e-01, 2.1251e-02, 1.1959e-01,
        2.1574e+00, 8.5575e-01, 7.0999e+00, 1.1897e+00, 2.5522e-01, 2.2374e-01,
        8.8222e-02, 1.2575e-01, 3.0168e+00, 8.6543e-02, 4.2230e-01, 3.0598e-01,
        3.9630e-01, 9.6665e-01, 5.3727e-01, 1.1939e+01, 1.6523e+00, 1.3524e+01,
        2.3841e+00, 5.6295e+00, 9.4343e+00, 1.4491e+01, 5.5296e-01, 1.2473e+00,
        3.5514e-01, 8.4800e-01, 5.7638e-02, 2.0372e-01, 1.3425e-01, 6.4885e-01,
        3.5692e-01, 1.3664e+00, 1.6072e+01, 2.6123e+00, 4.5318e+00, 8.0281e-01,
        1.8193e+01, 1.2887e+00, 6.0261e+00, 1.1610e+00, 7.2259e-01, 1.3561e+00,
        1.3012e+00, 2.1200e+00, 5.9683e-01, 2.9197e+00, 2.4852e-02, 1.6644e+00,
        3.5527e+00, 4.5096e+00, 1.2361e+01, 1.1260e+01, 4.0402e+00, 1.0092e+01,
        1.0916e-02, 7.2974e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [3/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 241.9
Sum Train Loss:  tensor([1.7767e+01, 2.2433e+00, 1.0924e+01, 3.0941e+00, 1.2006e+00, 1.0986e+00,
        3.0413e-01, 1.3903e+00, 5.0700e-01, 1.2830e+00, 3.4713e-01, 3.5781e-01,
        9.9604e-02, 2.4565e+00, 4.0234e+00, 7.8188e-01, 3.4689e+00, 1.3532e+00,
        3.2440e-01, 2.2635e+00, 1.1022e-01, 3.5801e-02, 1.2931e-01, 4.5422e-01,
        1.9489e+00, 1.1844e+00, 6.4122e+00, 3.4398e-01, 7.4547e-01, 2.0170e-01,
        3.3853e-01, 2.0606e-01, 2.0700e+00, 7.3215e-02, 1.7026e+00, 7.8074e-01,
        7.8475e-01, 4.9818e-01, 6.9959e-01, 7.9889e+00, 3.2647e+00, 2.0109e+01,
        1.7013e+00, 6.4927e+00, 4.9161e+00, 9.6191e+00, 5.7694e-01, 4.3160e-01,
        2.9575e-01, 1.0464e+00, 4.0657e-02, 3.8589e-02, 1.7732e-01, 3.2095e-01,
        5.7834e-01, 1.4716e+00, 1.9595e+01, 2.3970e+00, 3.3005e+00, 5.6671e-01,
        2.1151e+01, 1.7280e+00, 3.4419e+00, 2.8467e+00, 5.7186e-01, 1.2122e+00,
        1.3366e+00, 3.6275e+00, 7.3419e-01, 5.0542e+00, 2.1405e-02, 1.5622e+00,
        5.1134e+00, 7.7061e+00, 6.7266e+00, 1.4923e+01, 9.0370e-01, 5.8846e+00,
        2.3077e-03, 2.4570e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [3/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 243.7
Sum Train Loss:  tensor([1.9372e+01, 3.0048e+00, 1.1039e+01, 1.0978e+00, 4.0964e-01, 1.5924e+00,
        6.5024e-01, 1.6964e+00, 1.4511e+00, 9.8553e-01, 2.1555e-01, 5.2588e-01,
        1.5460e-02, 2.2163e+00, 1.8997e+00, 9.4667e-01, 4.6976e+00, 1.2480e+00,
        3.3689e-01, 3.1562e-01, 6.9479e-02, 6.3881e-02, 1.7200e-02, 3.2841e-01,
        3.0154e+00, 1.3361e+00, 2.8151e+00, 1.1561e+00, 9.4317e-01, 5.8654e-01,
        2.3881e-01, 1.0699e-01, 4.2363e+00, 1.4988e-01, 6.8211e-01, 3.0208e-01,
        4.4000e-01, 7.1866e-01, 3.9892e-01, 9.4828e+00, 5.9269e+00, 1.8578e+01,
        3.3047e+00, 1.0836e+01, 3.2454e+00, 8.4055e+00, 2.5232e-01, 3.3570e-01,
        3.0191e-01, 6.2417e-01, 7.1333e-02, 1.7632e-01, 1.3235e-01, 3.2161e-01,
        4.9041e-01, 7.4754e-01, 1.8143e+01, 1.0068e+00, 4.1263e+00, 4.2473e-01,
        1.9214e+01, 6.7329e-01, 6.0754e+00, 1.7114e+00, 4.1670e-01, 8.4600e-01,
        1.5830e+00, 1.3142e+00, 4.5884e+00, 4.4620e+00, 1.7863e-02, 2.3153e+00,
        6.0857e+00, 4.4000e+00, 1.3298e+01, 1.2040e+01, 1.8467e-01, 7.9751e+00,
        5.3815e-03, 2.0161e+00], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [3/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 247.5
Sum Train Loss:  tensor([1.8681e+01, 1.7295e+00, 1.1805e+01, 2.4208e+00, 3.1637e-01, 2.1081e+00,
        8.4298e-01, 2.5022e+00, 8.3615e-01, 1.7552e+00, 3.3297e-01, 1.7572e-01,
        2.5981e-02, 2.2526e+00, 3.2128e+00, 8.0857e-01, 2.6851e+00, 1.2693e+00,
        5.5059e-01, 2.2390e+00, 1.3134e-01, 6.8436e-02, 1.6977e-02, 1.9130e-01,
        1.7501e+00, 1.2682e+00, 7.0435e+00, 8.0200e-01, 3.7767e-01, 3.9325e-01,
        6.7459e-01, 3.6875e-01, 1.3151e+00, 2.6919e-02, 4.1530e-01, 1.7963e-01,
        4.0543e-01, 4.8908e-01, 5.1141e-01, 6.7429e+00, 7.0028e+00, 1.1868e+01,
        2.4307e+00, 3.9347e+00, 5.0034e+00, 1.1962e+01, 3.4933e-01, 5.8644e-01,
        2.3365e-01, 7.4953e-01, 6.1758e-02, 2.1854e-01, 5.9851e-01, 3.3169e-01,
        6.6804e-01, 8.9896e-01, 2.2179e+01, 3.0027e+00, 5.7423e+00, 4.4749e-01,
        2.0663e+01, 1.2955e+00, 8.7710e+00, 5.5986e-01, 5.9933e-01, 1.4826e+00,
        9.5811e-01, 3.2348e+00, 2.3260e+00, 1.6310e+00, 2.1173e-02, 2.0073e+00,
        4.7993e+00, 2.5054e+00, 1.0596e+01, 9.3213e+00, 8.2411e-01, 1.9096e+00,
        1.2152e-02, 7.9782e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [3/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 232.3
Sum Train Loss:  tensor([1.9553e+01, 2.0984e+00, 9.2902e+00, 3.2824e+00, 3.8118e-01, 1.8361e+00,
        2.0332e-01, 1.7703e+00, 3.6107e+00, 9.9969e-01, 1.3714e-01, 1.5198e-01,
        1.4261e-01, 3.0396e+00, 2.6722e+00, 1.5312e+00, 3.3286e+00, 6.8382e-01,
        2.5542e-01, 1.1009e+00, 1.3984e-01, 1.0755e-01, 1.5085e-02, 6.1658e-01,
        1.9844e+00, 9.6694e-01, 4.9251e+00, 6.3407e-01, 9.3185e-02, 7.3360e-01,
        4.3535e-01, 1.4819e-01, 2.0510e+00, 1.2662e-01, 3.6417e-01, 1.1336e-01,
        7.8505e-01, 2.7858e-01, 9.4366e-02, 1.0732e+01, 8.8197e+00, 1.9489e+01,
        1.7716e+00, 3.9558e+00, 6.2085e+00, 1.0736e+01, 6.9680e-01, 4.7805e-01,
        6.5331e-01, 3.7466e-01, 3.9100e-02, 5.5389e-02, 3.7201e-01, 2.9290e-01,
        5.2449e-01, 5.5139e-01, 1.2409e+01, 1.5932e+00, 5.2955e+00, 9.6815e-02,
        2.1627e+01, 8.4980e-01, 1.6535e+00, 1.1233e+00, 1.2665e+00, 9.0204e-01,
        1.1510e+00, 3.8296e+00, 4.1698e-01, 2.7564e+00, 9.0636e-03, 1.9911e+00,
        9.0421e-01, 5.9892e+00, 8.9409e+00, 1.2406e+01, 1.7992e-01, 1.5427e+01,
        6.4221e-03, 1.2897e+00], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [3/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 238.5
Sum Train Loss:  tensor([1.7658e+01, 2.5374e+00, 1.2054e+01, 3.6708e+00, 6.9526e-01, 1.4474e+00,
        3.0003e-01, 1.9774e+00, 1.7780e+00, 1.7442e+00, 4.9948e-01, 4.5417e-01,
        9.1683e-02, 2.4769e+00, 3.4919e+00, 1.0163e+00, 3.1266e+00, 3.3225e+00,
        1.8804e-01, 8.4116e-01, 3.4086e-02, 9.4023e-03, 1.3869e-01, 2.7729e-01,
        1.7695e+00, 2.0792e+00, 5.5469e+00, 1.0018e+00, 3.8479e-01, 3.7491e-01,
        2.5312e-01, 9.9617e-02, 1.9350e+00, 1.7216e-01, 3.3993e-01, 3.1418e-01,
        6.4413e-01, 1.2919e+00, 1.1065e+00, 1.0014e+01, 3.0907e+00, 8.6486e+00,
        1.9632e+00, 4.2407e+00, 6.5644e+00, 6.7536e+00, 2.1302e-01, 1.1659e+00,
        1.6898e-01, 5.3660e-01, 6.1794e-02, 9.8217e-02, 2.2761e-01, 1.4787e-01,
        6.9744e-02, 8.2379e-01, 1.9430e+01, 1.9549e+00, 3.8786e+00, 2.8095e-01,
        1.7273e+01, 2.7056e+00, 1.9724e+00, 8.8996e-01, 1.8093e-01, 8.4954e-01,
        3.7626e-01, 3.4982e+00, 7.8747e-01, 7.8356e-01, 3.1004e-01, 1.8615e+00,
        4.4358e+00, 5.3469e+00, 8.5631e+00, 1.1231e+01, 7.6929e-01, 6.2659e+00,
        6.6543e-03, 7.1557e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [3/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 216.3
Sum_Val Meta Model:  tensor([2.2010e+01, 1.4850e+01, 3.6236e+01, 9.5417e+00, 1.5037e-01, 1.5420e+00,
        2.7492e-01, 1.6976e+00, 1.1717e+00, 1.2785e+00, 1.7871e-01, 4.4679e-01,
        1.4547e-01, 1.7830e+00, 1.4276e+00, 7.7543e-01, 8.3241e+01, 2.6819e-01,
        6.5235e-02, 3.1379e-01, 3.2672e-02, 7.1885e-03, 3.9293e-02, 5.6714e-02,
        2.9207e+00, 1.7673e+00, 7.9562e+00, 2.1466e-01, 3.9264e-01, 6.1803e-01,
        1.7675e-01, 4.2649e-02, 1.5203e+00, 1.8681e-02, 1.3892e-01, 6.1416e-02,
        2.2665e-01, 4.9847e-01, 2.0285e-01, 1.7027e+01, 6.1106e+00, 1.3565e+01,
        1.9026e+00, 7.3468e+00, 1.0565e+01, 1.2149e+01, 1.0636e-01, 3.4900e-01,
        5.5165e-02, 4.6099e-01, 1.3209e-02, 1.9242e-02, 3.1375e-01, 2.1577e-01,
        3.8200e-02, 2.2290e-01, 1.4753e+01, 1.2820e+00, 5.6504e+00, 1.8897e-01,
        1.1050e+01, 4.6126e+00, 2.1292e+00, 7.3258e-01, 9.9640e-02, 2.4839e-01,
        2.3945e-01, 1.0772e+00, 3.1273e+00, 6.6379e+00, 2.2726e-02, 2.7835e+00,
        7.8935e+00, 4.2550e+00, 7.1641e+00, 6.4545e+00, 1.9682e-01, 4.1148e+00,
        5.7966e-03, 2.1331e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([2.0650e+01, 1.2312e+01, 3.0352e+01, 7.0179e+00, 5.5016e-02, 1.5044e+00,
        2.5699e-01, 1.6199e+00, 1.1774e+00, 1.0871e+00, 2.2008e-01, 4.2832e-01,
        1.5210e-01, 1.7182e+00, 1.3408e+00, 1.9026e+00, 6.3095e+01, 4.2748e-01,
        5.0231e-02, 3.4645e-01, 6.0575e-02, 1.0222e-02, 1.2680e-02, 6.1827e-02,
        2.6741e+00, 1.6806e+00, 7.3911e+00, 3.2361e-01, 4.9392e-01, 5.9889e-01,
        8.7621e-02, 2.3877e-02, 1.3837e+00, 1.4562e-02, 1.4345e-01, 5.8403e-02,
        4.2976e-01, 2.1872e-01, 8.2356e-02, 1.6888e+01, 6.3613e+00, 1.4785e+01,
        1.8584e+00, 6.0679e+00, 8.9358e+00, 1.0588e+01, 8.4071e-02, 3.1980e-01,
        3.0491e-02, 3.4455e-01, 4.3888e-03, 1.1041e-02, 3.1095e-01, 6.7518e-02,
        2.5904e-02, 1.2605e-01, 1.5211e+01, 1.9053e+00, 4.8232e+00, 3.7646e-01,
        1.0283e+01, 2.3524e+00, 1.8303e+00, 8.9033e-01, 5.6596e-02, 3.6036e-01,
        1.7341e-01, 1.2084e+00, 3.3060e+00, 5.0659e+00, 4.2503e-02, 2.5606e+00,
        6.9380e+00, 4.1388e+00, 6.3945e+00, 5.5728e+00, 1.9698e-01, 4.2825e+00,
        7.9973e-03, 3.1044e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.0107e+01, 6.9651e+01, 7.9383e+01, 3.7180e+01, 8.6714e-01, 1.3257e+01,
        5.8242e+00, 1.8562e+01, 6.6077e+00, 1.1981e+01, 7.2948e+00, 1.1422e+01,
        6.2027e+00, 1.5391e+01, 7.4800e+00, 1.4811e+01, 3.5789e+02, 3.8007e+00,
        9.3386e-01, 2.5416e+00, 3.3578e+00, 8.2224e-01, 3.6349e-01, 1.9183e+00,
        2.7592e+01, 1.8192e+01, 2.5845e+01, 4.9818e+00, 1.1325e+01, 1.0414e+01,
        1.4372e+00, 8.9005e-01, 7.6691e+00, 1.1978e+00, 1.7308e+00, 1.4868e+00,
        8.1480e+00, 2.4529e+00, 1.1277e+00, 4.6443e+01, 1.1205e+01, 2.6436e+01,
        9.0629e+00, 1.3099e+01, 1.3761e+01, 1.7848e+01, 1.9247e+00, 5.4305e+00,
        6.8254e-01, 3.7369e+00, 2.9220e-01, 5.4583e-01, 8.6779e+00, 8.1454e-01,
        6.2361e-01, 1.4769e+00, 2.8387e+01, 1.1115e+01, 1.6890e+01, 9.9215e+00,
        1.4041e+01, 1.2401e+01, 5.1137e+00, 5.4296e+00, 5.4851e-01, 3.4649e+00,
        1.0484e+00, 8.1924e+00, 1.0162e+01, 1.5643e+01, 4.9985e-01, 1.3840e+01,
        1.2088e+01, 1.2732e+01, 1.1504e+01, 6.7262e+00, 1.1059e+00, 6.9519e+00,
        2.5367e-01, 1.7393e+00], device='cuda:0')
Outer loop valEpocw Maximum [3/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 349.7
model_train val_loss valEpocw [3/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 306.6
model_train val_loss valEpocw [3/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1234.0
Sum_Val Meta Model:  tensor([2.4287e+01, 2.9153e+00, 2.9463e+01, 1.7235e+00, 7.2058e-02, 4.9912e+00,
        7.0458e+00, 6.6435e+00, 6.9625e+00, 3.4557e+00, 6.5834e-01, 1.3060e+01,
        3.3548e-01, 6.8758e-01, 2.9636e+00, 3.1112e+00, 2.1148e+00, 2.0974e+00,
        5.4837e-01, 5.1447e+00, 1.3533e-02, 3.0960e-03, 3.4406e-02, 1.8491e-01,
        2.6306e+00, 2.3895e+00, 8.6670e+00, 1.2966e+00, 8.6133e-01, 2.3304e-02,
        4.0542e-02, 1.3597e-02, 1.0326e-01, 9.3349e-03, 2.7567e-02, 1.2068e-02,
        4.1806e-01, 5.3963e-02, 2.7601e-02, 8.7752e-01, 2.8957e-01, 2.6643e+00,
        1.2896e-01, 3.3752e-01, 4.9964e-01, 2.3123e+00, 8.6000e-02, 5.5131e-02,
        2.2049e-02, 5.4122e-01, 6.4931e-03, 1.1957e-02, 1.0631e-02, 2.9874e-02,
        2.6053e-02, 3.8387e-01, 7.4907e+00, 1.3047e+00, 2.3875e+00, 1.7925e-01,
        2.5842e+00, 1.2141e-01, 1.0789e+00, 4.9376e-01, 1.0786e-01, 1.6584e-01,
        1.9652e-01, 1.7983e+00, 1.8030e-01, 1.2274e+00, 1.8346e-02, 1.7663e-01,
        2.5554e+00, 2.0757e+00, 2.4532e+00, 6.6198e-01, 7.4533e-02, 7.1161e-01,
        5.6308e-03, 6.3247e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([2.4638e+01, 2.9472e+00, 2.0314e+01, 2.0436e+00, 2.3847e-01, 4.7105e+00,
        5.4999e+00, 5.1373e+00, 6.7075e+00, 2.8223e+00, 6.0026e-01, 7.5459e+00,
        2.9539e-01, 9.9115e-01, 2.7894e+00, 4.6658e-01, 2.0255e+00, 1.9737e+00,
        1.1216e-01, 2.8484e+00, 6.9354e-02, 9.8335e-03, 5.3005e-02, 1.8875e-01,
        2.5701e+00, 2.4578e+00, 8.6517e+00, 1.3897e+00, 7.7028e-01, 8.9384e-02,
        6.1353e-02, 1.5747e-02, 1.4220e-01, 2.6209e-02, 3.2244e-02, 1.2249e-02,
        3.1765e-01, 4.2273e-01, 3.0285e-02, 9.4788e-01, 1.7639e-01, 1.5830e+00,
        1.1391e-01, 2.7709e-01, 3.7428e-01, 1.2655e+00, 5.1848e-02, 7.6861e-02,
        1.7419e-02, 4.6173e-01, 2.7781e-03, 7.0204e-03, 1.4046e-02, 8.0094e-02,
        1.2513e-02, 2.5661e-01, 5.5074e+00, 7.0972e-01, 2.1175e+00, 5.5630e-02,
        2.5128e+00, 1.3475e-01, 2.8536e-01, 5.4633e-02, 1.7097e-02, 4.5623e-02,
        4.6677e-02, 1.9355e+00, 8.2475e-02, 1.2625e+00, 3.0775e-03, 1.6067e-01,
        1.7276e+00, 1.1521e+00, 3.4093e+00, 4.8569e-01, 8.5474e-02, 3.6113e-01,
        1.7088e-03, 5.7218e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([7.6156e+01, 1.6739e+01, 5.0684e+01, 1.0242e+01, 3.7905e+00, 3.6901e+01,
        6.6392e+01, 4.5243e+01, 3.5017e+01, 3.1458e+01, 1.6825e+01, 1.3838e+02,
        8.4291e+00, 9.6697e+00, 1.4540e+01, 2.8008e+00, 1.1081e+01, 1.4854e+01,
        1.3475e+00, 1.4585e+01, 2.6900e+00, 5.6095e-01, 1.0626e+00, 5.0544e+00,
        2.4946e+01, 2.1848e+01, 3.2063e+01, 1.6056e+01, 1.2334e+01, 1.2977e+00,
        8.5949e-01, 4.3123e-01, 8.0480e-01, 1.6429e+00, 3.9199e-01, 3.0057e-01,
        5.0596e+00, 4.3564e+00, 3.8898e-01, 3.4554e+00, 4.2398e-01, 3.2877e+00,
        6.4106e-01, 7.1657e-01, 6.8269e-01, 2.3780e+00, 9.2452e-01, 1.1138e+00,
        2.8311e-01, 4.3570e+00, 1.2254e-01, 2.6407e-01, 3.0209e-01, 8.7618e-01,
        2.2842e-01, 2.6814e+00, 1.1715e+01, 3.9176e+00, 8.0150e+00, 1.0744e+00,
        3.6761e+00, 8.8719e-01, 8.4080e-01, 3.1391e-01, 1.6247e-01, 4.0475e-01,
        2.9911e-01, 1.6317e+01, 2.8903e-01, 4.7389e+00, 3.3227e-02, 1.0584e+00,
        3.7057e+00, 3.6842e+00, 7.9030e+00, 5.8925e-01, 4.0841e-01, 5.1996e-01,
        4.0263e-02, 3.5398e-01], device='cuda:0')
Outer loop valEpocw Maximum [3/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 171.5
model_train val_loss valEpocw [3/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 139.9
model_train val_loss valEpocw [3/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 831.0
Sum_Val Meta Model:  tensor([2.5776e+01, 6.3472e-01, 4.2020e+00, 4.4701e-01, 7.0898e-02, 2.7054e-01,
        6.9555e-02, 1.0816e+00, 3.8492e-01, 2.6161e-01, 4.9427e-02, 5.2341e-02,
        1.4198e-02, 1.8253e+00, 3.4078e-01, 3.5923e-01, 8.9658e-01, 2.3519e-01,
        7.7846e-02, 2.4149e-01, 2.3892e-02, 2.8875e-02, 3.4223e-01, 8.6988e-02,
        6.6172e-01, 1.8423e-01, 5.4598e+00, 4.7037e-01, 6.4784e-02, 1.7118e-01,
        3.5786e-01, 3.0675e-01, 2.0012e+00, 5.5090e-03, 3.0864e-01, 1.5114e-01,
        1.0995e+00, 1.3834e+00, 3.2368e-02, 9.2538e+00, 1.5961e+01, 4.3990e+01,
        1.9756e+01, 4.2814e+01, 3.7396e+01, 4.5203e+01, 1.0366e+00, 1.1658e+00,
        4.3229e+00, 2.2210e+00, 9.4441e-01, 1.3799e+00, 3.6702e+00, 1.9221e+00,
        4.2595e+00, 1.1655e+01, 8.5004e+01, 2.2496e+00, 2.2800e+00, 9.1511e-02,
        7.4135e+01, 2.3442e-01, 3.7624e+00, 2.4282e+00, 1.5439e+00, 1.0370e-01,
        1.4439e+00, 1.3951e+00, 1.5547e+00, 5.4658e-01, 2.8576e-02, 1.3665e+00,
        2.3587e+00, 1.0910e+01, 8.2861e-01, 1.1719e+01, 1.9099e+00, 7.0015e-01,
        7.7621e-03, 9.5731e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.2082e+01, 8.5971e-02, 1.6323e+00, 7.7361e-02, 8.2187e-03, 2.3910e-02,
        7.9557e-03, 1.3402e+00, 4.3430e-02, 1.9016e-02, 1.6365e-02, 3.4137e-03,
        2.7250e-03, 1.6079e+00, 8.6532e-02, 3.7364e-01, 5.6664e-01, 7.2348e-02,
        2.2513e-02, 6.9364e-02, 1.1454e-02, 3.6634e-03, 1.7651e-03, 4.1127e-03,
        4.8401e-01, 1.1692e-01, 5.1848e+00, 5.1213e-01, 8.8090e-02, 3.0984e-02,
        5.6870e-02, 2.2311e-01, 1.4557e+00, 1.5829e-03, 8.6838e-02, 5.8800e-02,
        9.7323e-01, 1.3502e+00, 2.2861e-02, 1.1482e+01, 1.3768e+01, 4.0740e+01,
        1.8973e+01, 3.9050e+01, 2.7350e+01, 4.2281e+01, 6.1715e-01, 7.4236e-01,
        3.3865e+00, 1.3354e+00, 6.2426e-01, 1.4737e+00, 3.9108e+00, 2.6487e+00,
        4.0814e+00, 1.1009e+01, 5.3100e+01, 2.9650e+00, 2.3252e+00, 2.2278e-01,
        7.3019e+01, 2.3056e-01, 3.8227e+00, 2.1001e+00, 1.2972e+00, 5.3783e-01,
        1.4102e+00, 1.3631e+00, 1.4232e+00, 1.2724e+00, 1.6698e-02, 9.7297e-01,
        3.7469e+00, 1.0962e+01, 1.6335e+00, 1.1160e+01, 1.7583e+00, 1.1149e+00,
        1.0787e-02, 3.2489e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.0569e+01, 5.4343e-01, 4.6614e+00, 4.5496e-01, 1.5184e-01, 2.3799e-01,
        2.0702e-01, 1.6905e+01, 2.7699e-01, 2.3619e-01, 5.6196e-01, 9.0489e-02,
        1.1448e-01, 1.6246e+01, 5.6242e-01, 3.1911e+00, 3.3568e+00, 6.4359e-01,
        4.0267e-01, 5.3973e-01, 7.3234e-01, 1.9828e-01, 3.4356e-02, 1.2498e-01,
        5.5161e+00, 1.6414e+00, 2.5165e+01, 9.5640e+00, 2.1700e+00, 4.9373e-01,
        8.6206e-01, 6.4748e+00, 7.3278e+00, 1.2005e-01, 1.0483e+00, 1.2402e+00,
        1.4906e+01, 1.4232e+01, 3.2932e-01, 4.0571e+01, 2.8869e+01, 7.0322e+01,
        6.8765e+01, 7.8921e+01, 4.3462e+01, 6.7396e+01, 1.0998e+01, 9.6274e+00,
        3.9841e+01, 1.1258e+01, 2.4039e+01, 4.7705e+01, 7.6458e+01, 2.8822e+01,
        7.8851e+01, 1.0170e+02, 1.1361e+02, 1.9251e+01, 1.0196e+01, 5.1606e+00,
        9.3163e+01, 1.6355e+00, 1.2516e+01, 1.3867e+01, 1.4385e+01, 5.5051e+00,
        9.8358e+00, 1.3319e+01, 5.8577e+00, 5.3200e+00, 2.1719e-01, 7.0688e+00,
        8.7469e+00, 3.9714e+01, 4.1960e+00, 1.3616e+01, 1.0519e+01, 1.7461e+00,
        3.1594e-01, 2.1996e+00], device='cuda:0')
Outer loop valEpocw Maximum [3/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 504.1
model_train val_loss valEpocw [3/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 429.1
model_train val_loss valEpocw [3/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1421.7
Sum_Val Meta Model:  tensor([3.5148e+01, 2.8273e-01, 8.1236e+00, 1.6242e-01, 3.9221e-02, 1.0078e+00,
        2.2873e-01, 1.2824e+00, 2.2770e-01, 1.2263e+00, 1.4111e-01, 1.5727e-01,
        5.2068e-03, 1.5057e+00, 2.2205e+00, 1.9833e-01, 2.8330e-01, 8.1641e-02,
        2.6435e-02, 6.5481e-02, 9.3893e-03, 5.3501e-03, 1.6394e-02, 2.4184e-02,
        1.8173e+00, 1.2433e-01, 6.5878e+00, 9.8859e-02, 5.3927e-01, 2.0610e-02,
        3.4377e-02, 5.6246e-03, 6.2682e-01, 4.9616e-02, 2.5873e-01, 2.4586e-01,
        2.0573e-02, 1.3951e-01, 3.1342e-01, 6.4588e+00, 5.5287e+00, 1.2651e+01,
        1.3106e+00, 3.5244e+00, 3.4072e+00, 6.1572e+00, 2.9495e-02, 3.7265e-02,
        7.6633e-02, 4.8679e-02, 1.7518e-02, 1.5484e-02, 1.4486e-02, 5.2490e-01,
        2.7481e-02, 1.3904e-01, 1.2048e+01, 6.7016e-01, 4.1470e+00, 6.9948e-02,
        2.5172e+01, 1.3867e-01, 1.8659e+00, 1.2244e+00, 5.1698e-01, 2.0045e-01,
        9.8918e-01, 7.5927e+00, 2.9524e-01, 6.9887e-01, 8.2708e-03, 2.4430e-01,
        1.8632e+00, 3.4215e+00, 1.7909e+02, 1.2031e+01, 1.6119e-01, 7.3438e+00,
        3.1521e-03, 1.6708e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.4289e+01, 3.3297e-01, 7.7884e+00, 3.0924e-01, 5.3430e-02, 9.2084e-01,
        6.1584e-01, 9.8297e-01, 2.6847e-01, 1.3951e+00, 1.1001e-01, 2.4148e-01,
        1.1122e-02, 1.5540e+00, 2.6313e+00, 2.6101e-01, 2.1216e-01, 1.1535e-01,
        1.6462e-02, 9.2171e-02, 2.0294e-02, 1.9239e-03, 3.9863e-02, 2.0093e-01,
        2.0463e+00, 1.8606e-01, 6.6869e+00, 8.8898e-02, 5.7533e-01, 1.3709e-02,
        2.4507e-02, 8.4424e-03, 7.3440e-02, 7.0976e-03, 1.4260e-02, 6.4086e-03,
        8.7191e-02, 3.7649e-02, 2.2257e-02, 1.2506e+00, 2.4446e-01, 8.5349e-01,
        8.7212e-02, 2.9781e-01, 4.2206e-01, 1.1463e+00, 2.8733e-02, 2.7006e-02,
        6.2029e-03, 2.3961e-02, 1.7738e-03, 2.6907e-03, 3.9160e-03, 2.5421e-02,
        5.1792e-03, 4.5787e-02, 3.0783e+00, 1.8842e-01, 3.1404e+00, 5.2289e-02,
        5.4071e+00, 1.3083e-01, 3.7490e-01, 7.9818e-02, 3.6271e-02, 7.4200e-02,
        9.6073e-02, 1.1296e+00, 1.3468e-01, 3.9686e-01, 3.1116e-03, 1.5347e-01,
        5.3824e-01, 1.6999e+00, 5.7662e+01, 1.9883e+00, 1.1322e-01, 3.0033e+00,
        2.1987e-03, 3.4681e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.3276e+01, 2.3988e+00, 2.4598e+01, 2.0882e+00, 1.0724e+00, 1.0549e+01,
        1.8037e+01, 1.4345e+01, 1.9623e+00, 2.1826e+01, 4.7842e+00, 7.6922e+00,
        5.6327e-01, 1.6936e+01, 1.9516e+01, 2.5592e+00, 1.4752e+00, 1.2745e+00,
        3.4438e-01, 8.9777e-01, 1.5269e+00, 1.4875e-01, 1.3234e+00, 7.5478e+00,
        2.6742e+01, 2.9304e+00, 2.9041e+01, 1.8273e+00, 1.5888e+01, 2.9199e-01,
        5.1421e-01, 3.9710e-01, 3.8749e-01, 6.2702e-01, 1.8664e-01, 1.0975e-01,
        2.1710e+00, 5.1583e-01, 2.7695e-01, 4.1203e+00, 4.5901e-01, 1.5319e+00,
        4.5269e-01, 7.0623e-01, 7.1933e-01, 2.1361e+00, 8.8198e-01, 5.8307e-01,
        1.6254e-01, 3.8380e-01, 1.4505e-01, 1.4787e-01, 1.3218e-01, 3.2448e-01,
        1.5221e-01, 6.3620e-01, 6.1768e+00, 1.2636e+00, 1.1475e+01, 1.6529e+00,
        7.3373e+00, 9.8902e-01, 1.0729e+00, 4.6460e-01, 3.2342e-01, 7.9046e-01,
        5.3932e-01, 7.0149e+00, 5.4333e-01, 1.6475e+00, 4.8010e-02, 1.1725e+00,
        1.0832e+00, 5.5942e+00, 1.3019e+02, 2.2065e+00, 5.9223e-01, 3.8016e+00,
        7.2412e-02, 2.1282e-01], device='cuda:0')
Outer loop valEpocw Maximum [3/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 363.3
model_train val_loss valEpocw [3/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 126.3
model_train val_loss valEpocw [3/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 488.6
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [80.04166616 97.2137279  91.26716292 97.60967824 97.96420609 97.19179835
 97.74978375 94.90137791 97.6035867  96.62284816 98.53193796 98.67082516
 99.41399349 95.31925781 97.28926305 96.67889036 96.29634142 97.47931921
 98.68057163 98.30533254 98.31873393 99.1922613  99.38475408 98.32117055
 95.20351848 96.65086926 94.15577296 96.77879168 98.01293844 98.16279041
 97.93009344 98.51609995 96.67158051 98.17862843 98.34797334 98.44421973
 97.04316468 97.99588212 98.34431842 92.92528112 97.86064985 92.58659129
 96.99565064 96.30121465 97.02367174 94.24958273 98.02633984 98.54168443
 97.99588212 98.57214215 98.54046612 98.55630414 98.99976852 97.98979057
 98.70615611 97.45617134 90.48500871 96.37309487 96.24029922 97.00052387
 91.95916229 97.9934455  96.52051023 97.08093225 98.57092384 97.37210804
 98.33822687 95.95156004 98.72686736 97.88867095 99.81603538 96.81655925
 98.13476931 95.56413786 96.71909455 97.13453783 99.18007822 98.18106505
 99.84405648 99.14596557]
Accuracy th:0.7 is [74.71887526 97.2137279  90.46429746 97.47200936 97.78633301 97.02001681
 97.67059368 94.75761748 97.49272061 96.50954545 98.53193796 98.61112803
 99.41399349 95.31682119 97.28195319 96.59726368 96.29512311 97.48053752
 98.65864207 98.31142408 98.27731144 99.18616976 99.3311485  98.19081152
 95.21935649 96.65086926 94.11556877 96.75198889 98.01293844 98.16279041
 97.78633301 98.579452   96.56071442 98.08847358 98.18593828 98.29436776
 96.97128446 98.03974123 98.18959321 92.7681193  97.84359352 92.17236632
 96.9201155  96.22689782 96.9761577  94.12775185 98.03974123 98.57336046
 97.99588212 98.579452   98.43447326 98.55508583 98.99976852 97.96664271
 98.70615611 97.46591781 89.98672043 96.36822163 96.26953863 96.94082674
 91.17944469 97.82166397 96.33167237 97.02245343 98.51731826 97.34530525
 98.31142408 95.95277835 98.6903181  97.78998794 99.81603538 96.46203141
 98.08116373 95.46423655 96.63503125 97.06631254 99.18007822 98.17131858
 99.84405648 99.14718388]
Avg Prec: is [91.89357525 18.80877416 56.53613616 51.88957036 59.78149227 51.98888998
 56.79739687 37.37695936 34.30208768 36.98277021 11.22220455 32.75850213
  8.04548415 17.00280293 16.74031553 31.53372966 15.76510744 18.99674794
 28.04989163 22.84275112 36.98448366 27.20619735 82.42501553 56.7282064
 19.42843074 14.90698547 28.50534948 23.02549812 10.59290536 25.07192369
 57.15492247 24.80503772 46.59667134 41.12830384 56.15967554 64.58460125
 31.53096251 62.70406787 71.86187583 35.55615322 20.56080887 46.13456339
 37.01514305 34.28992039 31.9066207  43.63722265 18.39326585 17.7313789
 25.51245996 27.27105967 45.30992257 21.89776116  8.80170277 58.12651865
  9.26884399 18.99849573 49.7038036  40.91012472 23.85604783 32.31662387
 60.27423279 64.52889561 46.5517937  32.71427287 38.88091011 27.01844252
 35.67256076 15.42758016 32.01584978 45.59951892  5.44590763 56.67128136
 40.13599124 32.58657213 39.07683053 36.13258322  5.70642309 20.10960367
  1.91591649  9.5241318 ]
Accuracy th:0.5 is [45.40758519 97.2137279  73.57001011 97.02489005 97.26733349 78.49563236
 78.86477991 77.55875294 79.7273425  96.432792   80.14765902 98.52097319
 99.41399349 80.79701758 79.6432792  96.56680596 96.29512311 79.44713149
 98.65376884 98.30776915 80.96392588 80.63863744 98.38695922 79.52023002
 80.95783433 96.65086926 94.0778012  79.06092762 98.01293844 79.98927888
 97.30875598 98.57457877 96.36213009 98.02024829 86.60591367 79.60551163
 79.34844848 90.21210755 97.11504489 76.46471169 80.02339153 92.05906361
 78.8635616  78.42740707 96.9627563  93.87434364 98.02877645 98.57336046
 88.19702489 88.11905313 87.17608216 98.55508583 98.99976852 79.04021637
 98.70615611 79.4349484  73.7771226  93.2700625  96.24273583 96.9067141
 89.79300934 97.17717864 90.91872662 79.33870201 98.42838172 79.91739867
 98.20786784 78.5431464  80.49853194 97.55972759 81.12230601 95.99054592
 80.1452224  95.45083515 78.67106882 82.73412848 85.84690732 80.02948307
 81.14301726 99.14718388]
Accuracy th:0.7 is [45.4368246  97.2137279  73.57001011 97.02489005 97.26733349 78.49563236
 78.86477991 77.65499933 79.7273425  96.47177788 80.14765902 98.52097319
 99.41399349 81.19905947 79.6432792  96.56680596 96.29512311 79.44713149
 98.65376884 98.30776915 81.37327762 80.63863744 98.38695922 79.52632156
 81.42322827 96.65086926 94.0778012  79.06092762 98.01293844 79.98927888
 97.30875598 98.57457877 96.36213009 98.02024829 86.81058954 79.60551163
 79.34844848 90.46673408 97.11504489 76.46471169 80.59965156 92.05906361
 78.8635616  78.42740707 96.9627563  93.87434364 98.02877645 98.57336046
 90.1767766  88.83298205 87.32227921 98.55508583 98.99976852 79.04021637
 98.70615611 79.4349484  73.7771226  93.62093542 96.24273583 96.9067141
 89.79300934 97.17717864 91.09050816 79.33870201 98.42838172 79.91739867
 98.20786784 78.5431464  80.49853194 97.55972759 81.12230601 95.99054592
 80.14644071 95.45083515 78.67106882 82.82550164 85.94559033 80.02948307
 81.14301726 99.14718388]
Avg Prec: is [55.79468316  3.09481429 11.16199067  3.37991214  2.26690643  3.68413828
  3.21240252  5.56863374  2.48807003  3.71605519  1.52121906  1.59912104
  0.61950987  5.22889174  2.70295785  3.18570799  3.48407032  2.64917298
  1.39335888  1.7470164   1.94811083  0.88962975  1.84532595  2.39618703
  5.05765197  3.64995227  6.35744737  3.37660815  1.99757448  1.80971484
  2.64215639  1.36491277  3.69319511  1.7927966   2.36340841  2.37786984
  3.05948074  2.62844724  2.7823934   7.18592667  2.18738571  8.08405199
  3.25461755  3.89744897  3.18348223  6.30305897  2.06589119  1.44850778
  2.13373741  1.58596371  1.91716574  1.65720106  0.99557656  2.97586143
  1.28595195  2.56083103 11.25777405  3.62623641  3.81249547  2.88968451
 10.67018102  2.20138221  3.8924404   3.02097292  1.63881855  2.48592295
  1.88243187  4.21282886  1.2941374   2.31414723  0.16582331  3.39485293
  1.89524907  4.634599    4.00143649  3.10265698  0.83987779  1.84730734
  0.14435226  0.76266601]
mAP score regular 34.67, mAP score EMA 3.74
starting validation
Accuracy th:0.5 is [83.12778733 97.229489   91.7856342  97.86481302 98.58235543 97.2967586
 97.83242395 95.06689588 97.64556394 96.62904552 98.53501756 98.75426664
 99.34972718 95.11423375 97.25440367 96.52938685 96.23290231 97.50853327
 98.82402771 98.25099036 98.47522236 99.20771358 99.53907866 98.5698981
 95.40573536 96.52938685 94.36679373 96.90559833 97.81747515 98.14385729
 98.4353589  98.54249197 96.81092259 98.41791863 98.63965917 98.84395944
 97.48860154 98.00433515 98.52256023 92.98652117 97.86979595 93.07621397
 97.1323218  96.51443805 97.04013753 94.22976306 98.17126342 98.66208237
 97.96696315 98.6147445  98.55245783 98.55744077 98.87385704 98.00433515
 98.6969629  97.58078581 90.35802377 96.65645165 96.10334604 96.93300446
 92.29389342 98.24351596 96.59416498 97.12983033 98.56740663 97.43129781
 98.39051249 95.7769639  98.75426664 98.00184369 99.81563146 97.080001
 98.20365249 95.57764656 96.77853352 97.27682687 99.24757705 98.26593916
 99.82559733 99.14542691]
Accuracy th:0.7 is [79.38809577 97.22450607 91.10546379 97.74771408 98.50013703 97.1846426
 97.87477888 94.95976281 97.47863567 96.48454045 98.5250517  98.6894885
 99.34972718 95.11423375 97.24194633 96.36993298 96.21047911 97.50604181
 98.81406184 98.33819169 98.4951541  99.14791838 99.51914692 98.42290156
 95.43314149 96.52938685 94.38921693 96.82337992 97.81747515 98.13139996
 98.29832823 98.67204823 96.62406259 98.27092209 98.51508583 98.6595909
 97.31669034 98.03174129 98.32822583 92.83454169 97.82993248 92.58041209
 97.08249246 96.49699778 97.03764606 94.3543364  98.17624636 98.7343349
 97.96198022 98.68450557 98.46276503 98.55993223 98.87385704 98.14884022
 98.6969629  97.58576874 90.0864539  96.72621272 96.21047911 96.85826046
 91.97747714 98.09651942 96.40730498 97.042629   98.5250517  97.41136607
 98.34068316 95.7769639  98.73184344 97.92959115 99.81563146 96.80095672
 98.17375489 95.44559882 96.65146872 97.21204873 99.24757705 98.21860129
 99.82559733 99.15040985]
Avg Prec: is [93.56274622 19.11972069 60.24163129 62.02487743 63.75380169 53.56417254
 64.97064158 38.13460561 41.23482932 39.45356698 19.36577687 37.97611319
  8.44635137 18.64186014 20.93312546 40.08200581 19.60950319 23.1864843
 30.29015565 22.87082008 47.94047107 37.36173309 88.12327813 64.9200449
 19.92509233 19.06356556 27.55646434 29.67878236 12.76848039 28.11329765
 67.50199064 27.96228903 49.9204183  47.32324826 62.68822579 72.4035113
 37.44283294 68.99607321 81.2142017  38.01330569 24.94096576 47.28023821
 36.21989981 32.53900199 30.14842859 42.77522015 17.83299706 15.62229105
 29.659031   31.02360067 51.03236293 21.41802349 12.54867993 66.16685099
 11.66863832 22.07889195 49.42660655 43.42244498 24.18180593 36.63522429
 61.40776348 73.88555069 49.93209534 40.44359144 47.54312783 26.62614009
 44.40934832 18.20016569 32.40062616 51.93196739 10.35335261 61.13800643
 41.47905395 33.42038032 50.68064309 38.65170699  4.81057701 24.04400296
  1.53472369 10.60114281]
Accuracy th:0.5 is [45.32227122 97.22450607 72.1752996  96.96290206 97.90716795 77.84089493
 77.97294267 76.69731171 79.33577497 96.41976231 79.59488751 98.5325261
 99.34972718 78.9969355  79.41301044 96.31262924 96.21047911 78.92468296
 98.78167277 98.34068316 80.03587712 80.23519446 98.31327703 79.0044099
 79.07417096 96.52938685 94.3393876  78.91222563 97.81747515 79.54256671
 97.52597354 98.67204823 96.39983058 98.18870369 87.16396343 79.14891497
 79.05174776 91.8304806  97.0276802  76.02212422 79.08164536 92.37362035
 78.25696988 77.89072427 97.03764606 94.02795426 98.18621222 98.77668984
 88.55918479 87.92386078 85.71891272 98.55993223 98.87385704 78.31178215
 98.6969629  78.92468296 72.54403667 94.32194733 96.16314124 96.78102499
 90.13379176 97.04761193 90.67194858 78.83748163 98.32075143 79.68208885
 98.13139996 77.99785734 80.21775419 97.53593941 80.73099634 96.07843137
 79.90382938 95.44559882 77.9256048  83.76809428 87.50778583 79.57993871
 80.80574034 99.15040985]
Accuracy th:0.7 is [45.52408003 97.22450607 72.1752996  96.96290206 97.90716795 77.84089493
 77.97294267 76.71973491 79.33577497 96.41976231 79.59488751 98.5325261
 99.34972718 79.39307871 79.41301044 96.31262924 96.21047911 78.92468296
 98.78167277 98.34068316 80.34232753 80.23519446 98.31327703 79.00690136
 79.44539951 96.52938685 94.3393876  78.91222563 97.81747515 79.54256671
 97.52597354 98.67204823 96.39983058 98.18870369 87.4280589  79.14891497
 79.05174776 92.01235767 97.0276802  76.02212422 79.35819817 92.37362035
 78.25696988 77.89072427 97.03764606 94.02795426 98.18621222 98.77668984
 90.57727284 88.24775145 85.84597753 98.55993223 98.87385704 78.31178215
 98.6969629  78.92468296 72.54403667 94.57358547 96.16314124 96.78102499
 90.13379176 97.04761193 90.85631711 78.83748163 98.32075143 79.68208885
 98.13139996 77.99785734 80.21775419 97.53593941 80.73099634 96.07843137
 79.90382938 95.44559882 77.9256048  83.84034681 87.64730797 79.57993871
 80.80574034 99.15040985]
Avg Prec: is [53.38959041  3.70329798 14.9378472   4.53332221  1.46148026  4.52772695
 14.69118131  8.75356794  8.5706485   5.51545495  3.52486399  5.02334854
  2.49836712  5.76340342  2.99595619  3.73654681 15.22662918  6.50127022
  1.57274796  2.87529361  3.58109423  1.58376365  1.22438271  5.13333041
  5.51779585  7.73507409  7.75615866  4.65702428  3.88618022  4.31059189
  2.03942281  0.83780075  2.8808371   1.1378544   1.66209972  2.11778205
  1.91994827  2.18770415  2.18996186  6.27749871  1.70023512  6.022885
  2.15129858  2.68823536  2.33935149  4.82725937  1.6484945   1.01884617
  1.39496925  1.15706064  1.19505348  0.96352843  0.73643308  2.25457397
  0.84661087  1.841808    9.94065397  2.96394547  3.82837203  2.69895641
  7.7787589   2.10843618  3.27241271  2.52117522  1.35600971  1.93595713
  1.52608715  3.51658943  1.10270385  2.26390535  0.19723966  3.34192405
  1.65875481  3.95253196  3.18976955  2.30382503  0.5579032   1.43349973
  0.11901994  0.60148349]
mAP score regular 38.46, mAP score EMA 4.19
Train_data_mAP: current_mAP = 34.67, highest_mAP = 34.67
Val_data_mAP: current_mAP = 38.46, highest_mAP = 38.46
tensor([0.3032, 0.1452, 0.3188, 0.1521, 0.0539, 0.0932, 0.0379, 0.0744, 0.1433,
        0.0707, 0.0257, 0.0350, 0.0223, 0.0955, 0.1440, 0.1078, 0.1472, 0.0980,
        0.0508, 0.1100, 0.0155, 0.0149, 0.0329, 0.0293, 0.0795, 0.0682, 0.2362,
        0.0541, 0.0404, 0.0516, 0.0526, 0.0238, 0.1742, 0.0097, 0.0675, 0.0485,
        0.0444, 0.0753, 0.0737, 0.2998, 0.5267, 0.5554, 0.2244, 0.4432, 0.6574,
        0.5761, 0.0365, 0.0518, 0.0429, 0.0697, 0.0139, 0.0212, 0.0341, 0.0748,
        0.0377, 0.0771, 0.4892, 0.1486, 0.2553, 0.0343, 0.7352, 0.1398, 0.3278,
        0.1486, 0.0941, 0.0936, 0.1526, 0.1331, 0.3180, 0.2781, 0.0725, 0.1384,
        0.5661, 0.2990, 0.5936, 0.8915, 0.3713, 0.7692, 0.0433, 0.1613],
       device='cuda:0')
Sum Train Loss:  tensor([1.8380e+01, 1.1982e+00, 8.4987e+00, 2.3701e+00, 1.0134e+00, 1.3476e+00,
        2.4229e-01, 1.6539e+00, 2.2925e+00, 4.8817e-01, 1.0087e-01, 2.3444e-01,
        8.9387e-02, 2.3953e+00, 3.9784e+00, 1.4413e+00, 3.6901e+00, 7.6108e-01,
        2.7497e-01, 7.1484e-01, 9.3088e-02, 5.3104e-02, 1.6646e-01, 2.5685e-01,
        2.5762e+00, 1.2483e+00, 6.5050e+00, 4.9682e-01, 6.5410e-01, 4.7767e-01,
        2.6600e-01, 1.2506e-01, 1.5760e+00, 3.3431e-02, 3.0992e-01, 3.0025e-01,
        5.4804e-01, 6.8057e-01, 6.4148e-01, 9.0439e+00, 8.1061e+00, 1.1059e+01,
        3.4155e+00, 3.4695e+00, 3.9713e+00, 1.3745e+01, 5.3077e-01, 4.7141e-01,
        1.4341e-01, 7.9153e-01, 7.3313e-02, 1.1798e-01, 2.7575e-02, 3.9175e-01,
        3.0401e-02, 5.3488e-01, 1.6337e+01, 1.4147e+00, 5.3531e+00, 2.1505e-01,
        1.6896e+01, 6.3759e-01, 3.4515e+00, 2.0998e+00, 4.2820e-01, 6.7104e-01,
        1.4287e+00, 2.4932e+00, 1.2028e+00, 2.2592e+00, 1.7488e-02, 1.7178e+00,
        5.3720e+00, 2.4862e+00, 1.0365e+01, 1.0967e+01, 1.5623e+00, 1.0884e+00,
        1.0717e-02, 2.1204e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [4/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 212.8
Sum Train Loss:  tensor([1.6961e+01, 1.6242e+00, 9.8931e+00, 1.3998e+00, 1.4483e-01, 8.7958e-01,
        3.8407e-01, 1.3339e+00, 9.4318e-01, 7.4043e-01, 2.0247e-01, 2.2702e-01,
        2.5705e-02, 1.1866e+00, 3.4860e+00, 9.1117e-01, 2.2963e+00, 7.2165e-01,
        4.8989e-01, 3.0062e-01, 1.6471e-01, 5.7018e-02, 2.0237e-01, 1.7319e-01,
        1.2116e+00, 1.2963e+00, 5.5048e+00, 6.0491e-01, 2.5700e-01, 7.6149e-02,
        4.7777e-01, 2.1447e-01, 8.9387e-01, 5.9898e-02, 2.1644e-01, 3.9311e-01,
        3.7292e-01, 6.7993e-01, 1.8288e-01, 6.5231e+00, 5.5434e+00, 1.8509e+01,
        3.6770e+00, 9.2964e+00, 8.0645e+00, 1.4978e+01, 7.0640e-01, 6.3461e-01,
        3.8763e-01, 5.9248e-01, 7.6351e-02, 1.1819e-01, 3.5970e-01, 4.7551e-01,
        6.8304e-02, 5.6668e-01, 1.2995e+01, 1.9527e+00, 6.6121e+00, 2.0963e-01,
        1.5588e+01, 1.1797e+00, 3.1229e+00, 1.8081e+00, 8.1882e-01, 1.0491e+00,
        7.5472e-01, 2.0901e+00, 8.8645e-01, 2.7784e+00, 2.1041e-02, 1.2672e+00,
        2.2456e+00, 5.1928e+00, 9.1976e+00, 1.5270e+01, 3.0615e+00, 1.1638e+01,
        7.7517e-03, 9.0495e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [4/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 228.4
Sum Train Loss:  tensor([1.8222e+01, 3.2048e+00, 8.6141e+00, 1.4589e+00, 9.0534e-01, 1.1834e+00,
        2.0143e-01, 1.3060e+00, 2.1393e+00, 5.7460e-01, 2.2748e-01, 2.8470e-01,
        1.2102e-01, 2.7295e+00, 2.2025e+00, 1.8539e+00, 1.7464e+00, 1.3585e+00,
        5.6503e-02, 1.0086e+00, 9.5187e-02, 8.0026e-02, 9.9610e-03, 8.6592e-02,
        3.0501e+00, 9.1675e-01, 6.4781e+00, 7.6605e-01, 5.4516e-01, 1.2702e-01,
        3.5890e-01, 9.5647e-02, 1.5631e+00, 1.5509e-01, 1.9581e-01, 1.3740e-01,
        5.9881e-01, 9.6418e-01, 2.0586e-01, 9.0053e+00, 9.4071e+00, 1.4889e+01,
        3.1517e+00, 5.4245e+00, 6.7854e+00, 5.6188e+00, 7.6402e-02, 1.7679e-01,
        3.6778e-01, 2.2565e-01, 2.4178e-01, 7.7099e-02, 2.3603e-01, 4.5799e-01,
        1.5249e-01, 1.5400e-01, 1.9793e+01, 2.0674e+00, 1.4719e+00, 3.7133e-01,
        1.1085e+01, 1.2580e+00, 4.6380e+00, 8.9279e-01, 5.1775e-01, 6.9234e-01,
        8.2483e-01, 3.3693e+00, 2.0522e+00, 1.4639e+00, 1.5906e-02, 1.1301e+00,
        2.8302e+00, 3.9331e+00, 1.0569e+01, 1.4054e+01, 4.1360e+00, 4.2907e+00,
        7.5248e-03, 1.0805e+00], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [4/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 214.8
Sum Train Loss:  tensor([1.5076e+01, 2.4249e+00, 8.9214e+00, 1.4387e+00, 3.9692e-01, 6.7614e-01,
        1.9096e-01, 1.2543e+00, 1.1406e+00, 7.9847e-01, 1.2360e-01, 1.0634e-01,
        8.0079e-02, 1.2998e+00, 1.7081e+00, 2.0044e+00, 4.0308e+00, 9.9541e-01,
        5.4458e-01, 1.4387e+00, 8.1886e-02, 1.1169e-02, 3.8927e-02, 1.9655e-01,
        2.1062e+00, 1.4597e+00, 6.5992e+00, 8.5384e-01, 5.0513e-01, 4.1578e-01,
        1.1145e-01, 4.7718e-02, 3.4138e+00, 5.3943e-02, 4.9545e-01, 1.7111e-01,
        2.3180e-01, 1.7619e-01, 8.2378e-01, 8.6543e+00, 8.0107e+00, 8.8441e+00,
        1.3575e+00, 6.9384e+00, 6.8725e+00, 1.1355e+01, 3.3780e-01, 5.9441e-01,
        1.4241e-01, 1.2576e+00, 4.4214e-02, 2.4111e-01, 3.0287e-02, 2.0069e-01,
        1.6232e-01, 1.1875e+00, 1.6947e+01, 2.8375e+00, 6.9883e+00, 2.6044e-01,
        2.0128e+01, 7.0368e-01, 6.4669e+00, 2.0069e+00, 4.8543e-01, 9.6163e-01,
        8.2809e-01, 3.5827e+00, 2.0264e+00, 2.9432e+00, 3.6085e-02, 1.2881e+00,
        9.3479e+00, 2.5302e+00, 9.8514e+00, 9.0161e+00, 3.4227e-01, 8.5190e+00,
        9.3244e-03, 5.8057e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [4/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 227.4
Sum Train Loss:  tensor([1.5907e+01, 1.4836e+00, 1.1827e+01, 2.8412e+00, 1.3046e-01, 1.0213e+00,
        4.7257e-01, 1.5731e+00, 2.4354e+00, 9.2870e-01, 7.4551e-02, 1.9328e-01,
        1.9815e-01, 2.2344e+00, 2.5224e+00, 9.6417e-01, 4.2848e+00, 6.9654e-01,
        9.5980e-02, 1.1736e+00, 3.8716e-02, 1.1664e-02, 6.9827e-02, 5.7908e-02,
        1.5148e+00, 6.0336e-01, 4.4241e+00, 7.5369e-01, 5.4657e-01, 8.2266e-01,
        4.6000e-01, 1.9315e-01, 1.1610e+00, 8.8161e-02, 2.1670e-01, 6.4065e-01,
        6.3395e-01, 1.2846e+00, 3.6989e-01, 6.6757e+00, 5.2004e+00, 1.0527e+01,
        2.5785e+00, 5.6661e+00, 1.8874e+00, 6.7854e+00, 1.9052e-01, 4.5644e-01,
        9.7200e-02, 1.5020e-01, 3.9709e-02, 1.2501e-01, 2.9392e-01, 3.8079e-01,
        2.9257e-01, 7.0237e-01, 1.8569e+01, 2.3872e+00, 3.1808e+00, 4.2982e-01,
        1.4774e+01, 7.7243e-01, 6.2868e+00, 2.6546e+00, 6.0640e-01, 1.0424e+00,
        1.0578e+00, 3.9022e+00, 7.8846e-01, 2.4742e+00, 1.1280e-02, 1.1227e+00,
        3.0746e+00, 4.4672e+00, 2.6421e+00, 9.1147e+00, 1.8179e+00, 3.2369e+00,
        6.5680e-03, 5.6373e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [4/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 192.0
Sum Train Loss:  tensor([1.2603e+01, 8.3256e-01, 9.0090e+00, 9.8738e-01, 7.1301e-01, 4.5817e-01,
        1.7335e-01, 8.1002e-01, 3.0384e+00, 7.9341e-01, 2.7630e-01, 9.0511e-02,
        1.0269e-01, 2.0762e+00, 1.2940e+00, 1.3299e+00, 1.1303e+00, 8.3325e-01,
        4.1607e-01, 1.8881e+00, 1.6253e-01, 4.8435e-02, 3.8142e-02, 1.0148e-01,
        1.2613e+00, 1.6377e+00, 4.9096e+00, 1.1427e+00, 3.1226e-01, 6.1398e-01,
        4.9308e-01, 1.8738e-01, 1.0581e+00, 7.9524e-02, 3.3146e-01, 4.0736e-01,
        7.2278e-01, 7.0756e-01, 7.5326e-01, 7.2780e+00, 4.9115e+00, 1.3195e+01,
        2.6437e+00, 4.9835e+00, 5.8645e+00, 8.3018e+00, 4.8100e-01, 6.2065e-01,
        1.6328e-01, 8.1895e-01, 9.4675e-02, 1.8718e-01, 2.7861e-01, 6.0515e-01,
        2.6280e-01, 9.9204e-01, 9.2641e+00, 1.2796e+00, 6.0549e+00, 2.3615e-01,
        1.5033e+01, 4.9067e-01, 4.7992e+00, 3.3137e+00, 3.4977e-01, 6.5199e-01,
        1.1767e+00, 2.7380e+00, 1.7684e+00, 2.1916e+00, 2.6528e-01, 1.3724e+00,
        7.7848e+00, 3.7555e+00, 7.4498e+00, 8.1410e+00, 2.0710e+00, 7.0220e+00,
        4.8772e-03, 9.4566e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [4/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 192.8
Sum Train Loss:  tensor([1.5862e+01, 3.3096e+00, 9.9827e+00, 4.3306e-01, 5.1622e-01, 4.7919e-01,
        4.4894e-01, 1.2731e+00, 1.3020e+00, 8.1624e-01, 4.0428e-01, 2.3197e-01,
        1.5677e-02, 2.2036e+00, 3.1821e+00, 1.2569e+00, 1.9172e+00, 7.7872e-01,
        2.2844e-01, 1.7414e+00, 2.9922e-01, 6.4251e-02, 1.0210e-01, 2.0959e-01,
        1.4632e+00, 1.2982e+00, 4.9637e+00, 1.0281e+00, 1.3186e-01, 3.4929e-01,
        2.0379e-01, 7.8993e-02, 2.8649e+00, 1.2644e-01, 1.0148e+00, 4.7789e-01,
        4.7640e-01, 4.4044e-01, 4.8697e-01, 7.5575e+00, 5.1364e+00, 9.4737e+00,
        1.4922e+00, 5.9016e+00, 5.7468e+00, 6.1300e+00, 3.4345e-01, 1.4602e-01,
        9.0511e-02, 1.5451e-01, 1.0635e-01, 1.2540e-01, 2.5209e-01, 5.3003e-01,
        2.0504e-01, 8.2813e-01, 1.9577e+01, 2.1356e+00, 2.8731e+00, 5.1530e-01,
        1.2474e+01, 9.5966e-01, 2.9912e+00, 5.5016e-01, 5.1318e-01, 7.3334e-01,
        3.4547e-01, 2.6360e+00, 8.4867e-01, 1.0278e+00, 2.3240e-02, 1.5762e+00,
        7.5808e+00, 3.3266e+00, 4.4700e+00, 1.8937e+00, 3.5318e-01, 1.1649e+01,
        2.1227e-01, 1.3995e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [4/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 186.1
Sum_Val Meta Model:  tensor([1.8222e+01, 1.0312e+01, 2.8911e+01, 6.8617e+00, 1.7406e-01, 1.2172e+00,
        2.0244e-01, 1.3541e+00, 1.0227e+00, 9.2895e-01, 1.4276e-01, 4.0123e-01,
        1.4246e-01, 1.6272e+00, 1.2395e+00, 6.0339e-01, 6.8249e+01, 2.8475e-01,
        4.7133e-02, 2.1309e-01, 2.0117e-02, 7.3804e-03, 2.5545e-02, 7.2369e-02,
        2.2788e+00, 1.3753e+00, 6.9247e+00, 2.2950e-01, 3.7480e-01, 5.7184e-01,
        9.3202e-02, 3.2388e-02, 1.3616e+00, 1.0429e-02, 1.3174e-01, 9.5940e-02,
        1.7203e-01, 3.7190e-01, 8.7540e-02, 1.3808e+01, 5.6875e+00, 1.2346e+01,
        1.7978e+00, 5.9869e+00, 1.0241e+01, 1.1824e+01, 5.6130e-02, 2.9091e-01,
        7.4822e-02, 3.7586e-01, 7.9383e-03, 2.3803e-02, 2.7088e-01, 2.2063e-01,
        4.5517e-02, 1.8568e-01, 1.3882e+01, 1.1993e+00, 4.6090e+00, 1.3989e-01,
        1.1274e+01, 2.9437e+00, 2.1749e+00, 6.9524e-01, 1.2180e-01, 2.3307e-01,
        1.9359e-01, 1.2684e+00, 2.5392e+00, 5.4318e+00, 3.6493e-02, 2.0082e+00,
        8.9259e+00, 4.2988e+00, 8.0736e+00, 6.3806e+00, 5.4129e-01, 5.0652e+00,
        8.0311e-03, 1.0434e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.5953e+01, 7.8180e+00, 2.4437e+01, 4.6366e+00, 8.3441e-02, 1.1392e+00,
        2.2780e-01, 1.3977e+00, 9.6931e-01, 9.0002e-01, 1.5220e-01, 3.6209e-01,
        1.4982e-01, 1.5621e+00, 1.1640e+00, 1.7161e+00, 4.9494e+01, 4.7290e-01,
        3.3667e-02, 2.6114e-01, 3.0969e-02, 8.0607e-03, 1.3492e-02, 5.6698e-02,
        2.1812e+00, 1.3222e+00, 6.7343e+00, 3.3431e-01, 4.9863e-01, 5.6439e-01,
        7.9344e-02, 2.6751e-02, 1.2120e+00, 1.1712e-02, 1.2717e-01, 8.1041e-02,
        3.8142e-01, 1.8002e-01, 5.0670e-02, 1.3933e+01, 5.8920e+00, 1.3589e+01,
        1.9896e+00, 6.1554e+00, 9.8424e+00, 1.1128e+01, 3.7534e-02, 2.7617e-01,
        4.8228e-02, 2.8543e-01, 2.1166e-03, 1.1205e-02, 2.5802e-01, 9.9349e-02,
        3.6241e-02, 1.1119e-01, 1.3041e+01, 1.2537e+00, 4.1114e+00, 2.7919e-01,
        1.0501e+01, 1.9905e+00, 1.6393e+00, 8.1287e-01, 1.1384e-01, 4.0955e-01,
        1.7427e-01, 1.7299e+00, 3.2146e+00, 5.3006e+00, 3.3621e-02, 2.1385e+00,
        8.6694e+00, 4.5451e+00, 6.7495e+00, 5.4982e+00, 7.1226e-01, 4.7762e+00,
        8.1899e-03, 1.2939e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.2619e+01, 5.3833e+01, 7.6648e+01, 3.0492e+01, 1.5473e+00, 1.2223e+01,
        6.0079e+00, 1.8799e+01, 6.7641e+00, 1.2738e+01, 5.9302e+00, 1.0349e+01,
        6.7167e+00, 1.6363e+01, 8.0860e+00, 1.5924e+01, 3.3617e+02, 4.8261e+00,
        6.6287e-01, 2.3745e+00, 1.9995e+00, 5.4059e-01, 4.1029e-01, 1.9383e+00,
        2.7437e+01, 1.9396e+01, 2.8511e+01, 6.1795e+00, 1.2330e+01, 1.0942e+01,
        1.5094e+00, 1.1236e+00, 6.9554e+00, 1.2134e+00, 1.8834e+00, 1.6725e+00,
        8.5851e+00, 2.3921e+00, 6.8720e-01, 4.6468e+01, 1.1185e+01, 2.4468e+01,
        8.8651e+00, 1.3888e+01, 1.4971e+01, 1.9318e+01, 1.0283e+00, 5.3349e+00,
        1.1244e+00, 4.0976e+00, 1.5275e-01, 5.2912e-01, 7.5636e+00, 1.3281e+00,
        9.6246e-01, 1.4427e+00, 2.6660e+01, 8.4386e+00, 1.6106e+01, 8.1371e+00,
        1.4285e+01, 1.4238e+01, 5.0010e+00, 5.4690e+00, 1.2101e+00, 4.3770e+00,
        1.1417e+00, 1.2996e+01, 1.0110e+01, 1.9062e+01, 4.6388e-01, 1.5449e+01,
        1.5315e+01, 1.5201e+01, 1.1370e+01, 6.1671e+00, 1.9182e+00, 6.2095e+00,
        1.8926e-01, 8.0205e-01], device='cuda:0')
Outer loop valEpocw Maximum [4/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 301.8
model_train val_loss valEpocw [4/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 268.4
model_train val_loss valEpocw [4/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1197.8
Sum_Val Meta Model:  tensor([2.3088e+01, 2.7066e+00, 2.3767e+01, 1.4937e+00, 6.7092e-02, 4.4145e+00,
        6.6068e+00, 5.9949e+00, 6.0609e+00, 3.0928e+00, 6.8480e-01, 1.3622e+01,
        3.5292e-01, 6.5766e-01, 2.5693e+00, 2.4592e+00, 1.9608e+00, 1.9588e+00,
        5.6569e-01, 4.1263e+00, 1.1350e-02, 3.7151e-03, 2.7493e-02, 1.7842e-01,
        2.4083e+00, 1.8568e+00, 7.5359e+00, 1.2960e+00, 7.9523e-01, 1.9875e-02,
        3.0600e-02, 9.7360e-03, 8.5173e-02, 8.2259e-03, 2.0636e-02, 1.3979e-02,
        3.6603e-01, 3.5540e-02, 2.2397e-02, 7.8104e-01, 2.4236e-01, 2.4899e+00,
        1.0934e-01, 2.9986e-01, 4.0349e-01, 2.3488e+00, 7.3055e-02, 5.1021e-02,
        1.9650e-02, 4.6891e-01, 6.1668e-03, 1.2138e-02, 9.6788e-03, 2.2189e-02,
        2.1820e-02, 3.9868e-01, 6.7212e+00, 1.1490e+00, 2.2899e+00, 1.4154e-01,
        2.3609e+00, 1.0537e-01, 9.3707e-01, 3.8231e-01, 8.3527e-02, 1.2958e-01,
        1.6119e-01, 1.8245e+00, 1.4999e-01, 1.2054e+00, 1.3777e-02, 1.2894e-01,
        2.4356e+00, 1.9569e+00, 2.8659e+00, 6.4180e-01, 1.0029e-01, 7.3497e-01,
        6.0187e-03, 5.2268e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([2.1044e+01, 2.7884e+00, 1.7752e+01, 2.7006e+00, 3.5777e-01, 4.3843e+00,
        5.1678e+00, 4.3333e+00, 5.1545e+00, 2.5436e+00, 5.9104e-01, 5.5680e+00,
        3.3898e-01, 1.0340e+00, 2.3329e+00, 4.2037e-01, 1.8013e+00, 1.9247e+00,
        9.8622e-02, 2.3244e+00, 3.6635e-02, 1.0516e-02, 8.2931e-02, 1.4878e-01,
        2.3178e+00, 1.7992e+00, 7.1623e+00, 1.2889e+00, 7.4559e-01, 6.6929e-02,
        5.3495e-02, 2.0566e-02, 1.1143e-01, 2.3467e-02, 3.6498e-02, 1.8605e-02,
        3.2863e-01, 5.9606e-01, 1.8898e-02, 7.3882e-01, 2.8829e-01, 1.8338e+00,
        1.2973e-01, 3.8052e-01, 2.9628e-01, 1.1481e+00, 3.5632e-02, 4.6991e-02,
        2.9851e-02, 3.9525e-01, 2.0737e-03, 1.0140e-02, 2.2901e-02, 9.1378e-02,
        2.3052e-02, 2.6181e-01, 5.1335e+00, 7.0166e-01, 1.8862e+00, 4.5952e-02,
        2.4760e+00, 7.0800e-02, 2.3503e-01, 1.1576e-01, 3.3721e-02, 6.5411e-02,
        4.5988e-02, 1.9664e+00, 7.4633e-02, 1.1022e+00, 4.5478e-03, 5.9614e-02,
        1.9304e+00, 1.3886e+00, 4.1447e+00, 3.8059e-01, 3.1869e-01, 5.4067e-01,
        1.9890e-03, 2.8000e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([7.1067e+01, 1.8109e+01, 5.1273e+01, 1.5238e+01, 5.5697e+00, 3.6193e+01,
        6.5821e+01, 4.1311e+01, 3.0821e+01, 3.1553e+01, 1.6296e+01, 1.0189e+02,
        8.9550e+00, 1.0775e+01, 1.3646e+01, 2.8223e+00, 1.0658e+01, 1.5383e+01,
        1.1250e+00, 1.3620e+01, 1.3435e+00, 4.6139e-01, 1.5803e+00, 3.8896e+00,
        2.4539e+01, 1.9848e+01, 3.0978e+01, 1.6075e+01, 1.2198e+01, 9.6189e-01,
        7.4117e-01, 5.4456e-01, 6.1918e-01, 1.3498e+00, 4.7898e-01, 3.5627e-01,
        5.5873e+00, 6.4427e+00, 2.1684e-01, 3.0499e+00, 7.7425e-01, 3.8974e+00,
        6.5495e-01, 1.0058e+00, 5.4188e-01, 2.1854e+00, 6.4156e-01, 6.6676e-01,
        4.4861e-01, 4.3211e+00, 8.4581e-02, 3.1853e-01, 4.6320e-01, 1.0197e+00,
        4.1070e-01, 2.7083e+00, 1.1358e+01, 4.0921e+00, 7.8397e+00, 8.6933e-01,
        3.7556e+00, 5.2791e-01, 7.3192e-01, 6.9239e-01, 3.0358e-01, 5.9143e-01,
        2.7483e-01, 1.6423e+01, 2.6573e-01, 4.4119e+00, 5.3596e-02, 4.5574e-01,
        4.1200e+00, 4.6939e+00, 8.8377e+00, 4.3133e-01, 8.8010e-01, 6.9903e-01,
        3.3520e-02, 1.7783e-01], device='cuda:0')
Outer loop valEpocw Maximum [4/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 155.3
model_train val_loss valEpocw [4/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 126.0
model_train val_loss valEpocw [4/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 786.1
Sum_Val Meta Model:  tensor([2.3946e+01, 6.3349e-01, 4.3996e+00, 5.1333e-01, 7.5032e-02, 3.3926e-01,
        9.0543e-02, 9.3863e-01, 4.6867e-01, 2.5656e-01, 6.2241e-02, 6.8111e-02,
        2.1732e-02, 1.7235e+00, 3.8826e-01, 3.7425e-01, 9.0493e-01, 2.8689e-01,
        9.6802e-02, 2.9123e-01, 3.2895e-02, 5.4554e-02, 3.6627e-01, 9.8959e-02,
        6.1639e-01, 1.8977e-01, 4.4572e+00, 4.2081e-01, 8.5028e-02, 2.0262e-01,
        3.7279e-01, 1.9992e-01, 1.9329e+00, 6.1158e-03, 2.9872e-01, 2.1084e-01,
        8.5981e-01, 1.1464e+00, 5.2540e-02, 7.9912e+00, 1.2155e+01, 4.1160e+01,
        1.8017e+01, 3.6849e+01, 3.4891e+01, 4.2694e+01, 9.2051e-01, 1.0592e+00,
        3.6484e+00, 1.8159e+00, 8.5567e-01, 1.2985e+00, 3.5438e+00, 1.5367e+00,
        3.3243e+00, 9.0986e+00, 7.8971e+01, 2.2929e+00, 2.1879e+00, 1.1163e-01,
        7.2458e+01, 2.9204e-01, 3.5281e+00, 2.2270e+00, 1.4892e+00, 1.3881e-01,
        1.5373e+00, 1.1984e+00, 1.3330e+00, 6.7418e-01, 3.8542e-02, 1.1240e+00,
        2.6891e+00, 9.2330e+00, 1.2821e+00, 1.2163e+01, 3.2448e+00, 1.3046e+00,
        1.4449e-02, 1.4875e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([8.3078e+00, 1.1996e-01, 1.2508e+00, 1.0217e-01, 9.8854e-03, 1.5446e-02,
        6.0719e-03, 1.0665e+00, 5.8594e-02, 1.5591e-02, 4.3813e-03, 4.0703e-03,
        1.4340e-03, 1.3194e+00, 7.9973e-02, 2.3997e-01, 4.4436e-01, 5.7245e-02,
        9.5420e-03, 3.9034e-02, 3.5370e-03, 2.1599e-03, 2.6323e-03, 2.3141e-03,
        3.7035e-01, 6.1082e-02, 4.4491e+00, 4.2590e-01, 7.0870e-02, 2.3393e-02,
        3.5387e-02, 1.6453e-01, 1.4172e+00, 1.6356e-03, 5.6217e-02, 4.1029e-02,
        6.9053e-01, 1.1217e+00, 1.9464e-02, 9.1153e+00, 1.1723e+01, 3.6958e+01,
        1.8535e+01, 3.4207e+01, 2.7793e+01, 4.2148e+01, 4.5721e-01, 4.4785e-01,
        2.9528e+00, 9.2949e-01, 5.5119e-01, 1.3957e+00, 3.1274e+00, 2.3121e+00,
        3.0306e+00, 8.8870e+00, 5.0234e+01, 2.2997e+00, 1.8340e+00, 1.3076e-01,
        6.7628e+01, 1.4104e-01, 3.2907e+00, 1.8490e+00, 1.1048e+00, 6.2191e-01,
        1.4021e+00, 1.5554e+00, 1.4157e+00, 5.8312e-01, 2.6018e-02, 7.3103e-01,
        2.7368e+00, 9.5910e+00, 1.8970e+00, 1.3270e+01, 2.9488e+00, 1.3679e+00,
        9.3910e-03, 1.4875e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.1103e+01, 9.6514e-01, 4.0967e+00, 7.5256e-01, 2.1286e-01, 1.8472e-01,
        1.9675e-01, 1.6829e+01, 4.4492e-01, 2.4909e-01, 1.7732e-01, 1.3086e-01,
        7.2908e-02, 1.5827e+01, 6.3290e-01, 2.4375e+00, 3.0438e+00, 6.0924e-01,
        1.9328e-01, 3.7986e-01, 2.7953e-01, 1.1293e-01, 6.1875e-02, 8.6574e-02,
        5.3276e+00, 1.1070e+00, 2.5608e+01, 9.7700e+00, 2.0856e+00, 4.4118e-01,
        6.5329e-01, 5.7572e+00, 7.8484e+00, 1.5485e-01, 8.4112e-01, 8.1859e-01,
        1.3455e+01, 1.4026e+01, 2.7751e-01, 3.8347e+01, 2.9253e+01, 6.7056e+01,
        7.0266e+01, 7.4232e+01, 4.3698e+01, 6.8619e+01, 1.0133e+01, 7.0384e+00,
        4.1443e+01, 1.0693e+01, 2.6053e+01, 4.7872e+01, 7.0962e+01, 3.0433e+01,
        6.9249e+01, 9.6712e+01, 1.1320e+02, 1.6939e+01, 8.8219e+00, 3.5405e+00,
        8.9374e+01, 1.3072e+00, 1.1531e+01, 1.3696e+01, 1.2939e+01, 7.1895e+00,
        9.8121e+00, 1.7112e+01, 6.1039e+00, 2.7120e+00, 4.4076e-01, 6.7384e+00,
        6.3244e+00, 3.8306e+01, 4.2796e+00, 1.4792e+01, 1.0011e+01, 1.7987e+00,
        2.3572e-01, 1.1692e+00], device='cuda:0')
Outer loop valEpocw Maximum [4/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 468.0
model_train val_loss valEpocw [4/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 393.5
model_train val_loss valEpocw [4/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1367.7
Sum_Val Meta Model:  tensor([3.1329e+01, 1.9746e-01, 6.4514e+00, 1.1917e-01, 2.8078e-02, 8.2844e-01,
        1.5681e-01, 9.4204e-01, 1.9833e-01, 1.0585e+00, 9.2386e-02, 1.2881e-01,
        4.0165e-03, 1.1939e+00, 1.6949e+00, 1.6464e-01, 2.4479e-01, 6.9939e-02,
        2.1237e-02, 5.3783e-02, 6.7764e-03, 4.7570e-03, 1.3211e-02, 1.8404e-02,
        1.4819e+00, 8.9969e-02, 5.4273e+00, 6.1296e-02, 4.2098e-01, 1.6178e-02,
        2.7458e-02, 4.4211e-03, 5.8196e-01, 3.2736e-02, 2.0415e-01, 2.2313e-01,
        1.4961e-02, 8.3901e-02, 3.0455e-01, 5.0047e+00, 4.2812e+00, 1.2409e+01,
        1.4590e+00, 3.3174e+00, 3.3333e+00, 6.1496e+00, 1.9333e-02, 2.6181e-02,
        5.5526e-02, 3.0060e-02, 1.1495e-02, 1.1762e-02, 9.7237e-03, 4.5538e-01,
        1.7271e-02, 1.1026e-01, 1.0695e+01, 5.5146e-01, 3.1980e+00, 4.9668e-02,
        2.4893e+01, 9.3974e-02, 1.5182e+00, 8.1537e-01, 3.4594e-01, 1.4056e-01,
        7.3365e-01, 6.6633e+00, 2.6928e-01, 5.6126e-01, 6.1632e-03, 1.8289e-01,
        1.7841e+00, 2.8492e+00, 2.0270e+02, 1.1039e+01, 2.4545e-01, 8.0535e+00,
        3.0609e-03, 1.2535e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.2051e+01, 6.2486e-01, 5.7469e+00, 6.5761e-01, 6.6825e-02, 7.1944e-01,
        2.3523e-01, 8.8065e-01, 2.4238e-01, 1.0785e+00, 6.7555e-02, 2.6209e-01,
        8.4064e-03, 1.2177e+00, 2.3015e+00, 1.7586e-01, 1.8190e-01, 1.0783e-01,
        5.1419e-03, 3.8104e-02, 5.7990e-03, 1.2491e-03, 4.6389e-02, 1.2330e-01,
        1.5343e+00, 9.4794e-02, 5.2835e+00, 1.0870e-01, 4.5079e-01, 8.1966e-03,
        1.5640e-02, 9.2464e-03, 5.6016e-02, 4.9261e-03, 1.8678e-02, 9.2002e-03,
        1.0133e-01, 2.5434e-02, 1.5659e-02, 8.4705e-01, 3.5024e-01, 1.1762e+00,
        1.3832e-01, 5.4721e-01, 4.2986e-01, 7.3183e-01, 1.4538e-02, 1.2327e-02,
        1.0828e-02, 1.6521e-02, 1.2301e-03, 3.1594e-03, 5.0632e-03, 4.1871e-02,
        8.9927e-03, 5.3660e-02, 2.0434e+00, 1.2188e-01, 2.5363e+00, 2.0189e-02,
        6.2036e+00, 9.5997e-02, 3.1233e-01, 1.2920e-01, 5.6535e-02, 9.7482e-02,
        7.5839e-02, 1.1006e+00, 1.2255e-01, 1.0557e-01, 4.1296e-03, 5.7893e-02,
        1.4133e-01, 1.4043e+00, 4.9776e+01, 1.4437e+00, 3.9944e-01, 3.6903e+00,
        2.1051e-03, 1.6964e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.1939e+01, 6.0261e+00, 2.0903e+01, 5.7186e+00, 1.6211e+00, 1.0280e+01,
        9.0499e+00, 1.5802e+01, 2.1114e+00, 2.1295e+01, 3.5397e+00, 1.0353e+01,
        5.1984e-01, 1.6452e+01, 2.0821e+01, 1.9996e+00, 1.4809e+00, 1.4625e+00,
        1.2640e-01, 4.5595e-01, 5.7861e-01, 1.0198e-01, 1.8013e+00, 6.0999e+00,
        2.5179e+01, 1.9310e+00, 2.8503e+01, 2.8193e+00, 1.5829e+01, 2.1206e-01,
        4.0020e-01, 4.9645e-01, 3.2581e-01, 5.8811e-01, 2.9906e-01, 1.6499e-01,
        3.0856e+00, 4.1740e-01, 1.9718e-01, 3.5392e+00, 7.7620e-01, 2.1968e+00,
        6.6869e-01, 1.3809e+00, 7.3188e-01, 1.3689e+00, 5.4900e-01, 3.3551e-01,
        3.2844e-01, 3.2505e-01, 1.2262e-01, 1.9086e-01, 1.9857e-01, 6.3451e-01,
        3.3451e-01, 8.6741e-01, 4.4467e+00, 9.5597e-01, 1.0661e+01, 7.9292e-01,
        8.6537e+00, 9.8847e-01, 1.0034e+00, 8.8503e-01, 5.6101e-01, 1.2641e+00,
        4.3869e-01, 8.1169e+00, 5.1985e-01, 4.9697e-01, 8.4750e-02, 5.7728e-01,
        2.8812e-01, 5.3489e+00, 1.0357e+02, 1.5362e+00, 1.2375e+00, 4.3248e+00,
        6.1624e-02, 1.2662e-01], device='cuda:0')
Outer loop valEpocw Maximum [4/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 368.2
model_train val_loss valEpocw [4/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 108.9
model_train val_loss valEpocw [4/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 452.5
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [82.42224144 97.22469268 91.39508534 97.7010514  98.09456512 97.29048135
 97.8131358  94.96229334 97.68886831 96.71543963 98.53071965 98.72564905
 99.41399349 95.31560288 97.33677709 96.71787624 96.2939048  97.50734031
 98.68909979 98.33457195 98.34797334 99.19591623 99.40546533 98.52584642
 95.20595509 96.65208757 94.21912501 96.80072124 98.01293844 98.19812137
 98.04461447 98.54046612 96.60213691 98.20664953 98.46249437 98.52097319
 97.14550261 98.0494877  98.31507901 93.01787259 97.8825794  92.89604171
 97.04194637 96.33167237 97.00539711 94.29466015 98.02999476 98.60869141
 98.02268491 98.59163509 98.54655767 98.55264921 98.99976852 98.06776233
 98.70615611 97.46835443 90.56176216 96.50589052 96.28172171 97.04194637
 92.41115483 98.05801586 96.59848199 97.13331953 98.6342759  97.39038267
 98.3833043  95.9320671  98.71712089 97.83019213 99.81603538 96.7251861
 98.08481865 95.55073647 96.84092543 97.20885467 99.18007822 98.23832556
 99.84405648 99.14718388]
Accuracy th:0.7 is [79.60429332 97.21860114 90.56298047 97.74734713 98.04217785 97.02610836
 97.62795288 94.86117372 97.58287545 96.59482706 98.53193796 98.69275472
 99.41399349 95.31682119 97.31484753 96.62528478 96.2939048  97.48053752
 98.65376884 98.32117055 98.28096636 99.18738807 99.38597239 98.39061415
 95.21935649 96.65086926 94.120442   96.77757337 98.01293844 98.1603538
 97.93862161 98.57579708 96.4193906  98.07994542 98.31629732 98.41498032
 96.99565064 98.1323327  98.09578343 92.7547179  97.85821323 92.42455623
 96.91646057 96.24273583 96.96519292 93.95475201 98.03121307 98.5928534
 97.99710043 98.58798017 98.43325496 98.55508583 98.99976852 98.09212851
 98.70615611 97.46591781 89.98062889 96.349947   96.25979216 96.95057321
 91.81052862 97.84603014 96.42670045 97.09798857 98.61843788 97.35505172
 98.3418818  95.95156004 98.68909979 97.67790353 99.81603538 96.3913695
 97.98369903 95.56170125 96.83605219 97.13697445 99.18007822 98.18593828
 99.84405648 99.14718388]
Avg Prec: is [93.03552003 20.90918397 58.36442221 54.95138701 63.31610771 55.12191513
 60.16175976 39.43548685 38.97034287 39.61403992 12.39377489 36.06316817
 10.23893875 18.51554398 20.42606297 34.3673366  17.70367596 22.64230781
 30.2529526  25.53735317 41.30925811 30.48851277 83.65816976 63.23253063
 20.71728803 17.64697961 30.95868693 25.06574505 12.04380055 29.96277437
 61.56177369 25.78684378 48.82041294 46.80195758 61.55976607 68.26383961
 37.72771355 65.51891591 73.89183555 38.362806   25.79545994 48.26031548
 40.47296822 36.81078311 33.89125249 46.90414257 21.1857786  23.14280432
 28.24541721 29.51824897 47.03036308 22.94151815 10.6656866  62.87498363
 10.64993312 22.62044964 51.26920458 43.36848619 26.72226061 35.56085597
 64.19526246 67.55045921 48.21200565 34.89121018 42.3272422  28.41105818
 39.13334159 17.25271639 33.49493842 50.35589093  6.82111992 60.19695725
 44.56869125 34.42572799 43.78142246 41.64660605  6.23697461 27.92567744
  2.20732519 11.31563841]
Accuracy th:0.5 is [45.33448667 97.2137279  73.33853145 97.02489005 97.26733349 78.37136487
 78.80873771 77.4283939  79.60551163 96.44010185 80.04532109 98.52097319
 99.41399349 80.72148244 79.58967362 96.56680596 96.29512311 79.27656827
 98.65376884 98.30776915 80.87498934 80.49731363 98.38695922 79.42276532
 80.77386971 96.65086926 94.0778012  78.99270233 98.01293844 79.87475786
 97.30875598 98.57457877 96.36213009 98.02024829 86.56570948 79.54459619
 79.24367393 90.25840328 97.11504489 76.31120479 79.8942508  92.05906361
 78.74904058 78.30801282 96.9627563  93.87434364 98.02877645 98.57336046
 89.09491843 88.29083466 87.23334267 98.55508583 98.99976852 78.95980799
 98.70615611 79.31068091 73.66260158 93.32488639 96.24273583 96.9067141
 89.79300934 97.17717864 91.13314896 79.25585702 98.42838172 79.68348339
 98.20786784 78.32628745 80.40837709 97.55729097 80.99072867 95.99054592
 80.09161682 95.45083515 78.59553368 82.70123415 85.80426652 79.95151131
 81.05529903 99.14718388]
Accuracy th:0.7 is [45.43195137 97.2137279  73.33853145 97.02489005 97.26733349 78.37136487
 78.80873771 77.52098537 79.60551163 96.4742145  80.04532109 98.52097319
 99.41399349 81.13327079 79.58967362 96.56680596 96.29512311 79.27656827
 98.65376884 98.30776915 81.2867777  80.49731363 98.38695922 79.42763855
 81.29043262 96.65086926 94.0778012  78.99270233 98.01293844 79.87475786
 97.30875598 98.57457877 96.36213009 98.02024829 86.77647689 79.54459619
 79.24367393 90.49962842 97.11504489 76.31120479 80.49731363 92.05906361
 78.74904058 78.30801282 96.9627563  93.87434364 98.02877645 98.57336046
 91.23183197 88.95724955 87.40877913 98.55508583 98.99976852 78.95980799
 98.70615611 79.31068091 73.66260158 93.72083673 96.24273583 96.9067141
 89.79300934 97.17717864 91.33173329 79.25585702 98.42838172 79.68348339
 98.20786784 78.32628745 80.40837709 97.55972759 80.99072867 95.99054592
 80.09161682 95.45083515 78.59553368 82.7779876  85.89563967 79.95151131
 81.05529903 99.14718388]
Avg Prec: is [56.16765559  3.08784618 11.41052471  3.23014285  2.31424202  3.75843429
  3.37913367  5.6335161   2.42260232  3.83683647  1.55536822  1.55656581
  0.65444107  5.04786846  2.64635223  3.11864616  3.56888608  2.59499867
  1.32341546  1.70765933  1.85682508  0.87586349  1.80475189  2.317623
  5.08371602  3.74720258  6.61601074  3.20916451  2.20151123  1.88618182
  2.62146605  1.27857072  3.74200979  1.72140559  2.32696141  2.4021979
  3.05832902  2.6913003   2.82975259  7.40855238  2.22895645  8.05666955
  3.33086516  4.08142109  3.2460984   6.46273563  2.03710535  1.46584225
  2.06220774  1.60971822  1.80155811  1.56383938  1.03320349  2.97708673
  1.30644338  2.72075514 11.28024841  3.75025445  4.08815922  2.91359516
 10.68694189  2.21226096  3.89797555  3.15018926  1.68590459  2.58452775
  1.84303052  4.27557837  1.25704606  2.38206953  0.21172108  3.44814831
  1.92924607  4.63182142  3.89284919  3.05931144  0.84252889  1.84423894
  0.156076    0.7362819 ]
mAP score regular 37.60, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [84.85437377 97.192117   91.91269901 97.91713382 98.64962503 97.3864514
 97.99436929 94.99962628 97.79256048 96.71126392 98.5250517  98.82402771
 99.34972718 95.08433615 97.32167327 96.65894312 96.23041084 97.53593941
 98.82153624 98.26095622 98.57986397 99.19525625 99.54655306 98.77668984
 95.40075242 96.53187832 94.44652067 97.00525699 97.81498368 98.18122929
 98.5101029  98.6072701  96.68634925 98.4278845  98.71689464 98.86887411
 97.58327728 98.04419862 98.6820141  93.0214017  97.91215088 93.11358597
 97.12235593 96.50696365 97.05259486 94.4888756  98.17873782 98.7791813
 97.97443755 98.66457383 98.6222189  98.57488103 98.87385704 98.04170715
 98.6969629  97.58327728 90.58474724 96.80095672 96.22293644 97.00774846
 92.49321075 98.37556369 96.71873832 97.2220146  98.6595909  97.3939258
 98.4428333  95.7321175  98.75924957 97.89471062 99.81563146 97.09993273
 98.14634876 95.56518923 96.86324339 97.34658794 99.24508558 98.31576849
 99.82559733 99.15040985]
Accuracy th:0.7 is [82.64942572 97.23945487 91.03570272 98.01928395 98.65460797 97.14228766
 97.90218502 95.04945561 97.63310661 96.56675885 98.5250517  98.80409597
 99.34972718 95.11423375 97.27682687 96.44965991 96.21297058 97.50604181
 98.78914717 98.33819169 98.53501756 99.15290131 99.53409572 98.63467623
 95.43314149 96.52938685 94.43157187 96.89314099 97.81747515 98.12392555
 98.39798689 98.67952263 96.49450632 98.29085383 98.5325261  98.76672397
 97.37399407 98.12890849 98.37307223 92.80713556 97.86232155 92.81959289
 97.08249246 96.47955752 97.03017166 94.19986546 98.18372076 98.7567581
 97.96447168 98.6820141  98.48518823 98.5624237  98.87385704 98.22109276
 98.6969629  97.58576874 90.1537235  96.74863592 96.19802178 96.90808979
 92.20669208 98.21860129 96.52689538 97.18962553 98.5997957  97.41385754
 98.37307223 95.7769639  98.73682637 97.72279941 99.81563146 96.69133219
 98.05167302 95.59259536 96.77355059 97.28430127 99.24757705 98.25348182
 99.82559733 99.15040985]
Avg Prec: is [94.48297837 20.90244165 62.06366169 64.58516072 66.38909954 56.7986725
 69.1876892  39.86061244 46.50648409 41.63730294 21.67774502 43.44949121
 11.01678761 20.11837087 24.76840727 43.11885467 20.75254498 26.20933156
 32.7260449  25.75719143 52.71532235 40.08485491 89.19566789 72.18925084
 21.10908113 21.45459415 29.88536391 33.31823394 15.04819977 30.16699151
 70.30935864 30.9129196  51.59436208 52.88675777 66.845453   75.09652081
 43.4495454  70.41171637 84.63556455 39.70824275 28.38713556 48.32552649
 37.67770301 34.13726395 30.9470181  45.87849207 20.22030432 19.21863396
 31.76336757 33.58409887 56.08617539 25.20290857 15.77810778 68.42969449
 12.16397096 25.1580877  51.48870356 46.29116167 27.41617162 40.83849144
 63.15417299 77.44706525 52.62852492 43.00659726 51.99329227 27.73901527
 48.00353298 19.12960284 35.33026865 55.75350091 10.19137415 64.37471724
 43.95566174 35.46673042 53.76591801 43.17167535  5.32212453 30.14290225
  1.55306477 12.22717238]
Accuracy th:0.5 is [45.29984802 97.22450607 72.07564093 96.96290206 97.90716795 77.71133867
 77.83342053 76.56526397 79.20123577 96.41976231 79.46533124 98.5325261
 99.34972718 78.92468296 79.2884371  96.31262924 96.21047911 78.78516082
 98.78167277 98.34068316 79.95615019 80.09567232 98.31327703 78.86488776
 79.01437576 96.52938685 94.3393876  78.79761816 97.81747515 79.40304457
 97.52597354 98.67204823 96.39983058 98.18870369 87.35082343 79.01437576
 78.91720856 91.85539527 97.0276802  75.89755089 78.97700376 92.37362035
 78.12243067 77.77113387 97.03764606 94.02795426 98.18621222 98.77668984
 89.84478162 87.95126691 85.70147246 98.55993223 98.87385704 78.17724294
 98.6969629  78.80010962 72.43939507 94.30949    96.16314124 96.78102499
 90.13379176 97.04761193 90.81147071 78.71789122 98.32075143 79.55751551
 98.13139996 77.8583352  80.07823206 97.53593941 80.59645713 96.07843137
 79.76929018 95.44559882 77.79604853 83.76809428 87.58003837 79.44539951
 80.6662182  99.15040985]
Accuracy th:0.7 is [45.50913123 97.22450607 72.07564093 96.96290206 97.90716795 77.71133867
 77.83342053 76.58768717 79.20123577 96.41976231 79.46533124 98.5325261
 99.34972718 79.33826644 79.2884371  96.31262924 96.21047911 78.78516082
 98.78167277 98.34068316 80.26260059 80.09567232 98.31327703 78.86737923
 79.37812991 96.52938685 94.3393876  78.79761816 97.81747515 79.40304457
 97.52597354 98.67204823 96.39983058 98.18870369 87.61242744 79.01437576
 78.91720856 92.03228941 97.0276802  75.89755089 79.27847124 92.37362035
 78.12243067 77.77113387 97.03764606 94.02795426 98.18621222 98.77668984
 91.837955   88.27017465 85.85096046 98.55993223 98.87385704 78.17724294
 98.6969629  78.80010962 72.43939507 94.571094   96.16314124 96.78102499
 90.13379176 97.04761193 90.97341605 78.71789122 98.32075143 79.55751551
 98.13139996 77.8583352  80.07823206 97.53593941 80.59645713 96.07843137
 79.76929018 95.44559882 77.79604853 83.86027855 87.69215437 79.44539951
 80.6662182  99.15040985]
Avg Prec: is [53.3330476   3.68470975 14.92592258  4.53297214  1.46896253  4.46328594
 14.63450384  8.72495089  8.61255731  5.39050918  3.4945337   5.41581759
  2.39034579  5.78638411  3.02347843  3.76145939 16.05325545  6.50010875
  1.5705622   3.31781122  3.48254738  1.58377468  1.11685597  5.13719328
  5.5297998   7.84824907  7.71789272  4.58706394  3.87166649  4.40448197
  2.10361601  0.84312464  2.97727642  1.10300713  1.61954993  2.11456633
  1.94702058  2.18456933  2.24375109  6.17602964  1.71886506  5.98691449
  2.16016699  2.69083299  2.3688674   4.82796793  1.67512089  1.00877632
  1.36651417  1.1566279   1.1970241   0.94543644  0.73008826  2.28803116
  0.83888614  1.81596023  9.78311211  2.82671261  3.69061723  2.61782026
  7.71099094  2.084801    3.0468986   2.50028669  1.34262918  1.81225681
  1.48992537  3.4214212   1.10529661  2.29143198  0.19081802  3.32556192
  1.61166787  3.84608558  3.53727523  2.30624508  0.59192578  1.48209103
  0.12775011  0.59209782]
mAP score regular 41.20, mAP score EMA 4.20
Train_data_mAP: current_mAP = 37.60, highest_mAP = 37.60
Val_data_mAP: current_mAP = 41.20, highest_mAP = 41.20
tensor([0.2715, 0.1125, 0.2821, 0.1240, 0.0473, 0.0791, 0.0306, 0.0621, 0.1226,
        0.0597, 0.0227, 0.0296, 0.0196, 0.0807, 0.1210, 0.0947, 0.1288, 0.0825,
        0.0456, 0.0922, 0.0127, 0.0152, 0.0294, 0.0237, 0.0659, 0.0563, 0.1965,
        0.0447, 0.0347, 0.0452, 0.0454, 0.0223, 0.1635, 0.0081, 0.0586, 0.0491,
        0.0380, 0.0664, 0.0761, 0.2390, 0.4422, 0.5376, 0.2364, 0.4102, 0.6562,
        0.5691, 0.0311, 0.0434, 0.0392, 0.0589, 0.0118, 0.0199, 0.0310, 0.0653,
        0.0315, 0.0686, 0.4517, 0.1280, 0.2226, 0.0302, 0.7146, 0.1090, 0.2947,
        0.1327, 0.0896, 0.0795, 0.1530, 0.1138, 0.2976, 0.2421, 0.0567, 0.1126,
        0.5567, 0.2593, 0.6230, 0.9293, 0.5312, 0.8377, 0.0566, 0.1374],
       device='cuda:0')
Sum Train Loss:  tensor([16.4867,  1.4058,  5.8931,  1.7261,  0.6741,  0.9331,  0.3906,  1.1034,
         1.0022,  0.8826,  0.1094,  0.2866,  0.1116,  2.0172,  2.4518,  1.8223,
         2.7484,  0.4266,  0.3022,  0.6760,  0.0346,  0.1472,  0.1443,  0.3028,
         0.7049,  0.5772,  5.4686,  0.3593,  0.3710,  0.7272,  0.3013,  0.1374,
         1.6498,  0.0196,  0.4799,  0.1027,  0.5711,  0.7847,  0.6156,  5.0855,
         2.5538, 12.8328,  1.3464,  4.5709,  4.7392, 11.9817,  0.0516,  0.0758,
         0.1822,  0.1242,  0.0230,  0.1494,  0.1068,  0.1601,  0.2570,  0.3548,
         9.5701,  1.4553,  3.0729,  0.2413, 15.4250,  1.9196,  2.7649,  2.1658,
         0.1760,  0.6279,  0.7892,  1.8084,  1.9548,  1.5007,  0.0234,  0.9154,
         4.1688,  4.5028,  3.4601,  8.4664,  7.5105, 11.8966,  0.3022,  0.1245],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [5/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 184.4
Sum Train Loss:  tensor([1.6301e+01, 2.2029e+00, 9.5681e+00, 2.6667e+00, 4.4801e-01, 7.7561e-01,
        5.2574e-01, 1.3511e+00, 2.7524e+00, 1.0285e+00, 4.0601e-01, 2.8194e-01,
        2.2318e-01, 1.7614e+00, 8.5189e-01, 3.8100e-01, 2.4525e+00, 9.8578e-01,
        3.1086e-01, 5.1154e-01, 1.1080e-01, 4.3046e-02, 2.3918e-01, 1.8395e-01,
        1.4632e+00, 3.6569e-01, 4.4153e+00, 6.4311e-01, 7.6646e-02, 4.6545e-01,
        1.8348e-01, 1.1888e-01, 1.4745e+00, 7.3650e-02, 5.6750e-01, 2.9244e-01,
        5.6627e-01, 6.2149e-01, 4.1284e-01, 3.3440e+00, 1.1798e+00, 7.1243e+00,
        1.0663e+00, 3.5432e+00, 2.9748e+00, 7.7884e+00, 3.5394e-01, 3.0729e-01,
        2.6649e-01, 4.4866e-01, 1.9083e-02, 2.1245e-02, 1.9551e-02, 2.2322e-01,
        1.7802e-01, 6.5508e-01, 9.7189e+00, 1.0184e+00, 2.5936e+00, 2.3147e-01,
        1.3374e+01, 4.5901e-01, 2.3234e+00, 1.0316e+00, 1.8150e-01, 4.4774e-01,
        3.4129e-01, 2.0224e+00, 1.8962e+00, 1.1004e+00, 5.8895e-03, 1.2238e+00,
        4.6266e+00, 1.7638e+00, 6.3359e+00, 9.9502e+00, 5.3461e+00, 8.9489e+00,
        8.5915e-03, 1.6688e+00], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [5/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 164.2
Sum Train Loss:  tensor([1.1673e+01, 1.7071e+00, 6.4502e+00, 7.9715e-01, 5.0224e-01, 4.4271e-01,
        3.4606e-01, 4.6998e-01, 4.3386e-01, 8.3941e-01, 1.1503e-01, 3.5005e-01,
        2.1827e-01, 1.7549e+00, 2.6173e+00, 6.9404e-01, 2.0805e+00, 8.6757e-01,
        7.5683e-01, 9.6992e-01, 1.0025e-01, 2.3038e-02, 2.1292e-01, 2.1822e-01,
        7.6293e-01, 7.0174e-01, 4.3173e+00, 4.5877e-01, 2.1065e-01, 4.6190e-01,
        1.0309e-01, 1.8181e-01, 1.0140e+00, 3.1515e-02, 3.3038e-01, 8.0057e-02,
        1.0518e-01, 1.3776e-01, 3.4276e-01, 5.6583e+00, 3.8260e+00, 1.3411e+01,
        2.5470e+00, 5.4587e+00, 9.6955e+00, 1.9327e+01, 4.7541e-01, 7.1821e-01,
        4.0521e-01, 7.6594e-01, 1.4366e-01, 1.3656e-01, 2.0971e-01, 2.5545e-01,
        2.4728e-01, 6.8567e-01, 1.1704e+01, 1.1036e+00, 5.2981e+00, 2.2012e-01,
        1.7504e+01, 4.0012e-01, 4.9095e+00, 6.9668e-01, 1.6086e-01, 6.3574e-01,
        6.9851e-01, 8.4550e-01, 2.0844e+00, 1.7852e+00, 1.4471e-02, 1.5988e+00,
        3.0253e+00, 3.2363e+00, 1.4902e+01, 6.3947e+00, 6.5875e+00, 3.1610e+00,
        2.6934e-01, 5.5004e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [5/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 195.6
Sum Train Loss:  tensor([1.2469e+01, 2.3907e+00, 9.4796e+00, 8.6175e-01, 8.5923e-02, 1.2061e+00,
        3.6519e-01, 1.0618e+00, 8.0089e-01, 5.7200e-01, 1.1657e-01, 2.8493e-01,
        1.6757e-02, 1.8605e+00, 1.3125e+00, 6.4875e-01, 3.1499e+00, 1.0470e+00,
        2.7089e-01, 4.7872e-01, 1.6335e-02, 8.7257e-03, 3.1016e-02, 1.3069e-01,
        1.0820e+00, 1.0796e+00, 3.1559e+00, 4.0528e-01, 3.0397e-01, 4.2203e-01,
        1.5128e-01, 1.4821e-01, 2.1811e+00, 7.2038e-02, 2.8843e-01, 1.1466e-01,
        5.5161e-01, 5.8104e-01, 7.0639e-01, 8.0006e+00, 6.3991e+00, 1.6219e+01,
        2.1661e+00, 3.8549e+00, 7.6218e+00, 9.1255e+00, 1.6945e-01, 2.7901e-01,
        5.1526e-01, 2.1123e-01, 3.9228e-02, 2.2342e-01, 4.8049e-01, 6.7402e-01,
        3.2148e-01, 5.5888e-01, 1.1521e+01, 1.0611e+00, 2.8188e+00, 3.4368e-01,
        2.2972e+01, 6.2827e-01, 3.5169e+00, 8.5113e-01, 1.6499e-01, 9.6166e-01,
        9.5318e-01, 1.1926e+00, 2.0251e+00, 1.3981e+00, 2.2561e-01, 8.1252e-01,
        2.0226e+00, 4.8627e+00, 5.3411e+00, 9.2113e+00, 4.3551e-01, 9.4653e+00,
        9.5402e-03, 1.9637e+00], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [5/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 191.5
Sum Train Loss:  tensor([12.3261,  1.0851,  6.1600,  0.8662,  0.1634,  0.8364,  0.0683,  1.1048,
         0.5706,  0.2508,  0.0931,  0.2511,  0.1286,  1.4142,  1.3250,  1.1379,
         1.4889,  0.9548,  0.3571,  0.5045,  0.1088,  0.0821,  0.0316,  0.1660,
         2.0410,  1.3622,  4.4616,  0.7314,  0.3805,  0.1657,  0.2145,  0.1614,
         2.2608,  0.0573,  0.1362,  0.1631,  0.5032,  0.3183,  0.4522,  6.9511,
         4.2519, 14.7486,  2.4531,  6.6284,  3.3512, 12.0323,  0.3509,  0.2658,
         0.2887,  0.8413,  0.1135,  0.2580,  0.1933,  0.5085,  0.0341,  0.9618,
        12.5854,  1.8528,  3.0914,  0.5505, 14.6735,  0.4156,  3.3280,  1.6482,
         0.4542,  0.7784,  1.3917,  1.7708,  1.3142,  1.4839,  0.5678,  1.2149,
         2.8687,  4.6021, 11.2398,  5.1193,  0.5482,  8.8831,  0.3843,  0.7745],
       device='cuda:0', grad_fn=<NegBackward0>)
Epoch [5/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 180.7
Sum Train Loss:  tensor([1.4107e+01, 1.3139e+00, 9.0775e+00, 1.2366e+00, 5.6792e-01, 6.5153e-01,
        5.4372e-01, 9.1379e-01, 1.9152e+00, 4.3356e-01, 2.4782e-01, 4.3414e-02,
        1.5623e-01, 2.5461e+00, 9.7487e-01, 7.1864e-01, 2.1871e+00, 7.1664e-01,
        6.3335e-02, 5.7401e-01, 1.1850e-01, 7.9155e-02, 1.1071e-02, 1.3869e-01,
        2.0102e+00, 8.2930e-01, 5.1532e+00, 6.1680e-01, 2.4562e-01, 1.3846e-01,
        6.5113e-01, 2.2208e-01, 3.6117e+00, 4.8984e-02, 6.7299e-01, 1.8641e-01,
        6.5714e-01, 6.8581e-01, 2.6286e-01, 7.0970e+00, 4.0645e+00, 1.0122e+01,
        2.3139e+00, 5.5556e+00, 1.0502e+01, 6.8236e+00, 2.5041e-01, 1.3609e-01,
        4.8019e-01, 6.5654e-01, 3.6820e-02, 1.4172e-01, 5.1772e-01, 3.6026e-01,
        2.8981e-01, 9.0123e-01, 1.2941e+01, 1.1134e+00, 4.3594e+00, 3.9114e-01,
        1.7290e+01, 3.9912e-01, 1.9725e+00, 6.7767e-01, 2.9649e-01, 1.1458e+00,
        6.2158e-01, 2.1542e+00, 1.0361e+00, 1.6355e+00, 2.5664e-02, 8.6364e-01,
        2.8581e+00, 3.6052e+00, 8.9246e+00, 1.0516e+01, 3.9707e+00, 1.0044e+01,
        3.5393e-03, 8.7636e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [5/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 193.3
Sum Train Loss:  tensor([1.3941e+01, 8.1706e-01, 8.6450e+00, 1.9453e+00, 1.1933e-01, 5.9557e-01,
        2.9514e-01, 8.5316e-01, 4.8849e-01, 6.2811e-01, 1.4671e-01, 4.2620e-02,
        8.0470e-02, 1.0919e+00, 6.7818e-01, 1.2319e+00, 1.7184e+00, 5.8309e-01,
        3.2753e-01, 1.5122e-01, 9.6125e-02, 7.3083e-02, 1.1404e-01, 1.8699e-01,
        1.6622e+00, 1.0895e+00, 4.8613e+00, 4.9015e-01, 6.7704e-01, 3.8028e-01,
        4.3120e-01, 2.9021e-01, 6.2717e-01, 7.8449e-02, 3.2174e-01, 3.8215e-01,
        2.1179e-01, 4.2300e-01, 2.3032e-01, 7.4425e+00, 2.8065e+00, 1.7451e+01,
        3.3097e+00, 3.1298e+00, 1.2609e+01, 1.0743e+01, 3.5620e-01, 6.3210e-02,
        3.3393e-01, 5.5756e-01, 1.1890e-02, 3.6074e-02, 1.6844e-01, 4.9310e-01,
        2.2483e-01, 1.0282e+00, 9.8475e+00, 1.2828e+00, 2.7809e+00, 3.1567e-01,
        1.6167e+01, 1.6754e+00, 4.6983e+00, 2.3192e+00, 1.1959e-01, 5.3407e-01,
        1.1582e+00, 1.8438e+00, 2.4258e+00, 3.7831e+00, 1.2059e-02, 1.6715e+00,
        2.2745e+00, 7.2842e+00, 5.9813e+00, 1.0485e+01, 3.1217e+00, 1.0364e+01,
        1.4909e-02, 6.7294e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [5/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 198.6
Sum_Val Meta Model:  tensor([1.5019e+01, 9.0386e+00, 2.4482e+01, 5.5202e+00, 1.7217e-01, 1.1122e+00,
        1.3619e-01, 1.1303e+00, 7.7802e-01, 7.7504e-01, 1.3282e-01, 3.3355e-01,
        1.2801e-01, 1.2734e+00, 9.9915e-01, 6.6168e-01, 6.0395e+01, 1.8152e-01,
        5.4258e-02, 2.1097e-01, 1.6640e-02, 1.1793e-02, 2.0421e-02, 2.8703e-02,
        1.8536e+00, 1.0409e+00, 5.5976e+00, 1.6889e-01, 3.2754e-01, 4.8841e-01,
        9.1056e-02, 1.8091e-02, 1.5154e+00, 1.0796e-02, 1.8438e-01, 1.3174e-01,
        1.6150e-01, 2.8771e-01, 9.8113e-02, 1.1492e+01, 5.5431e+00, 1.1821e+01,
        2.0699e+00, 6.8315e+00, 1.1392e+01, 1.2486e+01, 8.2185e-02, 2.4188e-01,
        7.5248e-02, 3.3997e-01, 1.0860e-02, 2.6595e-02, 2.3345e-01, 1.0070e-01,
        3.6618e-02, 1.4740e-01, 1.3223e+01, 9.6416e-01, 3.7135e+00, 1.7131e-01,
        1.1547e+01, 2.3552e+00, 1.8788e+00, 6.4537e-01, 7.5388e-02, 1.4551e-01,
        2.1409e-01, 9.9572e-01, 2.7438e+00, 4.1933e+00, 1.6616e-02, 1.6037e+00,
        8.4566e+00, 3.3532e+00, 8.2504e+00, 8.2458e+00, 5.2284e-01, 6.7849e+00,
        8.5663e-03, 9.2926e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.6018e+01, 6.9423e+00, 1.9217e+01, 3.8521e+00, 6.3974e-02, 1.0641e+00,
        1.5727e-01, 9.8986e-01, 7.2484e-01, 7.7340e-01, 1.3418e-01, 3.2030e-01,
        1.4829e-01, 1.1677e+00, 9.1713e-01, 2.2560e+00, 3.7642e+01, 3.8659e-01,
        7.2345e-02, 4.4867e-01, 3.3765e-02, 3.7071e-02, 8.9940e-03, 2.2897e-02,
        1.7702e+00, 1.0166e+00, 5.1851e+00, 1.6760e-01, 3.6633e-01, 5.0121e-01,
        5.5438e-02, 1.1490e-02, 1.3750e+00, 1.5584e-02, 1.9109e-01, 1.1738e-01,
        3.4813e-01, 1.2936e-01, 4.2411e-02, 1.1538e+01, 5.5532e+00, 1.2871e+01,
        2.0673e+00, 6.1936e+00, 1.0901e+01, 1.1003e+01, 5.5361e-02, 2.0457e-01,
        3.2315e-02, 2.6554e-01, 2.7879e-03, 1.2254e-02, 1.8829e-01, 2.9898e-02,
        1.9509e-02, 7.1496e-02, 1.2330e+01, 1.1440e+00, 3.2680e+00, 3.1649e-01,
        1.0208e+01, 1.1985e+00, 1.4415e+00, 6.2314e-01, 4.1614e-02, 1.4872e-01,
        1.2507e-01, 1.0750e+00, 3.1675e+00, 4.1735e+00, 1.8420e-02, 1.7386e+00,
        8.4360e+00, 3.1601e+00, 8.6077e+00, 5.8507e+00, 4.9176e-01, 8.9961e+00,
        6.8830e-03, 1.0022e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.9000e+01, 6.1723e+01, 6.8112e+01, 3.1075e+01, 1.3519e+00, 1.3450e+01,
        5.1447e+00, 1.5931e+01, 5.9130e+00, 1.2953e+01, 5.9052e+00, 1.0806e+01,
        7.5593e+00, 1.4476e+01, 7.5795e+00, 2.3810e+01, 2.9231e+02, 4.6883e+00,
        1.5866e+00, 4.8683e+00, 2.6551e+00, 2.4393e+00, 3.0551e-01, 9.6734e-01,
        2.6844e+01, 1.8065e+01, 2.6390e+01, 3.7480e+00, 1.0555e+01, 1.1077e+01,
        1.2202e+00, 5.1626e-01, 8.4100e+00, 1.9201e+00, 3.2636e+00, 2.3905e+00,
        9.1500e+00, 1.9483e+00, 5.5745e-01, 4.8284e+01, 1.2557e+01, 2.3939e+01,
        8.7456e+00, 1.5100e+01, 1.6613e+01, 1.9334e+01, 1.7830e+00, 4.7188e+00,
        8.2357e-01, 4.5070e+00, 2.3529e-01, 6.1608e-01, 6.0750e+00, 4.5802e-01,
        6.1881e-01, 1.0419e+00, 2.7301e+01, 8.9368e+00, 1.4684e+01, 1.0482e+01,
        1.4284e+01, 1.0992e+01, 4.8922e+00, 4.6947e+00, 4.6447e-01, 1.8715e+00,
        8.1750e-01, 9.4437e+00, 1.0642e+01, 1.7238e+01, 3.2514e-01, 1.5439e+01,
        1.5153e+01, 1.2188e+01, 1.3817e+01, 6.2956e+00, 9.2583e-01, 1.0740e+01,
        1.2160e-01, 7.2936e-01], device='cuda:0')
Outer loop valEpocw Maximum [5/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 277.7
model_train val_loss valEpocw [5/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 242.4
model_train val_loss valEpocw [5/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1158.6
Sum_Val Meta Model:  tensor([1.8984e+01, 1.9302e+00, 2.0165e+01, 1.2614e+00, 5.3956e-02, 3.5959e+00,
        5.2724e+00, 4.5021e+00, 5.5630e+00, 2.4047e+00, 5.3923e-01, 9.6425e+00,
        2.6934e-01, 6.1212e-01, 2.2046e+00, 2.1138e+00, 1.7688e+00, 1.5492e+00,
        5.5047e-01, 3.9490e+00, 1.0134e-02, 3.9448e-03, 3.6369e-02, 1.6040e-01,
        1.9169e+00, 1.4502e+00, 6.1594e+00, 9.3582e-01, 6.6154e-01, 1.8446e-02,
        2.8978e-02, 9.5999e-03, 9.6262e-02, 6.7809e-03, 1.6346e-02, 1.3235e-02,
        3.3265e-01, 3.7438e-02, 2.4011e-02, 6.4580e-01, 2.3294e-01, 2.8962e+00,
        1.3974e-01, 2.7064e-01, 4.5542e-01, 2.2282e+00, 7.7512e-02, 4.6274e-02,
        2.0044e-02, 3.6661e-01, 4.9744e-03, 1.2947e-02, 8.6099e-03, 1.8780e-02,
        2.2467e-02, 3.6876e-01, 6.6983e+00, 1.0895e+00, 2.0611e+00, 1.7142e-01,
        2.5189e+00, 8.6915e-02, 9.1001e-01, 4.0248e-01, 9.2554e-02, 1.3715e-01,
        1.8626e-01, 1.5484e+00, 1.6394e-01, 1.1966e+00, 1.3767e-02, 1.1885e-01,
        2.4547e+00, 1.8028e+00, 3.1401e+00, 7.5369e-01, 2.0196e-01, 1.0075e+00,
        7.4134e-03, 4.6946e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.8831e+01, 1.9286e+00, 1.5215e+01, 1.8211e+00, 3.7808e-01, 3.2608e+00,
        4.3030e+00, 3.8716e+00, 4.7380e+00, 2.0489e+00, 5.0879e-01, 4.7142e+00,
        2.3596e-01, 6.9588e-01, 2.0410e+00, 3.6156e-01, 1.6891e+00, 1.5150e+00,
        1.2197e-01, 1.6469e+00, 2.7816e-02, 2.7656e-02, 3.6533e-02, 1.4934e-01,
        1.8499e+00, 1.3148e+00, 6.1393e+00, 8.8280e-01, 6.3253e-01, 9.5046e-02,
        5.1458e-02, 7.9200e-03, 1.1484e-01, 3.2396e-02, 4.9817e-02, 2.6696e-02,
        2.4649e-01, 3.4225e-01, 2.2823e-02, 4.8954e-01, 1.4344e-01, 2.0558e+00,
        1.2812e-01, 2.7386e-01, 3.0513e-01, 1.0598e+00, 4.3161e-02, 3.8817e-02,
        2.8877e-02, 2.5315e-01, 3.7979e-03, 1.1029e-02, 1.2314e-02, 3.5890e-02,
        1.1906e-02, 2.1871e-01, 4.6708e+00, 5.4800e-01, 1.4670e+00, 4.1712e-02,
        3.0980e+00, 6.2576e-02, 2.0947e-01, 6.7546e-02, 1.2583e-02, 2.3139e-02,
        3.1815e-02, 1.6979e+00, 5.2115e-02, 1.0013e+00, 1.5202e-03, 5.4696e-02,
        1.8665e+00, 9.5008e-01, 2.9862e+00, 4.8176e-01, 3.2214e-01, 9.9296e-01,
        1.3116e-03, 2.2069e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([7.1792e+01, 1.5991e+01, 4.9283e+01, 1.2986e+01, 7.7331e+00, 3.1665e+01,
        7.3247e+01, 4.5563e+01, 3.3324e+01, 3.1607e+01, 1.7067e+01, 1.0814e+02,
        8.1494e+00, 8.1185e+00, 1.4686e+01, 2.8445e+00, 1.1420e+01, 1.4707e+01,
        1.6810e+00, 1.1787e+01, 1.4359e+00, 1.3759e+00, 8.5030e-01, 5.0400e+00,
        2.3863e+01, 1.8548e+01, 3.0741e+01, 1.4442e+01, 1.3372e+01, 1.7164e+00,
        8.9993e-01, 2.5701e-01, 7.1262e-01, 2.4960e+00, 7.8114e-01, 5.6651e-01,
        5.2050e+00, 4.3529e+00, 2.9017e-01, 2.4923e+00, 4.2822e-01, 4.5074e+00,
        5.5637e-01, 7.9550e-01, 5.2520e-01, 2.0280e+00, 9.7914e-01, 7.2105e-01,
        5.3049e-01, 3.4080e+00, 2.0395e-01, 4.1940e-01, 3.1909e-01, 4.9507e-01,
        2.7246e-01, 2.6856e+00, 1.1211e+01, 3.6843e+00, 6.8044e+00, 9.8084e-01,
        4.7041e+00, 5.9986e-01, 7.0200e-01, 4.5891e-01, 1.2718e-01, 2.4635e-01,
        2.0076e-01, 1.6726e+01, 1.9832e-01, 4.7657e+00, 2.4437e-02, 5.3268e-01,
        4.0316e+00, 3.7100e+00, 5.6527e+00, 5.2166e-01, 5.4283e-01, 1.1752e+00,
        1.8975e-02, 1.6681e-01], device='cuda:0')
Outer loop valEpocw Maximum [5/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 133.5
model_train val_loss valEpocw [5/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 107.8
model_train val_loss valEpocw [5/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 787.9
Sum_Val Meta Model:  tensor([1.9756e+01, 4.6009e-01, 3.2179e+00, 3.4939e-01, 6.0141e-02, 2.1748e-01,
        5.7573e-02, 9.0033e-01, 3.6616e-01, 1.9143e-01, 4.9241e-02, 5.1281e-02,
        1.1986e-02, 1.6617e+00, 3.1579e-01, 3.0539e-01, 7.9705e-01, 2.0623e-01,
        7.5061e-02, 1.9021e-01, 1.8460e-02, 4.1136e-02, 3.0182e-01, 8.0923e-02,
        5.1084e-01, 1.3901e-01, 4.0043e+00, 3.6554e-01, 6.8982e-02, 1.7240e-01,
        2.7908e-01, 2.1339e-01, 1.7890e+00, 5.1067e-03, 2.0381e-01, 1.6460e-01,
        8.0144e-01, 1.0288e+00, 3.8328e-02, 6.7796e+00, 1.2031e+01, 3.9078e+01,
        2.2438e+01, 3.5397e+01, 3.5766e+01, 4.1409e+01, 7.5142e-01, 7.6731e-01,
        3.7475e+00, 1.4194e+00, 7.9301e-01, 1.3428e+00, 3.4307e+00, 1.4742e+00,
        3.0011e+00, 8.8063e+00, 7.1577e+01, 2.1083e+00, 1.8906e+00, 9.5030e-02,
        6.9184e+01, 1.7083e-01, 3.3947e+00, 1.9145e+00, 1.3520e+00, 1.0679e-01,
        1.5250e+00, 1.1690e+00, 1.3922e+00, 4.6347e-01, 2.6842e-02, 9.0249e-01,
        2.5197e+00, 8.9493e+00, 1.2398e+00, 1.3541e+01, 6.5772e+00, 1.0865e+00,
        1.6159e-02, 1.0366e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([7.7762e+00, 5.7714e-02, 1.2277e+00, 1.0347e-01, 1.4081e-02, 2.3001e-02,
        3.7148e-03, 9.8658e-01, 4.8344e-02, 1.2121e-02, 3.8173e-03, 2.1520e-03,
        1.1301e-03, 1.4249e+00, 6.3528e-02, 2.4711e-01, 5.4024e-01, 7.4231e-02,
        2.6808e-02, 8.1326e-02, 4.6636e-03, 6.6116e-03, 2.2095e-03, 1.6332e-03,
        4.2293e-01, 6.6263e-02, 4.2089e+00, 3.5583e-01, 3.8306e-02, 3.6690e-02,
        2.3153e-02, 2.3060e-01, 1.3597e+00, 1.8923e-03, 1.5303e-01, 1.0020e-01,
        7.1466e-01, 1.1569e+00, 1.8999e-02, 7.4095e+00, 9.6549e+00, 3.4636e+01,
        2.3170e+01, 3.2001e+01, 3.0996e+01, 4.1638e+01, 4.9203e-01, 4.3412e-01,
        3.1156e+00, 9.3827e-01, 4.7177e-01, 1.2888e+00, 2.9991e+00, 1.5890e+00,
        2.6353e+00, 8.5804e+00, 4.4564e+01, 2.3353e+00, 1.8889e+00, 1.7330e-01,
        5.7812e+01, 6.1739e-02, 2.6614e+00, 1.6688e+00, 1.2129e+00, 2.9314e-01,
        1.3248e+00, 1.2072e+00, 1.5755e+00, 8.5262e-01, 8.9968e-03, 5.8241e-01,
        3.2349e+00, 9.0001e+00, 8.6015e-01, 1.2597e+01, 5.5252e+00, 4.5105e+00,
        6.8612e-03, 1.2382e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.2284e+01, 5.4275e-01, 4.5031e+00, 8.8556e-01, 3.4089e-01, 2.8872e-01,
        1.3229e-01, 1.6171e+01, 4.0255e-01, 2.0341e-01, 1.5459e-01, 7.3584e-02,
        5.8016e-02, 1.7164e+01, 5.4688e-01, 2.6997e+00, 3.9307e+00, 8.5759e-01,
        5.5190e-01, 8.6441e-01, 4.1449e-01, 3.1841e-01, 5.0017e-02, 6.5759e-02,
        6.5500e+00, 1.3585e+00, 2.6973e+01, 9.0524e+00, 1.1954e+00, 7.2710e-01,
        4.5189e-01, 7.9178e+00, 7.7208e+00, 1.7596e-01, 2.4531e+00, 1.9899e+00,
        1.4731e+01, 1.4872e+01, 2.6001e-01, 3.6559e+01, 2.5556e+01, 6.4858e+01,
        6.9677e+01, 7.3278e+01, 4.6693e+01, 6.8206e+01, 1.1920e+01, 8.0646e+00,
        4.3954e+01, 1.2220e+01, 2.3841e+01, 4.2650e+01, 7.1043e+01, 2.2307e+01,
        6.6702e+01, 9.8064e+01, 1.0758e+02, 1.7735e+01, 9.7310e+00, 4.7404e+00,
        7.8121e+01, 6.5291e-01, 9.5409e+00, 1.3048e+01, 1.3975e+01, 3.5502e+00,
        8.9698e+00, 1.3615e+01, 6.9672e+00, 4.4946e+00, 1.7728e-01, 6.2664e+00,
        7.3145e+00, 3.9204e+01, 1.7084e+00, 1.3593e+01, 1.0078e+01, 5.4196e+00,
        1.2373e-01, 1.0241e+00], device='cuda:0')
Outer loop valEpocw Maximum [5/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 449.2
model_train val_loss valEpocw [5/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 377.8
model_train val_loss valEpocw [5/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1343.2
Sum_Val Meta Model:  tensor([2.6314e+01, 1.7486e-01, 5.7143e+00, 1.0303e-01, 2.4148e-02, 7.3192e-01,
        1.3454e-01, 8.0228e-01, 1.5036e-01, 7.9575e-01, 7.7016e-02, 1.0354e-01,
        3.2004e-03, 1.0774e+00, 1.4797e+00, 1.3536e-01, 2.1068e-01, 6.1286e-02,
        2.0492e-02, 4.4912e-02, 5.1827e-03, 5.5144e-03, 1.3335e-02, 1.7294e-02,
        1.1865e+00, 7.9593e-02, 4.2938e+00, 5.8078e-02, 3.6478e-01, 1.5130e-02,
        2.4489e-02, 3.4108e-03, 6.2597e-01, 3.0168e-02, 1.3906e-01, 2.2403e-01,
        1.2434e-02, 8.5099e-02, 3.3423e-01, 4.7526e+00, 4.7731e+00, 1.3793e+01,
        2.0395e+00, 3.6935e+00, 4.8154e+00, 7.2564e+00, 1.8094e-02, 2.5004e-02,
        5.7644e-02, 3.0974e-02, 1.0766e-02, 1.2923e-02, 1.0340e-02, 3.7201e-01,
        1.8708e-02, 1.0439e-01, 1.0656e+01, 5.2722e-01, 3.2474e+00, 4.9906e-02,
        2.8116e+01, 8.3158e-02, 1.5694e+00, 8.7874e-01, 4.3945e-01, 1.3165e-01,
        9.1138e-01, 5.4473e+00, 3.1616e-01, 6.1139e-01, 6.0094e-03, 1.5700e-01,
        2.3258e+00, 2.5819e+00, 2.2527e+02, 1.3157e+01, 7.8199e-01, 8.7024e+00,
        5.0170e-03, 1.4030e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.0661e+01, 2.2579e-01, 5.1370e+00, 3.4506e-01, 6.0023e-02, 5.2716e-01,
        1.7893e-01, 6.5702e-01, 1.3422e-01, 7.9948e-01, 5.0841e-02, 1.6133e-01,
        7.3304e-03, 1.0758e+00, 1.6911e+00, 1.3593e-01, 1.7228e-01, 5.9666e-02,
        6.8429e-03, 3.5294e-02, 4.3238e-03, 1.7339e-03, 1.4692e-02, 1.9188e-02,
        1.1433e+00, 1.0037e-01, 4.0525e+00, 8.0223e-02, 4.3468e-01, 9.4189e-03,
        2.0790e-02, 4.3797e-03, 6.3036e-02, 4.4987e-03, 2.1932e-02, 1.2790e-02,
        5.6195e-02, 1.1445e-02, 1.6269e-02, 6.2647e-01, 2.2976e-01, 1.1804e+00,
        1.2339e-01, 3.6432e-01, 3.4169e-01, 1.0132e+00, 1.7252e-02, 1.0631e-02,
        1.0163e-02, 1.2236e-02, 1.6044e-03, 3.4574e-03, 2.6852e-03, 1.1954e-02,
        5.4439e-03, 3.7563e-02, 2.4312e+00, 1.0651e-01, 2.3261e+00, 2.2573e-02,
        7.5589e+00, 4.7982e-02, 2.6613e-01, 7.0620e-02, 2.0451e-02, 3.9904e-02,
        6.0353e-02, 7.8580e-01, 9.7642e-02, 1.8215e-01, 1.1624e-03, 4.4814e-02,
        1.5164e-01, 1.2857e+00, 7.8901e+01, 2.0613e+00, 3.4637e-01, 4.4004e+00,
        1.1840e-03, 1.2730e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.3064e+01, 2.8047e+00, 2.1538e+01, 3.9066e+00, 1.9010e+00, 8.9193e+00,
        8.7370e+00, 1.4179e+01, 1.4464e+00, 1.9371e+01, 3.2226e+00, 8.0738e+00,
        5.7700e-01, 1.6423e+01, 1.8317e+01, 1.8764e+00, 1.6333e+00, 9.4981e-01,
        1.9498e-01, 5.2679e-01, 5.7655e-01, 1.5793e-01, 6.4616e-01, 1.1795e+00,
        2.3329e+01, 2.6562e+00, 2.6552e+01, 2.7226e+00, 1.9090e+01, 3.0662e-01,
        6.4114e-01, 2.9185e-01, 4.1219e-01, 6.9891e-01, 4.3222e-01, 2.5979e-01,
        2.1488e+00, 2.1431e-01, 2.1422e-01, 3.0506e+00, 5.7375e-01, 2.2612e+00,
        4.9104e-01, 9.4421e-01, 5.2972e-01, 1.8600e+00, 8.2465e-01, 3.7503e-01,
        3.6328e-01, 3.0633e-01, 2.2288e-01, 2.5151e-01, 1.3051e-01, 2.3194e-01,
        2.5679e-01, 7.5571e-01, 5.6885e+00, 9.2068e-01, 1.0559e+01, 9.9827e-01,
        1.0546e+01, 6.0825e-01, 9.0400e-01, 5.5549e-01, 2.2023e-01, 6.0160e-01,
        3.5826e-01, 6.7560e+00, 4.3493e-01, 9.7636e-01, 3.2676e-02, 5.8202e-01,
        3.0155e-01, 5.7465e+00, 1.4758e+02, 2.1431e+00, 5.6098e-01, 4.9045e+00,
        2.9956e-02, 1.0683e-01], device='cuda:0')
Outer loop valEpocw Maximum [5/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 393.6
model_train val_loss valEpocw [5/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 133.4
model_train val_loss valEpocw [5/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 475.7
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [82.86936075 97.22591099 91.98962001 97.7838964  98.18228335 97.35992495
 97.9374033  94.88066666 97.73272743 96.69472838 98.53071965 98.75610677
 99.41399349 95.31682119 97.37088973 96.7946297  96.30852451 97.49881215
 98.70981104 98.32482548 98.40767047 99.22150071 99.47734555 98.33579026
 95.18158892 96.64965095 94.23252641 96.80072124 98.01293844 98.21030446
 98.17131858 98.57701539 96.70203823 98.32604379 98.34066349 98.57701539
 97.20154482 98.15304394 98.37599444 93.01421766 97.89110756 93.02640075
 97.02489005 96.34020053 97.02245343 94.51517404 98.05923417 98.60016325
 97.97395256 98.64402237 98.57336046 98.58310693 98.99976852 98.0909102
 98.70615611 97.4634812  90.91141677 96.54243979 96.29999635 97.1296646
 92.54273218 98.24563541 96.66183404 97.21494621 98.64524068 97.41596715
 98.46858591 95.94790512 98.74270538 98.04339616 99.81603538 97.06387593
 98.21030446 95.68962366 96.93229858 97.35748833 99.18007822 98.0775088
 99.84405648 99.14718388]
Accuracy th:0.7 is [79.69201155 97.21616452 91.10512786 97.78755132 98.2297974  97.27829827
 97.76440346 94.75518086 97.6169881  96.6009186  98.53193796 98.6903181
 99.41399349 95.31682119 97.33799539 96.74224242 96.29999635 97.48175583
 98.66717023 98.32969871 98.31995224 99.21784579 99.46638077 98.18228335
 95.21691987 96.65086926 94.20450531 96.75929874 98.01293844 98.18471997
 97.98491734 98.57457877 96.45106663 98.25294526 98.52828304 98.63671252
 97.04316468 98.02755814 98.14086086 92.78761224 97.85943154 92.6158307
 96.92377042 96.2378626  96.96884785 94.40308963 98.03243138 98.59650833
 98.03243138 98.60503649 98.61356465 98.5648323  98.99976852 97.9654244
 98.70615611 97.46591781 90.32784688 96.41329906 96.30974282 97.05291115
 92.42455623 98.13111439 96.38527796 97.11260828 98.57336046 97.35017848
 98.37964937 95.95156004 98.70250119 97.9240019  99.81603538 96.74224242
 98.0909102  95.52149706 96.80437617 97.2405307  99.18007822 98.29924099
 99.84405648 99.14718388]
Avg Prec: is [93.45618219 22.38636812 61.81560279 56.65688602 66.85991828 57.03657091
 64.42947177 41.30984784 42.09491433 42.54662033 15.44398092 39.1883097
 12.24017572 19.64080109 23.55231779 38.43690307 19.10275176 26.1795355
 32.52473956 27.85078841 44.68545489 33.32266356 86.25709098 67.88802253
 21.21046805 18.56445646 31.89242379 26.06784168 13.38177599 30.58646398
 64.68422279 28.46862453 49.97579388 49.57549143 63.0387765  70.80091253
 41.46050698 67.63311345 77.89007077 39.4143428  26.6981495  50.43937289
 43.07982082 38.1141789  34.90347521 49.77911936 24.04393674 23.38615802
 31.71245944 33.58934539 50.68511202 26.37670086 10.30682099 64.79149965
 12.65781627 23.83981248 54.2616723  45.83139019 28.83130232 39.31893091
 66.11645218 70.58983817 51.60909549 38.50402674 47.90121963 31.24672525
 46.57168437 17.95031431 35.18488842 53.06149731  7.51248004 62.10079831
 48.42863572 37.05269616 48.82611808 46.75493096  9.15082343 31.30998871
  2.36418246 13.4142919 ]
Accuracy th:0.5 is [45.40393026 97.2137279  73.1362922  97.02489005 97.26733349 78.13501297
 78.5431464  77.2602673  79.4203287  96.43157369 79.80409595 98.52097319
 99.41399349 80.63498252 79.32651893 96.56680596 96.29512311 79.1839768
 98.65376884 98.30776915 80.76046832 80.28289129 98.38695922 79.24001901
 80.7580317  96.65086926 94.0778012  78.81726587 98.01293844 79.68226508
 97.30875598 98.57457877 96.36213009 98.02024829 86.70459668 79.30093444
 79.10722335 90.42287496 97.11504489 76.31851464 79.7553636  92.05906361
 78.53705486 78.16181577 96.9627563  93.87434364 98.02877645 98.57336046
 90.35343137 88.05082784 87.0834907  98.55508583 98.99976852 78.71614625
 98.70615611 79.06701916 73.63336217 93.27615404 96.24273583 96.9067141
 89.79300934 97.17717864 91.01131809 78.93422351 98.42838172 79.4617512
 98.20786784 78.16547069 80.20613784 97.55850928 80.78361618 95.99054592
 79.90399727 95.45083515 78.35187193 82.81819179 85.8432524  79.68348339
 80.86524287 99.14718388]
Accuracy th:0.7 is [45.40636688 97.2137279  73.1362922  97.02489005 97.26733349 78.13501297
 78.5431464  77.36991508 79.4203287  96.47177788 79.80409595 98.52097319
 99.41399349 81.01996808 79.32651893 96.56680596 96.29512311 79.1839768
 98.65376884 98.30776915 81.18443976 80.28289129 98.38695922 79.2668218
 81.24170027 96.65086926 94.0778012  78.81726587 98.01293844 79.68226508
 97.30875598 98.57457877 96.36213009 98.02024829 86.92511056 79.30093444
 79.10722335 90.7201423  97.11504489 76.31851464 80.39984893 92.05906361
 78.53705486 78.16181577 96.9627563  93.87434364 98.02877645 98.57336046
 92.35267602 88.76232015 87.25892716 98.55508583 98.99976852 78.71614625
 98.70615611 79.06701916 73.63336217 93.65626637 96.24273583 96.9067141
 89.79300934 97.17717864 91.24157844 78.93422351 98.42838172 79.4617512
 98.20786784 78.16547069 80.20613784 97.55972759 80.78361618 95.99054592
 79.9076522  95.45083515 78.35187193 82.90591002 85.94680864 79.68348339
 80.86524287 99.14718388]
Avg Prec: is [56.03414514  2.99151539 11.27277136  3.28871151  2.3463705   3.79458865
  3.23292226  5.52974853  2.42101919  3.90475433  1.61192863  1.59614843
  0.62323343  5.08336803  2.59357517  3.08857494  3.70045464  2.60072846
  1.42927275  1.70567774  1.99230655  0.90766864  1.84470014  2.33434498
  5.06310394  3.61637687  6.47667689  3.36756667  2.01481495  2.00745975
  2.55400921  1.3213223   3.85121899  1.63430147  2.49728607  2.4287377
  3.02017181  2.55110428  2.94809718  7.33045108  2.24691753  8.38081556
  3.3661946   4.00229238  3.25862046  6.57635339  2.09923158  1.51852167
  2.07821493  1.51244632  1.8361669   1.58705167  1.02855386  2.98132548
  1.31372267  2.69374726 11.28694829  3.77024577  3.96368295  2.94836578
 10.85737065  2.2075065   3.84471452  3.10929073  1.62856231  2.55974736
  1.84240075  4.06255419  1.26784355  2.39615796  0.28243623  3.38335018
  1.90775242  4.6519131   3.996386    3.1293292   0.84203037  1.9342956
  0.14068775  0.71150235]
mAP score regular 40.10, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [84.70737723 97.23447193 92.47327902 97.95450582 98.64215063 97.45621247
 98.12143409 95.11921668 97.88972768 96.73368712 98.5250517  98.90126317
 99.35221865 95.11423375 97.35157087 96.82088846 96.19553031 97.57081994
 98.85890824 98.24849889 98.64215063 99.21020505 99.59139946 98.59481277
 95.26123029 96.54433565 94.25467773 96.93549593 97.81747515 98.11894262
 98.5400005  98.67204823 96.80095672 98.5773725  98.70194584 98.94860104
 97.62314074 98.19119516 98.73184344 93.09116277 97.88474475 93.38764731
 97.13730473 96.54184418 97.042629   94.46396093 98.11146822 98.69447144
 97.91464235 98.6820141  98.63467623 98.5773725  98.87385704 98.32573436
 98.6969629  97.57829434 90.55983257 96.88815806 96.27027431 97.04761193
 92.35867155 98.51259436 96.80593966 97.266861   98.6296933  97.50105887
 98.46774796 95.75204923 98.77419837 98.09651942 99.81563146 97.35904527
 98.25348182 95.59010389 96.91805566 97.42880634 99.24757705 98.02675835
 99.82559733 99.15040985]
Accuracy th:0.7 is [82.83130279 97.22450607 91.78065127 98.10648529 98.76174104 97.45870394
 97.95948875 94.90993348 97.68791888 96.59914792 98.52754316 98.83648504
 99.34972718 95.11423375 97.27931833 96.70378952 96.22293644 97.50604181
 98.82651917 98.34566609 98.5698981  99.24010265 99.58641652 98.39798689
 95.43314149 96.52938685 94.50631587 96.81839699 97.81747515 98.20614396
 98.41542716 98.67204823 96.56177592 98.47771383 98.82153624 98.97849864
 97.48611007 98.05665595 98.44532476 92.83952463 97.84737275 92.91426863
 97.08249246 96.47706605 97.0351546  94.43904627 98.20614396 98.7642325
 97.99187782 98.71689464 98.7193861  98.57986397 98.87385704 98.26344769
 98.6969629  97.58576874 90.32065177 96.71126392 96.21795351 97.08498393
 92.57293769 98.34068316 96.45713431 97.12983033 98.55993223 97.42880634
 98.38054663 95.7769639  98.7492837  98.00682662 99.81563146 97.15723647
 98.16129756 95.49542816 96.82337992 97.41884047 99.24757705 98.34317463
 99.82559733 99.15040985]
Avg Prec: is [94.4370455  23.41824952 64.81226667 67.33928925 69.26459481 59.29803217
 71.69424911 42.14303202 49.86178553 43.93934441 24.00273862 47.25149532
 14.90786139 21.9079651  26.23659563 48.24639299 23.26697118 30.82635633
 35.58779699 27.50036577 56.59145418 44.14027552 90.50316836 76.0168835
 21.83029263 23.39955151 30.79692516 32.15760485 16.42108385 32.71917917
 72.35524849 30.92040657 52.49911951 53.52330558 69.52309686 77.86193922
 47.35023757 72.88375662 86.33278684 40.64235963 28.61829501 49.4873454
 40.21307067 35.56570959 30.48838709 46.6696717  22.32850031 19.9576334
 33.55752108 35.75051202 57.8329771  26.74797788 15.48868432 71.11239375
 15.92571415 24.70070938 52.99661593 47.90518818 29.2647578  45.78103778
 64.34708658 78.68494218 56.64038224 44.72283489 54.52993004 30.43558909
 52.46151546 19.21076772 36.2618915  58.62249176  9.53633017 67.97948909
 47.56344447 36.27825472 56.52515938 46.8433105   6.90953549 33.96594176
  1.71473988 14.88342636]
Accuracy th:0.5 is [45.30732242 97.22450607 71.96352493 96.96290206 97.90716795 77.56932506
 77.69638986 76.43072477 79.0642051  96.41976231 79.32331764 98.5325261
 99.34972718 78.85492189 79.1464235  96.31262924 96.21047911 78.64813015
 98.78167277 98.34068316 79.87393178 79.95864165 98.31327703 78.73533149
 78.97202083 96.52938685 94.3393876  78.66058749 97.81747515 79.2660139
 97.52597354 98.67204823 96.39983058 98.18870369 87.48287117 78.88731096
 78.78017789 91.87781847 97.0276802  75.76550315 78.84495603 92.37362035
 77.99038294 77.6341032  97.03764606 94.02795426 98.18621222 98.77668984
 91.04068565 87.95375838 85.71143832 98.55993223 98.87385704 78.03522934
 98.6969629  78.67304482 72.3173132  94.30949    96.16314124 96.78102499
 90.13379176 97.04761193 90.90863792 78.57587762 98.32075143 79.43543364
 98.13139996 77.73625333 79.94120139 97.53593941 80.45444353 96.07843137
 79.63724244 95.44559882 77.67396666 83.76311134 87.67222264 79.30836884
 80.5242046  99.15040985]
Accuracy th:0.7 is [45.52158856 97.22450607 71.96352493 96.96290206 97.90716795 77.56932506
 77.69638986 76.45314797 79.0642051  96.41976231 79.32331764 98.5325261
 99.34972718 79.2884371  79.1464235  96.31262924 96.21047911 78.64813015
 98.78167277 98.34068316 80.19283952 79.95864165 98.31327703 78.73782296
 79.30836884 96.52938685 94.3393876  78.66058749 97.81747515 79.2660139
 97.52597354 98.67204823 96.39983058 98.18870369 87.70959464 78.88731096
 78.78017789 92.05471261 97.0276802  75.76550315 79.18130403 92.37362035
 77.99038294 77.6341032  97.03764606 94.02795426 98.18621222 98.77668984
 92.93420036 88.27515759 85.86092633 98.55993223 98.87385704 78.03522934
 98.6969629  78.67304482 72.3173132  94.5785684  96.16314124 96.78102499
 90.13379176 97.04761193 91.05812592 78.57587762 98.32075143 79.43543364
 98.13139996 77.73625333 79.94120139 97.53593941 80.45444353 96.07843137
 79.63724244 95.44559882 77.67396666 83.85031268 87.80925331 79.30836884
 80.5242046  99.15040985]
Avg Prec: is [53.4964314   3.70341182 14.86622847  4.536595    1.47056609  4.44438912
 14.55424249  8.71670054  8.62650326  5.31520574  3.52038884  5.69645557
  2.34970741  5.78974937  2.98281866  3.66918595 16.8888367   6.48793903
  1.57116727  2.83013121  3.48559475  1.57757783  1.22890146  5.13044369
  5.5297585   7.91964159  7.76658206  4.62103311  3.86203088  4.54989523
  2.12916691  0.83826994  2.89658689  1.13657213  1.64390273  2.13531343
  1.94352494  2.1164258   2.19111886  6.20964471  1.70700457  6.05940555
  2.1446623   2.6786668   2.33798926  4.90059525  1.66217664  1.01488904
  1.41813053  1.15528138  1.18440817  1.01713604  0.73478976  2.25141105
  0.8499766   1.84990722  9.94561247  2.99465933  3.8387945   2.72589154
  7.80930044  2.00220177  3.26696045  2.51668451  1.35455329  1.92985459
  1.520211    3.52442749  1.09723062  2.25004416  0.19341781  3.31200798
  1.65616566  3.96046776  3.17502624  2.2988988   0.565279    1.42497894
  0.1195103   0.61127228]
mAP score regular 43.29, mAP score EMA 4.22
Train_data_mAP: current_mAP = 40.10, highest_mAP = 40.10
Val_data_mAP: current_mAP = 43.29, highest_mAP = 43.29
tensor([0.2408, 0.0900, 0.2549, 0.1000, 0.0385, 0.0715, 0.0257, 0.0553, 0.1034,
        0.0515, 0.0200, 0.0251, 0.0166, 0.0754, 0.1030, 0.0821, 0.1161, 0.0733,
        0.0419, 0.0774, 0.0103, 0.0146, 0.0278, 0.0205, 0.0566, 0.0451, 0.1673,
        0.0361, 0.0291, 0.0380, 0.0394, 0.0192, 0.1509, 0.0069, 0.0504, 0.0454,
        0.0323, 0.0609, 0.0759, 0.2101, 0.3888, 0.5208, 0.2822, 0.4018, 0.6900,
        0.5701, 0.0263, 0.0352, 0.0359, 0.0501, 0.0094, 0.0179, 0.0268, 0.0538,
        0.0265, 0.0582, 0.4232, 0.1194, 0.2098, 0.0283, 0.7130, 0.0916, 0.2836,
        0.1201, 0.0855, 0.0729, 0.1520, 0.1079, 0.2824, 0.2154, 0.0439, 0.0894,
        0.5525, 0.2277, 0.6589, 0.9515, 0.7724, 0.8875, 0.0681, 0.1259],
       device='cuda:0')
Sum Train Loss:  tensor([1.1230e+01, 1.3601e+00, 5.3075e+00, 1.2376e+00, 5.0099e-01, 5.3000e-01,
        3.0657e-01, 1.1178e+00, 7.8913e-01, 7.3175e-01, 3.3503e-02, 3.7967e-01,
        1.2359e-02, 1.1295e+00, 7.0854e-01, 4.6706e-01, 1.6826e+00, 7.2932e-01,
        1.9954e-01, 3.2491e-01, 2.5094e-02, 3.4354e-02, 1.4729e-02, 1.7721e-01,
        1.3098e+00, 6.5049e-01, 3.1434e+00, 5.5358e-01, 3.8426e-01, 2.7131e-01,
        3.4014e-01, 2.8164e-01, 2.0250e+00, 8.5390e-03, 3.2935e-01, 3.6479e-01,
        3.6828e-01, 8.8437e-02, 9.2934e-01, 5.0070e+00, 3.7579e+00, 1.1463e+01,
        2.6584e+00, 2.7184e+00, 3.8486e+00, 1.2373e+01, 3.4023e-01, 1.8582e-01,
        1.5161e-01, 1.7044e-01, 5.3046e-02, 5.3747e-02, 8.4473e-02, 4.5750e-01,
        3.5812e-01, 4.1479e-01, 1.0445e+01, 1.2965e+00, 2.5993e+00, 2.4361e-01,
        1.2865e+01, 2.3179e-01, 2.7946e+00, 1.7340e+00, 4.5119e-01, 1.3824e+00,
        3.2900e-01, 1.9165e+00, 5.8316e-01, 1.4424e+00, 1.0242e-02, 5.9421e-01,
        4.3319e+00, 3.3356e+00, 6.1899e+00, 1.4207e+01, 8.0094e+00, 4.6372e+00,
        1.1871e-02, 8.6236e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [6/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 164.7
Sum Train Loss:  tensor([1.3143e+01, 1.0304e+00, 5.1645e+00, 9.0680e-01, 1.7880e-01, 5.9147e-01,
        2.2365e-01, 8.6178e-01, 6.6701e-01, 5.0450e-01, 5.4832e-02, 1.7218e-01,
        5.9225e-02, 8.8960e-01, 1.1195e+00, 9.9240e-01, 1.2297e+00, 1.0990e+00,
        4.5981e-01, 3.7469e-01, 5.6957e-02, 7.0327e-02, 4.4941e-02, 7.7282e-02,
        9.7039e-01, 5.0085e-01, 4.3479e+00, 7.3691e-01, 5.3088e-01, 2.9649e-01,
        2.7205e-01, 1.0566e-01, 1.9340e+00, 3.0123e-02, 1.2015e-01, 1.4932e-01,
        4.2559e-01, 6.0937e-01, 5.2337e-01, 4.0323e+00, 4.4572e+00, 1.0608e+01,
        2.6423e+00, 4.5286e+00, 4.9158e+00, 6.7177e+00, 3.2051e-01, 8.9491e-02,
        1.0486e-01, 4.4818e-01, 2.6126e-02, 6.5542e-02, 1.5151e-01, 1.6502e-01,
        1.7897e-01, 5.9196e-01, 1.1453e+01, 1.5201e+00, 2.1623e+00, 3.9989e-01,
        1.3949e+01, 1.2681e+00, 3.0964e+00, 1.0079e+00, 4.8822e-01, 5.6587e-01,
        9.3592e-01, 2.0989e+00, 8.2437e-01, 1.1333e+00, 8.4957e-03, 1.1959e+00,
        3.5835e+00, 2.8729e+00, 6.7476e+00, 1.2568e+01, 9.7721e-01, 4.6612e+00,
        6.8307e-03, 8.5175e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [6/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 155.9
Sum Train Loss:  tensor([1.2293e+01, 1.2778e+00, 7.5848e+00, 1.2153e+00, 3.4811e-02, 1.4780e+00,
        1.4731e-01, 1.1498e+00, 1.4046e+00, 4.8573e-01, 1.5292e-01, 1.3201e-01,
        7.1182e-02, 6.7335e-01, 1.0896e+00, 8.6292e-01, 3.5461e+00, 9.1369e-01,
        4.9304e-02, 2.2808e-01, 1.1426e-01, 1.5221e-02, 1.1092e-01, 6.7988e-02,
        9.5270e-01, 1.8373e+00, 3.7133e+00, 9.5515e-01, 4.8557e-01, 1.0206e+00,
        1.4046e-01, 6.5767e-02, 1.7706e+00, 1.9244e-02, 2.2682e-01, 8.7244e-02,
        5.0470e-01, 2.3159e-01, 1.6629e-01, 5.5891e+00, 4.2187e+00, 8.5879e+00,
        3.4673e+00, 5.2874e+00, 3.4333e+00, 1.5381e+01, 2.1806e-01, 1.0829e-01,
        4.3380e-01, 2.5897e-01, 7.6009e-02, 8.8862e-02, 1.0319e-01, 1.9014e-01,
        4.3371e-02, 5.0910e-01, 1.4501e+01, 9.4729e-01, 4.6588e+00, 3.4977e-01,
        1.5617e+01, 8.3341e-01, 1.4881e+00, 1.6750e+00, 6.3911e-01, 5.8672e-01,
        1.0012e+00, 2.3262e+00, 1.5904e+00, 2.0655e+00, 1.4023e-01, 8.9601e-01,
        3.4022e+00, 5.7530e+00, 5.4641e+00, 1.0123e+01, 3.6819e+00, 4.9261e+00,
        1.4421e-02, 1.9476e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [6/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 178.1
Sum Train Loss:  tensor([1.3033e+01, 9.3488e-01, 6.1952e+00, 3.6068e-01, 1.7514e-01, 9.5039e-01,
        3.8755e-01, 7.1966e-01, 6.2716e-01, 3.2477e-01, 1.4457e-01, 1.9906e-01,
        9.5523e-02, 1.1047e+00, 8.0728e-01, 7.5532e-01, 1.8386e+00, 8.5635e-01,
        1.4763e-01, 1.4451e-01, 7.8332e-02, 1.6143e-01, 7.5440e-02, 9.0883e-02,
        7.0859e-01, 6.7793e-01, 1.9553e+00, 4.3488e-01, 2.3809e-01, 1.0263e+00,
        2.4049e-01, 1.9931e-01, 8.9104e-01, 3.0067e-02, 1.7063e-01, 1.7109e-01,
        1.4305e-01, 5.9830e-01, 9.3257e-01, 3.8907e+00, 4.8808e+00, 1.0112e+01,
        2.7117e+00, 3.9542e+00, 9.4936e+00, 7.5992e+00, 4.4059e-02, 1.5601e-01,
        2.6624e-01, 2.1371e-01, 4.7013e-02, 2.5027e-01, 2.0205e-01, 6.1357e-01,
        2.0574e-01, 6.4119e-01, 9.4400e+00, 1.8589e+00, 1.9865e+00, 1.9904e-01,
        1.7754e+01, 4.4491e-01, 5.3667e+00, 8.4445e-01, 9.5411e-01, 1.1562e+00,
        1.9285e+00, 1.1952e+00, 6.4580e-01, 2.5067e-01, 8.9792e-03, 7.6257e-01,
        1.4508e+00, 3.2558e+00, 5.0847e+00, 1.5567e+01, 3.9657e-01, 5.6150e+00,
        5.1932e-01, 6.5969e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [6/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 161.3
Sum Train Loss:  tensor([1.1973e+01, 1.3994e+00, 7.4131e+00, 1.6959e+00, 2.1349e-01, 9.6224e-01,
        4.5094e-01, 8.2342e-01, 1.4180e+00, 6.2184e-01, 2.8812e-01, 1.3516e-01,
        4.6282e-03, 1.8854e+00, 1.0196e+00, 2.0886e+00, 2.2472e+00, 1.4180e+00,
        4.1832e-01, 4.4096e-01, 7.6418e-02, 2.5090e-01, 5.7096e-02, 1.2568e-01,
        9.3793e-01, 9.8371e-01, 2.7956e+00, 3.2566e-01, 8.1891e-01, 9.5978e-01,
        5.1944e-01, 1.6936e-01, 1.5937e+00, 6.5308e-02, 5.2875e-02, 3.6597e-02,
        2.2296e-01, 1.6351e-01, 1.4929e+00, 5.0112e+00, 2.2700e+00, 1.0261e+01,
        1.8716e+00, 3.8265e+00, 1.1186e+01, 1.0426e+01, 1.7856e-01, 2.2592e-01,
        4.7311e-02, 2.4979e-01, 4.0835e-02, 4.0633e-02, 3.0573e-02, 1.0334e-01,
        2.5282e-02, 6.4384e-01, 9.7511e+00, 1.2550e+00, 4.0037e+00, 4.5194e-01,
        1.4338e+01, 1.7274e+00, 6.0440e+00, 1.7621e+00, 1.1786e+00, 7.9172e-01,
        8.1086e-01, 1.2110e+00, 1.6580e+00, 4.9740e-01, 5.9704e-03, 8.4379e-01,
        3.9906e+00, 3.7668e+00, 6.1457e+00, 1.3791e+01, 3.0071e+00, 1.4216e+01,
        5.5531e-01, 5.5237e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [6/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 187.4
Sum Train Loss:  tensor([1.0079e+01, 6.9004e-01, 7.1901e+00, 5.7826e-01, 4.8563e-01, 8.8135e-01,
        5.7594e-02, 1.1941e+00, 4.2427e-01, 3.7506e-01, 3.3948e-02, 3.5932e-02,
        7.2611e-02, 1.1347e+00, 1.3443e+00, 7.6754e-01, 1.7632e+00, 1.1588e+00,
        2.7303e-01, 2.9845e-01, 6.6852e-02, 2.2149e-02, 1.9900e-01, 1.8745e-01,
        8.6555e-01, 6.4221e-01, 4.2130e+00, 3.1137e-01, 2.5448e-01, 2.3783e-01,
        2.2384e-01, 1.6817e-01, 1.4538e+00, 5.2238e-02, 2.8593e-01, 9.8047e-02,
        3.9480e-01, 1.0197e-01, 5.1756e-01, 6.8219e+00, 2.4904e+00, 1.1850e+01,
        1.4535e+00, 4.5831e+00, 4.7660e+00, 8.1572e+00, 4.6498e-01, 2.0468e-01,
        2.9270e-01, 6.0444e-01, 1.5950e-02, 6.8290e-02, 3.5561e-02, 1.6095e-01,
        9.7448e-02, 9.6067e-01, 1.0307e+01, 7.6720e-01, 3.1377e+00, 5.6410e-01,
        1.6794e+01, 7.4737e-01, 3.3515e+00, 9.5975e-01, 2.1461e-01, 1.1201e+00,
        8.3995e-01, 1.8041e+00, 3.9946e+00, 2.1834e+00, 5.4253e-01, 1.4812e+00,
        4.0651e+00, 3.8529e+00, 7.0899e+00, 8.7771e+00, 3.4136e+00, 1.4907e+00,
        2.8054e-02, 4.3233e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [6/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 160.1
Sum Train Loss:  tensor([8.8265e+00, 7.4486e-01, 7.7394e+00, 7.0412e-01, 6.8151e-01, 7.7139e-01,
        4.2290e-01, 8.4978e-01, 7.6549e-01, 4.8086e-01, 1.0457e-01, 1.3967e-01,
        1.4500e-02, 1.3480e+00, 1.8650e+00, 6.1830e-01, 2.7972e+00, 6.4860e-01,
        1.0710e-01, 5.3610e-01, 8.3576e-02, 1.8327e-02, 7.1742e-02, 2.4650e-02,
        1.2908e+00, 1.1086e+00, 3.8144e+00, 5.6003e-01, 1.7944e-01, 5.2577e-01,
        1.8323e-01, 6.5432e-02, 1.1591e+00, 4.8315e-02, 4.3786e-01, 5.0517e-01,
        4.8421e-01, 2.4156e-01, 5.9039e-01, 4.7227e+00, 3.3933e+00, 1.4892e+01,
        2.8136e+00, 4.1587e+00, 7.1612e+00, 9.9120e+00, 4.5926e-02, 1.3961e-01,
        7.6533e-01, 8.2115e-02, 7.5959e-02, 7.5557e-02, 1.0482e-01, 5.7473e-01,
        3.4565e-02, 4.1427e-01, 1.2127e+01, 1.5157e+00, 2.7321e+00, 3.5635e-01,
        1.6724e+01, 4.9815e-01, 4.3580e+00, 1.1932e+00, 6.3961e-01, 4.4599e-01,
        1.6935e+00, 3.0081e+00, 2.1692e+00, 2.0164e+00, 1.6886e-02, 8.3359e-01,
        2.5453e+00, 2.5496e+00, 2.2161e+00, 6.4760e+00, 8.1295e-01, 7.6168e+00,
        1.3305e-02, 2.2750e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [6/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 163.7
Sum_Val Meta Model:  tensor([1.4680e+01, 6.8555e+00, 2.2881e+01, 5.2012e+00, 1.2483e-01, 8.4088e-01,
        1.3499e-01, 9.1303e-01, 6.6622e-01, 6.4975e-01, 1.2843e-01, 3.1140e-01,
        1.0629e-01, 1.2794e+00, 8.1342e-01, 5.0203e-01, 5.6168e+01, 1.9062e-01,
        5.5976e-02, 7.2583e-02, 1.6206e-02, 8.0758e-03, 1.2742e-02, 2.9458e-02,
        1.6355e+00, 7.9454e-01, 5.3269e+00, 1.9914e-01, 2.7111e-01, 3.7108e-01,
        5.8479e-02, 1.8254e-02, 1.5873e+00, 6.7110e-03, 2.1227e-01, 1.2216e-01,
        1.0124e-01, 2.7798e-01, 3.2679e-01, 9.8420e+00, 4.3734e+00, 1.1619e+01,
        2.0093e+00, 6.2270e+00, 1.1132e+01, 1.2290e+01, 4.0114e-02, 1.9088e-01,
        4.6787e-02, 2.9177e-01, 8.9542e-03, 2.1803e-02, 2.0599e-01, 6.9612e-02,
        5.3333e-02, 1.2921e-01, 1.2038e+01, 1.0505e+00, 3.3923e+00, 1.1653e-01,
        9.8010e+00, 2.0194e+00, 1.7365e+00, 5.0399e-01, 9.0120e-02, 1.4473e-01,
        1.3979e-01, 7.5585e-01, 2.7345e+00, 4.1909e+00, 1.4041e-02, 1.4563e+00,
        1.0162e+01, 2.8903e+00, 8.6777e+00, 6.7732e+00, 5.7820e-01, 7.0505e+00,
        6.8055e-03, 8.1852e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.4425e+01, 4.9506e+00, 1.7652e+01, 3.1864e+00, 3.9881e-02, 8.4194e-01,
        1.2586e-01, 1.0090e+00, 6.4026e-01, 5.9419e-01, 1.3839e-01, 2.7798e-01,
        1.2633e-01, 1.2120e+00, 7.9219e-01, 1.2323e+00, 4.1072e+01, 3.8364e-01,
        3.1845e-02, 9.6298e-02, 2.4396e-02, 1.0832e-02, 4.5912e-03, 3.0945e-02,
        1.5297e+00, 8.3541e-01, 5.1480e+00, 3.0283e-01, 3.7167e-01, 4.2303e-01,
        3.4248e-02, 1.0959e-02, 1.3883e+00, 1.0550e-02, 1.1560e-01, 5.8814e-02,
        2.2569e-01, 1.2082e-01, 1.7846e-01, 1.0097e+01, 4.8606e+00, 1.2744e+01,
        2.1446e+00, 6.5613e+00, 1.1853e+01, 1.0539e+01, 2.1411e-02, 1.8859e-01,
        1.3314e-02, 2.5433e-01, 1.2023e-03, 6.3756e-03, 1.8668e-01, 1.8740e-02,
        2.0448e-02, 4.4777e-02, 1.1797e+01, 1.1700e+00, 2.8520e+00, 1.6626e-01,
        1.0211e+01, 1.4163e+00, 1.5298e+00, 4.5909e-01, 5.0008e-02, 2.3588e-01,
        8.1028e-02, 9.5679e-01, 2.6765e+00, 3.5253e+00, 1.3267e-02, 1.5031e+00,
        1.0130e+01, 2.7966e+00, 1.0090e+01, 5.8244e+00, 4.4832e-01, 5.9036e+00,
        4.9332e-03, 9.2438e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.9916e+01, 5.5010e+01, 6.9251e+01, 3.1876e+01, 1.0359e+00, 1.1783e+01,
        4.8949e+00, 1.8240e+01, 6.1922e+00, 1.1529e+01, 6.9153e+00, 1.1089e+01,
        7.5938e+00, 1.6064e+01, 7.6920e+00, 1.5017e+01, 3.5386e+02, 5.2360e+00,
        7.5955e-01, 1.2436e+00, 2.3793e+00, 7.4411e-01, 1.6514e-01, 1.5088e+00,
        2.7045e+01, 1.8531e+01, 3.0773e+01, 8.3849e+00, 1.2757e+01, 1.1142e+01,
        8.6983e-01, 5.7082e-01, 9.2006e+00, 1.5282e+00, 2.2955e+00, 1.2949e+00,
        6.9866e+00, 1.9850e+00, 2.3512e+00, 4.8056e+01, 1.2503e+01, 2.4470e+01,
        7.5996e+00, 1.6331e+01, 1.7179e+01, 1.8486e+01, 8.1542e-01, 5.3607e+00,
        3.7107e-01, 5.0766e+00, 1.2764e-01, 3.5533e-01, 6.9755e+00, 3.4806e-01,
        7.7160e-01, 7.6917e-01, 2.7873e+01, 9.7992e+00, 1.3591e+01, 5.8679e+00,
        1.4320e+01, 1.5464e+01, 5.3941e+00, 3.8211e+00, 5.8461e-01, 3.2368e+00,
        5.3302e-01, 8.8702e+00, 9.4793e+00, 1.6369e+01, 3.0218e-01, 1.6813e+01,
        1.8333e+01, 1.2283e+01, 1.5314e+01, 6.1212e+00, 5.8042e-01, 6.6516e+00,
        7.2435e-02, 7.3449e-01], device='cuda:0')
Outer loop valEpocw Maximum [6/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 258.9
model_train val_loss valEpocw [6/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 233.1
model_train val_loss valEpocw [6/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1213.7
Sum_Val Meta Model:  tensor([1.7136e+01, 1.5779e+00, 1.8637e+01, 9.9189e-01, 3.3008e-02, 3.5391e+00,
        4.1335e+00, 4.2022e+00, 4.9806e+00, 2.3185e+00, 5.1313e-01, 8.6877e+00,
        2.3389e-01, 4.9643e-01, 1.6599e+00, 1.4739e+00, 1.4896e+00, 1.5896e+00,
        4.4944e-01, 2.8342e+00, 4.2573e-03, 1.8154e-03, 1.5896e-02, 1.1995e-01,
        1.6373e+00, 1.2467e+00, 5.1985e+00, 7.1845e-01, 5.3998e-01, 6.9895e-03,
        1.4053e-02, 4.9056e-03, 5.5294e-02, 3.0141e-03, 8.0093e-03, 7.6363e-03,
        2.1099e-01, 1.7721e-02, 1.4076e-02, 4.7582e-01, 1.2851e-01, 2.1020e+00,
        9.2853e-02, 1.8072e-01, 3.1947e-01, 2.0044e+00, 3.6080e-02, 2.4681e-02,
        1.0139e-02, 3.1138e-01, 1.7779e-03, 5.3095e-03, 3.8012e-03, 1.2460e-02,
        9.0300e-03, 2.9876e-01, 5.2517e+00, 8.2832e-01, 1.8113e+00, 1.0137e-01,
        1.9199e+00, 5.4060e-02, 6.5671e-01, 2.4230e-01, 4.8718e-02, 6.9861e-02,
        1.0329e-01, 1.6892e+00, 9.1192e-02, 9.6889e-01, 5.4997e-03, 6.4685e-02,
        2.7347e+00, 1.4354e+00, 2.9726e+00, 4.6611e-01, 1.4320e-01, 6.1454e-01,
        4.3623e-03, 2.9939e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.5943e+01, 1.6843e+00, 1.3785e+01, 1.1257e+00, 1.8085e-01, 2.9851e+00,
        2.4876e+00, 2.9724e+00, 4.0762e+00, 1.7154e+00, 3.6741e-01, 2.5255e+00,
        1.8921e-01, 6.4651e-01, 1.7685e+00, 2.5011e-01, 1.3030e+00, 1.5059e+00,
        6.3053e-02, 2.0709e+00, 1.8032e-02, 8.4948e-03, 1.2460e-02, 1.1628e-01,
        1.6213e+00, 1.0824e+00, 5.3195e+00, 7.4972e-01, 5.0524e-01, 6.4302e-02,
        2.5115e-02, 7.0456e-03, 1.1504e-01, 1.3635e-02, 4.3508e-02, 1.8873e-02,
        1.1720e-01, 1.9803e-01, 3.0631e-02, 2.8268e-01, 1.0623e-01, 1.1692e+00,
        1.1348e-01, 1.6739e-01, 2.3204e-01, 1.0511e+00, 1.3760e-02, 1.2653e-02,
        1.0104e-02, 2.5462e-01, 1.4454e-03, 4.7708e-03, 5.2972e-03, 4.4801e-02,
        1.2615e-02, 1.6252e-01, 3.6109e+00, 5.6540e-01, 1.3520e+00, 1.3366e-02,
        1.4481e+00, 1.3850e-02, 1.8411e-01, 3.5839e-02, 7.8767e-03, 1.8337e-02,
        1.1307e-02, 1.6570e+00, 4.8381e-02, 6.1421e-01, 1.0196e-03, 2.3518e-02,
        2.1981e+00, 8.2150e-01, 3.4880e+00, 2.2309e-01, 1.6058e-01, 3.5792e-01,
        7.4298e-04, 1.6898e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.6260e+01, 1.7198e+01, 4.8194e+01, 9.6523e+00, 4.2435e+00, 3.2870e+01,
        5.0308e+01, 3.9864e+01, 3.3705e+01, 2.8772e+01, 1.4559e+01, 6.8752e+01,
        7.8263e+00, 8.5188e+00, 1.4550e+01, 2.4138e+00, 9.9529e+00, 1.6294e+01,
        9.3724e-01, 1.6885e+01, 1.1504e+00, 4.3744e-01, 3.1365e-01, 4.6577e+00,
        2.3598e+01, 1.8527e+01, 3.1967e+01, 1.4993e+01, 1.2282e+01, 1.3581e+00,
        4.9755e-01, 2.6591e-01, 7.6214e-01, 1.2453e+00, 7.7035e-01, 3.9820e-01,
        2.8560e+00, 2.7332e+00, 3.8672e-01, 1.5821e+00, 3.4548e-01, 2.6847e+00,
        4.5271e-01, 4.7331e-01, 3.8295e-01, 2.0124e+00, 3.7583e-01, 2.7676e-01,
        2.0249e-01, 4.0353e+00, 9.8369e-02, 1.9906e-01, 1.5435e-01, 7.0163e-01,
        3.5088e-01, 2.3947e+00, 9.3198e+00, 4.1549e+00, 6.7540e+00, 3.4159e-01,
        2.2165e+00, 1.5014e-01, 6.5655e-01, 2.6954e-01, 8.4955e-02, 2.1832e-01,
        7.4157e-02, 1.6969e+01, 1.8845e-01, 3.1864e+00, 1.9627e-02, 2.6311e-01,
        4.6909e+00, 3.5151e+00, 6.0476e+00, 2.3605e-01, 1.9975e-01, 4.0199e-01,
        9.0254e-03, 1.3993e-01], device='cuda:0')
Outer loop valEpocw Maximum [6/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 115.1
model_train val_loss valEpocw [6/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 88.2
model_train val_loss valEpocw [6/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 687.8
Sum_Val Meta Model:  tensor([1.9515e+01, 3.9485e-01, 3.2785e+00, 3.1038e-01, 5.4011e-02, 2.3159e-01,
        6.3852e-02, 7.1873e-01, 3.0334e-01, 1.7514e-01, 4.6300e-02, 5.8126e-02,
        1.2290e-02, 1.4996e+00, 3.1923e-01, 3.7931e-01, 9.2356e-01, 2.0780e-01,
        9.1644e-02, 2.4792e-01, 2.2808e-02, 5.4225e-02, 3.3087e-01, 6.7370e-02,
        4.4554e-01, 1.3475e-01, 3.4092e+00, 3.0165e-01, 6.4389e-02, 1.7265e-01,
        2.5181e-01, 1.5723e-01, 1.6951e+00, 5.1617e-03, 2.0926e-01, 1.7819e-01,
        6.0222e-01, 1.0002e+00, 5.8034e-02, 5.5495e+00, 1.0522e+01, 3.6286e+01,
        2.4460e+01, 3.3635e+01, 3.5423e+01, 4.1283e+01, 6.2214e-01, 6.5073e-01,
        3.2274e+00, 1.2168e+00, 6.1845e-01, 1.0434e+00, 2.1238e+00, 1.1283e+00,
        2.3883e+00, 6.7264e+00, 6.5030e+01, 2.2948e+00, 1.9799e+00, 1.4153e-01,
        6.6958e+01, 2.4123e-01, 2.8330e+00, 2.0737e+00, 1.5088e+00, 1.3158e-01,
        1.5735e+00, 1.0622e+00, 1.3051e+00, 5.6278e-01, 3.2587e-02, 7.4593e-01,
        2.4342e+00, 7.4983e+00, 1.7169e+00, 1.4151e+01, 9.6609e+00, 1.9453e+00,
        2.9325e-02, 1.3621e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.9096e+00, 6.1530e-02, 1.0636e+00, 4.8807e-02, 8.0440e-03, 1.3556e-02,
        8.8364e-03, 7.1749e-01, 4.1706e-02, 1.2092e-02, 2.2991e-03, 1.9269e-03,
        2.2677e-03, 1.1631e+00, 4.6441e-02, 1.3311e-01, 2.5310e-01, 4.7796e-02,
        8.0086e-03, 1.7885e-02, 2.5124e-03, 1.8466e-03, 8.9873e-04, 2.2570e-03,
        2.9405e-01, 7.4540e-02, 3.7540e+00, 3.5486e-01, 4.7878e-02, 2.3842e-02,
        3.9084e-02, 1.5016e-01, 1.1586e+00, 1.0814e-03, 1.2199e-01, 7.9326e-02,
        5.0770e-01, 9.6036e-01, 5.0637e-02, 7.2559e+00, 8.7840e+00, 3.2560e+01,
        2.3141e+01, 3.4110e+01, 2.8679e+01, 4.1174e+01, 3.3222e-01, 3.1744e-01,
        2.3991e+00, 6.6220e-01, 3.0169e-01, 1.1102e+00, 2.6177e+00, 1.3670e+00,
        1.7535e+00, 6.4046e+00, 4.0063e+01, 2.2651e+00, 1.9048e+00, 7.3855e-02,
        7.8342e+01, 2.2613e-02, 2.9676e+00, 1.4416e+00, 1.0389e+00, 4.5186e-01,
        1.4942e+00, 1.1189e+00, 1.3157e+00, 8.9669e-01, 9.0118e-03, 5.4201e-01,
        2.6363e+00, 8.2623e+00, 1.2631e+00, 1.5676e+01, 7.8428e+00, 1.3703e+00,
        5.4797e-03, 1.3271e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.7417e+01, 7.5419e-01, 4.2734e+00, 5.1674e-01, 2.4400e-01, 1.9586e-01,
        3.9562e-01, 1.4939e+01, 4.3057e-01, 2.4654e-01, 1.2181e-01, 8.0829e-02,
        1.6015e-01, 1.6164e+01, 4.7275e-01, 1.8165e+00, 2.1545e+00, 6.6777e-01,
        1.9101e-01, 2.2174e-01, 2.7772e-01, 1.0681e-01, 2.4427e-02, 1.1867e-01,
        5.2683e+00, 1.9690e+00, 2.8331e+01, 1.1116e+01, 1.8099e+00, 5.8933e-01,
        9.6182e-01, 6.7314e+00, 7.6666e+00, 1.3659e-01, 2.3475e+00, 1.5984e+00,
        1.4107e+01, 1.4393e+01, 7.3958e-01, 4.1316e+01, 2.6163e+01, 6.3351e+01,
        6.6535e+01, 7.6345e+01, 4.1900e+01, 6.6786e+01, 1.0649e+01, 7.5472e+00,
        4.1612e+01, 1.1028e+01, 2.2403e+01, 4.5949e+01, 7.7719e+01, 2.4462e+01,
        5.8929e+01, 9.5095e+01, 1.0172e+02, 1.9684e+01, 1.0869e+01, 2.3310e+00,
        1.0629e+02, 2.9052e-01, 1.1598e+01, 1.2690e+01, 1.3192e+01, 6.4629e+00,
        1.0699e+01, 1.3796e+01, 6.1079e+00, 5.4752e+00, 2.2093e-01, 7.2029e+00,
        5.9024e+00, 3.8871e+01, 2.2420e+00, 1.6438e+01, 9.8647e+00, 1.5313e+00,
        8.9512e-02, 1.2495e+00], device='cuda:0')
Outer loop valEpocw Maximum [6/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 430.9
model_train val_loss valEpocw [6/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 381.3
model_train val_loss valEpocw [6/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1352.4
Sum_Val Meta Model:  tensor([2.3321e+01, 1.5981e-01, 4.9204e+00, 1.0286e-01, 2.0654e-02, 5.8249e-01,
        1.0935e-01, 6.3305e-01, 1.4731e-01, 6.3672e-01, 8.3364e-02, 8.0805e-02,
        3.3604e-03, 9.6949e-01, 1.1974e+00, 1.1641e-01, 2.0374e-01, 6.1158e-02,
        2.3150e-02, 4.8501e-02, 5.4713e-03, 5.5278e-03, 1.5412e-02, 1.4384e-02,
        9.7328e-01, 7.0238e-02, 3.6285e+00, 5.8963e-02, 3.0334e-01, 1.5160e-02,
        2.4937e-02, 3.5934e-03, 5.7180e-01, 1.9634e-02, 1.2223e-01, 2.1425e-01,
        1.3543e-02, 6.9092e-02, 3.3070e-01, 4.3920e+00, 5.1020e+00, 1.4836e+01,
        2.5262e+00, 4.2326e+00, 5.6069e+00, 8.2281e+00, 1.7592e-02, 2.1762e-02,
        5.5172e-02, 2.7399e-02, 9.1345e-03, 1.2829e-02, 9.5549e-03, 3.3896e-01,
        1.5831e-02, 9.6880e-02, 1.0200e+01, 5.5533e-01, 3.1341e+00, 5.6251e-02,
        2.9396e+01, 7.8267e-02, 1.4710e+00, 8.7673e-01, 3.9964e-01, 1.1255e-01,
        9.1097e-01, 6.4630e+00, 3.6373e-01, 5.9869e-01, 6.0536e-03, 1.4558e-01,
        2.5250e+00, 2.4164e+00, 2.4292e+02, 1.4755e+01, 1.2476e+00, 9.4335e+00,
        5.7715e-03, 1.3481e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([9.7022e+00, 1.7282e-01, 4.5532e+00, 1.6834e-01, 3.2260e-02, 4.7096e-01,
        1.8436e-01, 6.2295e-01, 1.0032e-01, 6.9952e-01, 3.4181e-02, 2.1940e-01,
        1.8439e-02, 9.6838e-01, 1.3289e+00, 5.6751e-02, 8.4741e-02, 3.3596e-02,
        3.0770e-03, 5.8537e-03, 3.2928e-03, 5.1199e-04, 3.3558e-03, 1.8827e-02,
        1.1689e+00, 6.8948e-02, 3.4151e+00, 7.2218e-02, 3.5820e-01, 5.8853e-03,
        5.2716e-03, 3.0107e-03, 3.0674e-02, 1.5052e-03, 8.2099e-03, 2.5441e-03,
        2.4535e-02, 1.0267e-02, 1.4495e-02, 5.7955e-01, 2.2383e-01, 9.5166e-01,
        1.5271e-01, 1.9564e-01, 3.8826e-01, 1.0778e+00, 4.4403e-03, 3.7835e-03,
        3.5652e-03, 4.7578e-03, 7.9583e-04, 1.7910e-03, 9.2812e-04, 6.3097e-03,
        5.5404e-03, 2.2961e-02, 1.6914e+00, 1.3772e-01, 2.1493e+00, 1.2742e-02,
        7.0816e+00, 1.3672e-02, 3.2854e-01, 3.7277e-02, 1.5602e-02, 5.1420e-02,
        2.4824e-02, 7.3563e-01, 8.1558e-02, 1.0925e-01, 9.4100e-04, 2.5436e-02,
        1.1006e-01, 1.2980e+00, 7.9942e+01, 2.4675e+00, 2.6965e-01, 5.6076e+00,
        7.3078e-04, 9.5452e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.3517e+01, 2.7629e+00, 2.2065e+01, 2.3579e+00, 1.2699e+00, 9.5679e+00,
        1.1317e+01, 1.7280e+01, 1.2995e+00, 2.0787e+01, 2.7530e+00, 1.3982e+01,
        1.9761e+00, 1.7701e+01, 1.7216e+01, 9.9705e-01, 9.5960e-01, 6.4725e-01,
        1.0048e-01, 1.0108e-01, 5.3416e-01, 5.4743e-02, 1.7590e-01, 1.3996e+00,
        2.8899e+01, 2.3071e+00, 2.7130e+01, 2.9150e+00, 1.8316e+01, 2.2490e-01,
        1.9706e-01, 2.3719e-01, 2.3120e-01, 3.0031e-01, 1.9138e-01, 5.0940e-02,
        1.1670e+00, 2.2292e-01, 2.1358e-01, 3.3123e+00, 6.2199e-01, 1.8618e+00,
        5.6998e-01, 5.0408e-01, 5.8474e-01, 1.9336e+00, 2.6839e-01, 1.6401e-01,
        1.4823e-01, 1.4486e-01, 1.3794e-01, 1.4540e-01, 5.3852e-02, 1.4695e-01,
        3.4222e-01, 5.7067e-01, 4.2173e+00, 1.3095e+00, 1.1032e+01, 6.2654e-01,
        9.9093e+00, 2.1303e-01, 1.2454e+00, 3.3274e-01, 1.9827e-01, 8.9328e-01,
        1.6676e-01, 6.9582e+00, 3.8282e-01, 6.9661e-01, 3.1800e-02, 3.9833e-01,
        2.2145e-01, 6.1443e+00, 1.3527e+02, 2.5296e+00, 3.2025e-01, 6.0140e+00,
        1.7198e-02, 9.4038e-02], device='cuda:0')
Outer loop valEpocw Maximum [6/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 413.6
model_train val_loss valEpocw [6/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 130.5
model_train val_loss valEpocw [6/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 474.2
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [83.8513176  97.2405307  92.02373265 97.87648786 98.27121989 97.27464334
 97.91912867 94.98544121 97.78998794 96.76173536 98.53437458 98.83529684
 99.41764842 95.29854656 97.34774186 96.89818594 96.32558083 97.52074171
 98.69397303 98.3138607  98.4113254  99.21175424 99.47369062 98.69397303
 95.21691987 96.67158051 94.33242772 96.70203823 98.01902998 98.19933968
 98.13476931 98.57214215 96.78853815 98.34919165 98.50148025 98.62696605
 97.24905886 98.25660019 98.62209281 93.00325288 97.89110756 93.13970346
 97.09433365 96.34751039 97.1016435  94.70279358 98.06167079 98.60259987
 97.99953704 98.61112803 98.65864207 98.58554355 98.99976852 98.15669887
 98.70737442 97.47079105 90.73841693 96.43401031 96.34141884 97.21738283
 92.65603489 98.09943836 96.74955227 97.19058004 98.71102935 97.43302348
 98.43203665 95.95399666 98.71468428 98.06776233 99.81603538 96.8933127
 98.18350166 95.66891242 97.1016435  97.50368538 99.18129652 98.36259305
 99.84405648 99.14718388]
Accuracy th:0.7 is [82.70001584 97.21738283 91.33416991 97.78145978 98.19568475 97.08336887
 97.9800441  94.91356099 97.68155846 96.71787624 98.53193796 98.85600809
 99.41399349 95.31682119 97.31606584 96.72884102 96.29877804 97.48906568
 98.65864207 98.30776915 98.37355783 99.1922613  99.42983151 98.50757179
 95.2205748  96.65208757 94.16795604 96.86407329 98.01415675 98.16522703
 97.92643852 98.57457877 96.57898905 98.16644534 98.46493098 98.49782532
 97.04438299 98.15548056 98.5782337  92.79126716 97.87039632 92.67552783
 96.97981262 96.23908091 96.99321402 94.51273742 98.034868   98.59407171
 97.99710043 98.5782337  98.57579708 98.56117737 98.99976852 97.96176947
 98.70615611 97.46591781 90.4971918  96.56071442 96.36578502 97.11138997
 91.78250752 97.8131358  96.66914389 97.08336887 98.60259987 97.42571362
 98.35284658 95.95277835 98.69275472 97.94349484 99.81603538 96.52416515
 98.02390322 95.55926463 96.97737601 97.36114326 99.18007822 98.29193114
 99.84405648 99.14718388]
Avg Prec: is [94.15114281 24.58226887 62.66840637 59.5822126  68.79631098 57.8926508
 65.18821629 41.36161735 44.85973    44.1407692  16.39003803 45.92957759
 14.82393957 20.76498286 24.33435223 43.37600828 21.15748539 27.01615043
 33.41382021 29.1749833  45.65921286 37.40774744 86.84557413 72.26271302
 22.01238006 21.13250529 32.96911617 29.5450811  15.81262456 30.82160144
 65.87789633 29.484316   51.13083327 52.62960003 64.29353921 72.28432672
 45.15523466 69.08720329 79.34766609 39.33142699 27.79544306 52.52107781
 44.15405657 40.14085306 39.10743627 51.9870561  23.60886281 23.6661561
 30.34270029 31.93614991 51.21173488 26.24310012 13.38600822 65.35899057
 14.18121279 27.08889273 54.05513799 46.26453584 30.98503556 42.86888738
 66.99167457 74.10639853 52.62149003 38.68364146 47.72519906 33.44991688
 45.45189814 18.84629638 35.56231122 54.51811372  5.74135241 63.74332971
 51.76704227 37.00948578 51.97645824 50.91328462 12.49166525 37.06620847
  2.31801125 14.70982276]
Accuracy th:0.5 is [45.57814841 97.2137279  72.95232758 97.02489005 97.26733349 78.04363982
 78.40791413 77.12503503 79.35575834 96.43644692 79.6566806  98.52097319
 99.41399349 80.70077119 79.21321621 96.56680596 96.29512311 78.95858969
 98.65376884 98.30776915 80.76412324 80.17689843 98.38695922 79.01219527
 80.80310912 96.65086926 94.0778012  78.70883639 98.01293844 79.52510325
 97.30875598 98.57457877 96.36213009 98.02024829 86.77160366 79.17057541
 78.89645594 90.32662857 97.11504489 76.05170502 79.74683544 92.05906361
 78.41644229 77.91449909 96.9627563  93.87434364 98.02877645 98.57336046
 91.42432475 88.21529952 87.16268077 98.55508583 98.99976852 78.5565478
 98.70615611 78.90011087 73.29345403 93.22011184 96.24273583 96.9067141
 89.79300934 97.17717864 91.22086719 78.95006152 98.42838172 79.36794142
 98.20786784 78.03998489 80.05141263 97.55729097 80.6191445  95.99054592
 79.67617354 95.45083515 78.29948466 82.64275533 85.90416783 79.56043421
 80.69589795 99.14718388]
Accuracy th:0.7 is [45.62078922 97.2137279  72.95232758 97.02489005 97.26733349 78.04363982
 78.40791413 77.25295744 79.35575834 96.4742145  79.6566806  98.52097319
 99.41399349 81.1198694  79.21321621 96.56680596 96.29512311 78.95858969
 98.65376884 98.30776915 81.22220733 80.17689843 98.38695922 79.05727269
 81.31723541 96.65086926 94.0778012  78.70883639 98.01293844 79.52510325
 97.30875598 98.57457877 96.36213009 98.02024829 87.00551894 79.17057541
 78.89645594 90.58856495 97.11504489 76.05170502 80.35233489 92.05906361
 78.41644229 77.91449909 96.9627563  93.87434364 98.02877645 98.57336046
 93.40042153 88.95359462 87.34420877 98.55508583 98.99976852 78.5565478
 98.70615611 78.90011087 73.29345403 93.62337203 96.24273583 96.9067141
 89.79300934 97.17717864 91.42188813 78.95006152 98.42838172 79.36794142
 98.20786784 78.03998489 80.05141263 97.55972759 80.6191445  95.99054592
 79.67861015 95.45083515 78.29948466 82.73047356 86.00772408 79.56043421
 80.69589795 99.14718388]
Avg Prec: is [55.88468882  3.02188982 11.17862855  3.23564787  2.28879989  3.81058544
  3.38958127  5.64173575  2.35880567  3.76165619  1.56208305  1.54613779
  0.63342626  5.05520033  2.65776229  3.21312737  3.76248606  2.66007074
  1.31601523  1.73263386  1.93479733  0.83770519  1.88498308  2.45106539
  5.08924616  3.64243398  6.5567917   3.44158815  2.02589871  1.91013368
  2.6305531   1.31349636  3.6727062   1.70292207  2.39756649  2.39984282
  3.05898667  2.61799122  2.88285838  7.44537955  2.41335847  8.48353257
  3.44210452  4.122937    3.27070368  6.51762325  1.9940932   1.51152414
  2.20975022  1.58586937  1.85153555  1.5958237   1.08468997  3.04730985
  1.31073906  2.653156   11.45604547  3.84360776  3.94774691  2.88266658
 10.84400855  2.15093053  3.90094633  3.04178132  1.59237356  2.63634555
  1.7898625   4.21954359  1.26732838  2.40069034  0.19472925  3.43760479
  1.97974285  4.52708112  3.89016233  3.17783801  0.77626364  1.90195402
  0.11533571  0.72036572]
mAP score regular 41.69, mAP score EMA 3.78
starting validation
Accuracy th:0.5 is [85.89580686 97.24692927 92.52559982 98.14634876 98.77668984 97.41385754
 98.02675835 95.17652042 97.91215088 96.73119565 98.53501756 98.89129731
 99.35471012 95.09181055 97.34658794 96.88068366 96.28273164 97.56583701
 98.85143384 98.33071729 98.63467623 99.24010265 99.61880559 98.98846451
 95.47549642 96.56426738 94.421606   96.82088846 97.81747515 98.14136582
 98.50761143 98.6595909  96.87819219 98.5474749  98.79662157 99.00590478
 97.68293594 98.19368662 98.91870344 93.08617983 97.87976182 93.39761317
 97.13730473 96.55928445 97.042629   94.533722   98.18122929 98.78416424
 97.97443755 98.70692877 98.72187757 98.5923213  98.87385704 98.32573436
 98.70692877 97.59822608 90.59969604 96.67638339 96.24785111 97.2369634
 92.74484889 98.38802103 96.92802153 97.2369634  98.68699704 97.43628074
 98.46774796 95.7694895  98.76672397 98.12641702 99.81563146 97.30174154
 98.22109276 95.64491616 97.06256073 97.43129781 99.25006852 98.43785036
 99.82559733 99.15539278]
Accuracy th:0.7 is [85.48471485 97.22699753 91.94757954 98.05416449 98.73682637 97.13481326
 98.15880609 95.12918255 97.83740688 96.78600792 98.52754316 98.96354984
 99.35471012 95.11174228 97.2818098  96.57921618 96.21795351 97.52846501
 98.79662157 98.34068316 98.6072701  99.18030745 99.58392506 98.85890824
 95.43065002 96.52938685 94.46894387 97.03266313 97.81747515 98.15880609
 98.31825996 98.66955677 96.71375539 98.37556369 98.74430077 98.83399357
 97.43628074 98.11146822 98.90873757 92.8644393  97.84238981 93.09863717
 97.17965967 96.49201485 97.04013753 94.65082094 98.18621222 98.78914717
 97.95948875 98.6820141  98.6371677  98.56740663 98.87385704 98.24351596
 98.6969629  97.58576874 90.52744351 96.90559833 96.28771458 97.12235593
 92.32877395 98.08406209 96.81092259 97.10740713 98.62471037 97.51600767
 98.38552956 95.7769639  98.75924957 98.03921569 99.81563146 96.95293619
 98.09901089 95.55771483 96.85576899 97.45870394 99.24757705 98.37058076
 99.82559733 99.15040985]
Avg Prec: is [95.25576923 24.04933624 65.81012871 68.45130465 71.36334083 60.67593407
 73.63431666 43.59988122 51.19881415 47.90137211 25.78028809 51.64138862
 15.50632707 23.07447735 27.67474764 50.81375639 25.41818839 31.80541117
 36.55188421 28.41779256 57.08769711 45.60858396 90.73358058 79.56696471
 23.60657027 26.91501982 31.62277761 37.50598015 19.47371352 32.42859066
 73.09047007 33.66434957 52.96613544 56.28040515 68.68503308 78.72551933
 51.98246097 73.63217481 87.02737576 41.20021821 28.19158734 50.81050121
 40.48621178 36.37441219 32.25084681 49.3095648  22.22253102 21.41642338
 34.15423151 37.34244719 57.88726222 26.36048133 17.84583004 71.50677143
 19.72450009 29.78049861 53.47305346 49.08291549 30.52678148 49.87049599
 64.49159766 81.33486835 57.28196974 44.38354644 54.13329286 32.19579157
 52.87106813 20.43786434 37.916168   59.94872587 10.33872062 68.20161395
 47.88211754 37.77755131 58.00541571 48.13644407  9.98086949 40.34242418
  2.10080706 15.09579729]
Accuracy th:0.5 is [45.26745895 97.22450607 71.85140892 96.96290206 97.90716795 77.44226026
 77.56434213 76.31362583 78.93714029 96.41976231 79.19625283 98.5325261
 99.34972718 78.81754989 79.02932456 96.31262924 96.21047911 78.53103122
 98.78167277 98.34068316 79.82161098 79.84154272 98.31327703 78.60577522
 78.90225976 96.52938685 94.3393876  78.52853975 97.81747515 79.13396617
 97.52597354 98.67204823 96.39983058 98.18870369 87.63485064 78.76024616
 78.65311309 91.89525874 97.0276802  75.65837008 78.75277176 92.37362035
 77.86331814 77.50205546 97.03764606 94.02795426 98.18621222 98.77668984
 92.26897875 87.99860478 85.73386152 98.55993223 98.87385704 77.90816454
 98.6969629  78.55096295 72.220146   94.3169644  96.16314124 96.78102499
 90.13379176 97.04761193 91.00331365 78.44382988 98.32075143 79.30836884
 98.13139996 77.61417146 79.81911952 97.53593941 80.32239579 96.07843137
 79.50519471 95.44559882 77.54690186 83.78055161 87.75942397 79.18628697
 80.39215686 99.15040985]
Accuracy th:0.7 is [45.49169096 97.22450607 71.85140892 96.96290206 97.90716795 77.44226026
 77.56434213 76.33355757 78.93714029 96.41976231 79.19625283 98.5325261
 99.34972718 79.2361163  79.02932456 96.31262924 96.21047911 78.53103122
 98.78167277 98.34068316 80.12556992 79.84154272 98.31327703 78.60577522
 79.23362483 96.52938685 94.3393876  78.52853975 97.81747515 79.13396617
 97.52597354 98.67204823 96.39983058 98.18870369 87.86655704 78.76024616
 78.65311309 92.07215288 97.0276802  75.65837008 79.11901737 92.37362035
 77.86331814 77.50205546 97.03764606 94.02795426 98.18621222 98.77668984
 94.07280066 88.30256372 85.86840073 98.55993223 98.87385704 77.90816454
 98.6969629  78.55096295 72.220146   94.5785684  96.16314124 96.78102499
 90.13379176 97.04761193 91.17771632 78.44382988 98.32075143 79.30836884
 98.13139996 77.61417146 79.81911952 97.53593941 80.32239579 96.07843137
 79.50519471 95.44559882 77.54690186 83.87522735 87.89396318 79.18628697
 80.39215686 99.15040985]
Avg Prec: is [53.51385308  3.71029117 14.86952878  4.53917314  1.47186059  4.41276853
 14.54019533  8.70710781  8.66763808  5.33459019  3.16142085  6.51589499
  2.36358515  5.80281549  2.97356439  3.66918627 17.61295314  6.46946819
  1.5683137   2.73956357  3.48688738  1.55600696  1.23005891  5.12278533
  5.54700951  7.96439991  7.75637992  4.62143897  3.85071798  4.59027655
  2.13860476  0.83955407  2.88328054  1.13706331  1.6115618   2.13434649
  1.94166851  2.18664737  2.19458637  6.26895871  1.71037101  6.03190507
  2.1546223   2.69369761  2.33470778  4.84308876  1.64777155  1.01315199
  1.41368239  1.15328619  1.1930419   0.98133842  0.7366854   2.25530168
  0.85485249  1.84818256  9.9838742   2.98694513  3.80852824  2.77143572
  7.8122831   2.09462063  3.36167658  2.51752508  1.35710218  1.93165391
  1.52264616  3.51504153  1.10243598  2.26359738  0.1940306   3.32170654
  1.6458968   3.97008712  3.1808644   2.29947449  0.55396376  1.43962577
  0.11941259  0.61023685]
mAP score regular 44.77, mAP score EMA 4.24
Train_data_mAP: current_mAP = 41.69, highest_mAP = 41.69
Val_data_mAP: current_mAP = 44.77, highest_mAP = 44.77
tensor([0.2146, 0.0698, 0.2215, 0.0816, 0.0302, 0.0583, 0.0200, 0.0428, 0.0847,
        0.0411, 0.0155, 0.0192, 0.0120, 0.0630, 0.0861, 0.0652, 0.0985, 0.0590,
        0.0353, 0.0675, 0.0080, 0.0120, 0.0232, 0.0163, 0.0474, 0.0350, 0.1368,
        0.0295, 0.0242, 0.0315, 0.0320, 0.0157, 0.1290, 0.0056, 0.0420, 0.0477,
        0.0253, 0.0515, 0.0670, 0.1779, 0.3444, 0.5000, 0.3033, 0.4068, 0.7059,
        0.5883, 0.0204, 0.0278, 0.0301, 0.0400, 0.0073, 0.0157, 0.0220, 0.0442,
        0.0198, 0.0481, 0.4017, 0.1096, 0.1858, 0.0245, 0.7124, 0.0729, 0.2571,
        0.1049, 0.0752, 0.0617, 0.1360, 0.0894, 0.2635, 0.1797, 0.0361, 0.0718,
        0.5463, 0.2198, 0.6988, 0.9701, 0.9082, 0.9283, 0.0727, 0.1077],
       device='cuda:0')
Sum Train Loss:  tensor([1.0062e+01, 1.0823e+00, 5.6161e+00, 8.6165e-01, 1.9400e-01, 7.7844e-01,
        1.5890e-01, 5.0823e-01, 1.0333e+00, 6.7850e-01, 1.8529e-02, 9.0327e-02,
        8.1470e-03, 1.2636e+00, 1.8473e+00, 4.2156e-01, 1.4870e+00, 4.3550e-01,
        6.7195e-02, 3.0593e-01, 1.2607e-01, 3.3129e-02, 1.1656e-01, 1.8219e-01,
        7.0363e-01, 5.0457e-01, 2.1931e+00, 5.2990e-01, 1.2850e-01, 2.7120e-01,
        2.2837e-01, 2.1660e-02, 4.0962e-01, 1.8678e-02, 5.8360e-02, 4.5362e-02,
        2.3154e-01, 5.9031e-01, 1.9354e-01, 2.9719e+00, 1.6952e+00, 1.1303e+01,
        3.0577e+00, 4.5752e+00, 8.6640e+00, 7.4248e+00, 1.2487e-01, 2.4176e-01,
        2.2963e-01, 2.1806e-01, 3.0662e-02, 1.1319e-01, 2.0471e-01, 3.1738e-01,
        1.5365e-01, 5.5278e-01, 1.3086e+01, 2.3053e+00, 1.3226e+00, 2.1327e-01,
        1.8781e+01, 1.6828e-01, 2.7696e+00, 9.6528e-01, 1.2906e-01, 7.4507e-01,
        6.0729e-01, 8.3134e-01, 3.4122e-01, 1.5898e+00, 6.5783e-03, 3.6019e-01,
        8.0912e-01, 3.0079e+00, 5.6595e+00, 7.0832e+00, 6.3881e+00, 6.4856e+00,
        1.0671e-02, 8.5619e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [7/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 149.9
Sum Train Loss:  tensor([9.9881e+00, 7.8946e-01, 3.4822e+00, 8.7522e-01, 3.1701e-01, 3.8996e-01,
        1.4225e-01, 6.5095e-01, 6.7581e-01, 6.8224e-01, 1.7691e-02, 1.4062e-01,
        9.1502e-03, 1.4633e+00, 4.2300e-01, 4.5930e-01, 6.5553e-01, 1.0351e+00,
        3.1117e-01, 5.2493e-01, 6.3631e-02, 3.9143e-02, 5.0925e-02, 1.3675e-01,
        8.2121e-01, 3.8919e-01, 3.2532e+00, 3.5427e-01, 1.1103e-01, 2.2830e-01,
        3.3262e-01, 2.1892e-01, 1.6384e+00, 4.0112e-02, 4.3529e-01, 3.1588e-01,
        1.0929e-01, 5.2488e-01, 2.9943e-01, 2.7230e+00, 2.0373e+00, 8.8908e+00,
        2.1615e+00, 3.5552e+00, 4.9625e+00, 1.0192e+01, 2.5852e-01, 2.7859e-01,
        2.4308e-01, 4.4498e-01, 1.3622e-02, 3.3407e-02, 3.5706e-01, 3.5853e-01,
        1.3558e-01, 3.0012e-01, 1.1453e+01, 8.1248e-01, 1.3229e+00, 1.0871e-01,
        1.9113e+01, 3.9670e-01, 3.1803e+00, 1.3126e+00, 4.3684e-01, 6.8059e-01,
        8.8351e-01, 1.3898e+00, 7.5427e-01, 9.4855e-01, 9.7757e-02, 3.7840e-01,
        1.9210e+00, 1.7218e+00, 8.4221e+00, 7.1003e+00, 1.0067e+00, 1.2072e+01,
        1.1659e-02, 9.9182e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [7/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 145.0
Sum Train Loss:  tensor([1.1874e+01, 1.2083e+00, 9.0198e+00, 9.0946e-01, 1.2847e-01, 4.6387e-01,
        1.4666e-01, 7.0732e-01, 1.2757e+00, 5.7845e-01, 1.3574e-01, 1.1388e-01,
        5.3160e-02, 8.2650e-01, 3.8727e-01, 6.8740e-01, 1.9633e+00, 1.0904e+00,
        8.3424e-02, 2.0274e-01, 4.1744e-02, 9.7798e-03, 1.4056e-02, 2.0119e-01,
        6.0104e-01, 6.6557e-01, 3.7530e+00, 3.9895e-01, 2.6031e-01, 3.1462e-01,
        1.2327e-01, 3.7643e-02, 9.6555e-01, 5.0220e-02, 3.3906e-01, 3.0721e-01,
        1.5530e-01, 4.9702e-01, 6.5436e-01, 2.7728e+00, 7.2064e+00, 1.0045e+01,
        5.3983e+00, 3.4766e+00, 1.1295e+01, 1.1150e+01, 2.9053e-01, 3.9400e-01,
        2.4483e-01, 1.6740e-01, 4.1685e-02, 1.9102e-02, 2.5296e-02, 2.8137e-01,
        3.6216e-02, 6.5898e-01, 1.1175e+01, 1.0630e+00, 2.3738e+00, 2.7212e-01,
        1.6136e+01, 5.8628e-01, 1.1726e+00, 9.8948e-01, 6.8418e-01, 6.9621e-01,
        1.0955e+00, 1.2285e+00, 1.4199e+00, 5.1432e-01, 2.5210e-01, 1.1147e+00,
        8.7826e-01, 2.7157e+00, 1.4247e+01, 1.4039e+01, 5.4605e+00, 1.0651e+01,
        1.0804e-02, 3.1383e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [7/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 183.8
Sum Train Loss:  tensor([8.1161e+00, 1.0664e+00, 5.3453e+00, 6.5872e-01, 3.6258e-01, 7.3399e-01,
        3.3253e-01, 4.7842e-01, 1.1398e+00, 2.9933e-01, 1.9318e-01, 7.5342e-02,
        8.3542e-03, 1.0414e+00, 1.1894e+00, 4.3651e-01, 9.2196e-01, 4.8308e-01,
        2.9607e-01, 9.8231e-01, 2.9824e-02, 6.1329e-02, 1.1844e-01, 7.5765e-02,
        1.8444e+00, 5.0003e-01, 3.0443e+00, 1.7302e-01, 1.7312e-01, 4.6197e-01,
        4.7004e-01, 1.7534e-01, 9.9467e-01, 2.6709e-02, 5.7161e-02, 6.6925e-02,
        1.6300e-01, 5.8372e-01, 2.5692e-01, 3.2624e+00, 3.1766e+00, 6.1098e+00,
        1.1326e+00, 6.3597e+00, 6.6382e+00, 6.9334e+00, 1.5626e-01, 1.3287e-01,
        2.3377e-01, 5.0304e-02, 1.3733e-02, 4.3351e-02, 1.8163e-02, 1.4562e-01,
        1.0308e-01, 4.7203e-01, 7.8541e+00, 8.2713e-01, 2.0423e+00, 2.9161e-01,
        1.5486e+01, 3.4802e-01, 4.0370e+00, 6.2028e-01, 5.1851e-01, 4.5424e-01,
        3.8743e-01, 1.7238e+00, 1.2426e+00, 1.0535e+00, 1.3419e-02, 6.8353e-01,
        1.8004e+00, 2.7181e+00, 2.3325e+00, 4.1444e+00, 9.5301e-01, 8.3464e+00,
        8.3260e-03, 2.6822e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [7/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 126.6
Sum Train Loss:  tensor([1.2093e+01, 2.2190e-01, 6.1205e+00, 3.8568e-01, 3.3883e-01, 5.6800e-01,
        2.0917e-01, 8.0160e-01, 1.3869e+00, 6.5706e-01, 1.0571e-01, 2.7803e-01,
        3.1318e-02, 7.3703e-01, 5.3374e-01, 1.4626e-01, 2.1534e+00, 7.1205e-01,
        4.2567e-01, 5.3336e-01, 1.4481e-02, 3.7737e-02, 2.6941e-02, 6.9691e-02,
        1.2176e+00, 9.9225e-01, 3.4799e+00, 2.6407e-01, 3.4847e-01, 1.7462e-01,
        4.2486e-01, 9.1206e-02, 2.8378e+00, 6.4151e-02, 4.9101e-01, 6.5883e-01,
        2.1970e-01, 2.3005e-01, 3.5469e-01, 3.6082e+00, 2.6717e+00, 1.3546e+01,
        1.9573e+00, 5.4424e+00, 5.5440e+00, 8.0497e+00, 1.9137e-01, 1.9211e-01,
        3.6509e-01, 3.2048e-01, 1.2949e-02, 1.0850e-01, 6.6990e-02, 2.0825e-01,
        2.2721e-01, 6.4864e-01, 1.3153e+01, 9.7870e-01, 2.5758e+00, 1.2325e-01,
        1.7172e+01, 1.1120e+00, 9.0961e-01, 9.9310e-01, 4.5163e-01, 2.5717e-01,
        2.3438e-01, 1.3768e+00, 4.7410e-01, 7.8666e-01, 4.6249e-03, 7.7904e-01,
        5.5341e-01, 3.4813e+00, 5.0028e+00, 1.2064e+01, 5.4788e+00, 6.1264e+00,
        1.2967e-02, 4.7024e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [7/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 158.2
Sum Train Loss:  tensor([1.1286e+01, 1.5331e+00, 7.2080e+00, 7.4052e-01, 1.7701e-01, 5.0582e-01,
        1.5690e-01, 8.3461e-01, 1.7983e+00, 7.3091e-01, 1.6106e-01, 1.0944e-01,
        1.1252e-02, 1.9655e+00, 1.9206e+00, 8.5009e-01, 2.8403e+00, 5.9795e-01,
        1.4435e-01, 5.3286e-01, 3.4659e-02, 8.6633e-03, 7.9374e-02, 1.3091e-01,
        1.3193e+00, 7.0049e-01, 5.1701e+00, 3.6632e-01, 6.6995e-02, 2.5481e-01,
        1.3626e-01, 6.7533e-02, 2.1423e+00, 3.7385e-02, 1.2836e-01, 8.8390e-02,
        3.3053e-01, 3.2240e-01, 2.0402e-01, 5.5504e+00, 3.7690e+00, 1.5864e+01,
        2.3060e+00, 2.6588e+00, 4.7265e+00, 1.0788e+01, 3.4761e-01, 1.4750e-01,
        1.2118e-01, 1.5700e-01, 1.4416e-02, 1.0939e-01, 9.7333e-02, 5.0210e-02,
        2.9043e-02, 3.0443e-01, 1.3510e+01, 1.4599e+00, 4.0013e+00, 3.6677e-01,
        1.3806e+01, 7.2409e-01, 2.1824e+00, 6.5014e-01, 9.5793e-02, 5.5949e-01,
        2.4923e-01, 1.0507e+00, 2.3297e+00, 1.1407e+00, 1.0051e-02, 4.2919e-01,
        3.3640e+00, 2.7460e+00, 1.1530e+01, 2.3034e+01, 4.9462e-01, 1.2350e+01,
        5.2100e-01, 4.5437e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [7/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 189.8
Sum Train Loss:  tensor([6.8528e+00, 1.0245e+00, 4.0138e+00, 4.4513e-01, 3.8517e-02, 1.5707e-01,
        1.8249e-01, 4.1901e-01, 5.6726e-01, 2.6794e-01, 3.4456e-02, 7.9108e-02,
        6.6586e-03, 1.0073e+00, 1.1420e+00, 6.8214e-01, 1.3420e+00, 4.4598e-01,
        3.2820e-02, 1.8822e-01, 3.5604e-02, 4.5800e-02, 3.8494e-02, 1.0890e-02,
        8.5855e-01, 6.6156e-01, 2.4440e+00, 4.9137e-01, 3.1877e-01, 3.0556e-01,
        3.3214e-01, 1.4382e-01, 1.4468e+00, 2.5780e-02, 1.9617e-01, 1.1486e-01,
        4.6472e-01, 4.6491e-01, 3.8785e-01, 3.9345e+00, 5.8386e+00, 1.3130e+01,
        4.1924e+00, 5.3955e+00, 1.0327e+01, 1.1118e+01, 5.3623e-01, 4.7962e-01,
        1.2929e-01, 7.2259e-01, 6.8846e-02, 1.5322e-01, 2.3946e-02, 2.8345e-01,
        4.4449e-02, 2.8085e-01, 1.0994e+01, 1.1339e+00, 2.2861e+00, 3.7960e-01,
        1.4009e+01, 7.8877e-01, 3.5485e+00, 9.9625e-01, 7.1098e-01, 6.8376e-01,
        1.1516e+00, 1.1367e+00, 2.0344e+00, 1.3342e+00, 9.0541e-03, 4.8971e-01,
        3.8358e+00, 5.9998e+00, 6.5565e+00, 1.0665e+01, 2.4120e+00, 1.1471e+01,
        1.2641e-02, 4.9534e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [7/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 167.5
Sum_Val Meta Model:  tensor([1.3415e+01, 4.4311e+00, 1.8513e+01, 3.5763e+00, 5.6574e-02, 7.5026e-01,
        9.9183e-02, 7.1205e-01, 4.6071e-01, 4.8395e-01, 1.1613e-01, 2.5133e-01,
        7.8124e-02, 9.4363e-01, 6.8455e-01, 4.1248e-01, 4.5559e+01, 1.9297e-01,
        6.6969e-02, 1.2581e-01, 1.1964e-02, 8.8757e-03, 2.8071e-02, 2.7519e-02,
        1.3022e+00, 6.9613e-01, 3.9495e+00, 1.0890e-01, 2.5385e-01, 3.0717e-01,
        4.0021e-02, 1.1386e-02, 1.3411e+00, 6.9895e-03, 5.9603e-02, 1.0453e-01,
        1.0295e-01, 2.5946e-01, 5.0178e-02, 9.1003e+00, 4.3874e+00, 1.1718e+01,
        2.1758e+00, 5.8203e+00, 1.1310e+01, 1.2294e+01, 2.9327e-02, 1.4910e-01,
        2.6789e-02, 2.1984e-01, 5.7991e-03, 1.7085e-02, 1.7518e-01, 3.5673e-02,
        4.3584e-02, 1.0388e-01, 1.0674e+01, 7.0835e-01, 3.0665e+00, 1.2046e-01,
        9.2536e+00, 1.6039e+00, 1.4526e+00, 5.2926e-01, 4.8910e-02, 8.5888e-02,
        1.7105e-01, 7.5476e-01, 2.4894e+00, 3.5317e+00, 1.4113e-02, 9.6020e-01,
        7.3048e+00, 2.4455e+00, 9.2210e+00, 7.9696e+00, 6.8795e-01, 7.1365e+00,
        9.1988e-03, 8.0446e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.1776e+01, 3.0451e+00, 1.3475e+01, 2.3521e+00, 1.1242e-02, 7.2905e-01,
        9.7258e-02, 7.4453e-01, 4.7940e-01, 4.1969e-01, 1.6064e-01, 2.3078e-01,
        9.6564e-02, 9.5635e-01, 6.6435e-01, 1.1636e+00, 3.0200e+01, 4.0039e-01,
        4.8415e-02, 1.9735e-01, 2.2956e-02, 1.9729e-02, 8.8359e-03, 3.6907e-02,
        1.2384e+00, 7.0535e-01, 4.0995e+00, 1.4644e-01, 2.8265e-01, 4.2495e-01,
        2.2771e-02, 7.5407e-03, 1.2225e+00, 1.0190e-02, 1.1171e-01, 1.5136e-01,
        2.1336e-01, 1.2037e-01, 2.0145e-02, 8.6827e+00, 4.3089e+00, 1.3012e+01,
        2.0380e+00, 5.0913e+00, 1.2023e+01, 1.0269e+01, 2.4599e-02, 1.4656e-01,
        1.3976e-02, 1.8194e-01, 1.5538e-03, 9.3101e-03, 1.4447e-01, 1.3006e-02,
        3.1738e-02, 6.2923e-02, 1.0200e+01, 6.9352e-01, 2.3064e+00, 1.4244e-01,
        9.1925e+00, 9.2292e-01, 1.2082e+00, 5.6104e-01, 2.8361e-02, 1.2653e-01,
        1.0344e-01, 8.8691e-01, 2.8863e+00, 3.0260e+00, 2.1594e-02, 9.9703e-01,
        6.7608e+00, 2.3875e+00, 9.8678e+00, 6.6420e+00, 9.5677e-01, 6.0353e+00,
        7.9156e-03, 1.0230e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.4866e+01, 4.3653e+01, 6.0833e+01, 2.8837e+01, 3.7274e-01, 1.2501e+01,
        4.8721e+00, 1.7378e+01, 5.6602e+00, 1.0221e+01, 1.0362e+01, 1.2036e+01,
        8.0189e+00, 1.5185e+01, 7.7183e+00, 1.7846e+01, 3.0674e+02, 6.7839e+00,
        1.3711e+00, 2.9256e+00, 2.8868e+00, 1.6473e+00, 3.8103e-01, 2.2620e+00,
        2.6119e+01, 2.0176e+01, 2.9977e+01, 4.9642e+00, 1.1697e+01, 1.3471e+01,
        7.1249e-01, 4.7927e-01, 9.4743e+00, 1.8244e+00, 2.6597e+00, 3.1755e+00,
        8.4364e+00, 2.3388e+00, 3.0047e-01, 4.8819e+01, 1.2510e+01, 2.6027e+01,
        6.7189e+00, 1.2515e+01, 1.7031e+01, 1.7457e+01, 1.2087e+00, 5.2754e+00,
        4.6440e-01, 4.5429e+00, 2.1261e-01, 5.9424e-01, 6.5649e+00, 2.9434e-01,
        1.6001e+00, 1.3078e+00, 2.5388e+01, 6.3278e+00, 1.2411e+01, 5.8197e+00,
        1.2904e+01, 1.2653e+01, 4.6989e+00, 5.3463e+00, 3.7716e-01, 2.0509e+00,
        7.6043e-01, 9.9217e+00, 1.0954e+01, 1.6837e+01, 5.9851e-01, 1.3882e+01,
        1.2375e+01, 1.0860e+01, 1.4122e+01, 6.8470e+00, 1.0535e+00, 6.5015e+00,
        1.0893e-01, 9.4954e-01], device='cuda:0')
Outer loop valEpocw Maximum [7/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 227.5
model_train val_loss valEpocw [7/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 198.2
model_train val_loss valEpocw [7/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1128.1
Sum_Val Meta Model:  tensor([1.5750e+01, 1.4676e+00, 1.6826e+01, 8.2677e-01, 2.4256e-02, 3.1242e+00,
        3.6646e+00, 3.8787e+00, 4.5333e+00, 2.2391e+00, 4.3256e-01, 7.3599e+00,
        1.8886e-01, 4.2304e-01, 1.5428e+00, 1.4310e+00, 1.3116e+00, 1.3913e+00,
        3.2900e-01, 2.3567e+00, 2.7979e-03, 1.5304e-03, 1.6269e-02, 1.1692e-01,
        1.5505e+00, 1.0751e+00, 4.6181e+00, 6.4225e-01, 4.4405e-01, 5.6507e-03,
        1.1024e-02, 4.1664e-03, 4.0739e-02, 2.9281e-03, 6.7125e-03, 6.6867e-03,
        2.0741e-01, 1.6997e-02, 1.0635e-02, 3.7343e-01, 9.0441e-02, 2.2246e+00,
        6.8869e-02, 1.5734e-01, 2.8548e-01, 2.0442e+00, 2.6513e-02, 1.8679e-02,
        7.5027e-03, 3.1586e-01, 1.2825e-03, 3.9212e-03, 2.6892e-03, 6.1025e-03,
        5.8054e-03, 2.9589e-01, 5.0694e+00, 9.0746e-01, 1.6860e+00, 8.0009e-02,
        1.4259e+00, 4.5449e-02, 6.3090e-01, 2.1693e-01, 4.2574e-02, 6.5469e-02,
        9.6326e-02, 1.5095e+00, 8.0415e-02, 9.2313e-01, 4.0598e-03, 5.7784e-02,
        2.4509e+00, 1.4041e+00, 2.9174e+00, 4.7060e-01, 1.6785e-01, 5.1039e-01,
        4.9296e-03, 2.6752e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.5115e+01, 1.7449e+00, 1.2862e+01, 1.0001e+00, 8.2962e-02, 2.6641e+00,
        2.2817e+00, 2.7603e+00, 4.1437e+00, 1.7513e+00, 4.5145e-01, 2.2608e+00,
        1.6869e-01, 5.8948e-01, 1.6105e+00, 2.7449e-01, 1.2537e+00, 1.2263e+00,
        1.3411e-01, 1.4819e+00, 2.2918e-02, 8.1599e-03, 2.9002e-02, 1.2962e-01,
        1.4270e+00, 9.9198e-01, 4.8248e+00, 6.8757e-01, 4.8412e-01, 9.3755e-02,
        2.2480e-02, 7.4241e-03, 1.0034e-01, 2.7905e-02, 1.9991e-02, 1.9280e-02,
        1.6275e-01, 1.5707e-01, 7.9470e-03, 2.7425e-01, 1.1511e-01, 1.6198e+00,
        1.2095e-01, 1.8369e-01, 1.8636e-01, 8.0427e-01, 1.5486e-02, 1.1903e-02,
        9.8784e-03, 2.4567e-01, 1.5533e-03, 6.0834e-03, 9.3055e-03, 3.0482e-02,
        1.9318e-02, 1.5792e-01, 3.9530e+00, 6.6059e-01, 1.3264e+00, 1.3865e-02,
        1.6682e+00, 2.7789e-02, 1.5244e-01, 3.0677e-02, 6.2367e-03, 1.2718e-02,
        2.1064e-02, 1.4570e+00, 4.2961e-02, 8.3708e-01, 1.0759e-03, 2.2342e-02,
        1.8577e+00, 9.2099e-01, 4.3281e+00, 5.8684e-01, 4.0229e-01, 4.0233e-01,
        1.1556e-03, 2.3947e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.5625e+01, 2.0472e+01, 5.1300e+01, 9.7032e+00, 2.1279e+00, 3.2997e+01,
        5.5328e+01, 4.1998e+01, 3.7614e+01, 3.1749e+01, 1.9746e+01, 7.1449e+01,
        7.9697e+00, 8.5047e+00, 1.4500e+01, 2.9421e+00, 1.0515e+01, 1.4840e+01,
        2.1435e+00, 1.3342e+01, 1.5636e+00, 4.2820e-01, 7.5717e-01, 5.3766e+00,
        2.2667e+01, 1.9170e+01, 3.2923e+01, 1.5038e+01, 1.2886e+01, 2.1733e+00,
        4.7457e-01, 2.9499e-01, 6.9690e-01, 2.5282e+00, 3.6987e-01, 3.5165e-01,
        4.2764e+00, 2.3351e+00, 9.5794e-02, 1.6477e+00, 3.7998e-01, 3.6102e+00,
        4.4850e-01, 4.8095e-01, 2.9436e-01, 1.4972e+00, 4.7884e-01, 2.8330e-01,
        2.0533e-01, 4.1504e+00, 1.1211e-01, 2.6001e-01, 2.8705e-01, 5.0328e-01,
        6.1283e-01, 2.5199e+00, 1.0237e+01, 4.7508e+00, 7.2280e+00, 3.6481e-01,
        2.5157e+00, 3.2645e-01, 5.5944e-01, 2.4162e-01, 6.8595e-02, 1.5512e-01,
        1.4148e-01, 1.6657e+01, 1.6619e-01, 4.6053e+00, 2.1376e-02, 2.7260e-01,
        4.0108e+00, 3.9578e+00, 7.1900e+00, 6.1231e-01, 4.4133e-01, 4.3612e-01,
        1.1678e-02, 2.0613e-01], device='cuda:0')
Outer loop valEpocw Maximum [7/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 104.6
model_train val_loss valEpocw [7/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 85.6
model_train val_loss valEpocw [7/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 722.2
Sum_Val Meta Model:  tensor([1.7259e+01, 3.0134e-01, 2.5552e+00, 2.2314e-01, 4.3650e-02, 1.5586e-01,
        3.7658e-02, 5.3227e-01, 2.4702e-01, 1.2006e-01, 2.9652e-02, 3.7549e-02,
        7.8470e-03, 1.1300e+00, 2.4531e-01, 2.5536e-01, 6.6502e-01, 1.3101e-01,
        6.8347e-02, 1.8298e-01, 1.1516e-02, 3.0648e-02, 2.4078e-01, 5.2425e-02,
        3.1752e-01, 8.1208e-02, 2.5736e+00, 2.1973e-01, 5.0160e-02, 1.0881e-01,
        1.8949e-01, 1.1943e-01, 1.2994e+00, 3.8964e-03, 1.4984e-01, 1.7240e-01,
        4.3897e-01, 7.6524e-01, 4.5040e-02, 4.8704e+00, 9.6751e+00, 3.6951e+01,
        2.4034e+01, 3.5688e+01, 3.6957e+01, 4.2158e+01, 4.2276e-01, 4.4213e-01,
        2.3941e+00, 8.4714e-01, 4.2128e-01, 8.1176e-01, 1.5752e+00, 9.1153e-01,
        1.5105e+00, 5.0292e+00, 6.3692e+01, 2.0924e+00, 1.6853e+00, 1.0724e-01,
        6.7377e+01, 2.1025e-01, 2.5801e+00, 1.6311e+00, 1.2730e+00, 1.1588e-01,
        1.3235e+00, 7.9583e-01, 1.2173e+00, 5.5322e-01, 2.4078e-02, 5.8442e-01,
        2.6821e+00, 6.8424e+00, 1.7417e+00, 1.7832e+01, 9.8548e+00, 1.8868e+00,
        3.1876e-02, 1.3773e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([4.6654e+00, 8.0600e-02, 9.1819e-01, 3.0867e-02, 1.7529e-03, 1.2974e-02,
        3.6451e-03, 5.2741e-01, 1.8489e-02, 7.7200e-03, 6.9231e-03, 1.5805e-03,
        1.1006e-03, 9.1788e-01, 2.5532e-02, 9.0039e-02, 2.6823e-01, 3.8975e-02,
        1.1795e-02, 2.4766e-02, 1.9604e-03, 1.9446e-03, 1.2753e-03, 1.8109e-03,
        2.5241e-01, 3.7428e-02, 2.4889e+00, 2.0746e-01, 1.6985e-02, 2.3317e-02,
        1.6377e-02, 1.2004e-01, 9.9749e-01, 1.0691e-03, 2.7989e-02, 3.3580e-02,
        3.4873e-01, 7.8633e-01, 8.7033e-03, 6.1579e+00, 8.4035e+00, 3.1491e+01,
        2.3707e+01, 3.3494e+01, 3.1962e+01, 4.1218e+01, 2.1734e-01, 2.1889e-01,
        1.6393e+00, 4.4726e-01, 1.9779e-01, 7.4926e-01, 1.8433e+00, 8.4060e-01,
        1.0991e+00, 4.2174e+00, 3.8646e+01, 1.5820e+00, 1.7199e+00, 6.4831e-02,
        6.6970e+01, 3.0429e-02, 2.3684e+00, 1.1949e+00, 9.8343e-01, 2.0619e-01,
        1.2097e+00, 9.1333e-01, 1.3441e+00, 3.8930e-01, 6.7907e-03, 4.0564e-01,
        3.5702e+00, 7.6148e+00, 1.7715e+00, 1.9072e+01, 9.6491e+00, 1.9790e+00,
        7.4141e-03, 8.8675e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.4983e+01, 1.3346e+00, 4.4720e+00, 4.2489e-01, 7.2624e-02, 2.4941e-01,
        2.3958e-01, 1.4672e+01, 2.3711e-01, 2.1503e-01, 5.3434e-01, 1.0106e-01,
        1.1718e-01, 1.6634e+01, 3.3929e-01, 1.5651e+00, 2.7967e+00, 7.6639e-01,
        3.5547e-01, 3.8220e-01, 3.4120e-01, 1.6567e-01, 4.6819e-02, 1.2771e-01,
        6.1590e+00, 1.3895e+00, 2.4683e+01, 9.3900e+00, 9.8109e-01, 8.1891e-01,
        5.5957e-01, 7.6711e+00, 7.9747e+00, 1.9303e-01, 6.9520e-01, 7.2431e-01,
        1.3798e+01, 1.5260e+01, 1.4625e-01, 4.0667e+01, 2.7094e+01, 5.9764e+01,
        6.8937e+01, 7.1048e+01, 4.4900e+01, 6.5108e+01, 1.0213e+01, 7.4166e+00,
        3.9926e+01, 1.0060e+01, 2.2746e+01, 4.1618e+01, 7.4517e+01, 1.9822e+01,
        5.4909e+01, 8.4325e+01, 1.0238e+02, 1.5520e+01, 1.1355e+01, 2.7217e+00,
        8.8767e+01, 5.1854e-01, 9.7846e+00, 1.2758e+01, 1.4509e+01, 3.6054e+00,
        1.0083e+01, 1.5068e+01, 6.5469e+00, 2.7819e+00, 2.2485e-01, 7.0598e+00,
        8.0399e+00, 3.9028e+01, 2.9373e+00, 1.9588e+01, 1.0401e+01, 2.1027e+00,
        1.1806e-01, 9.9423e-01], device='cuda:0')
Outer loop valEpocw Maximum [7/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 422.1
model_train val_loss valEpocw [7/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 362.7
model_train val_loss valEpocw [7/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1291.6
Sum_Val Meta Model:  tensor([2.0985e+01, 1.0089e-01, 4.2164e+00, 5.7075e-02, 1.2035e-02, 4.9044e-01,
        9.5378e-02, 5.5318e-01, 9.6610e-02, 5.8944e-01, 5.5632e-02, 6.1053e-02,
        1.5359e-03, 8.0832e-01, 1.0875e+00, 8.1589e-02, 1.2688e-01, 3.1945e-02,
        1.1937e-02, 2.9479e-02, 2.6769e-03, 2.8412e-03, 9.8539e-03, 9.3101e-03,
        8.4852e-01, 4.1131e-02, 2.9666e+00, 2.9565e-02, 2.3320e-01, 7.5006e-03,
        1.5530e-02, 1.8500e-03, 3.2271e-01, 8.7484e-03, 6.6662e-02, 1.9603e-01,
        5.7734e-03, 5.3793e-02, 1.8889e-01, 3.2181e+00, 3.4690e+00, 1.2512e+01,
        2.1443e+00, 3.7192e+00, 4.8727e+00, 7.1943e+00, 9.1405e-03, 1.2668e-02,
        3.4430e-02, 1.5600e-02, 5.7153e-03, 7.1882e-03, 4.8628e-03, 2.3267e-01,
        8.2137e-03, 6.0381e-02, 7.6853e+00, 3.4596e-01, 2.5801e+00, 2.8581e-02,
        2.7192e+01, 4.1619e-02, 1.0464e+00, 5.4162e-01, 2.8301e-01, 8.0960e-02,
        5.8723e-01, 4.5547e+00, 2.7726e-01, 4.3288e-01, 2.6946e-03, 9.1952e-02,
        2.2865e+00, 1.9384e+00, 2.8935e+02, 1.1737e+01, 1.0197e+00, 9.1719e+00,
        4.0309e-03, 8.3844e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([8.5274e+00, 3.9032e-01, 4.6342e+00, 8.4844e-02, 6.3072e-03, 4.1666e-01,
        9.7568e-02, 4.8474e-01, 4.7820e-02, 6.1960e-01, 5.3611e-02, 1.1714e-01,
        8.6643e-03, 7.8144e-01, 1.2125e+00, 2.5255e-02, 6.9045e-02, 3.5142e-02,
        4.0881e-03, 9.4697e-03, 2.6792e-03, 4.4156e-04, 5.0522e-03, 1.3448e-02,
        7.9062e-01, 4.3963e-02, 2.4277e+00, 3.3653e-02, 2.8136e-01, 3.9096e-03,
        4.9293e-03, 3.2041e-03, 2.3183e-02, 1.8006e-03, 4.6453e-03, 6.5807e-03,
        1.9058e-02, 5.8766e-03, 1.8847e-03, 4.2953e-01, 1.7701e-01, 4.8675e-01,
        7.2241e-02, 1.2565e-01, 1.7445e-01, 4.4928e-01, 3.5161e-03, 2.2401e-03,
        1.5637e-03, 2.3602e-03, 5.4867e-04, 1.4366e-03, 9.2773e-04, 2.3491e-03,
        5.9578e-03, 2.8536e-02, 1.1354e+00, 5.8952e-02, 1.9111e+00, 6.3950e-03,
        6.0643e+00, 2.0728e-02, 1.8354e-01, 1.6848e-02, 4.6013e-03, 2.3441e-02,
        1.9020e-02, 6.4138e-01, 6.8293e-02, 4.4645e-02, 8.0169e-04, 2.7668e-02,
        1.5775e-01, 1.0205e+00, 4.4345e+01, 7.1113e+00, 2.1514e-01, 4.4043e+00,
        8.9320e-04, 7.4292e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.2964e+01, 7.3588e+00, 2.5566e+01, 1.3948e+00, 2.9252e-01, 9.7382e+00,
        7.2532e+00, 1.5363e+01, 7.0531e-01, 2.1056e+01, 5.1238e+00, 9.2615e+00,
        1.1591e+00, 1.6317e+01, 1.8557e+01, 5.0537e-01, 8.8309e-01, 8.4149e-01,
        1.4789e-01, 1.9135e-01, 5.4063e-01, 5.1530e-02, 2.9525e-01, 1.1585e+00,
        2.3252e+01, 1.7870e+00, 2.3758e+01, 1.6855e+00, 1.9708e+01, 1.8383e-01,
        2.1951e-01, 2.9347e-01, 1.9667e-01, 4.5431e-01, 1.3152e-01, 1.3144e-01,
        1.0475e+00, 1.4856e-01, 3.0311e-02, 2.8083e+00, 5.3142e-01, 9.4494e-01,
        2.5508e-01, 3.0912e-01, 2.5738e-01, 8.0579e-01, 2.4686e-01, 1.1765e-01,
        7.4302e-02, 8.1969e-02, 1.1183e-01, 1.4189e-01, 6.2706e-02, 6.1208e-02,
        4.5746e-01, 8.7183e-01, 3.0380e+00, 6.4896e-01, 1.1245e+01, 3.6993e-01,
        8.3363e+00, 3.8006e-01, 7.4475e-01, 1.7594e-01, 6.1642e-02, 4.7454e-01,
        1.4212e-01, 7.5285e+00, 3.2924e-01, 3.1512e-01, 3.0913e-02, 5.1726e-01,
        3.2016e-01, 5.2805e+00, 7.0982e+01, 7.2444e+00, 2.2876e-01, 4.6136e+00,
        1.9146e-02, 8.2832e-02], device='cuda:0')
Outer loop valEpocw Maximum [7/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 433.4
model_train val_loss valEpocw [7/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 90.8
model_train val_loss valEpocw [7/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 391.0
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [85.04282355 97.25027717 92.33074646 97.87526955 98.19081152 97.48784737
 98.08481865 95.11092701 97.80948088 96.78244661 98.4527479  98.85966302
 99.41886673 95.32169442 97.35870664 96.96884785 96.31948928 97.57922053
 98.77194479 98.35893812 98.47589576 99.21662748 99.49562018 98.79387434
 95.1231101  96.66061573 94.12409693 96.88722116 98.01415675 98.22005093
 98.22248754 98.52219149 96.90062256 98.41619863 98.55752245 98.67813501
 97.38550944 98.29680438 98.45518451 93.11046405 97.8959808  93.18234427
 97.15524908 96.33776392 97.11017166 94.6772091  98.08238204 98.61234634
 98.00319197 98.65011391 98.63183928 98.55995906 98.99855021 98.15791718
 98.71224766 97.48297414 91.18188131 96.59848199 96.42548215 97.30266444
 93.0885345  98.3833043  96.83361557 97.21494621 98.662297   97.44520656
 98.47102253 95.96130651 98.7597617  98.07141726 99.81603538 97.20154482
 98.40279724 95.68475043 97.21738283 97.58774869 99.18495145 98.51244502
 99.84405648 99.14840219]
Accuracy th:0.7 is [82.89494524 97.27464334 92.11754243 97.82897382 97.9934455  97.41109392
 97.95324131 94.97325812 97.63891765 96.78853815 98.54899429 98.82555037
 99.41399349 95.31682119 97.32703062 96.77026352 96.29755973 97.52926987
 98.70006457 98.32360717 98.46249437 99.2336838  99.47734555 98.75367016
 95.23032127 96.65086926 94.32024464 96.82143249 98.01293844 98.20786784
 98.06532571 98.57336046 96.67158051 98.33822687 98.47102253 98.69397303
 97.22469268 98.19933968 98.19568475 92.84121782 97.86308646 92.63897857
 96.97006615 96.2378626  96.98590417 94.24958273 98.05070601 98.58919847
 97.99710043 98.61478296 98.63549421 98.56605061 98.99976852 97.95933285
 98.70981104 97.4646995  90.30713563 96.3219259  96.37431318 97.15037585
 91.91530318 98.15426225 96.55705949 97.10773504 98.57579708 97.37088973
 98.40279724 95.9588699  98.70615611 97.90572727 99.81603538 96.91036903
 98.28462129 95.52880691 97.17230541 97.54510788 99.18007822 98.4673676
 99.84405648 99.14718388]
Avg Prec: is [94.96845967 27.06347027 65.08491902 60.83487926 69.74617879 60.48333022
 66.90709059 43.66714763 48.82987927 46.44413332 18.65767784 45.5272982
 14.84872907 22.35465696 26.505198   45.76292034 22.73046542 30.55288261
 37.19975355 30.8087191  49.46284033 36.20149899 87.74194716 74.25319987
 22.72600259 22.39401561 34.56503167 31.93442588 17.18324205 32.94638065
 68.02946064 30.04470455 52.78622185 54.57299022 66.74332541 73.31568116
 48.16704816 71.12214188 81.66354079 41.61349409 29.35146801 54.66622651
 46.46438533 41.46845111 40.83543073 54.03244796 28.10865387 25.40891527
 33.10430042 37.27424852 52.99282373 28.32336597 13.2460339  66.80178508
 14.95282034 27.87296382 58.14370975 50.15760391 33.45282975 46.54084163
 71.29168553 76.18095419 54.92673018 42.36689918 50.79750267 34.31689715
 48.36458404 21.0239402  39.08318653 57.64964435  7.23019315 65.52892616
 54.96265343 39.56876735 55.43019335 56.51881067 16.21520298 47.08893529
  2.78337696 13.44387254]
Accuracy th:0.5 is [45.5476907  97.2137279  73.18867948 97.02489005 97.26733349 78.04851305
 78.36892825 77.22493634 79.25585702 96.43888354 79.60794825 98.52097319
 99.41399349 80.58868679 79.17423033 96.56680596 96.29512311 78.93178689
 98.65376884 98.30776915 80.76777817 80.15740549 98.38695922 78.95615307
 80.81772883 96.65086926 94.0778012  78.64548434 98.01293844 79.49586384
 97.30875598 98.57457877 96.36213009 98.02024829 86.94704012 79.10478674
 78.92082211 90.5398326  97.11504489 75.98591635 79.71515942 92.05906361
 78.35309024 77.89500615 96.9627563  93.87434364 98.02877645 98.57336046
 92.45257733 88.24575724 87.14196952 98.55508583 98.99976852 78.60528015
 98.70615611 78.91716719 73.24715829 93.31270331 96.24273583 96.9067141
 89.79300934 97.17717864 91.20502918 78.87939962 98.42838172 79.37037804
 98.20786784 77.98881593 80.03922954 97.55972759 80.62889097 95.99054592
 79.63718766 95.45083515 78.24831569 82.76580451 86.05523812 79.50926524
 80.66178531 99.14718388]
Accuracy th:0.7 is [45.57936672 97.2137279  73.18867948 97.02489005 97.26733349 78.04851305
 78.36892825 77.33458413 79.25585702 96.4742145  79.60794825 98.52097319
 99.41399349 81.0370244  79.17423033 96.56680596 96.29512311 78.93178689
 98.65376884 98.30776915 81.20515101 80.15740549 98.38695922 79.00488542
 81.34403821 96.65086926 94.0778012  78.64548434 98.01293844 79.49586384
 97.30875598 98.57457877 96.36213009 98.02024829 87.14440614 79.10478674
 78.92082211 90.79324082 97.11504489 75.98591635 80.39863062 92.05906361
 78.35309024 77.89500615 96.9627563  93.87434364 98.02877645 98.57336046
 94.23861795 88.91460874 87.31618767 98.55508583 98.99976852 78.60528015
 98.70615611 78.91716719 73.24715829 93.69647056 96.24273583 96.9067141
 89.79300934 97.17717864 91.42432475 78.87939962 98.42838172 79.37037804
 98.20786784 77.98881593 80.03922954 97.55972759 80.62889097 95.99054592
 79.64937074 95.45083515 78.24831569 82.83890303 86.18681546 79.50926524
 80.66178531 99.14718388]
Avg Prec: is [55.90751876  3.05731495 11.26305288  3.32402371  2.20413989  3.82749611
  3.31264107  5.44798956  2.44721835  3.77743139  1.5536715   1.58812102
  0.62533392  5.15014943  2.71516979  3.30183366  3.64007218  2.72205064
  1.46453127  1.72064302  1.91003157  0.90540935  1.80051222  2.46962602
  5.06181872  3.52872915  6.60207742  3.35473001  1.98934026  1.95486474
  2.68043112  1.33556721  3.62969043  1.67445593  2.34615615  2.3363964
  2.99297988  2.59030687  2.7247701   7.63650044  2.42690818  8.40143523
  3.34902922  4.25379497  3.34637275  6.5255918   2.07851522  1.45392369
  2.12100543  1.5340161   1.77711367  1.63927597  1.07335814  3.06154357
  1.41125602  2.71288041 11.40617624  3.78462986  3.96648712  2.7992125
 11.0289611   2.15691307  3.89973644  2.95006759  1.56233971  2.48613808
  1.75125675  4.17247971  1.31268938  2.46220016  0.17132479  3.4069518
  1.96787859  4.54846353  3.90516092  3.16956345  0.81783999  1.87781903
  0.13653203  0.73782974]
mAP score regular 43.93, mAP score EMA 3.78
starting validation
Accuracy th:0.5 is [86.71051648 97.14228766 92.41348382 98.20116102 98.7268605  97.64556394
 98.30081969 95.29112789 97.90218502 96.75112739 98.44532476 98.94112664
 99.34225278 95.11174228 97.34409647 96.93051299 96.27525724 97.65802128
 98.83897651 98.33071729 98.6072701  99.26252585 99.58392506 99.05324264
 95.28863642 96.56177592 94.02546279 97.042629   97.81996661 98.05416449
 98.55744077 98.6147445  96.89314099 98.6072701  98.84146797 99.03580238
 97.83989835 98.20614396 98.77668984 93.1933129  97.89720208 93.39761317
 97.229489   96.55928445 97.04512046 94.67573561 98.21860129 98.7791813
 97.96447168 98.69945437 98.78416424 98.5848469  98.87385704 98.37058076
 98.69945437 97.60320901 90.98587338 96.92802153 96.31013778 97.30672447
 92.99648703 98.58733837 96.94047886 97.27682687 98.6521165  97.49109301
 98.47023943 95.7246431  98.7567581  98.17873782 99.81064853 97.37648554
 98.33570023 95.7171687  97.19460847 97.3117074  99.25754292 98.50511996
 99.82559733 99.15539278]
Accuracy th:0.7 is [85.46976605 97.26935247 92.69003662 98.14136582 98.60477863 97.57331141
 98.21610982 95.21887535 97.76764581 96.87071779 98.58235543 98.95856691
 99.36218452 95.11672522 97.2593866  96.59665645 96.21546204 97.58576874
 98.86389117 98.36061489 98.69197997 99.26750878 99.60385679 99.05075118
 95.45058176 96.53436978 94.571094   96.92553006 97.81747515 98.19866956
 98.42290156 98.67703117 96.75112739 98.54996637 98.73931784 99.05075118
 97.66798714 98.07907915 98.48269676 92.9317089  97.85983008 92.96658943
 97.12235593 96.48952338 97.03017166 94.39419987 98.19368662 98.79662157
 97.96198022 98.7044373  98.75177517 98.5923213  98.87385704 98.17375489
 98.6894885  97.58826021 90.42031044 96.70129805 96.34252685 97.1995914
 92.47826195 98.41791863 96.71375539 97.11488153 98.5848469  97.44873807
 98.44034183 95.7620151  98.76921544 98.01430102 99.81563146 97.28679273
 98.29583676 95.55522336 97.26436953 97.55088821 99.25006852 98.48269676
 99.82559733 99.15040985]
Avg Prec: is [95.65485379 25.55921138 67.4957852  70.1007615  72.65951271 62.79450076
 75.83958265 44.69275316 54.32411332 50.03242374 28.82852981 52.76331405
 16.73724222 24.95342628 29.0732839  54.982135   27.07861139 35.70305987
 39.35587755 30.03769057 59.89853775 48.73201222 90.69984107 81.46314668
 23.54596679 27.91340501 32.70477161 37.90948804 20.72470325 34.13361954
 74.19005076 33.61523658 53.47751137 55.98767544 71.25848712 79.89962947
 53.54546627 74.85693624 87.75165197 42.52048926 30.11959456 51.43148906
 42.30956466 37.67499946 32.78724407 50.08187527 26.46208674 24.07934612
 36.28581177 38.1807479  62.24088624 30.00050954 16.58145286 72.09783398
 16.80031468 29.93809999 55.46166236 50.22816942 33.49666632 52.32702931
 67.03482331 82.7096545  59.29829618 46.67979699 56.4394328  31.86276237
 55.27897479 21.6334327  39.61007159 63.67651465 10.89914398 70.14198941
 50.0060128  38.86259022 62.12893419 51.21679328 10.70960971 45.08547721
  2.01716185 15.04074635]
Accuracy th:0.5 is [45.25749309 97.22450607 71.72683559 96.96290206 97.90716795 77.31270399
 77.42481999 76.19154396 78.80758402 96.41976231 79.06171363 98.5325261
 99.34972718 78.76273762 78.88980243 96.31262924 96.21047911 78.39649201
 98.78167277 98.34068316 79.75434138 79.70202058 98.31327703 78.46874455
 78.82502429 96.52938685 94.3393876  78.39898348 97.81747515 78.99942696
 97.52597354 98.67204823 96.39983058 98.18870369 87.77437277 78.62072402
 78.52853975 91.93263074 97.0276802  75.52383088 78.64314722 92.37362035
 77.72877893 77.36751626 97.03764606 94.02795426 98.18621222 98.77668984
 93.32286917 88.06089145 85.72638712 98.55993223 98.87385704 77.77362533
 98.6969629  78.41144082 72.11052146 94.3244388  96.16314124 96.78102499
 90.13379176 97.04761193 91.15778459 78.31925655 98.32075143 79.17382963
 98.13139996 77.48959813 79.67959738 97.53593941 80.18287366 96.07843137
 79.37563844 95.44559882 77.43229439 83.76560281 87.85908264 79.05174776
 80.25263473 99.15040985]
Accuracy th:0.7 is [45.50414829 97.22450607 71.72683559 96.96290206 97.90716795 77.31270399
 77.42481999 76.21396716 78.80758402 96.41976231 79.06171363 98.5325261
 99.34972718 79.1837955  78.88980243 96.31262924 96.21047911 78.39649201
 98.78167277 98.34068316 80.06079179 79.70202058 98.31327703 78.47372748
 79.17382963 96.52938685 94.3393876  78.39898348 97.81747515 78.99942696
 97.52597354 98.67204823 96.39983058 98.18870369 87.99113038 78.62072402
 78.52853975 92.11450781 97.0276802  75.52383088 79.03679896 92.37362035
 77.72877893 77.36751626 97.03764606 94.02795426 98.18621222 98.77668984
 94.97471161 88.34491865 85.86341779 98.55993223 98.87385704 77.77362533
 98.6969629  78.41144082 72.11052146 94.58355134 96.16314124 96.78102499
 90.13379176 97.04761193 91.35211899 78.31925655 98.32075143 79.17382963
 98.13139996 77.48959813 79.67959738 97.53593941 80.18287366 96.07843137
 79.37563844 95.44559882 77.43229439 83.84283828 87.97618158 79.05174776
 80.25263473 99.15040985]
Avg Prec: is [53.43376259  3.70107998 14.9015026   4.53572751  1.47056221  4.36796168
 14.52396569  8.70971195  8.64370968  5.31911553  2.93314176  5.72271424
  2.35567213  5.81505764  3.0247399   3.70731634 18.70599553  6.48044523
  1.55685362  2.7612339   3.47489377  1.54948066  1.11721982  5.10245406
  5.54685422  8.04118712  7.74818958  4.597436    3.82764311  4.69168089
  2.15425531  0.84191445  3.00106609  1.10549421  1.63929331  2.12300017
  1.95681668  2.18776059  2.24144174  6.2063455   1.72237453  6.00359306
  2.18652896  2.71415982  2.37401993  4.83712035  1.68158111  1.02013515
  1.35619789  1.15274101  1.19337598  0.97064497  0.73319837  2.29616455
  0.83879626  1.82405043  9.81952876  2.86057636  3.69064506  2.69851406
  7.74735535  2.0704443   3.0829852   2.49875554  1.33808442  1.81068464
  1.49235447  3.44463632  1.10502069  2.28818467  0.19091075  3.28798658
  1.60931801  3.85264198  3.53225717  2.30603047  0.59575008  1.50347651
  0.1278029   0.59029889]
mAP score regular 46.41, mAP score EMA 4.23
Train_data_mAP: current_mAP = 43.93, highest_mAP = 43.93
Val_data_mAP: current_mAP = 46.41, highest_mAP = 46.41
tensor([0.1914, 0.0588, 0.1940, 0.0687, 0.0251, 0.0490, 0.0160, 0.0368, 0.0740,
        0.0356, 0.0127, 0.0154, 0.0095, 0.0548, 0.0728, 0.0562, 0.0867, 0.0474,
        0.0318, 0.0579, 0.0062, 0.0106, 0.0198, 0.0139, 0.0397, 0.0282, 0.1112,
        0.0234, 0.0172, 0.0254, 0.0266, 0.0132, 0.1147, 0.0044, 0.0348, 0.0470,
        0.0215, 0.0442, 0.0611, 0.1555, 0.3125, 0.5099, 0.3070, 0.4137, 0.7135,
        0.5833, 0.0170, 0.0227, 0.0257, 0.0342, 0.0060, 0.0125, 0.0183, 0.0386,
        0.0155, 0.0385, 0.3773, 0.0951, 0.1657, 0.0207, 0.7228, 0.0615, 0.2427,
        0.0916, 0.0694, 0.0526, 0.1212, 0.0754, 0.2543, 0.1602, 0.0312, 0.0601,
        0.5300, 0.1984, 0.7326, 0.9782, 0.9631, 0.9524, 0.0788, 0.0953],
       device='cuda:0')
Sum Train Loss:  tensor([7.3033e+00, 1.0566e+00, 3.0409e+00, 2.6399e-01, 7.1353e-02, 4.6965e-01,
        6.6085e-02, 4.7191e-01, 7.0284e-01, 3.2270e-01, 9.1075e-02, 1.8227e-01,
        6.3390e-03, 9.3464e-01, 1.0938e+00, 6.5340e-01, 1.4219e+00, 3.8211e-01,
        2.8630e-01, 2.6151e-01, 3.7248e-02, 2.7277e-02, 1.3227e-01, 3.6541e-02,
        5.8982e-01, 3.0049e-01, 1.5730e+00, 2.6207e-01, 1.3551e-01, 1.7320e-01,
        9.6955e-02, 2.8857e-02, 1.2469e+00, 1.8069e-02, 1.4006e-01, 1.4705e-01,
        2.3420e-01, 3.7334e-01, 6.5063e-01, 3.2820e+00, 3.9862e+00, 9.7086e+00,
        3.7730e+00, 5.3118e+00, 8.8658e+00, 8.2340e+00, 1.3717e-01, 1.1765e-01,
        1.2252e-01, 3.4973e-02, 9.8184e-03, 1.1093e-01, 2.2695e-02, 1.2524e-01,
        8.7285e-02, 3.2804e-01, 6.5528e+00, 6.6249e-01, 3.4910e+00, 1.6820e-01,
        1.4775e+01, 3.7617e-01, 2.4781e+00, 1.4342e+00, 5.4077e-01, 3.9647e-01,
        1.0703e+00, 1.0499e+00, 1.8665e+00, 1.1180e+00, 1.0731e-02, 2.7435e-01,
        4.4916e+00, 2.8223e+00, 9.1375e+00, 8.4049e+00, 1.1108e+01, 1.0446e+01,
        1.2468e-02, 3.3349e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [8/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 152.6
Sum Train Loss:  tensor([8.0277e+00, 1.2886e+00, 5.3901e+00, 6.3836e-01, 1.2376e-01, 3.9858e-01,
        7.7117e-02, 5.1931e-01, 4.9881e-01, 4.0970e-01, 8.7710e-02, 1.0728e-01,
        8.9341e-02, 5.3422e-01, 7.1237e-01, 5.6095e-01, 6.6309e-01, 3.9034e-01,
        8.0023e-02, 2.3473e-01, 3.2411e-02, 8.9035e-03, 7.5316e-02, 2.6012e-01,
        1.0538e+00, 1.3399e-01, 3.7662e+00, 3.2847e-01, 1.5150e-01, 1.7671e-01,
        2.0219e-01, 5.5581e-02, 6.1183e-01, 5.6847e-02, 8.9911e-02, 5.2214e-02,
        1.4073e-01, 4.4180e-01, 6.0773e-01, 3.6612e+00, 2.4267e+00, 4.7425e+00,
        1.8347e+00, 5.2798e+00, 9.2335e+00, 8.7653e+00, 1.8350e-01, 1.6673e-01,
        2.6540e-01, 3.9928e-01, 3.3835e-02, 6.4429e-02, 1.8838e-01, 2.9188e-01,
        1.1693e-01, 2.0294e-01, 1.1385e+01, 7.7625e-01, 2.0737e+00, 1.2526e-01,
        1.0495e+01, 3.8003e-01, 2.5744e+00, 1.1028e+00, 4.7667e-01, 4.4911e-01,
        7.4499e-01, 1.7611e+00, 5.0271e-01, 5.5976e-01, 6.4278e-03, 8.5215e-01,
        1.8196e+00, 2.1638e+00, 1.2645e+01, 4.2142e+00, 7.2893e-01, 1.4618e+00,
        1.6487e-02, 1.2420e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [8/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 124.4
Sum Train Loss:  tensor([9.1265e+00, 1.1724e+00, 4.2185e+00, 1.1468e+00, 1.5626e-01, 6.6946e-01,
        1.1066e-01, 1.0928e+00, 8.4234e-01, 2.7161e-01, 1.1137e-01, 2.6890e-01,
        4.0648e-02, 1.4342e+00, 5.2157e-01, 6.4597e-01, 7.1864e-01, 7.5613e-01,
        1.4862e-01, 8.7385e-02, 8.7078e-02, 8.7969e-02, 1.0339e-02, 9.3514e-02,
        4.9801e-01, 2.9500e-01, 2.8338e+00, 1.3775e-01, 1.8839e-01, 1.8387e-01,
        2.2028e-01, 9.1644e-02, 1.4413e+00, 1.2944e-02, 2.6668e-01, 1.6284e-01,
        1.7094e-01, 1.5165e-01, 2.8119e-01, 2.8444e+00, 1.0940e+00, 1.1677e+01,
        1.2678e+00, 3.8702e+00, 7.9237e+00, 1.3632e+01, 2.6234e-01, 3.4340e-01,
        1.3753e-01, 3.6432e-01, 1.2150e-02, 4.8423e-02, 1.6579e-02, 4.2616e-01,
        1.6369e-01, 1.4421e-01, 8.6313e+00, 7.7856e-01, 2.9161e+00, 1.2172e-01,
        9.9177e+00, 1.7384e-01, 2.4024e+00, 4.8312e-01, 1.2320e-01, 3.9805e-01,
        4.1128e-01, 7.7127e-01, 3.0274e+00, 1.9769e+00, 4.4172e-03, 5.8317e-01,
        1.9521e+00, 2.3811e+00, 1.3315e+01, 9.3224e+00, 1.5076e+00, 8.3903e+00,
        1.2428e-02, 4.1343e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [8/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 145.0
Sum Train Loss:  tensor([6.2274e+00, 5.3772e-01, 6.1652e+00, 2.5124e-01, 1.2268e-01, 5.7142e-01,
        2.1562e-01, 3.9682e-01, 5.9988e-01, 3.7646e-01, 5.2432e-02, 9.5295e-02,
        6.5497e-03, 6.5996e-01, 7.3916e-01, 4.4481e-01, 9.3493e-01, 9.0549e-01,
        1.5480e-01, 6.0584e-01, 7.5102e-02, 2.0292e-02, 1.3962e-02, 2.0824e-02,
        8.3591e-01, 8.6235e-01, 1.3652e+00, 4.4199e-01, 8.0142e-02, 1.9997e-01,
        6.4307e-02, 8.1623e-02, 2.2924e+00, 2.1605e-02, 2.4255e-01, 1.0449e-01,
        8.0287e-02, 6.9401e-02, 7.6498e-01, 3.6878e+00, 5.4038e+00, 1.0562e+01,
        2.1030e+00, 7.1567e+00, 1.0325e+01, 1.4689e+01, 7.7183e-02, 7.4529e-02,
        1.3550e-01, 9.7122e-02, 3.0805e-02, 1.3396e-02, 6.1991e-02, 3.3843e-01,
        9.7633e-02, 5.8129e-01, 7.8297e+00, 1.6424e+00, 1.5935e+00, 9.1584e-02,
        1.4074e+01, 3.1667e-01, 3.4599e+00, 5.5302e-01, 2.0053e-01, 5.6813e-01,
        5.5350e-01, 1.5303e+00, 2.0801e+00, 1.6934e+00, 7.9762e-03, 4.1383e-01,
        1.3003e+00, 3.0956e+00, 8.8548e+00, 1.4005e+01, 4.4680e+00, 7.0533e+00,
        9.7660e-03, 7.6367e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [8/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 159.3
Sum Train Loss:  tensor([8.6973e+00, 4.4604e-01, 4.3896e+00, 2.5531e-01, 1.9570e-01, 5.8687e-01,
        9.3018e-02, 5.8652e-01, 2.9324e-01, 2.3812e-01, 5.7599e-02, 3.1034e-02,
        1.3487e-01, 9.2581e-01, 4.5532e-01, 4.9209e-01, 1.4558e+00, 3.1132e-01,
        3.5958e-01, 3.3475e-01, 2.6196e-02, 1.0104e-02, 2.0997e-02, 3.7833e-02,
        9.3254e-01, 3.6665e-01, 2.0435e+00, 1.8550e-01, 4.2841e-02, 9.1972e-02,
        3.6452e-01, 1.0809e-01, 7.9271e-01, 2.6703e-02, 3.0087e-01, 5.8568e-01,
        8.5698e-02, 7.2648e-01, 2.7910e-01, 2.8161e+00, 1.1962e+00, 1.3319e+01,
        1.3118e+00, 5.7302e+00, 5.4630e+00, 7.1508e+00, 1.2853e-01, 1.2255e-01,
        1.3334e-01, 7.7641e-02, 3.2902e-02, 2.6560e-02, 4.2852e-02, 3.7336e-01,
        5.4515e-02, 8.7526e-02, 9.3023e+00, 9.5387e-01, 3.6775e+00, 2.7408e-01,
        9.1514e+00, 2.5547e-01, 2.5809e+00, 9.1700e-01, 8.2637e-01, 7.3495e-01,
        9.4192e-01, 7.1575e-01, 2.8817e+00, 1.1380e+00, 1.5683e-01, 2.6551e-01,
        1.8065e+00, 3.3637e+00, 7.5617e+00, 1.2044e+01, 1.1536e+00, 3.8891e+00,
        1.1587e-02, 3.0278e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [8/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 130.3
Sum Train Loss:  tensor([7.6773e+00, 9.1867e-01, 7.1193e+00, 9.3516e-01, 5.4244e-02, 7.3423e-01,
        2.8352e-01, 1.2116e+00, 2.5057e-01, 4.0741e-01, 7.7600e-02, 1.9501e-01,
        5.2222e-03, 9.9284e-01, 4.1522e-01, 7.2843e-01, 9.7224e-01, 5.2574e-01,
        2.3784e-01, 6.3417e-01, 1.5993e-02, 5.4675e-02, 1.1881e-02, 4.3136e-02,
        8.6795e-01, 5.6449e-01, 2.2641e+00, 3.0940e-01, 3.8250e-01, 1.8731e-01,
        9.7239e-02, 7.4551e-02, 1.9087e+00, 4.4265e-02, 2.4287e-01, 1.7821e-01,
        1.8350e-01, 1.8423e-01, 1.9058e-01, 4.2317e+00, 2.5667e+00, 1.0468e+01,
        1.4742e+00, 1.5607e+00, 4.6192e+00, 4.6490e+00, 1.1088e-01, 7.5422e-02,
        1.9445e-01, 6.3653e-02, 1.7600e-02, 6.8682e-02, 2.0986e-01, 1.0292e-01,
        9.0749e-02, 9.1392e-02, 7.7134e+00, 1.0019e+00, 1.9594e+00, 1.7229e-01,
        9.5837e+00, 2.2745e-01, 2.0066e+00, 6.9544e-01, 2.4061e-01, 3.8609e-01,
        4.5702e-01, 1.2075e+00, 3.8524e-01, 1.0581e+00, 6.1617e-03, 7.6156e-01,
        1.2293e+00, 3.7047e+00, 8.1442e+00, 7.1283e+00, 7.5382e-01, 1.5079e+01,
        1.3869e-02, 1.1967e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [8/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 126.8
Sum Train Loss:  tensor([9.2945e+00, 3.5535e-01, 6.8772e+00, 3.0990e-01, 1.2771e-01, 8.7579e-01,
        2.3512e-01, 7.3686e-01, 4.2837e-01, 6.0819e-01, 1.8217e-01, 1.9709e-01,
        1.0568e-02, 9.3372e-01, 1.2491e+00, 7.0230e-01, 7.8152e-01, 3.2092e-01,
        1.1358e-01, 1.4358e-01, 1.2624e-02, 8.2975e-03, 1.7537e-02, 4.0760e-02,
        9.1679e-01, 2.9782e-01, 2.8669e+00, 3.8162e-01, 2.9590e-01, 6.5940e-02,
        1.7245e-02, 9.6359e-03, 1.1128e+00, 8.7608e-03, 2.0899e-01, 3.5541e-01,
        2.3598e-01, 1.9167e-01, 4.1598e-01, 2.0828e+00, 2.9548e+00, 7.8917e+00,
        1.7375e+00, 3.0829e+00, 5.4680e+00, 5.3223e+00, 1.2079e-01, 1.4385e-01,
        5.3505e-02, 9.9675e-02, 1.0819e-02, 4.3095e-02, 2.1916e-02, 1.7029e-01,
        7.2512e-02, 3.2498e-01, 6.6285e+00, 1.8569e+00, 2.6231e+00, 2.5167e-01,
        9.6229e+00, 3.1056e-01, 5.0354e+00, 8.6542e-01, 4.7037e-01, 4.3990e-01,
        7.4374e-01, 1.3524e+00, 8.7772e-01, 2.9659e-01, 4.8204e-03, 3.9080e-01,
        9.8779e-01, 2.2625e+00, 1.4534e+01, 8.5449e+00, 4.4768e+00, 5.3187e+00,
        1.3744e-02, 4.3717e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [8/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 129.9
Sum_Val Meta Model:  tensor([1.1033e+01, 4.8726e+00, 1.6487e+01, 2.6978e+00, 2.8363e-02, 5.8322e-01,
        7.4090e-02, 6.1745e-01, 4.3454e-01, 4.2470e-01, 8.4404e-02, 1.8073e-01,
        6.8027e-02, 8.8563e-01, 5.4379e-01, 4.0047e-01, 3.6177e+01, 1.3553e-01,
        4.4682e-02, 5.3496e-02, 5.9866e-03, 4.6766e-03, 1.3219e-02, 1.0616e-02,
        1.1794e+00, 5.3559e-01, 3.1862e+00, 9.5045e-02, 1.7078e-01, 2.7316e-01,
        3.9935e-02, 1.3600e-02, 1.1507e+00, 5.8765e-03, 4.4078e-02, 9.2679e-02,
        8.2441e-02, 2.4205e-01, 1.5183e-01, 7.7001e+00, 3.8315e+00, 1.1875e+01,
        2.3631e+00, 6.0453e+00, 1.1106e+01, 1.0632e+01, 2.7250e-02, 1.2556e-01,
        2.7919e-02, 1.7798e-01, 2.6925e-03, 1.3272e-02, 1.3534e-01, 5.5095e-02,
        1.6867e-02, 5.4133e-02, 1.1260e+01, 7.8465e-01, 2.8779e+00, 4.4418e-02,
        9.6947e+00, 1.2521e+00, 1.6037e+00, 4.7090e-01, 6.5628e-02, 7.9139e-02,
        1.2157e-01, 6.3989e-01, 2.3207e+00, 2.3641e+00, 8.9576e-03, 8.7269e-01,
        7.8532e+00, 2.4746e+00, 1.0144e+01, 7.4819e+00, 6.4390e-01, 7.2015e+00,
        1.2331e-02, 5.2654e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.0347e+01, 3.5542e+00, 1.0895e+01, 1.8761e+00, 1.1038e-02, 5.4596e-01,
        5.8259e-02, 6.6946e-01, 4.1043e-01, 3.7356e-01, 9.7398e-02, 1.8028e-01,
        8.0522e-02, 8.6607e-01, 5.3667e-01, 1.1569e+00, 2.4032e+01, 2.4279e-01,
        3.3774e-02, 8.9415e-02, 6.4820e-03, 5.3246e-03, 3.4585e-03, 1.6211e-02,
        1.0478e+00, 4.8691e-01, 2.9683e+00, 9.4253e-02, 2.0266e-01, 3.0388e-01,
        1.2645e-02, 5.1920e-03, 1.0034e+00, 1.0987e-02, 3.2492e-02, 4.7300e-02,
        1.8431e-01, 1.2747e-01, 6.7375e-02, 6.8817e+00, 3.7993e+00, 1.2622e+01,
        2.0538e+00, 4.4688e+00, 1.1138e+01, 1.0605e+01, 2.4983e-02, 1.2341e-01,
        1.5366e-02, 1.3147e-01, 7.5655e-04, 7.6739e-03, 1.2206e-01, 1.9390e-02,
        1.2705e-02, 2.7545e-02, 1.0217e+01, 6.2632e-01, 2.4124e+00, 5.7248e-02,
        9.1313e+00, 4.1979e-01, 1.3475e+00, 5.2712e-01, 5.1788e-02, 8.1676e-02,
        9.1784e-02, 7.2127e-01, 2.5055e+00, 1.7204e+00, 2.4885e-02, 8.9680e-01,
        5.6415e+00, 2.3805e+00, 1.0607e+01, 8.6710e+00, 7.9596e-01, 6.7896e+00,
        1.3727e-02, 7.0349e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.4052e+01, 6.0473e+01, 5.6150e+01, 2.7307e+01, 4.3967e-01, 1.1149e+01,
        3.6323e+00, 1.8198e+01, 5.5451e+00, 1.0503e+01, 7.6460e+00, 1.1689e+01,
        8.4963e+00, 1.5797e+01, 7.3763e+00, 2.0577e+01, 2.7728e+02, 5.1238e+00,
        1.0613e+00, 1.5443e+00, 1.0422e+00, 5.0243e-01, 1.7441e-01, 1.1663e+00,
        2.6369e+01, 1.7288e+01, 2.6682e+01, 4.0326e+00, 1.1805e+01, 1.1983e+01,
        4.7459e-01, 3.9228e-01, 8.7515e+00, 2.5128e+00, 9.3433e-01, 1.0070e+00,
        8.5771e+00, 2.8822e+00, 1.1029e+00, 4.4246e+01, 1.2159e+01, 2.4753e+01,
        6.6904e+00, 1.0802e+01, 1.5610e+01, 1.8182e+01, 1.4682e+00, 5.4399e+00,
        5.9733e-01, 3.8462e+00, 1.2553e-01, 6.1254e-01, 6.6623e+00, 5.0264e-01,
        8.2052e-01, 7.1558e-01, 2.7079e+01, 6.5852e+00, 1.4560e+01, 2.7650e+00,
        1.2633e+01, 6.8265e+00, 5.5511e+00, 5.7560e+00, 7.4600e-01, 1.5530e+00,
        7.5714e-01, 9.5717e+00, 9.8513e+00, 1.0739e+01, 7.9840e-01, 1.4928e+01,
        1.0644e+01, 1.2001e+01, 1.4478e+01, 8.8642e+00, 8.2645e-01, 7.1288e+00,
        1.7427e-01, 7.3829e-01], device='cuda:0')
Outer loop valEpocw Maximum [8/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 207.6
model_train val_loss valEpocw [8/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 180.5
model_train val_loss valEpocw [8/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1070.5
Sum_Val Meta Model:  tensor([1.3602e+01, 1.2059e+00, 1.4771e+01, 6.1693e-01, 2.2311e-02, 2.8165e+00,
        3.2245e+00, 3.1375e+00, 3.8094e+00, 2.0757e+00, 4.1039e-01, 5.9483e+00,
        1.5457e-01, 3.7769e-01, 1.3007e+00, 1.1275e+00, 1.1754e+00, 1.2085e+00,
        3.9978e-01, 2.2202e+00, 2.1850e-03, 1.2641e-03, 1.5488e-02, 1.1656e-01,
        1.2864e+00, 9.0262e-01, 4.1484e+00, 5.3088e-01, 3.5792e-01, 4.6331e-03,
        9.7853e-03, 3.1920e-03, 3.9292e-02, 2.3065e-03, 5.4212e-03, 6.8789e-03,
        1.5414e-01, 1.3649e-02, 8.9645e-03, 3.3826e-01, 7.5111e-02, 2.0760e+00,
        7.0664e-02, 1.6102e-01, 3.0706e-01, 2.0539e+00, 2.1827e-02, 1.5534e-02,
        6.5681e-03, 2.6299e-01, 9.1282e-04, 3.0049e-03, 2.2114e-03, 5.0411e-03,
        5.5757e-03, 2.1902e-01, 4.5616e+00, 7.1574e-01, 1.6297e+00, 6.3221e-02,
        1.4134e+00, 3.7189e-02, 5.3430e-01, 1.7304e-01, 3.4043e-02, 4.3067e-02,
        7.2271e-02, 1.4108e+00, 6.2036e-02, 7.9819e-01, 2.9148e-03, 4.9908e-02,
        2.3479e+00, 1.1971e+00, 2.9395e+00, 4.0937e-01, 1.4498e-01, 4.8030e-01,
        5.0309e-03, 2.0846e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.4057e+01, 1.2819e+00, 1.0421e+01, 8.3364e-01, 8.9065e-02, 2.3246e+00,
        1.8587e+00, 2.5474e+00, 3.4685e+00, 1.3914e+00, 3.0834e-01, 1.9447e+00,
        1.4802e-01, 6.5969e-01, 1.4327e+00, 2.7541e-01, 1.2196e+00, 1.0415e+00,
        1.5271e-01, 1.5385e+00, 6.8710e-03, 2.5469e-03, 8.9237e-03, 1.2313e-01,
        1.2584e+00, 9.2145e-01, 4.0846e+00, 5.1501e-01, 3.7226e-01, 6.1209e-02,
        1.0175e-02, 4.9192e-03, 8.3744e-02, 2.0636e-02, 9.4274e-03, 1.0325e-02,
        1.2073e-01, 1.4456e-01, 2.8886e-02, 5.2944e-01, 2.1364e-01, 1.6278e+00,
        3.4457e-01, 6.4737e-01, 1.0732e+00, 1.2163e+00, 1.7084e-02, 1.4010e-02,
        1.5606e-02, 1.7463e-01, 1.0274e-03, 7.4362e-03, 5.9695e-03, 4.3175e-02,
        1.0305e-02, 1.3058e-01, 3.6233e+00, 5.9123e-01, 1.1154e+00, 5.7548e-03,
        3.6691e+00, 4.9993e-02, 1.8743e-01, 4.2138e-02, 1.8491e-02, 1.5627e-02,
        2.7211e-02, 1.3120e+00, 9.4483e-02, 3.6077e-01, 1.6512e-03, 6.3722e-02,
        1.3620e+00, 7.9905e-01, 5.8376e+00, 7.3285e-01, 2.8933e-01, 3.8028e-01,
        2.2385e-03, 1.6388e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.9363e+01, 1.7297e+01, 4.5304e+01, 9.3027e+00, 2.5706e+00, 3.2077e+01,
        5.4474e+01, 4.4604e+01, 3.5669e+01, 2.8151e+01, 1.5319e+01, 7.3681e+01,
        8.5987e+00, 1.0638e+01, 1.5013e+01, 3.3720e+00, 1.1589e+01, 1.4819e+01,
        2.6504e+00, 1.6076e+01, 5.7932e-01, 1.4692e-01, 2.7339e-01, 5.8034e+00,
        2.3082e+01, 2.0693e+01, 3.1533e+01, 1.3568e+01, 1.2544e+01, 1.6417e+00,
        2.4341e-01, 2.2327e-01, 6.3955e-01, 2.3601e+00, 2.0221e-01, 1.8870e-01,
        3.6385e+00, 2.4116e+00, 3.8928e-01, 3.5297e+00, 7.8332e-01, 3.6582e+00,
        1.2209e+00, 1.7042e+00, 1.6884e+00, 2.2742e+00, 6.1363e-01, 3.9062e-01,
        3.8297e-01, 3.2881e+00, 8.8701e-02, 3.7077e-01, 2.1555e-01, 8.0864e-01,
        3.9795e-01, 2.4020e+00, 9.9300e+00, 4.8059e+00, 6.4078e+00, 1.6823e-01,
        5.5740e+00, 6.6279e-01, 7.2267e-01, 3.6945e-01, 2.1877e-01, 2.2200e-01,
        1.9894e-01, 1.6385e+01, 3.9701e-01, 2.2548e+00, 3.7255e-02, 8.9395e-01,
        3.1319e+00, 3.7250e+00, 9.2142e+00, 7.5601e-01, 3.0056e-01, 4.0222e-01,
        2.3031e-02, 1.5364e-01], device='cuda:0')
Outer loop valEpocw Maximum [8/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 92.0
model_train val_loss valEpocw [8/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 81.5
model_train val_loss valEpocw [8/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 721.5
Sum_Val Meta Model:  tensor([1.5977e+01, 2.3889e-01, 2.3678e+00, 2.0387e-01, 3.6134e-02, 1.2936e-01,
        3.4532e-02, 5.1151e-01, 2.0572e-01, 9.9901e-02, 2.5122e-02, 2.7905e-02,
        6.3167e-03, 1.0292e+00, 2.1693e-01, 2.2490e-01, 6.4243e-01, 1.1799e-01,
        6.3835e-02, 1.6181e-01, 1.2578e-02, 3.4720e-02, 2.1634e-01, 4.3362e-02,
        2.8812e-01, 7.7308e-02, 2.4400e+00, 1.8442e-01, 4.3232e-02, 8.7675e-02,
        1.8300e-01, 1.5619e-01, 1.0995e+00, 2.5331e-03, 1.2016e-01, 1.5737e-01,
        4.0248e-01, 7.6758e-01, 3.4142e-02, 4.4210e+00, 8.9470e+00, 3.4514e+01,
        2.5008e+01, 3.3656e+01, 3.5294e+01, 4.0976e+01, 3.8286e-01, 3.6849e-01,
        2.1679e+00, 7.3606e-01, 4.2594e-01, 7.3698e-01, 1.5021e+00, 7.8082e-01,
        1.4280e+00, 4.4481e+00, 6.0026e+01, 1.8648e+00, 1.7041e+00, 9.4212e-02,
        6.5242e+01, 1.6970e-01, 2.6881e+00, 1.4483e+00, 1.2280e+00, 8.9718e-02,
        1.3994e+00, 7.2248e-01, 1.1768e+00, 4.7470e-01, 1.9722e-02, 5.1052e-01,
        2.2748e+00, 6.6843e+00, 1.6235e+00, 1.6909e+01, 1.0100e+01, 1.7593e+00,
        2.9162e-02, 1.1087e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([3.6506e+00, 2.1435e-02, 7.1306e-01, 2.2068e-02, 1.6702e-03, 4.2839e-03,
        2.2522e-03, 5.1719e-01, 1.9695e-02, 3.7234e-03, 1.7988e-03, 1.2591e-03,
        5.8340e-04, 7.7337e-01, 1.5294e-02, 5.2326e-02, 2.2301e-01, 1.7390e-02,
        6.2859e-03, 6.6708e-03, 4.6971e-04, 4.9531e-04, 5.6208e-04, 9.0242e-04,
        1.9432e-01, 2.3392e-02, 2.3372e+00, 1.7269e-01, 1.2322e-02, 1.3732e-02,
        1.9715e-02, 1.3026e-01, 8.6882e-01, 8.6667e-04, 1.8334e-02, 3.2350e-02,
        3.2843e-01, 7.1412e-01, 3.6013e-02, 6.2219e+00, 8.2357e+00, 3.2343e+01,
        2.3660e+01, 3.0996e+01, 3.0529e+01, 4.1267e+01, 2.1968e-01, 2.1101e-01,
        1.3748e+00, 4.5467e-01, 1.9096e-01, 6.7210e-01, 1.6462e+00, 1.0547e+00,
        1.1117e+00, 4.2547e+00, 3.2970e+01, 1.4342e+00, 1.5219e+00, 1.4722e-02,
        5.0206e+01, 4.1498e-02, 2.2949e+00, 1.1003e+00, 9.2488e-01, 1.5311e-01,
        1.1544e+00, 8.3497e-01, 1.4164e+00, 1.3985e+00, 8.9950e-03, 5.0279e-01,
        3.1109e+00, 6.9364e+00, 2.9910e+00, 1.5433e+01, 9.7003e+00, 9.1488e-01,
        1.0435e-02, 7.8424e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.1299e+01, 3.9139e-01, 3.7511e+00, 3.3735e-01, 7.1734e-02, 8.5958e-02,
        1.5456e-01, 1.4959e+01, 2.7951e-01, 1.0679e-01, 1.4353e-01, 8.5726e-02,
        6.5543e-02, 1.5142e+01, 2.2305e-01, 9.7290e-01, 2.5675e+00, 3.7320e-01,
        1.9007e-01, 1.1131e-01, 8.5823e-02, 4.1586e-02, 2.2355e-02, 6.5530e-02,
        5.1174e+00, 9.4998e-01, 2.5607e+01, 8.2851e+00, 7.9693e-01, 5.1851e-01,
        7.0471e-01, 8.6316e+00, 7.6258e+00, 1.7373e-01, 5.0793e-01, 6.7164e-01,
        1.3561e+01, 1.4891e+01, 6.4997e-01, 4.4845e+01, 2.8689e+01, 6.3251e+01,
        6.6470e+01, 6.5908e+01, 4.3064e+01, 6.7243e+01, 1.0991e+01, 7.8807e+00,
        3.7850e+01, 1.1131e+01, 2.3308e+01, 4.1362e+01, 7.3530e+01, 2.5985e+01,
        6.1811e+01, 9.4107e+01, 9.2266e+01, 1.5069e+01, 1.0310e+01, 6.3076e-01,
        6.7602e+01, 7.3758e-01, 9.8118e+00, 1.2792e+01, 1.4200e+01, 2.9544e+00,
        1.0153e+01, 1.4012e+01, 7.1732e+00, 1.1014e+01, 3.0716e-01, 9.5291e+00,
        7.5312e+00, 3.8042e+01, 4.6626e+00, 1.5758e+01, 9.9858e+00, 9.5384e-01,
        1.5895e-01, 8.9551e-01], device='cuda:0')
Outer loop valEpocw Maximum [8/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 402.8
model_train val_loss valEpocw [8/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 330.6
model_train val_loss valEpocw [8/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1274.2
Sum_Val Meta Model:  tensor([1.9441e+01, 9.9363e-02, 3.9591e+00, 5.6297e-02, 1.1212e-02, 5.4603e-01,
        8.1476e-02, 5.6001e-01, 9.5761e-02, 5.8433e-01, 5.2412e-02, 6.7668e-02,
        1.5780e-03, 7.6159e-01, 1.0105e+00, 7.5780e-02, 1.2284e-01, 3.3777e-02,
        1.1951e-02, 2.8919e-02, 2.9578e-03, 3.3590e-03, 9.5673e-03, 9.2607e-03,
        8.9844e-01, 4.0727e-02, 3.1693e+00, 2.9425e-02, 2.3054e-01, 7.3597e-03,
        1.4129e-02, 1.9609e-03, 3.2492e-01, 7.7980e-03, 6.2546e-02, 2.5243e-01,
        5.6969e-03, 3.5637e-02, 1.9676e-01, 3.2894e+00, 3.5752e+00, 1.1802e+01,
        2.1632e+00, 3.5543e+00, 4.7813e+00, 7.2060e+00, 9.3617e-03, 1.1579e-02,
        3.2269e-02, 1.4100e-02, 6.4186e-03, 7.1282e-03, 5.1171e-03, 1.6681e-01,
        8.3328e-03, 5.7196e-02, 7.1124e+00, 3.3142e-01, 2.5642e+00, 3.3196e-02,
        2.4696e+01, 3.6643e-02, 1.0855e+00, 4.6780e-01, 2.5206e-01, 8.1929e-02,
        5.2561e-01, 4.5741e+00, 2.6209e-01, 3.5885e-01, 2.8117e-03, 9.1856e-02,
        2.2714e+00, 1.9454e+00, 3.0250e+02, 1.1905e+01, 1.1914e+00, 9.3952e+00,
        5.2413e-03, 9.2797e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([7.0870e+00, 1.1577e-01, 3.7305e+00, 1.1310e-01, 1.2817e-02, 3.9044e-01,
        1.1797e-01, 5.2018e-01, 9.8892e-02, 4.5300e-01, 3.7790e-02, 1.1284e-01,
        5.3305e-03, 7.3735e-01, 1.0770e+00, 4.9537e-02, 1.2954e-01, 3.2944e-02,
        4.2834e-03, 5.1467e-03, 1.4707e-03, 3.6277e-04, 3.9649e-03, 7.6558e-03,
        9.1966e-01, 4.3777e-02, 2.6565e+00, 3.0690e-02, 2.6537e-01, 5.2031e-03,
        4.2151e-03, 3.2855e-03, 2.4686e-02, 2.0999e-03, 2.4529e-03, 3.8572e-03,
        2.0169e-02, 6.2447e-03, 1.2776e-02, 5.4442e-01, 2.7605e-01, 1.0591e+00,
        3.1896e-01, 5.4937e-01, 9.9001e-01, 1.9470e+00, 6.2105e-03, 3.3592e-03,
        5.8145e-03, 8.6755e-03, 6.3275e-04, 2.5379e-03, 1.3072e-03, 8.6737e-03,
        5.5534e-03, 2.8754e-02, 1.8863e+00, 5.9549e-02, 1.8175e+00, 3.1885e-03,
        6.9538e+00, 3.3031e-02, 2.9214e-01, 2.8978e-02, 1.6779e-02, 3.4876e-02,
        3.1922e-02, 5.8010e-01, 1.9692e-01, 4.0043e-01, 1.5840e-03, 5.8178e-02,
        3.5142e-01, 1.0891e+00, 3.9110e+01, 3.8249e+00, 2.0089e-01, 5.6058e+00,
        1.7014e-03, 7.0765e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.8413e+01, 2.2031e+00, 2.0858e+01, 1.9093e+00, 5.7698e-01, 9.0371e+00,
        8.8805e+00, 1.6109e+01, 1.5380e+00, 1.5132e+01, 3.4660e+00, 8.5577e+00,
        6.7163e-01, 1.5084e+01, 1.7196e+01, 9.9999e-01, 1.7484e+00, 7.9988e-01,
        1.5003e-01, 1.0329e-01, 2.9534e-01, 3.7248e-02, 2.2580e-01, 6.2791e-01,
        2.6301e+01, 1.8873e+00, 2.7162e+01, 1.5966e+00, 1.9249e+01, 2.3897e-01,
        1.8879e-01, 2.8989e-01, 2.2091e-01, 5.3186e-01, 7.3678e-02, 6.9942e-02,
        1.1029e+00, 1.7345e-01, 2.0817e-01, 3.6634e+00, 8.7795e-01, 2.1195e+00,
        1.0502e+00, 1.3391e+00, 1.4456e+00, 3.4775e+00, 4.1410e-01, 1.7270e-01,
        2.6275e-01, 2.9857e-01, 1.1831e-01, 2.3659e-01, 8.4176e-02, 2.4057e-01,
        4.2493e-01, 8.4038e-01, 5.2123e+00, 7.0085e-01, 1.0675e+01, 1.7433e-01,
        9.6692e+00, 6.1952e-01, 1.1332e+00, 3.1092e-01, 2.2173e-01, 7.3866e-01,
        2.4345e-01, 6.9145e+00, 9.5102e-01, 3.0819e+00, 6.1937e-02, 1.1486e+00,
        7.2538e-01, 5.6142e+00, 5.9233e+01, 3.8824e+00, 2.0549e-01, 5.7772e+00,
        3.0871e-02, 7.2671e-02], device='cuda:0')
Outer loop valEpocw Maximum [8/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 441.4
model_train val_loss valEpocw [8/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 87.2
model_train val_loss valEpocw [8/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 378.4
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [85.64710469 97.25149547 92.56953497 97.99100888 98.37721275 97.45982627
 98.11283976 95.1657509  97.86064985 96.78244661 98.53559289 98.88159257
 99.41886673 95.34849722 97.34286863 97.0321999  96.2939048  97.59505854
 98.81702221 98.32969871 98.480769   99.24343027 99.50049342 98.78290956
 95.21082833 96.66670728 94.31658971 96.92742535 98.01537506 98.22005093
 98.17984674 98.55021259 96.94448167 98.46249437 98.61234634 98.83286022
 97.41109392 98.32360717 98.73539552 93.19209074 97.92765683 93.54783689
 97.18692511 96.55340456 97.16743217 94.80878644 98.09456512 98.61356465
 98.06898064 98.70859273 98.6903181  98.59163509 99.00220514 98.28462129
 98.70737442 97.48175583 91.52422607 96.72153117 96.42060891 97.22103776
 93.42844264 98.44665635 96.93595351 97.21494621 98.76463493 97.47200936
 98.5087901  95.96374313 98.71955751 98.01537506 99.81603538 97.28682643
 98.46614929 95.78830667 97.36845311 97.75587529 99.19104299 98.59407171
 99.84405648 99.14840219]
Accuracy th:0.7 is [83.15931823 97.21860114 91.88971869 97.9093822  98.24563541 97.17596033
 98.03730461 94.87335681 97.74856544 96.61675662 98.53315627 98.85478978
 99.41643011 95.31560288 97.3111926  96.85432682 96.33776392 97.51099524
 98.73905045 98.31020577 98.35040996 99.20688101 99.47490893 98.64158575
 95.22422972 96.65208757 94.21181516 96.8226508  98.01293844 98.21030446
 97.9240019  98.58554355 96.67523544 98.3284804  98.46858591 98.71955751
 97.25027717 98.17131858 98.66838854 92.92649943 97.91303712 93.0605134
 97.1442843  96.35238362 97.10286181 94.98665952 98.06410741 98.59407171
 98.03730461 98.65986038 98.60259987 98.57336046 98.99976852 98.12258623
 98.70737442 97.46591781 90.77252957 96.47177788 96.3779681  97.05047453
 93.40163984 98.28096636 96.75198889 97.14793923 98.73661383 97.38185451
 98.48442392 95.95277835 98.76341663 98.21274107 99.81603538 97.15281247
 98.35284658 95.60434205 97.38550944 97.63648104 99.18251483 98.46980422
 99.84405648 99.14718388]
Avg Prec: is [95.32770869 27.72640068 66.69686712 63.12503407 73.04318827 61.55465838
 67.9115793  44.40552098 50.88556791 46.28564527 19.92282913 47.8297674
 14.20404582 23.47013709 27.72798632 47.18659407 23.98879569 33.11279465
 41.10097677 32.42945269 52.32422963 37.90269034 87.37946046 76.48249803
 23.11561067 24.07822895 34.77193862 32.51663752 16.36507066 33.34831648
 68.30529402 31.5521693  54.56398203 56.60912025 69.01164192 76.88517474
 50.37044707 72.63713264 82.15617003 42.58690062 33.2687335  57.26292562
 49.11720823 45.45085994 43.77757808 57.92357103 30.24773127 27.98516712
 36.16080147 39.70621356 56.15816363 29.68496755 15.98055573 67.97618821
 16.81689349 32.02586442 60.25372249 52.145272   34.48858613 48.13402588
 73.62108388 77.25011108 57.62158666 41.84392345 53.90231227 37.07526472
 50.56728974 21.23445851 42.15569163 60.94898649  8.26749865 68.90997249
 60.06238893 41.94250617 59.70760144 61.21655531 19.01705452 52.79838406
  2.80109731 16.56759996]
Accuracy th:0.5 is [45.50139496 97.2137279  73.04370073 97.02489005 97.26733349 77.9717596
 78.41156906 77.16889414 79.27169503 96.43766523 79.57261729 98.52097319
 99.41399349 80.53020797 79.10965997 96.56680596 96.29512311 78.82823065
 98.65376884 98.30776915 80.76655986 80.12694777 98.38695922 78.9890474
 80.69955288 96.65086926 94.0778012  78.62720971 98.01293844 79.48733568
 97.30875598 98.57457877 96.36213009 98.02024829 87.14318783 79.10356843
 78.87330807 90.68602965 97.11504489 76.0626698  79.78582132 92.05906361
 78.29826635 77.93521034 96.9627563  93.87434364 98.02877645 98.57336046
 93.19696397 88.30667268 87.22481451 98.55508583 98.99976852 78.52121685
 98.70615611 78.91594888 73.36777086 93.39676661 96.24273583 96.9067141
 89.79300934 97.17717864 91.46574725 78.8903644  98.42838172 79.32773724
 98.20786784 77.95104835 79.98196903 97.55972759 80.5765037  95.99054592
 79.63596935 95.45083515 78.2081115  82.90103678 86.21483656 79.50561031
 80.63620083 99.14718388]
Accuracy th:0.7 is [45.48433864 97.2137279  73.04370073 97.02489005 97.26733349 77.9717596
 78.41156906 77.29072502 79.27169503 96.4742145  79.57261729 98.52097319
 99.41399349 80.97367235 79.10965997 96.56680596 96.29512311 78.82823065
 98.65376884 98.30776915 81.18443976 80.12694777 98.38695922 79.05118115
 81.19662285 96.65086926 94.0778012  78.62720971 98.01293844 79.48733568
 97.30875598 98.57457877 96.36213009 98.02024829 87.34055384 79.10356843
 78.87330807 90.94187449 97.11504489 76.0626698  80.42787003 92.05906361
 78.29826635 77.93521034 96.9627563  93.87434364 98.02877645 98.57336046
 95.012244   88.97552418 87.41487068 98.55508583 98.99976852 78.52121685
 98.70615611 78.91594888 73.36777086 93.79880849 96.24273583 96.9067141
 89.79300934 97.17717864 91.626564   78.8903644  98.42838172 79.32773724
 98.20786784 77.95104835 79.98196903 97.55972759 80.5765037  95.99054592
 79.64937074 95.45083515 78.2081115  82.98388178 86.32326604 79.50561031
 80.63620083 99.14718388]
Avg Prec: is [55.5332708   3.13009452 11.21454006  3.45695803  2.27524833  3.87374246
  3.29514037  5.58792787  2.42397478  3.89299994  1.59310804  1.66636437
  0.67216037  5.00069589  2.65705994  3.12496742  3.66730108  2.67191861
  1.31145549  1.7307242   1.89209174  0.87802245  1.7822402   2.45944096
  5.11560692  3.61118246  6.40575146  3.28876474  2.14030015  1.97237
  2.61583165  1.36551744  3.73851004  1.63464811  2.22612054  2.34248692
  2.99098673  2.55507428  2.83269634  7.39593384  2.37635548  8.27276394
  3.44860091  4.06214064  3.2522322   6.55936643  2.10536681  1.52821159
  2.10676756  1.72145969  1.9056349   1.60436479  1.0922287   2.98862423
  1.29633129  2.61886205 11.2241173   3.82156972  4.02168031  2.85865651
 10.84198165  2.14461519  3.81164439  3.02933603  1.58154288  2.47045865
  1.8086223   4.09539886  1.27125241  2.40512608  0.1916929   3.41304233
  1.9539042   4.67448746  3.95248737  3.15643875  0.81621408  1.93802026
  0.1282632   0.74948531]
mAP score regular 45.91, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [87.12659142 97.25440367 92.83454169 98.18621222 98.82153624 97.54341381
 98.24849889 95.33846575 98.01928395 96.88068366 98.60228717 98.99095598
 99.36218452 95.11672522 97.304233   97.00525699 96.20300471 97.65802128
 98.84645091 98.34815756 98.69197997 99.28744052 99.61631412 99.03580238
 95.43314149 96.55928445 94.51628174 97.03266313 97.81996661 98.02177542
 98.5549493  98.6147445  96.89812393 98.61723597 98.84395944 99.10307198
 97.80750928 98.23105862 98.99095598 93.2082617  97.89471062 93.36522411
 96.89064953 96.47457458 96.79099086 94.18491666 98.22358422 98.8090789
 98.01928395 98.72436904 98.8016045  98.60976157 98.88133144 98.39051249
 98.70942024 97.61317488 90.89867205 97.04013753 96.35000125 97.17467673
 92.49819369 98.67204823 96.95542766 97.3715026  98.73682637 97.52348207
 98.50013703 95.72713456 98.70692877 98.07409622 99.81314    97.41136607
 98.34566609 95.76450657 97.20457433 97.52846501 99.25255998 98.5773725
 99.82559733 99.16535865]
Accuracy th:0.7 is [85.60679672 97.23198047 92.46082169 98.14385729 98.7119117  97.2369634
 98.21860129 95.13665695 97.85983008 96.65894312 98.53750903 98.97849864
 99.35720158 95.12419962 97.25689513 96.71126392 96.30266338 97.55337967
 98.87883997 98.33819169 98.58235543 99.21518798 99.56897626 98.90624611
 95.43812442 96.53187832 94.50631587 96.96041059 97.81747515 98.20116102
 98.37556369 98.66457383 96.77355059 98.5549493  98.68450557 98.96354984
 97.70037621 98.09153649 98.97849864 93.03385903 97.87228742 93.42003638
 97.2369634  96.54184418 97.117373   94.83518948 98.23355009 98.78416424
 97.99935222 98.72935197 98.6745397  98.58982983 98.87385704 98.34566609
 98.6969629  97.58576874 90.63208511 96.84580312 96.31262924 96.99529113
 93.09365423 98.51259436 96.76607619 97.23447193 98.69447144 97.45372101
 98.4727309  95.75952363 98.76672397 98.23355009 99.81563146 97.47116127
 98.34815756 95.58512096 97.266861   97.59573461 99.25505145 98.49764556
 99.82559733 99.15290131]
Avg Prec: is [95.96566912 27.91787526 68.56692286 70.56541902 74.42734107 63.19202968
 75.52504792 44.37693082 56.11330901 50.30535079 30.60929006 53.50994604
 18.22837251 25.85523056 29.99166068 54.49385866 28.56597442 36.5081076
 40.13328322 30.94991959 62.73693648 52.40233854 90.93316107 82.84179938
 23.05390548 29.12282392 32.9022452  38.40371274 19.35298848 34.02555213
 75.58039418 32.58384844 54.23061341 58.69213388 72.56100339 80.88771266
 54.70873177 75.42890746 88.03979147 42.63987694 29.93951826 52.96159758
 43.5324909  39.04331206 34.41309281 51.04424287 28.51355238 24.12975305
 36.76725482 38.9852705  63.37535368 31.22297953 17.86049351 73.91653968
 21.67973227 32.42768113 56.38661345 52.69431195 35.16138989 52.57281558
 67.67283789 83.44754483 58.93091743 47.26081549 57.67189385 33.73534
 55.6337883  21.88520529 38.89288136 64.58889316  9.98146301 72.55899763
 52.21046083 40.62816195 62.81404318 52.1072032  11.42838784 47.50855545
  2.48043794 18.62161098]
Accuracy th:0.5 is [45.28988215 97.22450607 71.60475372 96.96290206 97.90716795 77.18563919
 77.30273812 76.07444503 78.68550216 96.41976231 78.93464883 98.5325261
 99.34972718 78.72785709 78.77270349 96.31262924 96.21047911 78.26942721
 98.78167277 98.34068316 79.68956325 79.57495578 98.31327703 78.34666268
 78.76772056 96.52938685 94.3393876  78.27191868 97.81747515 78.88232803
 97.52597354 98.67204823 96.39983058 98.18870369 87.94379251 78.50362508
 78.41642375 91.98495154 97.0276802  75.41669781 78.54598002 92.37362035
 77.61168    77.25041732 97.03764606 94.02795426 98.18621222 98.77668984
 94.20733986 88.08082318 85.73884446 98.55993223 98.87385704 77.64656053
 98.6969629  78.28935895 72.00338839 94.33440466 96.16314124 96.78102499
 90.13379176 97.04761193 91.35710193 78.20714054 98.32075143 79.04676483
 98.13139996 77.37748212 79.56748138 97.53593941 80.05580885 96.07843137
 79.2585395  95.44559882 77.32017839 83.77806014 87.99362185 78.92966589
 80.12556992 99.15040985]
Accuracy th:0.7 is [45.52408003 97.22450607 71.60475372 96.96290206 97.90716795 77.18563919
 77.30273812 76.10932556 78.68550216 96.41976231 78.93464883 98.5325261
 99.34972718 79.12898323 78.77270349 96.31262924 96.21047911 78.26942721
 98.78167277 98.34068316 79.99352219 79.57495578 98.31327703 78.35912001
 79.10656003 96.52938685 94.3393876  78.27191868 97.81747515 78.88232803
 97.52597354 98.67204823 96.39983058 98.18870369 88.19293918 78.50362508
 78.41642375 92.17181155 97.0276802  75.41669781 78.95458056 92.37362035
 77.61168    77.25041732 97.03764606 94.02795426 98.18621222 98.77668984
 95.68477963 88.35488452 85.90577273 98.55993223 98.87385704 77.64656053
 98.6969629  78.28935895 72.00338839 94.5860428  96.16314124 96.78102499
 90.13379176 97.04761193 91.49164113 78.20714054 98.32075143 79.04676483
 98.13139996 77.37748212 79.56748138 97.53593941 80.05580885 96.07843137
 79.26352244 95.44559882 77.32017839 83.85529561 88.11321225 78.92966589
 80.12556992 99.15040985]
Avg Prec: is [53.53515919  3.70302446 14.94259467  4.54090762  1.44090733  4.35169311
 14.53642341  8.70835252  8.62254245  5.32757795  2.85846804  5.18758081
  2.35958182  5.83199487  2.99886896  3.67378198 19.60014581  6.4886371
  1.53888408  2.75865512  3.47731181  1.55218791  1.19176471  5.1069014
  5.56533192  8.12034533  7.78890295  4.60725194  3.81593842  4.84512007
  2.16375898  0.84140136  3.04267018  1.09992332  1.63789975  2.13357799
  1.94764889  2.17679203  2.25308543  6.18903431  1.73100993  6.01469134
  2.19032931  2.71374818  2.37082036  4.82395267  1.63940506  0.99028569
  1.40010102  1.12626769  1.15179552  0.95073794  0.72513506  2.29038344
  0.84105486  1.81331985  9.89098715  2.87410839  3.70241299  2.73695005
  7.77050188  2.07470603  3.05915103  2.50119292  1.33944914  1.82529377
  1.49140324  3.46099311  1.08882857  2.27550296  0.19131138  3.29420844
  1.59437202  3.86445309  3.5455212   2.30831254  0.59582411  1.50177402
  0.12572965  0.5840679 ]
mAP score regular 47.42, mAP score EMA 4.24
Train_data_mAP: current_mAP = 45.91, highest_mAP = 45.91
Val_data_mAP: current_mAP = 47.42, highest_mAP = 47.42
tensor([0.1664, 0.0496, 0.1745, 0.0553, 0.0202, 0.0398, 0.0117, 0.0295, 0.0611,
        0.0288, 0.0096, 0.0119, 0.0070, 0.0437, 0.0581, 0.0465, 0.0705, 0.0375,
        0.0266, 0.0473, 0.0044, 0.0085, 0.0156, 0.0109, 0.0322, 0.0211, 0.0917,
        0.0174, 0.0122, 0.0198, 0.0204, 0.0102, 0.0961, 0.0030, 0.0268, 0.0422,
        0.0166, 0.0333, 0.0488, 0.1364, 0.2760, 0.4888, 0.3114, 0.4118, 0.7239,
        0.5843, 0.0134, 0.0183, 0.0203, 0.0275, 0.0045, 0.0097, 0.0145, 0.0299,
        0.0115, 0.0312, 0.3494, 0.0769, 0.1541, 0.0164, 0.7205, 0.0494, 0.2383,
        0.0756, 0.0595, 0.0417, 0.1046, 0.0625, 0.2329, 0.1355, 0.0235, 0.0481,
        0.5139, 0.1827, 0.7659, 0.9868, 0.9876, 0.9744, 0.0797, 0.0859],
       device='cuda:0')
Sum Train Loss:  tensor([6.3285e+00, 1.9828e-01, 3.2051e+00, 5.3226e-01, 9.0240e-02, 1.9405e-01,
        5.2702e-02, 7.6484e-01, 2.9642e-01, 2.7350e-01, 2.0615e-02, 1.4823e-01,
        2.6961e-02, 9.9022e-01, 1.0250e+00, 3.8137e-01, 1.0002e+00, 3.7001e-01,
        1.1113e-01, 3.2926e-01, 4.1185e-02, 2.7472e-02, 4.9306e-02, 2.5027e-02,
        4.1124e-01, 4.2997e-01, 2.8469e+00, 2.4516e-01, 1.0341e-01, 9.7115e-02,
        2.9199e-02, 5.4138e-02, 1.1236e+00, 7.4920e-03, 2.4581e-01, 9.6589e-02,
        1.5700e-01, 6.6671e-02, 2.3699e-01, 4.6377e+00, 1.9158e+00, 1.1897e+01,
        3.1088e+00, 3.3057e+00, 6.9433e+00, 8.6361e+00, 6.8282e-02, 1.9688e-01,
        6.8096e-02, 2.3561e-01, 9.9439e-03, 2.7891e-02, 6.7136e-02, 4.1456e-01,
        3.2972e-02, 1.9060e-01, 1.0747e+01, 6.8424e-01, 1.6707e+00, 1.1907e-01,
        1.6083e+01, 3.8163e-01, 2.8364e+00, 6.4543e-01, 1.2201e-01, 4.9206e-01,
        2.2496e-01, 9.5137e-01, 5.1630e-01, 4.9289e-01, 1.5746e-02, 3.8825e-01,
        2.2861e+00, 2.8184e+00, 1.4197e+01, 4.8269e+00, 4.5684e+00, 3.2823e+00,
        2.1403e-02, 5.2615e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [9/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 133.3
Sum Train Loss:  tensor([7.7704e+00, 4.5446e-01, 5.5580e+00, 4.0531e-01, 1.9556e-01, 5.7500e-01,
        1.5415e-01, 5.1645e-01, 2.4057e-01, 3.2118e-01, 8.4882e-02, 5.7926e-02,
        4.4687e-03, 1.3415e+00, 1.0531e+00, 3.0774e-01, 8.0989e-01, 3.1237e-01,
        1.2464e-01, 1.4329e-01, 6.0291e-02, 1.5892e-02, 1.3860e-01, 3.4991e-02,
        7.3682e-01, 1.8820e-01, 1.3711e+00, 8.8502e-02, 2.2028e-02, 5.9159e-02,
        1.3657e-01, 7.1469e-02, 5.0302e-01, 9.5497e-03, 7.9814e-02, 9.1813e-02,
        2.3218e-01, 8.4115e-02, 9.3800e-02, 2.4485e+00, 2.3508e+00, 9.9933e+00,
        3.3787e+00, 5.5350e+00, 5.5612e+00, 1.1115e+01, 6.5942e-02, 7.2971e-02,
        7.5337e-02, 2.4320e-01, 8.1945e-03, 7.2207e-02, 8.7906e-02, 3.7654e-01,
        6.7005e-02, 3.6098e-01, 7.6663e+00, 2.6195e-01, 1.0703e+00, 5.6933e-02,
        1.3370e+01, 1.7255e-01, 1.7415e+00, 8.5915e-01, 1.0668e-01, 3.6250e-01,
        1.0794e+00, 7.5842e-01, 1.6306e+00, 2.1029e+00, 1.1218e-02, 6.6119e-01,
        4.6017e+00, 1.1851e+00, 8.3504e+00, 8.4810e+00, 8.5673e+00, 4.6469e+00,
        6.8736e-03, 5.8654e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [9/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 134.6
Sum Train Loss:  tensor([5.0659e+00, 7.3498e-01, 4.8914e+00, 5.4256e-01, 7.1772e-02, 6.3092e-01,
        9.2950e-02, 7.0362e-01, 4.2308e-01, 4.7109e-01, 5.1966e-02, 1.2768e-01,
        2.4622e-02, 1.1222e+00, 1.2292e+00, 1.3519e-01, 1.1722e+00, 5.2125e-01,
        2.4524e-01, 5.6131e-01, 1.9758e-02, 6.4672e-03, 1.2702e-02, 6.1628e-02,
        6.4775e-01, 2.1796e-01, 2.0300e+00, 1.8826e-01, 1.2606e-01, 1.9900e-01,
        1.4912e-01, 1.1517e-01, 7.0975e-01, 4.8145e-03, 1.0872e-01, 1.3211e-01,
        1.6009e-01, 3.6866e-01, 2.5052e-01, 1.8975e+00, 1.8442e+00, 9.4477e+00,
        2.7891e+00, 3.7173e+00, 8.7532e+00, 9.0216e+00, 1.7677e-01, 9.7408e-02,
        2.0253e-01, 3.4932e-02, 2.1220e-02, 2.3896e-02, 1.0908e-01, 1.4163e-01,
        4.6560e-02, 1.4817e-01, 1.0169e+01, 3.6040e-01, 1.1470e+00, 1.2751e-01,
        7.1734e+00, 1.4173e-01, 1.9877e+00, 3.3745e-01, 1.0122e-01, 5.1921e-01,
        2.0799e-01, 8.2844e-01, 1.3621e+00, 4.6795e-01, 7.8770e-03, 2.3319e-01,
        3.0353e+00, 2.2302e+00, 1.4685e+01, 1.0804e+01, 1.3828e+00, 1.0393e+01,
        6.7302e-01, 1.9784e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [9/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 131.2
Sum Train Loss:  tensor([7.0696e+00, 3.9670e-01, 4.3415e+00, 5.8114e-01, 2.4420e-01, 5.0822e-01,
        1.0920e-01, 4.3448e-01, 5.6068e-01, 5.0130e-01, 5.1610e-02, 1.0479e-01,
        3.4552e-02, 1.0222e+00, 8.4468e-01, 4.5415e-01, 1.4413e+00, 4.1550e-01,
        2.8267e-01, 1.9834e-01, 4.0250e-02, 2.0942e-02, 2.8175e-02, 9.5750e-03,
        5.1334e-01, 1.1624e-01, 3.0331e+00, 1.7224e-01, 7.6059e-02, 1.4461e-01,
        1.1919e-01, 1.1553e-02, 6.8477e-01, 5.8480e-03, 2.5560e-01, 1.8228e-01,
        9.9371e-02, 3.2880e-01, 3.2195e-01, 3.1619e+00, 5.5941e+00, 1.7735e+01,
        4.7282e+00, 5.2141e+00, 1.9045e+01, 1.5284e+01, 2.7449e-01, 8.3355e-02,
        2.7416e-01, 1.9924e-01, 1.2727e-02, 1.7693e-01, 1.1550e-01, 1.6978e-01,
        9.2660e-02, 3.7505e-01, 1.2592e+01, 4.2953e-01, 1.8151e+00, 8.7284e-02,
        8.0560e+00, 5.2570e-02, 1.0936e+00, 6.8627e-01, 4.0616e-02, 2.4076e-01,
        9.4844e-02, 9.0652e-01, 1.5216e+00, 2.5417e+00, 6.7261e-03, 3.2064e-01,
        3.7484e+00, 2.4964e+00, 6.9946e+00, 6.5195e+00, 8.7859e-01, 4.7046e+00,
        3.8506e-01, 7.5120e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [9/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 155.3
Sum Train Loss:  tensor([6.6007e+00, 6.2432e-01, 3.4932e+00, 1.2108e-01, 5.9397e-02, 2.5890e-01,
        2.2176e-02, 6.3188e-01, 3.7510e-01, 1.9703e-01, 5.6954e-02, 1.6960e-02,
        2.5695e-03, 1.0187e+00, 9.6119e-01, 9.2414e-01, 1.1097e+00, 5.7697e-01,
        4.5485e-01, 9.7686e-02, 2.0502e-02, 1.6692e-02, 1.1359e-01, 4.2781e-02,
        6.8725e-01, 4.0484e-01, 1.9081e+00, 3.0470e-01, 3.2871e-02, 3.5418e-01,
        1.4915e-01, 1.4412e-02, 1.7286e+00, 1.8128e-02, 1.4179e-01, 1.5048e-01,
        2.9341e-01, 2.3717e-01, 1.8214e-01, 3.4542e+00, 3.7504e+00, 1.0084e+01,
        2.2904e+00, 3.7818e+00, 1.4803e+01, 1.3516e+01, 1.2150e-01, 3.4970e-02,
        2.3511e-01, 2.3309e-01, 1.5132e-02, 4.4791e-02, 6.0731e-02, 1.4608e-01,
        6.3195e-02, 1.1823e-01, 5.9543e+00, 5.8828e-01, 1.4994e+00, 2.1264e-01,
        1.3772e+01, 1.7570e-01, 2.2607e+00, 1.4241e+00, 1.1822e-01, 3.4047e-01,
        3.8260e-01, 6.8146e-01, 5.4611e-01, 5.6149e-01, 1.5823e-01, 4.6805e-01,
        2.0386e+00, 2.1892e+00, 6.6511e+00, 7.8431e+00, 4.4795e+00, 2.2485e+00,
        1.2783e-02, 2.9004e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [9/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 132.1
Sum Train Loss:  tensor([6.7478e+00, 4.2292e-01, 5.7847e+00, 4.6111e-01, 7.3981e-02, 6.1800e-01,
        6.5685e-02, 8.7039e-01, 7.6801e-01, 2.2150e-01, 1.3836e-02, 1.3916e-02,
        2.2403e-02, 4.1486e-01, 5.8874e-01, 5.4227e-01, 7.6993e-01, 2.7594e-01,
        1.6175e-01, 6.2255e-01, 1.9961e-02, 1.3212e-01, 1.1847e-02, 4.1809e-02,
        5.2673e-01, 2.7476e-01, 2.1094e+00, 2.6879e-01, 1.6267e-01, 2.1925e-01,
        4.9609e-02, 5.8786e-02, 1.2737e+00, 1.9420e-02, 1.5412e-01, 2.5990e-01,
        9.1017e-02, 1.0426e-01, 2.4209e-01, 1.6535e+00, 7.9483e-01, 8.0415e+00,
        2.6177e+00, 5.1912e+00, 6.8078e+00, 1.4533e+01, 5.7206e-02, 1.7425e-01,
        1.6251e-01, 2.7727e-01, 1.3191e-02, 4.4953e-02, 1.6049e-01, 8.5604e-02,
        4.8454e-02, 2.0323e-01, 8.4851e+00, 5.5127e-01, 2.1121e+00, 7.9052e-02,
        1.2143e+01, 3.0218e-01, 1.6597e+00, 4.9766e-01, 2.9846e-01, 2.6196e-01,
        5.9109e-01, 8.0564e-01, 6.5537e-01, 9.5295e-01, 1.2318e-02, 4.3917e-01,
        2.9830e+00, 1.7261e+00, 4.5154e+00, 1.0803e+01, 4.5922e+00, 8.4539e+00,
        8.4443e-01, 1.3598e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [9/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 130.2
Sum Train Loss:  tensor([7.0010e+00, 7.2371e-01, 4.0497e+00, 3.6624e-01, 1.4973e-01, 6.5461e-01,
        1.1774e-01, 6.3897e-01, 3.0938e-01, 1.9662e-01, 5.3496e-02, 1.6188e-02,
        8.5473e-03, 1.1624e+00, 8.4341e-01, 3.4291e-01, 1.9575e+00, 1.0952e-01,
        1.2256e-01, 3.2280e-01, 2.4606e-02, 6.3098e-03, 4.9547e-02, 1.0036e-01,
        3.2177e-01, 1.5283e-01, 2.1580e+00, 2.3669e-01, 1.0322e-01, 2.2539e-01,
        3.0794e-02, 8.0110e-02, 1.7999e+00, 1.9028e-02, 4.3201e-01, 6.6346e-01,
        2.4802e-01, 7.0722e-02, 3.1017e-01, 2.5821e+00, 1.7714e+00, 1.3400e+01,
        5.5036e+00, 5.1440e+00, 4.7615e+00, 9.9145e+00, 5.1264e-02, 7.9019e-02,
        2.8102e-01, 9.6616e-02, 6.6513e-03, 1.9201e-02, 1.9460e-02, 3.7551e-01,
        7.1823e-02, 1.6132e-01, 1.0704e+01, 9.2752e-01, 2.2117e+00, 1.4337e-01,
        1.4977e+01, 2.3400e-01, 2.1567e+00, 1.0662e+00, 3.7361e-01, 5.7983e-01,
        6.5653e-01, 1.7839e+00, 2.8466e-01, 5.4245e-01, 1.3205e-03, 2.6723e-01,
        4.8699e+00, 2.3373e+00, 1.1450e+01, 3.5785e+00, 9.8392e+00, 4.5606e+00,
        2.6499e-01, 9.4999e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [9/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 144.3
Sum_Val Meta Model:  tensor([8.7954e+00, 3.2499e+00, 1.4294e+01, 2.2873e+00, 3.4002e-02, 4.4577e-01,
        5.4079e-02, 5.3777e-01, 4.0797e-01, 3.3015e-01, 6.4233e-02, 1.4575e-01,
        4.6330e-02, 7.0113e-01, 4.6442e-01, 3.9193e-01, 3.5640e+01, 9.8683e-02,
        4.8498e-02, 5.5657e-02, 4.3392e-03, 4.9019e-03, 1.2301e-02, 1.7225e-02,
        1.0338e+00, 4.1061e-01, 2.5965e+00, 7.5257e-02, 1.2856e-01, 2.1369e-01,
        5.1256e-02, 6.4940e-03, 8.7891e-01, 2.4961e-03, 6.7222e-02, 6.9103e-02,
        5.5396e-02, 1.4295e-01, 6.2539e-02, 7.2571e+00, 3.2948e+00, 1.1325e+01,
        2.2883e+00, 6.0607e+00, 9.9974e+00, 1.0573e+01, 2.0422e-02, 9.7256e-02,
        2.3279e-02, 1.4147e-01, 3.7472e-03, 9.7572e-03, 1.0125e-01, 5.2793e-02,
        1.5943e-02, 7.3122e-02, 9.4285e+00, 4.4925e-01, 2.6497e+00, 4.1584e-02,
        8.3483e+00, 7.7518e-01, 1.4239e+00, 4.9026e-01, 4.1495e-02, 3.8128e-02,
        1.4283e-01, 5.4779e-01, 2.1545e+00, 2.4760e+00, 5.5133e-03, 7.1174e-01,
        7.6580e+00, 2.0931e+00, 1.0129e+01, 8.7059e+00, 8.6060e-01, 7.7735e+00,
        5.8934e-03, 4.5467e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([8.7489e+00, 2.2350e+00, 8.9871e+00, 1.3165e+00, 1.7024e-02, 4.2895e-01,
        4.3389e-02, 5.2347e-01, 3.6057e-01, 2.8211e-01, 7.7247e-02, 1.4138e-01,
        5.6257e-02, 6.6162e-01, 4.5129e-01, 1.5305e+00, 2.3822e+01, 1.7022e-01,
        3.8404e-02, 9.3235e-02, 3.9063e-03, 7.3559e-03, 4.5704e-03, 2.4259e-02,
        8.7282e-01, 3.6280e-01, 2.4615e+00, 6.8450e-02, 1.5133e-01, 2.3874e-01,
        2.9118e-02, 5.2818e-03, 7.5038e-01, 4.8518e-03, 3.9916e-02, 3.1497e-02,
        1.2571e-01, 7.7913e-02, 4.1214e-02, 6.8385e+00, 3.7020e+00, 1.2475e+01,
        2.1771e+00, 5.2645e+00, 1.0947e+01, 9.7523e+00, 1.5651e-02, 9.2756e-02,
        9.2619e-03, 1.0931e-01, 8.4664e-04, 4.2316e-03, 9.7835e-02, 1.6587e-02,
        9.6334e-03, 2.8825e-02, 8.5705e+00, 3.8130e-01, 2.1628e+00, 7.1348e-02,
        8.2885e+00, 3.3475e-01, 1.2110e+00, 5.6494e-01, 5.0842e-02, 5.2693e-02,
        1.9137e-01, 6.4530e-01, 2.1043e+00, 1.8980e+00, 1.5795e-02, 7.6635e-01,
        5.7651e+00, 2.0657e+00, 1.0736e+01, 9.9000e+00, 1.2035e+00, 7.4055e+00,
        6.3827e-03, 7.2493e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.2586e+01, 4.5067e+01, 5.1516e+01, 2.3820e+01, 8.4313e-01, 1.0786e+01,
        3.7079e+00, 1.7724e+01, 5.8980e+00, 9.7867e+00, 8.0137e+00, 1.1879e+01,
        8.0417e+00, 1.5135e+01, 7.7736e+00, 3.2907e+01, 3.3770e+02, 4.5378e+00,
        1.4449e+00, 1.9705e+00, 8.9243e-01, 8.6918e-01, 2.9298e-01, 2.2295e+00,
        2.7133e+01, 1.7198e+01, 2.6854e+01, 3.9250e+00, 1.2415e+01, 1.2075e+01,
        1.4289e+00, 5.1551e-01, 7.8084e+00, 1.6358e+00, 1.4872e+00, 7.4662e-01,
        7.5692e+00, 2.3413e+00, 8.4448e-01, 5.0143e+01, 1.3413e+01, 2.5521e+01,
        6.9904e+00, 1.2785e+01, 1.5122e+01, 1.6690e+01, 1.1718e+00, 5.0688e+00,
        4.5621e-01, 3.9812e+00, 1.8614e-01, 4.3590e-01, 6.7644e+00, 5.5506e-01,
        8.3431e-01, 9.2346e-01, 2.4528e+01, 4.9603e+00, 1.4032e+01, 4.3434e+00,
        1.1504e+01, 6.7701e+00, 5.0817e+00, 7.4703e+00, 8.5426e-01, 1.2648e+00,
        1.8290e+00, 1.0328e+01, 9.0359e+00, 1.4007e+01, 6.7253e-01, 1.5941e+01,
        1.1218e+01, 1.1308e+01, 1.4016e+01, 1.0033e+01, 1.2186e+00, 7.6001e+00,
        8.0068e-02, 8.4354e-01], device='cuda:0')
Outer loop valEpocw Maximum [9/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 192.3
model_train val_loss valEpocw [9/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 171.3
model_train val_loss valEpocw [9/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1129.4
Sum_Val Meta Model:  tensor([1.2001e+01, 9.4983e-01, 1.2638e+01, 5.6522e-01, 1.5669e-02, 1.9078e+00,
        2.2102e+00, 2.3081e+00, 2.8238e+00, 1.2922e+00, 2.3933e-01, 4.0439e+00,
        9.4624e-02, 2.9867e-01, 9.6588e-01, 8.6909e-01, 9.5475e-01, 8.4784e-01,
        2.7563e-01, 1.7148e+00, 1.4007e-03, 7.2015e-04, 8.6307e-03, 6.7434e-02,
        1.0019e+00, 6.4023e-01, 3.1866e+00, 3.7108e-01, 2.5393e-01, 2.9843e-03,
        5.5846e-03, 1.7850e-03, 2.7445e-02, 1.1282e-03, 3.1378e-03, 4.6923e-03,
        1.0023e-01, 8.9379e-03, 5.7934e-03, 2.8427e-01, 6.3745e-02, 1.7855e+00,
        6.1014e-02, 1.5529e-01, 2.8284e-01, 2.0082e+00, 1.4069e-02, 1.0981e-02,
        3.8176e-03, 1.9901e-01, 4.8785e-04, 2.0396e-03, 1.1675e-03, 4.1193e-03,
        3.1464e-03, 1.5309e-01, 4.2071e+00, 5.6501e-01, 1.4958e+00, 5.4655e-02,
        1.6315e+00, 2.3802e-02, 5.6346e-01, 1.4868e-01, 2.9004e-02, 3.9420e-02,
        6.2383e-02, 1.0460e+00, 5.8373e-02, 6.5259e-01, 2.2059e-03, 3.5140e-02,
        2.5808e+00, 1.0490e+00, 2.9082e+00, 4.3723e-01, 1.6719e-01, 6.7374e-01,
        4.1205e-03, 1.7207e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.2023e+01, 1.1377e+00, 9.6116e+00, 6.0420e-01, 1.2942e-01, 1.6853e+00,
        1.1976e+00, 1.9567e+00, 2.3125e+00, 1.0070e+00, 2.3761e-01, 1.3304e+00,
        8.9211e-02, 4.4981e-01, 1.1002e+00, 2.2291e-01, 9.2391e-01, 7.4802e-01,
        1.1943e-01, 1.1068e+00, 3.7396e-03, 4.1394e-03, 1.0436e-02, 9.7111e-02,
        1.0349e+00, 5.9369e-01, 3.2203e+00, 3.6722e-01, 2.5016e-01, 3.0439e-02,
        2.6232e-02, 4.4584e-03, 6.5805e-02, 1.0165e-02, 1.5768e-02, 7.5115e-03,
        9.2144e-02, 1.5021e-01, 9.7615e-03, 2.0607e-01, 1.1630e-01, 1.4847e+00,
        2.2059e-01, 3.3092e-01, 3.6757e-01, 7.3938e-01, 1.1382e-02, 5.8562e-03,
        7.3858e-03, 1.7027e-01, 9.3490e-04, 3.6203e-03, 3.8653e-03, 3.5798e-02,
        6.3084e-03, 8.0983e-02, 3.5268e+00, 4.4431e-01, 1.1414e+00, 7.5544e-03,
        3.0600e+00, 4.9445e-02, 1.3775e-01, 5.7060e-02, 2.0244e-02, 8.7212e-03,
        6.4029e-02, 1.0448e+00, 2.7299e-02, 5.0352e-01, 5.1635e-04, 2.2386e-02,
        1.0402e+00, 6.3718e-01, 5.1014e+00, 4.3030e-01, 8.1880e-01, 4.7525e-01,
        1.4583e-03, 1.9101e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.8954e+01, 1.9803e+01, 4.7662e+01, 9.2330e+00, 5.2061e+00, 3.1951e+01,
        5.3678e+01, 4.5778e+01, 3.1576e+01, 2.8831e+01, 1.8972e+01, 7.3483e+01,
        8.0628e+00, 9.7239e+00, 1.5440e+01, 3.5888e+00, 1.1206e+01, 1.4710e+01,
        2.8261e+00, 1.5127e+01, 5.1829e-01, 3.4254e-01, 4.6351e-01, 6.8383e+00,
        2.5639e+01, 2.0076e+01, 3.1412e+01, 1.4378e+01, 1.3070e+01, 1.1593e+00,
        9.4046e-01, 2.9976e-01, 6.2778e-01, 2.0242e+00, 4.7779e-01, 1.6694e-01,
        4.0716e+00, 3.5723e+00, 1.6861e-01, 1.6805e+00, 4.9806e-01, 3.3843e+00,
        7.9456e-01, 8.7995e-01, 5.5504e-01, 1.3215e+00, 5.7100e-01, 2.2679e-01,
        2.5209e-01, 4.4652e+00, 1.2741e-01, 2.6275e-01, 1.9828e-01, 9.1922e-01,
        3.6241e-01, 2.0392e+00, 1.0616e+01, 4.5872e+00, 7.3975e+00, 3.0587e-01,
        4.5145e+00, 8.5867e-01, 5.5269e-01, 6.4892e-01, 2.9810e-01, 1.6031e-01,
        6.0544e-01, 1.7181e+01, 1.2082e-01, 3.6811e+00, 1.6407e-02, 4.2236e-01,
        2.4096e+00, 3.3296e+00, 7.3591e+00, 4.3745e-01, 8.2986e-01, 4.8846e-01,
        1.3533e-02, 2.1238e-01], device='cuda:0')
Outer loop valEpocw Maximum [9/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 76.2
model_train val_loss valEpocw [9/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 66.4
model_train val_loss valEpocw [9/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 727.6
Sum_Val Meta Model:  tensor([1.3180e+01, 1.7143e-01, 1.8975e+00, 1.2211e-01, 2.2763e-02, 9.2061e-02,
        1.6318e-02, 3.3461e-01, 1.3836e-01, 5.9318e-02, 1.6070e-02, 1.6880e-02,
        3.1691e-03, 7.6212e-01, 1.6089e-01, 1.7414e-01, 4.3325e-01, 7.3002e-02,
        3.4911e-02, 1.0392e-01, 7.1744e-03, 1.8726e-02, 1.2414e-01, 2.4297e-02,
        1.9830e-01, 3.9082e-02, 1.7374e+00, 1.2859e-01, 2.7782e-02, 5.6831e-02,
        1.0259e-01, 7.4810e-02, 7.9678e-01, 1.2721e-03, 5.9907e-02, 8.5999e-02,
        2.5875e-01, 4.9224e-01, 2.0560e-02, 3.5355e+00, 8.0755e+00, 3.4095e+01,
        2.6023e+01, 3.4154e+01, 3.6738e+01, 4.2949e+01, 2.4403e-01, 2.6030e-01,
        1.5681e+00, 5.2257e-01, 2.4331e-01, 4.6589e-01, 9.5088e-01, 5.7993e-01,
        8.8237e-01, 3.2712e+00, 5.5543e+01, 1.3102e+00, 1.3281e+00, 6.5539e-02,
        6.7420e+01, 1.1355e-01, 2.5878e+00, 1.0258e+00, 8.7716e-01, 6.7524e-02,
        9.2279e-01, 5.7525e-01, 1.1089e+00, 3.4080e-01, 1.2404e-02, 3.4693e-01,
        2.3345e+00, 6.1166e+00, 1.7037e+00, 1.7335e+01, 1.0289e+01, 1.6977e+00,
        2.7801e-02, 7.9897e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([3.4253e+00, 4.5297e-02, 7.2715e-01, 1.8798e-02, 2.9554e-03, 4.3646e-03,
        2.4317e-03, 3.4025e-01, 1.8077e-02, 4.5268e-03, 1.9205e-03, 1.2449e-03,
        4.1472e-04, 5.9755e-01, 1.5311e-02, 5.3739e-02, 1.2990e-01, 1.7989e-02,
        1.1425e-02, 1.0363e-02, 3.0692e-04, 3.1851e-04, 4.3824e-04, 5.2979e-04,
        1.2568e-01, 2.0859e-02, 1.7472e+00, 1.0226e-01, 7.9100e-03, 8.1550e-03,
        1.0462e-02, 8.0169e-02, 6.9998e-01, 3.7574e-04, 3.2420e-02, 3.0367e-02,
        1.9393e-01, 4.6479e-01, 1.0140e-02, 4.0874e+00, 6.3416e+00, 2.8398e+01,
        2.4347e+01, 3.2940e+01, 3.3383e+01, 4.3474e+01, 1.3369e-01, 1.4609e-01,
        9.6082e-01, 2.7493e-01, 1.0965e-01, 4.4207e-01, 1.1254e+00, 6.6609e-01,
        6.7785e-01, 2.6031e+00, 3.2550e+01, 1.1000e+00, 1.1130e+00, 2.3496e-02,
        6.1774e+01, 2.9138e-02, 2.0586e+00, 8.8075e-01, 7.3352e-01, 8.7071e-02,
        8.1418e-01, 7.1414e-01, 1.1371e+00, 3.4402e-01, 3.7080e-03, 3.1291e-01,
        3.0743e+00, 6.0994e+00, 2.2814e+00, 1.6076e+01, 9.6034e+00, 2.6783e+00,
        6.0437e-03, 5.7936e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.4058e+01, 1.1017e+00, 4.5373e+00, 4.1521e-01, 1.8988e-01, 1.2832e-01,
        2.8027e-01, 1.4772e+01, 3.6269e-01, 1.9898e-01, 2.6644e-01, 1.3159e-01,
        7.7434e-02, 1.6030e+01, 3.0740e-01, 1.3553e+00, 1.9819e+00, 5.5809e-01,
        4.9449e-01, 2.3736e-01, 1.0191e-01, 4.2537e-02, 2.7851e-02, 6.0889e-02,
        4.5890e+00, 1.3930e+00, 2.5401e+01, 8.1364e+00, 8.7336e-01, 4.5550e-01,
        6.1310e-01, 8.4620e+00, 8.0634e+00, 1.4724e-01, 1.3206e+00, 8.5266e-01,
        1.2386e+01, 1.4296e+01, 2.4420e-01, 3.6609e+01, 2.6036e+01, 5.5623e+01,
        6.6817e+01, 6.9919e+01, 4.4744e+01, 6.6436e+01, 9.7203e+00, 7.9142e+00,
        3.8901e+01, 9.4329e+00, 2.3334e+01, 4.1068e+01, 7.3630e+01, 2.3733e+01,
        6.0466e+01, 8.1711e+01, 9.9165e+01, 1.5309e+01, 8.6166e+00, 1.4875e+00,
        8.1253e+01, 7.1905e-01, 9.0880e+00, 1.3852e+01, 1.4332e+01, 2.3095e+00,
        9.4207e+00, 1.6681e+01, 6.1126e+00, 3.2373e+00, 1.8934e-01, 8.3739e+00,
        7.4273e+00, 3.7463e+01, 3.2025e+00, 1.6234e+01, 9.6833e+00, 2.7283e+00,
        8.0403e-02, 8.1722e-01], device='cuda:0')
Outer loop valEpocw Maximum [9/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 389.9
model_train val_loss valEpocw [9/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 332.6
model_train val_loss valEpocw [9/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1258.8
Sum_Val Meta Model:  tensor([1.6415e+01, 8.2001e-02, 3.4146e+00, 4.7259e-02, 9.8820e-03, 3.4753e-01,
        5.1995e-02, 3.6506e-01, 8.0028e-02, 4.0977e-01, 4.1237e-02, 4.7864e-02,
        1.0647e-03, 6.1067e-01, 7.5370e-01, 6.5459e-02, 1.1906e-01, 2.6178e-02,
        1.0677e-02, 2.3356e-02, 2.4802e-03, 3.2397e-03, 7.4720e-03, 6.5483e-03,
        6.1699e-01, 2.8240e-02, 2.4296e+00, 2.1676e-02, 1.3990e-01, 6.3223e-03,
        9.9124e-03, 1.2237e-03, 2.7415e-01, 4.6289e-03, 3.6795e-02, 1.9631e-01,
        4.0393e-03, 2.2581e-02, 1.5343e-01, 2.8681e+00, 3.8086e+00, 1.3960e+01,
        2.9250e+00, 4.4222e+00, 6.1076e+00, 9.0265e+00, 7.7253e-03, 1.0665e-02,
        2.7177e-02, 1.3614e-02, 4.2578e-03, 5.9346e-03, 5.0369e-03, 1.4941e-01,
        9.2310e-03, 5.0912e-02, 7.8601e+00, 3.1553e-01, 2.2479e+00, 2.5581e-02,
        3.0098e+01, 3.0292e-02, 1.2054e+00, 4.3935e-01, 2.4742e-01, 7.7845e-02,
        4.9307e-01, 3.5334e+00, 2.8552e-01, 3.6002e-01, 2.7869e-03, 7.0527e-02,
        2.5080e+00, 1.8403e+00, 3.2143e+02, 1.4017e+01, 1.4324e+00, 1.0485e+01,
        7.8652e-03, 9.0039e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.8164e+00, 1.6821e-01, 2.7188e+00, 5.1782e-02, 2.2530e-02, 3.1623e-01,
        6.0019e-02, 3.5822e-01, 8.9133e-02, 3.4171e-01, 2.8670e-02, 1.0254e-01,
        3.5806e-03, 5.4411e-01, 8.1935e-01, 2.9856e-02, 4.4100e-02, 1.7650e-02,
        4.1349e-03, 3.8917e-03, 7.6778e-04, 2.8791e-04, 2.9406e-03, 3.7572e-03,
        7.0491e-01, 4.1219e-02, 1.7920e+00, 2.6017e-02, 1.7440e-01, 2.4661e-03,
        4.8309e-03, 1.4446e-03, 2.7491e-02, 7.5714e-04, 5.6240e-03, 3.7653e-03,
        1.4031e-02, 8.3618e-03, 8.6626e-03, 2.9354e-01, 1.8022e-01, 7.1972e-01,
        2.9558e-01, 4.3735e-01, 7.4754e-01, 8.5431e-01, 3.1023e-03, 1.2302e-03,
        2.9406e-03, 1.7179e-03, 5.0632e-04, 1.0725e-03, 7.0067e-04, 7.2377e-03,
        2.8826e-03, 3.7605e-02, 1.6020e+00, 3.0148e-02, 1.6303e+00, 4.6327e-03,
        8.6185e+00, 3.2403e-02, 2.5700e-01, 3.3431e-02, 1.7225e-02, 1.4580e-02,
        6.2292e-02, 4.7584e-01, 5.6965e-02, 5.5165e-02, 6.6608e-04, 2.5566e-02,
        1.4325e-01, 9.0282e-01, 5.7908e+01, 3.2465e+00, 5.9879e-01, 5.3363e+00,
        1.2979e-03, 6.3560e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.7456e+01, 4.3666e+00, 1.8416e+01, 1.2670e+00, 1.4904e+00, 1.0403e+01,
        7.2966e+00, 1.6392e+01, 1.9034e+00, 1.7021e+01, 4.4102e+00, 1.1422e+01,
        7.0095e-01, 1.5137e+01, 1.8448e+01, 8.1279e-01, 7.8773e-01, 6.0752e-01,
        2.0354e-01, 1.0559e-01, 2.6913e-01, 4.7655e-02, 2.6620e-01, 4.7827e-01,
        2.7894e+01, 2.7911e+00, 2.4010e+01, 2.1275e+00, 2.0601e+01, 1.6280e-01,
        3.4065e-01, 1.9864e-01, 3.2089e-01, 3.5861e-01, 2.5579e-01, 8.5926e-02,
        1.1327e+00, 3.3837e-01, 1.8930e-01, 2.4506e+00, 6.6122e-01, 1.4457e+00,
        9.3566e-01, 1.0616e+00, 1.0413e+00, 1.4151e+00, 3.0733e-01, 9.0320e-02,
        1.9752e-01, 8.0951e-02, 1.6225e-01, 1.4572e-01, 6.6596e-02, 2.8834e-01,
        3.5402e-01, 1.5523e+00, 4.7420e+00, 4.5171e-01, 1.1106e+01, 3.6926e-01,
        1.1756e+01, 8.0489e-01, 1.0711e+00, 4.8922e-01, 2.8931e-01, 4.1611e-01,
        6.1466e-01, 7.6445e+00, 2.9156e-01, 4.9224e-01, 3.6831e-02, 6.8203e-01,
        3.0150e-01, 5.2854e+00, 8.0063e+01, 3.2704e+00, 6.0300e-01, 5.4142e+00,
        2.0445e-02, 8.2445e-02], device='cuda:0')
Outer loop valEpocw Maximum [9/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 469.4
model_train val_loss valEpocw [9/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 99.0
model_train val_loss valEpocw [9/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 399.1
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [85.89442136 97.2965729  92.42821116 97.96664271 98.41985356 97.45738965
 98.1189313  95.06463128 97.91547374 96.85920006 98.5782337  98.86575456
 99.41643011 95.3326592  97.37819958 97.02123512 96.33898222 97.56338251
 98.8011842  98.3552832  98.46005775 99.29459924 99.51389481 98.79752927
 95.22910296 96.6837636  94.37385022 96.94326336 98.02390322 98.19446644
 98.24076217 98.58676186 96.86163668 98.41985356 98.62940266 98.77194479
 97.43911502 98.36015643 98.80362081 93.22376677 97.90816389 93.42844264
 97.29413628 96.4888342  97.22347437 95.24615928 98.11649468 98.63183928
 98.11527637 98.68178994 98.63183928 98.58554355 99.00098683 98.27487482
 98.70859273 97.51708678 91.58879643 96.63868618 96.43644692 97.31606584
 93.78418879 98.52462811 97.00174218 97.25758702 98.7183392  97.43302348
 98.34797334 95.92597556 98.79143773 98.28462129 99.81481707 97.3392137
 98.48442392 95.75175741 97.4086573  97.72663588 99.21540917 98.65011391
 99.84405648 99.14596557]
Accuracy th:0.7 is [82.91321987 97.26367856 91.45112754 97.88014279 98.36015643 97.21250959
 98.08969189 94.84289909 97.79120625 96.64965095 98.53924782 98.82067714
 99.41399349 95.31682119 97.31606584 96.99443233 96.30486958 97.52439663
 98.79509265 98.31751562 98.32969871 99.2336838  99.49562018 98.64402237
 95.21935649 96.6557425  94.27029398 96.82508741 98.01293844 98.16522703
 98.20908615 98.57336046 96.78975646 98.23588894 98.60259987 98.63549421
 97.2271293  98.29802268 98.6208745  92.88751355 97.88501602 92.78030238
 97.13697445 96.31583436 97.13575614 94.80756813 98.08969189 98.6062548
 98.01537506 98.64036744 98.7037195  98.57701539 98.99976852 98.17131858
 98.70737442 97.4781009  90.74085355 96.35847516 96.32314421 97.15524908
 93.17990765 98.42350849 96.8238691  97.24296731 98.74757861 97.36114326
 98.5087901  95.95156004 98.71712089 98.15791718 99.81603538 97.07484071
 98.36746628 95.61896176 97.31972076 97.56460082 99.21540917 98.58310693
 99.84405648 99.14596557]
Avg Prec: is [95.41624296 28.9555868  66.91903038 62.55975337 73.26538456 60.64617783
 68.75147466 44.40972477 50.34140211 47.90925971 23.28668276 47.97024045
 17.44849597 24.0102112  29.34697036 47.43290276 24.41136666 32.1476376
 41.95675642 32.55654539 53.33706294 42.87872759 87.95527229 77.76932067
 23.97110675 24.75758554 35.90110505 33.70227219 18.68177042 31.97721307
 68.03125895 33.29825634 52.37502227 56.26020782 69.30865622 76.28488881
 51.41357929 72.28254887 83.45509762 44.43563341 32.807515   58.2513805
 50.7047218  44.70917526 45.82762296 60.32362813 30.30041163 28.18099914
 37.63392277 39.16049575 55.29609557 31.50082204 16.62938208 67.73450609
 18.08280778 33.17141697 61.45704723 53.45893957 36.05178378 49.79765352
 75.08544427 78.8366344  59.65768926 43.51995835 54.07563034 37.10669922
 51.65191477 21.48507956 42.5443412  62.25932067  9.34368208 69.13388484
 61.61895512 41.61293573 61.07557574 63.9140433  24.0984273  55.31205201
  4.14005839 17.26966338]
Accuracy th:0.5 is [45.40758519 97.2137279  72.84755303 97.02489005 97.26733349 77.80241469
 78.18374532 77.04340834 79.04874453 96.44863001 79.42763855 98.52097319
 99.41399349 80.55579245 79.02072343 96.56680596 96.29512311 78.72711102
 98.65376884 98.30776915 80.78239788 79.96978594 98.38695922 78.81604756
 80.76777817 96.65086926 94.0778012  78.40669582 98.01293844 79.29362459
 97.30875598 98.57457877 96.36213009 98.02024829 87.05425129 78.83675881
 78.66010404 90.6921212  97.11504489 75.89088827 79.73708897 92.05906361
 78.1386679  77.7341894  96.9627563  93.87434364 98.02877645 98.57336046
 94.04490686 88.36393319 87.28572995 98.55508583 98.99976852 78.35187193
 98.70615611 78.65644912 73.21304565 93.30173853 96.24273583 96.9067141
 89.79300934 97.17717864 91.51935283 78.69908992 98.42838172 79.12184306
 98.20786784 77.83530902 79.8248072  97.55972759 80.36329967 95.99054592
 79.48368075 95.45083515 78.06556938 82.7499665  86.26600553 79.28753305
 80.44492635 99.14718388]
Accuracy th:0.7 is [45.45997247 97.2137279  72.84755303 97.02489005 97.26733349 77.80241469
 78.18374532 77.17133076 79.04874453 96.4754328  79.42763855 98.52097319
 99.41399349 80.9663625  79.02072343 96.56680596 96.29512311 78.72711102
 98.65376884 98.30776915 81.22829888 79.96978594 98.38695922 78.90742072
 81.24901012 96.65086926 94.0778012  78.40669582 98.01293844 79.29362459
 97.30875598 98.57457877 96.36213009 98.02024829 87.2516173  78.83675881
 78.66010404 90.94674772 97.11504489 75.89088827 80.39984893 92.05906361
 78.1386679  77.7341894  96.9627563  93.87434364 98.02877645 98.57336046
 95.64454624 89.00841851 87.44898332 98.55508583 98.99976852 78.35187193
 98.70615611 78.65644912 73.21304565 93.6879424  96.24273583 96.9067141
 89.79300934 97.17717864 91.67285974 78.69908992 98.42838172 79.12184306
 98.20786784 77.83530902 79.8248072  97.55972759 80.36329967 95.99054592
 79.51413847 95.45083515 78.06556938 82.82793826 86.39027302 79.28753305
 80.44492635 99.14718388]
Avg Prec: is [55.7113543   3.07261696 11.06285033  3.41482095  2.25787421  3.89441108
  3.15610006  5.44006645  2.52096231  3.81032364  1.50836132  1.54369544
  0.5807807   5.02969622  2.68227802  3.20816963  3.67222705  2.75055149
  1.36410149  1.78862051  2.03141825  0.88348142  1.84197155  2.45050446
  5.05796107  3.54968369  6.43253176  3.30780829  1.99382648  1.86187138
  2.5955425   1.34647901  3.63866639  1.61181941  2.40107871  2.43560191
  3.07881783  2.61768465  2.87088667  7.38472021  2.2506918   8.09526845
  3.29430834  3.98504901  3.25134374  6.38079682  2.21031309  1.65794292
  2.15481139  1.67422014  1.85943217  1.61444712  1.12482801  2.92974081
  1.36059329  2.70725858 11.25047576  3.71046783  4.02699975  2.81641775
 10.78777001  2.22617901  3.86476089  2.91384835  1.62680015  2.54698259
  1.85434305  4.15720664  1.2536768   2.43597249  0.22980236  3.46282409
  1.89159566  4.57782387  3.94556925  3.17325527  0.80529176  1.97312735
  0.11888996  0.73383811]
mAP score regular 46.78, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [87.2935197  97.3117074  92.9092857  98.22358422 98.83399357 97.55587114
 98.26344769 95.40075242 98.04170715 96.90808979 98.61225303 98.97102424
 99.35969305 95.13914842 97.38395994 97.05259486 96.30515484 97.67795301
 98.82651917 98.37058076 98.70942024 99.33228692 99.60883972 99.05573411
 95.46303909 96.61409672 94.5262476  97.06006926 97.81996661 98.15631462
 98.58733837 98.64713357 96.94546179 98.62471037 98.84645091 99.02334504
 97.86232155 98.22856716 99.00092184 93.18832997 97.87228742 93.48979744
 97.154745   96.59914792 96.97785086 94.88252734 98.26843063 98.7941301
 98.05416449 98.72935197 98.72935197 98.60477863 98.87883997 98.33570023
 98.70692877 97.64805541 91.07307472 96.98034233 96.38239031 97.34409647
 93.05628223 98.7268605  97.07501806 97.33163914 98.67703117 97.51102474
 98.35563196 95.73460896 98.7492837  98.21361836 99.81563146 97.54590527
 98.39798689 95.7993871  97.34409647 97.61815781 99.20522211 98.56491517
 99.82559733 99.17532451]
Accuracy th:0.7 is [85.63171139 97.29426714 92.20669208 98.20863542 98.8016045  97.266861
 98.24351596 95.11423375 97.94453995 96.72621272 98.59481277 98.95358397
 99.35969305 95.11423375 97.29177567 96.99529113 96.25781698 97.56583701
 98.90375464 98.34068316 98.5698981  99.26252585 99.60385679 98.88382291
 95.44061589 96.55181005 94.50880733 96.95044473 97.81747515 98.18621222
 98.64215063 98.68450557 96.83832872 98.4802053  98.77668984 98.84894237
 97.68293594 98.19368662 98.88382291 92.92672596 97.86730448 93.06126517
 97.25689513 96.54931858 97.07501806 94.63587214 98.23853302 98.7791813
 97.97692902 98.72187757 98.77668984 98.59730423 98.87385704 98.4054613
 98.6969629  97.63061514 90.70932058 96.73867006 96.25034258 97.21204873
 93.0214017  98.63218477 96.91556419 97.39641727 98.7268605  97.44873807
 98.50761143 95.7620151  98.73931784 98.09402795 99.81563146 97.31419887
 98.34566609 95.61252709 97.2743354  97.59075168 99.24259412 98.5474749
 99.82559733 99.15539278]
Avg Prec: is [95.9839928  28.74325683 68.00454327 71.09871331 74.09212622 63.58830429
 76.39429895 46.50383456 55.59342908 51.03435939 31.43469253 51.7508635
 18.37979481 27.43707016 31.38258592 54.68900233 29.04932462 38.75227615
 43.69807496 31.76754429 64.41983195 52.91389928 91.36612707 84.39348554
 24.45944052 30.43313261 33.6026335  39.5106749  21.92841427 34.71094259
 75.26425743 35.6089752  54.95447401 58.81043808 71.46351786 80.74838111
 55.61502675 75.62754181 88.41851037 43.72010582 31.23280717 52.81157123
 44.31958985 39.32423564 34.69943178 52.34467892 29.56599383 22.97335274
 38.07966122 39.20466403 61.55676701 31.06209143 17.61048008 73.21446485
 22.96497925 34.30615358 57.18118218 54.09904334 34.91932859 54.57435834
 68.21285959 84.44570704 60.33713465 48.668108   57.82004444 34.46095395
 56.00816848 23.69916806 37.75900215 63.89577213  9.66610798 72.18269839
 53.86400817 40.84225432 63.37573948 54.14823596 11.71757115 47.90892875
  3.09022993 18.63091651]
Accuracy th:0.5 is [45.30732242 97.22450607 71.49263772 96.96290206 97.90716795 77.06854025
 77.18065625 75.96232902 78.57836909 96.41976231 78.81256696 98.5325261
 99.34972718 78.70792536 78.66058749 96.31262924 96.21047911 78.14734534
 98.78167277 98.34068316 79.64222538 79.45287391 98.31327703 78.23703814
 78.72038269 96.52938685 94.3393876  78.15980268 97.81747515 78.76522909
 97.52597354 98.67204823 96.39983058 98.18870369 88.10324638 78.38154321
 78.30929068 92.00239181 97.0276802  75.31953061 78.48120188 92.37362035
 77.48959813 77.13830132 97.03764606 94.02795426 98.18621222 98.77668984
 95.02952388 88.12566958 85.79365673 98.55993223 98.87385704 77.52447866
 98.6969629  78.17226001 71.91618706 94.34935346 96.16314124 96.78102499
 90.13379176 97.04761193 91.49164113 78.08505867 98.32075143 78.95458056
 98.13139996 77.26038319 79.45536537 97.53593941 79.93372699 96.07843137
 79.15140643 95.44559882 77.20307945 83.77806014 88.10822931 78.80758402
 80.00348805 99.15040985]
Accuracy th:0.7 is [45.52158856 97.22450607 71.49263772 96.96290206 97.90716795 77.06854025
 77.18065625 76.00717542 78.57836909 96.41976231 78.81256696 98.5325261
 99.34972718 79.08911976 78.66058749 96.31262924 96.21047911 78.14734534
 98.78167277 98.34068316 79.95864165 79.45287391 98.31327703 78.24451254
 79.06669656 96.52938685 94.3393876  78.15980268 97.81747515 78.76522909
 97.52597354 98.67204823 96.39983058 98.18870369 88.31502105 78.38154321
 78.30929068 92.21416648 97.0276802  75.31953061 78.89478536 92.37362035
 77.48959813 77.13830132 97.03764606 94.02795426 98.18621222 98.77668984
 96.27027431 88.42962852 85.9531106  98.55993223 98.87385704 77.52447866
 98.6969629  78.17226001 71.91618706 94.59850014 96.16314124 96.78102499
 90.13379176 97.04761193 91.61870593 78.08505867 98.32075143 78.95458056
 98.13139996 77.26038319 79.45536537 97.53593941 79.93372699 96.07843137
 79.15888083 95.44559882 77.20307945 83.86027855 88.25024292 78.80758402
 80.00348805 99.15040985]
Avg Prec: is [53.58061387  3.70671482 14.92372996  4.54224924  1.44196865  4.32861155
 14.54169193  8.69785629  8.54952534  5.3173877   2.78869232  5.00037716
  2.34739297  5.84476281  2.99347937  3.67184564 20.54171928  6.50415437
  1.53745852  2.74288849  3.4768612   1.55244569  1.19242016  5.10370173
  5.57473275  8.22261412  7.80935989  4.5879383   3.82707696  4.96492409
  2.17103779  0.84299254  3.03830433  1.10096568  1.64021338  2.13592287
  1.95731526  2.18143805  2.25324161  6.19355525  1.72946597  6.01634809
  2.18801384  2.71349032  2.36757354  4.82585004  1.64742635  0.99247638
  1.40152533  1.12780682  1.15159527  0.9505634   0.72536753  2.29279574
  0.83914971  1.82049358  9.9156295   2.87522598  3.66513023  2.75187591
  7.78653907  2.08638172  3.0610476   2.50618622  1.34309301  1.82551198
  1.49466506  3.47461129  1.09134805  2.27440234  0.19144414  3.29709476
  1.59081748  3.87491357  3.54615555  2.30831268  0.59527101  1.51755639
  0.12597491  0.58415628]
mAP score regular 48.05, mAP score EMA 4.25
Train_data_mAP: current_mAP = 46.78, highest_mAP = 46.78
Val_data_mAP: current_mAP = 48.05, highest_mAP = 48.05
tensor([0.1477, 0.0420, 0.1568, 0.0441, 0.0168, 0.0335, 0.0093, 0.0242, 0.0505,
        0.0231, 0.0074, 0.0104, 0.0059, 0.0378, 0.0479, 0.0390, 0.0594, 0.0319,
        0.0228, 0.0406, 0.0033, 0.0068, 0.0126, 0.0090, 0.0271, 0.0165, 0.0772,
        0.0139, 0.0095, 0.0170, 0.0159, 0.0083, 0.0822, 0.0022, 0.0214, 0.0381,
        0.0142, 0.0269, 0.0438, 0.1205, 0.2468, 0.4816, 0.3284, 0.4147, 0.7454,
        0.6218, 0.0114, 0.0155, 0.0170, 0.0244, 0.0036, 0.0087, 0.0123, 0.0251,
        0.0091, 0.0268, 0.3344, 0.0684, 0.1417, 0.0139, 0.7248, 0.0435, 0.2300,
        0.0632, 0.0528, 0.0361, 0.0896, 0.0535, 0.2399, 0.1276, 0.0205, 0.0415,
        0.5044, 0.1707, 0.7987, 0.9916, 0.9949, 0.9847, 0.1116, 0.0766],
       device='cuda:0')
Sum Train Loss:  tensor([6.3390e+00, 7.1214e-01, 4.5235e+00, 4.8773e-01, 1.0777e-01, 2.3949e-01,
        5.4884e-02, 3.1727e-01, 1.8528e-01, 2.1025e-01, 8.9171e-02, 1.6680e-02,
        7.5543e-02, 5.7102e-01, 4.9276e-01, 4.8947e-01, 5.4168e-01, 1.2960e-01,
        1.9974e-01, 2.1011e-01, 2.5277e-02, 1.9584e-02, 4.0964e-02, 5.5860e-02,
        5.2490e-01, 6.7320e-02, 1.3726e+00, 1.7741e-01, 6.7315e-02, 4.9269e-02,
        1.0432e-01, 1.2782e-02, 8.1028e-01, 3.3527e-03, 1.8436e-01, 9.7377e-02,
        1.3086e-01, 1.1309e-01, 2.2639e-01, 1.5223e+00, 4.4072e+00, 8.8563e+00,
        3.2876e+00, 7.7483e+00, 1.0503e+01, 1.1268e+01, 5.6478e-02, 1.3835e-01,
        1.7675e-01, 1.8260e-01, 2.2011e-02, 3.8847e-02, 9.4023e-03, 2.2012e-01,
        5.1784e-02, 3.5586e-01, 7.2051e+00, 1.4720e+00, 2.7502e+00, 2.2977e-01,
        1.4479e+01, 1.3301e-01, 3.3033e+00, 5.7038e-01, 5.8131e-01, 6.9359e-01,
        6.6730e-01, 2.0147e+00, 1.0284e+00, 1.6748e+00, 5.3736e-02, 5.9937e-01,
        3.8996e+00, 4.0518e+00, 7.7562e+00, 1.0957e+01, 9.3525e+00, 2.0830e+00,
        1.2581e-02, 5.4590e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [10/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 145.0
Sum Train Loss:  tensor([6.3274e+00, 2.8865e-01, 4.1466e+00, 1.6757e-01, 4.9450e-02, 7.2208e-01,
        1.1757e-01, 4.5939e-01, 3.0246e-01, 1.0450e-01, 7.2254e-02, 7.3939e-02,
        2.8393e-02, 2.6934e-01, 9.3397e-01, 4.1299e-01, 1.0703e+00, 2.7237e-01,
        7.9812e-02, 2.9873e-01, 1.7895e-02, 2.4199e-02, 3.9074e-02, 6.6536e-02,
        7.4727e-01, 2.8659e-01, 1.6725e+00, 1.5702e-01, 9.1897e-02, 2.3477e-01,
        5.6410e-02, 5.1594e-02, 7.3411e-01, 6.9063e-03, 1.3303e-01, 2.5178e-01,
        4.7689e-02, 9.8056e-02, 3.4471e-01, 2.4469e+00, 2.9413e+00, 1.1761e+01,
        1.4331e+00, 4.6909e+00, 6.2630e+00, 7.1310e+00, 1.6042e-01, 1.3088e-01,
        1.3790e-01, 1.9623e-01, 1.0232e-02, 1.9429e-02, 1.1126e-01, 1.5511e-01,
        3.8535e-02, 3.7201e-01, 9.9803e+00, 8.9577e-01, 1.2707e+00, 5.9728e-02,
        1.7278e+01, 2.4302e-01, 1.3194e+00, 4.5600e-01, 6.5949e-02, 3.0360e-01,
        1.3007e-01, 5.1630e-01, 2.9131e-01, 4.2873e-01, 4.1854e-03, 2.8914e-01,
        1.0477e+00, 3.0294e+00, 6.5657e+00, 8.2687e+00, 6.9346e-01, 6.7201e+00,
        2.6496e-02, 4.6835e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [10/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 119.6
Sum Train Loss:  tensor([5.1282e+00, 2.4668e-01, 3.5898e+00, 4.5179e-01, 1.1839e-01, 3.2548e-01,
        3.0054e-02, 6.9693e-01, 6.9490e-01, 2.2296e-01, 4.0816e-02, 1.3525e-02,
        6.7959e-03, 6.5991e-01, 1.0154e+00, 4.9057e-01, 5.7614e-01, 4.8207e-01,
        1.0272e-01, 3.8391e-01, 1.6681e-02, 8.5738e-02, 8.4928e-02, 2.0014e-02,
        4.6105e-01, 1.1184e-01, 1.5015e+00, 9.1749e-02, 9.1847e-02, 1.8807e-01,
        8.4987e-02, 3.0251e-02, 5.8648e-01, 5.6876e-03, 8.3912e-02, 2.5782e-01,
        1.0336e-01, 1.3555e-01, 1.2871e-01, 3.0164e+00, 1.6077e+00, 1.0456e+01,
        1.2698e+00, 3.7037e+00, 6.0708e+00, 6.4017e+00, 6.1453e-02, 6.0311e-02,
        1.0261e-01, 1.7946e-01, 1.2137e-02, 2.7075e-02, 3.9241e-02, 1.1992e-01,
        7.8519e-02, 2.2774e-01, 8.7500e+00, 1.0610e+00, 4.2413e+00, 1.0206e-01,
        1.6060e+01, 4.0048e-01, 3.3711e+00, 7.5645e-01, 3.7108e-01, 3.3265e-01,
        6.6355e-01, 1.5468e+00, 2.3424e-01, 6.9516e-01, 2.9718e-03, 4.5357e-01,
        1.7925e+00, 2.9038e+00, 4.0706e+00, 1.4639e+01, 3.6987e+00, 6.8854e+00,
        4.2889e-01, 5.4244e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [10/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 126.3
Sum Train Loss:  tensor([5.4971e+00, 1.4486e-01, 3.3350e+00, 3.0021e-01, 5.2858e-02, 3.4443e-01,
        6.7929e-02, 4.3592e-01, 2.3555e-01, 2.5262e-01, 1.7272e-02, 1.3242e-01,
        2.8128e-02, 7.6354e-01, 5.0084e-01, 2.8417e-01, 7.9092e-01, 2.5468e-01,
        1.4278e-01, 2.7995e-01, 9.0254e-03, 1.0169e-02, 6.2689e-03, 1.2055e-02,
        4.4674e-01, 1.9014e-01, 1.6100e+00, 1.5269e-01, 7.0548e-02, 3.3719e-01,
        9.9809e-02, 6.9148e-02, 8.5905e-01, 8.9581e-03, 9.6678e-02, 1.7410e-01,
        1.4399e-01, 4.5379e-02, 1.2356e-01, 3.6249e+00, 2.4130e+00, 1.8770e+01,
        2.4549e+00, 6.9109e+00, 6.9469e+00, 6.5733e+00, 9.4354e-02, 1.3425e-01,
        7.9150e-02, 1.6589e-01, 1.5283e-02, 2.2460e-02, 1.2173e-01, 1.1433e-01,
        8.8565e-02, 2.8861e-01, 9.6721e+00, 5.6798e-01, 1.8342e+00, 7.3437e-02,
        1.4288e+01, 2.5403e-01, 2.4574e+00, 3.9178e-01, 2.5043e-01, 5.7817e-01,
        3.7994e-01, 1.0882e+00, 4.9118e-01, 1.2511e+00, 5.4497e-03, 7.8403e-01,
        1.6122e+00, 1.9505e+00, 5.6172e+00, 1.0684e+01, 6.8307e-01, 1.4888e+00,
        3.5370e-02, 3.6231e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [10/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 123.9
Sum Train Loss:  tensor([6.6732e+00, 2.9128e-01, 5.5333e+00, 2.6058e-01, 8.4701e-02, 1.8810e-01,
        6.8730e-02, 4.6880e-01, 2.0229e-01, 2.9386e-01, 4.4803e-02, 5.7196e-02,
        3.6893e-03, 1.1707e+00, 1.0569e-01, 7.1383e-01, 7.7189e-01, 3.2759e-01,
        3.9932e-02, 1.1559e-01, 2.2317e-02, 4.4430e-03, 2.2366e-02, 3.0367e-02,
        5.5574e-01, 3.0468e-01, 1.8980e+00, 1.4110e-01, 8.8459e-02, 1.4862e-01,
        8.7245e-02, 2.1922e-02, 1.2669e+00, 3.4424e-02, 1.7586e-01, 1.4774e-01,
        1.9041e-01, 1.4517e-01, 3.9077e-01, 2.9599e+00, 1.0081e+00, 1.1761e+01,
        1.8528e+00, 4.7891e+00, 5.5831e+00, 8.5855e+00, 6.2847e-02, 6.5321e-02,
        9.9887e-02, 1.8897e-01, 2.4231e-02, 2.5943e-02, 8.0759e-02, 8.3563e-02,
        9.8515e-02, 2.6120e-01, 9.0536e+00, 5.5673e-01, 2.5308e+00, 1.0137e-01,
        2.3320e+01, 5.0183e-01, 1.7032e+00, 7.0488e-01, 7.3525e-02, 1.7667e-01,
        1.9523e-01, 9.4834e-01, 3.8442e-01, 5.5543e-01, 3.9880e-03, 2.0241e-01,
        1.1949e+00, 2.4963e+00, 1.3435e+01, 1.2196e+01, 5.6547e+00, 9.0887e+00,
        2.9750e-02, 5.0284e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [10/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 146.2
Sum Train Loss:  tensor([4.3439e+00, 9.6380e-01, 3.7045e+00, 1.1166e-01, 1.1557e-01, 1.6319e-01,
        4.3799e-02, 3.7205e-01, 8.0437e-02, 2.1521e-01, 3.9724e-02, 1.3536e-01,
        3.0211e-02, 6.6862e-01, 6.0861e-01, 4.2735e-01, 4.7391e-01, 3.3045e-01,
        1.4569e-01, 1.4126e-01, 2.8823e-02, 1.1319e-02, 5.9030e-02, 1.6002e-02,
        5.6611e-01, 2.6940e-01, 2.0863e+00, 1.9520e-01, 1.3082e-01, 2.0129e-01,
        8.7390e-02, 6.6309e-02, 6.4250e-01, 1.3472e-02, 7.1293e-02, 1.4844e-01,
        4.3156e-02, 9.3781e-02, 1.0231e-01, 3.5875e+00, 5.4785e-01, 7.8484e+00,
        1.9314e+00, 6.4888e+00, 1.1578e+01, 1.5350e+01, 4.3545e-02, 9.3821e-02,
        1.6322e-01, 6.4901e-02, 7.4022e-03, 1.9079e-02, 4.2055e-02, 9.3626e-02,
        3.4546e-02, 4.0915e-01, 9.1124e+00, 6.8789e-01, 2.0919e+00, 1.4606e-01,
        1.7223e+01, 5.3122e-01, 2.2728e+00, 6.9365e-01, 1.9301e-01, 3.7189e-01,
        9.4177e-01, 6.6957e-01, 4.4567e-01, 9.8579e-01, 3.4742e-03, 5.1493e-01,
        6.2987e+00, 3.5203e+00, 4.8013e+00, 1.4470e+01, 8.4666e+00, 7.3509e+00,
        2.2330e-02, 7.3270e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [10/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 148.1
Sum Train Loss:  tensor([5.5071e+00, 2.5498e-01, 3.2575e+00, 5.2091e-01, 6.2764e-02, 2.0663e-01,
        3.7531e-02, 3.6932e-01, 6.7191e-01, 1.4134e-01, 2.9450e-02, 6.3935e-02,
        3.2391e-02, 6.9879e-01, 3.2300e-01, 3.8818e-01, 1.3223e+00, 1.1816e-01,
        3.1179e-01, 2.8676e-01, 6.9257e-02, 8.0300e-03, 3.7506e-02, 3.6477e-02,
        4.7722e-01, 2.4729e-01, 1.3475e+00, 1.6947e-01, 1.0267e-01, 1.1183e-01,
        6.9013e-02, 3.9655e-02, 1.3280e+00, 2.0980e-02, 1.0371e-01, 8.6186e-02,
        2.1644e-01, 2.8540e-01, 4.9498e-01, 2.7595e+00, 2.2416e+00, 1.0043e+01,
        1.9409e+00, 2.5743e+00, 8.0853e+00, 6.2908e+00, 7.4535e-02, 1.3733e-01,
        1.2507e-01, 1.1740e-01, 6.1303e-03, 1.1311e-02, 1.4729e-01, 3.8317e-02,
        1.1918e-02, 2.9767e-01, 6.5873e+00, 1.1937e+00, 1.6305e+00, 2.2790e-01,
        6.8773e+00, 3.7404e-01, 1.3313e+00, 6.9152e-01, 8.8471e-02, 5.6330e-01,
        5.6249e-01, 7.5499e-01, 5.0969e-01, 3.0002e-01, 5.9795e-02, 2.6737e-01,
        3.5718e+00, 2.1835e+00, 1.3962e+01, 3.7662e+00, 7.6470e+00, 1.4936e+01,
        1.2090e-02, 6.2885e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [10/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 123.5
Sum_Val Meta Model:  tensor([8.1424e+00, 2.9016e+00, 1.2753e+01, 1.4461e+00, 3.9252e-02, 3.7524e-01,
        4.0677e-02, 4.4498e-01, 3.6521e-01, 2.4849e-01, 5.2039e-02, 1.3260e-01,
        3.8015e-02, 5.7876e-01, 4.0384e-01, 3.1144e-01, 2.8571e+01, 8.5409e-02,
        2.0807e-02, 6.8269e-02, 4.9640e-03, 5.8220e-03, 6.0708e-03, 9.1621e-03,
        8.1758e-01, 2.9453e-01, 2.1790e+00, 5.7588e-02, 1.0084e-01, 1.7426e-01,
        2.8887e-02, 6.7939e-03, 6.2776e-01, 2.7012e-03, 2.0599e-02, 5.6232e-02,
        4.5027e-02, 1.2171e-01, 4.5019e-02, 5.9675e+00, 2.9635e+00, 1.0325e+01,
        2.5872e+00, 6.7749e+00, 1.2406e+01, 1.2877e+01, 1.6271e-02, 8.2129e-02,
        2.0230e-02, 1.3032e-01, 1.7107e-03, 1.1561e-02, 8.3848e-02, 4.2409e-02,
        1.5248e-02, 4.2319e-02, 9.0900e+00, 5.0747e-01, 2.3531e+00, 7.0130e-02,
        7.4689e+00, 7.2024e-01, 1.3416e+00, 3.2173e-01, 5.7801e-02, 8.0083e-02,
        1.0446e-01, 3.9756e-01, 2.5066e+00, 2.3464e+00, 2.3917e-03, 6.3208e-01,
        7.3558e+00, 1.8993e+00, 1.1565e+01, 8.3070e+00, 5.5111e-01, 7.8921e+00,
        1.2353e-02, 6.5353e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([7.3856e+00, 1.9671e+00, 8.1831e+00, 1.0080e+00, 1.1373e-02, 3.9750e-01,
        4.0303e-02, 4.8028e-01, 3.1319e-01, 2.4600e-01, 6.6918e-02, 1.2926e-01,
        4.5845e-02, 5.5077e-01, 4.2139e-01, 8.3146e-01, 1.9754e+01, 1.7413e-01,
        1.1590e-02, 1.1012e-01, 5.8257e-03, 7.4381e-03, 1.8389e-03, 1.2226e-02,
        7.3251e-01, 2.8284e-01, 2.1186e+00, 7.0234e-02, 1.3683e-01, 1.9290e-01,
        2.4784e-02, 4.3194e-03, 5.9013e-01, 3.4309e-03, 2.0043e-02, 3.6462e-02,
        1.0041e-01, 7.1907e-02, 2.2014e-02, 5.5056e+00, 3.1533e+00, 1.1660e+01,
        2.3628e+00, 5.8942e+00, 1.2258e+01, 1.1070e+01, 9.1192e-03, 7.4485e-02,
        1.0509e-02, 1.1246e-01, 3.2522e-04, 4.7666e-03, 7.6351e-02, 2.3665e-02,
        7.7330e-03, 2.3748e-02, 8.1950e+00, 4.3719e-01, 2.0293e+00, 8.3319e-02,
        9.3965e+00, 2.3184e-01, 1.1226e+00, 3.2274e-01, 3.0634e-02, 9.2453e-02,
        5.4824e-02, 5.0408e-01, 2.4201e+00, 1.9093e+00, 2.2934e-03, 7.1047e-01,
        5.6513e+00, 2.0645e+00, 1.2933e+01, 8.0527e+00, 5.8081e-01, 6.8346e+00,
        1.0811e-02, 7.7130e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.0015e+01, 4.6890e+01, 5.2203e+01, 2.2881e+01, 6.7546e-01, 1.1872e+01,
        4.3384e+00, 1.9849e+01, 6.2060e+00, 1.0662e+01, 9.1039e+00, 1.2440e+01,
        7.8036e+00, 1.4571e+01, 8.7991e+00, 2.1309e+01, 3.3243e+02, 5.4569e+00,
        5.0878e-01, 2.7143e+00, 1.7544e+00, 1.0876e+00, 1.4650e-01, 1.3583e+00,
        2.7010e+01, 1.7192e+01, 2.7430e+01, 5.0686e+00, 1.4452e+01, 1.1336e+01,
        1.5570e+00, 5.1774e-01, 7.1834e+00, 1.5410e+00, 9.3733e-01, 9.5670e-01,
        7.0877e+00, 2.6760e+00, 5.0228e-01, 4.5691e+01, 1.2776e+01, 2.4213e+01,
        7.1946e+00, 1.4214e+01, 1.6447e+01, 1.7803e+01, 8.0042e-01, 4.8154e+00,
        6.1996e-01, 4.6108e+00, 9.1071e-02, 5.4750e-01, 6.2033e+00, 9.4375e-01,
        8.4798e-01, 8.8558e-01, 2.4507e+01, 6.3920e+00, 1.4322e+01, 5.9892e+00,
        1.2964e+01, 5.3277e+00, 4.8803e+00, 5.1087e+00, 5.8065e-01, 2.5609e+00,
        6.1166e-01, 9.4218e+00, 1.0086e+01, 1.4964e+01, 1.1179e-01, 1.7137e+01,
        1.1205e+01, 1.2093e+01, 1.6193e+01, 8.1213e+00, 5.8379e-01, 6.9406e+00,
        9.6841e-02, 1.0064e+00], device='cuda:0')
Outer loop valEpocw Maximum [10/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 181.6
model_train val_loss valEpocw [10/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 162.6
model_train val_loss valEpocw [10/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1120.4
Sum_Val Meta Model:  tensor([1.2480e+01, 9.9563e-01, 1.3336e+01, 6.2174e-01, 2.5651e-02, 2.1052e+00,
        2.1000e+00, 2.5764e+00, 2.8297e+00, 1.5173e+00, 3.1212e-01, 5.1639e+00,
        1.2520e-01, 3.0182e-01, 1.0467e+00, 1.3765e+00, 1.0728e+00, 8.7472e-01,
        3.5789e-01, 2.1784e+00, 1.0852e-03, 5.8112e-04, 5.9971e-03, 7.8240e-02,
        1.1243e+00, 6.8215e-01, 3.3702e+00, 4.5528e-01, 2.7895e-01, 2.5712e-03,
        5.6381e-03, 1.7477e-03, 2.0285e-02, 9.3305e-04, 2.2248e-03, 3.8379e-03,
        1.4021e-01, 7.1731e-03, 5.1304e-03, 2.5193e-01, 5.0230e-02, 2.2223e+00,
        5.3367e-02, 1.2767e-01, 2.2887e-01, 2.1299e+00, 1.7153e-02, 9.5926e-03,
        3.0477e-03, 2.4221e-01, 4.0213e-04, 1.6107e-03, 1.1222e-03, 2.5849e-03,
        2.3543e-03, 2.0923e-01, 4.3790e+00, 6.5688e-01, 1.6184e+00, 1.0693e-01,
        1.4583e+00, 2.3050e-02, 5.0721e-01, 1.9274e-01, 3.0135e-02, 3.3329e-02,
        6.4618e-02, 1.1874e+00, 5.0799e-02, 7.5025e-01, 2.1645e-03, 3.4054e-02,
        2.4549e+00, 1.2595e+00, 4.0298e+00, 3.4296e-01, 1.3023e-01, 4.8435e-01,
        4.8367e-03, 1.4558e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.1646e+01, 1.1013e+00, 9.9714e+00, 8.0568e-01, 1.2879e-01, 1.8677e+00,
        1.4084e+00, 1.8238e+00, 2.2222e+00, 1.3005e+00, 2.5208e-01, 1.7779e+00,
        1.2562e-01, 4.1047e-01, 1.1411e+00, 2.4587e-01, 8.1602e-01, 8.6696e-01,
        7.2625e-02, 1.1343e+00, 6.2185e-03, 6.6104e-03, 4.2738e-03, 8.1103e-02,
        1.0821e+00, 6.2989e-01, 3.4527e+00, 4.0452e-01, 2.7430e-01, 2.2033e-02,
        2.4687e-02, 5.0361e-03, 4.2775e-02, 1.2718e-02, 1.2279e-02, 1.1835e-02,
        8.3867e-02, 2.0881e-01, 5.6445e-03, 3.1891e-01, 1.3179e-01, 2.1261e+00,
        3.8638e-01, 8.2075e-01, 5.5441e-01, 4.0733e-01, 8.2334e-03, 6.4790e-03,
        1.2721e-02, 1.9265e-01, 5.3918e-04, 4.3408e-03, 5.3991e-03, 6.9315e-02,
        7.1821e-03, 7.6703e-02, 3.1819e+00, 5.7261e-01, 9.7188e-01, 1.1050e-02,
        2.7662e+00, 3.0820e-02, 2.0246e-01, 3.9351e-02, 1.8638e-02, 1.6102e-02,
        3.6310e-02, 1.2359e+00, 4.1697e-02, 4.4179e-01, 3.7962e-04, 2.6542e-02,
        1.1245e+00, 7.5887e-01, 3.8811e+00, 1.8743e-01, 2.4583e-01, 2.4723e-01,
        1.7169e-03, 2.0907e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.6174e+01, 1.7873e+01, 5.0272e+01, 1.1462e+01, 4.4544e+00, 3.2846e+01,
        5.9058e+01, 4.0257e+01, 2.9309e+01, 3.3368e+01, 1.6623e+01, 7.8423e+01,
        9.0631e+00, 7.9459e+00, 1.5096e+01, 3.6521e+00, 9.6967e+00, 1.5357e+01,
        1.4580e+00, 1.4261e+01, 7.1947e-01, 4.6151e-01, 1.7349e-01, 4.7226e+00,
        2.4376e+01, 2.0282e+01, 3.2375e+01, 1.4105e+01, 1.2924e+01, 7.0231e-01,
        8.2421e-01, 2.8398e-01, 3.8839e-01, 2.0131e+00, 3.5660e-01, 2.4224e-01,
        3.1059e+00, 4.5283e+00, 8.4889e-02, 2.5803e+00, 5.5021e-01, 4.7168e+00,
        1.2693e+00, 2.1871e+00, 8.0537e-01, 7.0095e-01, 3.4675e-01, 2.2047e-01,
        3.7728e-01, 4.4062e+00, 6.1733e-02, 2.6285e-01, 2.3276e-01, 1.5938e+00,
        3.7309e-01, 1.7027e+00, 9.1107e+00, 5.3333e+00, 5.9176e+00, 3.7308e-01,
        4.0255e+00, 4.9810e-01, 7.8313e-01, 4.2683e-01, 2.5815e-01, 2.7662e-01,
        3.2534e-01, 1.8263e+01, 1.7718e-01, 3.0923e+00, 1.0412e-02, 4.7102e-01,
        2.6033e+00, 3.6431e+00, 5.4973e+00, 1.9038e-01, 2.4818e-01, 2.5337e-01,
        1.1326e-02, 2.0812e-01], device='cuda:0')
Outer loop valEpocw Maximum [10/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 83.0
model_train val_loss valEpocw [10/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 66.7
model_train val_loss valEpocw [10/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 723.7
Sum_Val Meta Model:  tensor([9.8034e+00, 1.3629e-01, 1.8110e+00, 9.7336e-02, 1.6160e-02, 7.6938e-02,
        1.2118e-02, 2.7503e-01, 1.1752e-01, 4.8714e-02, 9.3958e-03, 1.1291e-02,
        2.2139e-03, 6.6223e-01, 1.2952e-01, 1.2321e-01, 3.5034e-01, 6.1083e-02,
        3.1617e-02, 9.1448e-02, 4.2100e-03, 1.5080e-02, 8.4009e-02, 1.9913e-02,
        1.6342e-01, 3.3075e-02, 1.5811e+00, 9.2261e-02, 1.7560e-02, 5.0465e-02,
        8.2624e-02, 5.4749e-02, 7.6610e-01, 6.5474e-04, 6.3697e-02, 1.2128e-01,
        2.0773e-01, 4.0569e-01, 1.7298e-02, 3.0402e+00, 6.8031e+00, 3.5525e+01,
        2.6028e+01, 3.4541e+01, 3.8045e+01, 4.8068e+01, 1.7547e-01, 2.0097e-01,
        1.2763e+00, 4.4244e-01, 1.5969e-01, 3.8205e-01, 9.6123e-01, 4.2432e-01,
        6.4509e-01, 2.8941e+00, 5.4036e+01, 1.0315e+00, 1.2722e+00, 4.1599e-02,
        6.4065e+01, 5.7498e-02, 2.4567e+00, 8.3973e-01, 7.1392e-01, 4.9534e-02,
        8.4253e-01, 4.4076e-01, 1.0142e+00, 2.4789e-01, 7.8727e-03, 2.7424e-01,
        2.1663e+00, 6.1630e+00, 1.6613e+00, 2.3963e+01, 1.1311e+01, 1.4127e+00,
        2.5564e-02, 5.0366e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([2.9134e+00, 2.7696e-02, 6.0578e-01, 2.5829e-02, 1.6709e-03, 4.9499e-03,
        9.8511e-04, 2.7391e-01, 1.8544e-02, 4.4618e-03, 1.0639e-03, 5.2349e-04,
        2.8025e-04, 5.4791e-01, 1.6337e-02, 3.4705e-02, 8.8518e-02, 1.2277e-02,
        2.3613e-03, 6.9805e-03, 3.6655e-04, 4.4002e-04, 2.7952e-04, 6.4629e-04,
        1.1158e-01, 2.0176e-02, 1.6301e+00, 9.2511e-02, 7.8581e-03, 4.4323e-03,
        9.5691e-03, 5.8281e-02, 6.3925e-01, 2.7712e-04, 7.9347e-03, 2.1781e-02,
        1.6522e-01, 3.4126e-01, 5.7109e-03, 4.1944e+00, 5.7782e+00, 3.5904e+01,
        2.5728e+01, 3.2385e+01, 3.4417e+01, 4.6313e+01, 1.0977e-01, 1.0270e-01,
        8.4393e-01, 2.5395e-01, 8.0148e-02, 3.6312e-01, 8.6327e-01, 7.5322e-01,
        4.6528e-01, 2.2598e+00, 3.2672e+01, 9.9128e-01, 1.1461e+00, 2.1878e-02,
        6.7191e+01, 1.7812e-02, 2.0310e+00, 6.4340e-01, 5.3687e-01, 1.1818e-01,
        7.0233e-01, 5.0382e-01, 1.2104e+00, 4.3205e-01, 1.4244e-03, 2.4925e-01,
        2.2677e+00, 6.1582e+00, 1.4611e+00, 1.6515e+01, 1.0343e+01, 1.8298e+00,
        7.8215e-03, 5.8504e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.4163e+01, 8.0629e-01, 4.1157e+00, 6.8315e-01, 1.3324e-01, 1.7397e-01,
        1.4471e-01, 1.4959e+01, 4.5274e-01, 2.4388e-01, 1.8979e-01, 6.5482e-02,
        6.4881e-02, 1.7194e+01, 3.9981e-01, 1.0488e+00, 1.6304e+00, 4.5950e-01,
        1.1543e-01, 1.9018e-01, 1.7114e-01, 7.1270e-02, 2.5133e-02, 9.0856e-02,
        4.9861e+00, 1.7323e+00, 2.6927e+01, 9.2220e+00, 1.1360e+00, 2.9649e-01,
        7.1961e-01, 7.8051e+00, 8.5161e+00, 1.5470e-01, 4.1832e-01, 6.2311e-01,
        1.3242e+01, 1.2915e+01, 1.6548e-01, 4.5107e+01, 2.5821e+01, 7.0083e+01,
        6.6336e+01, 6.9353e+01, 4.4094e+01, 6.7165e+01, 1.0081e+01, 6.9652e+00,
        4.1699e+01, 1.0645e+01, 2.3268e+01, 4.2114e+01, 7.0473e+01, 3.5045e+01,
        5.7135e+01, 8.4242e+01, 1.0569e+02, 1.5867e+01, 9.1410e+00, 1.7585e+00,
        8.6992e+01, 5.4575e-01, 9.5181e+00, 1.2140e+01, 1.2511e+01, 3.9548e+00,
        9.4933e+00, 1.3666e+01, 6.6539e+00, 4.5543e+00, 8.9294e-02, 8.5974e+00,
        5.6766e+00, 3.9313e+01, 1.9404e+00, 1.6610e+01, 1.0377e+01, 1.8491e+00,
        8.4579e-02, 9.2264e-01], device='cuda:0')
Outer loop valEpocw Maximum [10/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 391.4
model_train val_loss valEpocw [10/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 345.6
model_train val_loss valEpocw [10/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1304.0
Sum_Val Meta Model:  tensor([1.6823e+01, 8.1202e-02, 3.1584e+00, 4.3278e-02, 8.9980e-03, 3.6749e-01,
        5.6691e-02, 3.6493e-01, 6.8658e-02, 4.4932e-01, 3.3227e-02, 5.1450e-02,
        8.9510e-04, 5.8942e-01, 6.8818e-01, 5.8865e-02, 1.0118e-01, 2.3466e-02,
        1.0472e-02, 2.0762e-02, 1.8994e-03, 3.0370e-03, 6.4289e-03, 6.3475e-03,
        6.1049e-01, 2.4326e-02, 2.2956e+00, 2.1765e-02, 1.7323e-01, 5.6892e-03,
        9.3577e-03, 1.2263e-03, 2.7124e-01, 7.3343e-03, 4.0438e-02, 2.3340e-01,
        4.0961e-03, 1.8996e-02, 1.6847e-01, 2.6593e+00, 3.7270e+00, 1.5047e+01,
        3.0306e+00, 4.2143e+00, 6.8926e+00, 9.9414e+00, 6.6837e-03, 1.0272e-02,
        3.2104e-02, 1.2259e-02, 4.1031e-03, 6.1321e-03, 5.0366e-03, 1.1862e-01,
        8.0882e-03, 4.8127e-02, 7.3824e+00, 3.2285e-01, 2.4408e+00, 2.2216e-02,
        3.0876e+01, 2.2833e-02, 1.1161e+00, 3.2623e-01, 1.8696e-01, 6.6190e-02,
        3.8684e-01, 2.8807e+00, 3.4710e-01, 4.1515e-01, 2.6618e-03, 6.6779e-02,
        2.7083e+00, 1.7465e+00, 3.3624e+02, 1.1969e+01, 1.4822e+00, 1.0636e+01,
        1.1490e-02, 9.6892e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.2246e+00, 1.2946e-01, 3.0925e+00, 1.3964e-01, 1.5272e-02, 3.0421e-01,
        6.1507e-02, 3.3971e-01, 1.4308e-01, 4.3141e-01, 2.9134e-02, 8.8423e-02,
        3.8698e-03, 5.3791e-01, 7.1396e-01, 2.5168e-02, 5.1377e-02, 1.8497e-02,
        1.6071e-03, 5.1368e-03, 1.6537e-03, 6.0289e-04, 1.1107e-03, 5.0514e-03,
        6.1845e-01, 4.1539e-02, 1.8155e+00, 2.3079e-02, 1.7837e-01, 2.2593e-03,
        6.4352e-03, 2.9188e-03, 1.0629e-02, 9.1468e-04, 1.8217e-03, 3.2246e-03,
        8.4374e-03, 9.2804e-03, 3.5773e-03, 3.0757e-01, 3.2195e-01, 1.5934e+00,
        5.2121e-01, 8.5402e-01, 6.6569e-01, 1.4460e+00, 2.3932e-03, 1.7913e-03,
        3.2623e-03, 2.1046e-03, 3.5937e-04, 1.7502e-03, 8.4930e-04, 1.1450e-02,
        3.3965e-03, 3.7703e-02, 1.4071e+00, 3.7324e-02, 1.8061e+00, 7.2850e-03,
        8.4095e+00, 2.1696e-02, 2.8790e-01, 3.2073e-02, 3.4470e-02, 3.9929e-02,
        5.5682e-02, 4.4712e-01, 4.4046e-02, 4.5653e-02, 2.8455e-04, 1.9062e-02,
        8.3754e-02, 1.0851e+00, 6.1704e+01, 3.7875e+00, 1.4924e-01, 6.1937e+00,
        1.5896e-03, 8.1246e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.5627e+01, 3.3707e+00, 2.1368e+01, 3.3991e+00, 9.6742e-01, 9.5774e+00,
        6.6721e+00, 1.4759e+01, 3.2209e+00, 2.0362e+01, 4.0698e+00, 8.5433e+00,
        6.6915e-01, 1.4768e+01, 1.5954e+01, 7.0387e-01, 9.5166e-01, 5.9786e-01,
        7.2412e-02, 1.3717e-01, 4.9599e-01, 8.3690e-02, 9.3953e-02, 5.8098e-01,
        2.4383e+01, 2.7933e+00, 2.3966e+01, 1.7086e+00, 1.9393e+01, 1.3879e-01,
        4.4457e-01, 3.5773e-01, 1.2047e-01, 3.9074e-01, 8.4535e-02, 6.9764e-02,
        6.0669e-01, 3.7323e-01, 7.3906e-02, 2.8084e+00, 1.2074e+00, 3.1354e+00,
        1.5430e+00, 2.0853e+00, 8.9284e-01, 2.2790e+00, 2.1379e-01, 1.2069e-01,
        2.1087e-01, 9.3607e-02, 1.0047e-01, 2.1022e-01, 7.0187e-02, 4.4644e-01,
        4.0336e-01, 1.5107e+00, 4.2698e+00, 5.3255e-01, 1.1668e+01, 5.6232e-01,
        1.1406e+01, 5.3180e-01, 1.1809e+00, 4.8637e-01, 5.9948e-01, 1.1515e+00,
        6.0221e-01, 7.2216e+00, 2.0831e-01, 3.8689e-01, 1.4891e-02, 5.1883e-01,
        1.8070e-01, 6.3709e+00, 8.3136e+01, 3.8109e+00, 1.4987e-01, 6.2694e+00,
        1.6196e-02, 9.6634e-02], device='cuda:0')
Outer loop valEpocw Maximum [10/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 484.4
model_train val_loss valEpocw [10/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 105.6
model_train val_loss valEpocw [10/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 400.7
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [86.46946309 97.33068554 92.80466856 97.92765683 98.47224084 97.50855862
 98.16279041 95.16331429 97.88379771 96.77270014 98.58432524 98.91083198
 99.41764842 95.32900428 97.41718546 97.13210122 96.35116531 97.60967824
 98.73905045 98.40279724 98.56361399 99.26292321 99.48709202 98.86940949
 95.27418038 96.69594669 94.41892764 96.9481366  98.06167079 98.19202982
 98.32969871 98.59529002 96.79706631 98.4673676  98.4953887  98.74026876
 97.4646995  98.31142408 98.79021942 93.22864    97.94227653 93.52590734
 97.07240409 96.40355259 97.30388275 95.29245501 98.12380453 98.64645899
 98.09700174 98.71955751 98.66960685 98.60016325 99.00098683 98.27609313
 98.7183392  97.54632619 91.74961319 96.81534094 96.45593986 97.43546009
 94.05587164 98.57457877 97.10529842 97.26002364 98.68788148 97.5621642
 98.53437458 95.92963049 98.82798699 98.25660019 99.81603538 97.36845311
 98.55752245 95.87480659 97.63038949 97.90694558 99.24830351 98.80483912
 99.84405648 99.15327542]
Accuracy th:0.7 is [83.93050767 97.26124194 92.31369014 97.96055116 98.42107187 97.45251642
 98.01172013 95.01102569 97.93253006 96.88600285 98.54899429 98.87062779
 99.41399349 95.3180395  97.35505172 96.94691829 96.31217943 97.53901634
 98.67813501 98.34553673 98.47955069 99.28363446 99.45541599 98.6903181
 95.23641281 96.66183404 94.23374472 96.87381976 98.0214666  98.15913549
 98.2444171  98.57701539 96.43766523 98.30776915 98.29071283 98.55630414
 97.22834761 98.35893812 98.58554355 92.92040789 97.91303712 93.674541
 97.29779121 96.54731302 97.09311534 95.0390468  98.07872711 98.60259987
 98.02268491 98.65620546 98.58554355 98.56848723 98.99976852 98.29802268
 98.71102935 97.48053752 90.89314214 96.57046089 96.36822163 97.35992495
 93.24569632 98.47102253 96.96641123 97.21494621 98.82189544 97.4500798
 98.58919847 95.95156004 98.73539552 98.09456512 99.81603538 97.16743217
 98.44056481 95.71642646 97.53048818 97.76684007 99.23246549 98.71590258
 99.84405648 99.1496205 ]
Avg Prec: is [95.76042842 30.01070142 68.38953935 63.37683493 74.28339003 61.69379779
 70.54786786 45.12524402 52.56014055 48.69291275 24.99330355 51.34677104
 18.28264084 25.11171182 28.91362525 50.53596359 25.75639551 34.74740096
 41.84324145 34.10789152 54.33707002 44.69536313 87.92771034 78.13496763
 25.075129   26.79628754 36.39524432 34.46591805 20.31430042 34.69641673
 71.10222892 34.46459492 54.58756259 57.90281908 69.74529097 76.22168231
 53.01549984 72.86977603 83.8098182  44.07107005 34.72906594 59.4929885
 51.10962587 47.68170888 47.66783789 60.93328096 31.63679077 29.58340708
 37.34268039 40.71258671 55.81373935 32.28716061 15.89319211 69.40606762
 20.86802016 34.94668987 62.8249361  54.93456084 36.56222513 52.38059499
 76.91853439 79.67674862 61.36039235 44.18741194 55.07230315 38.78444834
 54.30507168 21.55609577 43.54091909 62.23198537  9.11476759 69.74786762
 63.67758419 44.42598449 66.19216712 66.60422987 29.43923498 62.79443782
  5.63882944 18.99743968]
Accuracy th:0.5 is [45.49530342 97.2137279  72.81344038 97.02489005 97.26733349 77.8316541
 78.18618194 76.95690842 79.14377261 96.44984832 79.41058223 98.52097319
 99.41399349 80.7019895  78.86965315 96.56680596 96.29512311 78.67106882
 98.65376884 98.30776915 80.81041898 79.91374374 98.38695922 78.79046308
 80.92737662 96.65086926 94.0778012  78.34090715 98.01293844 79.23027254
 97.30875598 98.57457877 96.36213009 98.02024829 87.18582863 78.85137852
 78.66497728 90.51912136 97.11504489 75.89332489 79.78460301 92.05906361
 78.11673834 77.6781472  96.9627563  93.87434364 98.02877645 98.57336046
 94.77223718 88.57713722 87.45507487 98.55508583 98.99976852 78.28608326
 98.70615611 78.71249132 73.21791889 93.31270331 96.24273583 96.9067141
 89.79300934 97.17717864 91.80443708 78.73563919 98.42838172 79.2388007
 98.20786784 77.83043579 79.78094809 97.55972759 80.3681729  95.99054592
 79.4763709  95.45083515 78.02171026 82.8620509  86.32204773 79.3362654
 80.39132077 99.14718388]
Accuracy th:0.7 is [45.5342893  97.2137279  72.81344038 97.02489005 97.26733349 77.8316541
 78.18618194 77.11528856 79.14377261 96.4754328  79.41058223 98.52097319
 99.41399349 81.15154542 78.86965315 96.56680596 96.29512311 78.67106882
 98.65376884 98.30776915 81.24413689 79.91374374 98.38695922 78.87939962
 81.42566489 96.65086926 94.0778012  78.34090715 98.01293844 79.23027254
 97.30875598 98.57457877 96.36213009 98.02024829 87.37344818 78.85137852
 78.66497728 90.78105774 97.11504489 75.89332489 80.43639819 92.05906361
 78.11673834 77.6781472  96.9627563  93.87434364 98.02877645 98.57336046
 96.18669363 89.23258732 87.64391272 98.55508583 98.99976852 78.28608326
 98.70615611 78.71249132 73.21791889 93.71474519 96.24273583 96.9067141
 89.79300934 97.17717864 92.00423971 78.73563919 98.42838172 79.2388007
 98.20786784 77.83043579 79.78094809 97.55972759 80.3681729  95.99054592
 79.50682862 95.45083515 78.02171026 82.93758604 86.44022368 79.3362654
 80.39132077 99.14718388]
Avg Prec: is [56.06388236  3.07214114 11.30242183  3.29102783  2.26591702  3.78084564
  3.3586167   5.62947708  2.45415155  3.89242385  1.55930966  1.66831814
  0.64572448  5.03343215  2.71404531  3.11885705  3.72176751  2.59904971
  1.37073844  1.73548544  1.89074004  0.87732645  1.85860509  2.42245632
  5.19530233  3.572863    6.56608896  3.53405777  2.09180614  1.99718749
  2.58463944  1.30282848  3.7089302   1.72846367  2.3877252   2.38389407
  3.01246572  2.56363289  2.83849892  7.29161684  2.24244742  8.19536421
  3.2663212   4.1799576   3.28892     6.37657701  2.12035992  1.50447808
  2.09709215  1.60863498  1.80209507  1.5649448   1.137526    2.95569814
  1.32957636  2.67854776 11.2841462   3.64999812  4.02536881  2.82606907
 10.77273139  2.21530548  3.69490305  2.89060633  1.52004469  2.48167206
  1.76540077  4.20245715  1.25744341  2.35896138  0.18150351  3.34141486
  1.8760636   4.67707755  3.99295793  3.13630403  0.83366438  1.77133172
  0.16901616  0.77244063]
mAP score regular 48.22, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [87.60246157 97.35406234 92.90679423 98.06662182 98.87883997 97.59075168
 98.34317463 95.24877295 97.94703142 96.72621272 98.63218477 99.01337918
 99.35969305 95.12419962 97.39641727 97.1397962  96.30017191 97.67546154
 98.86887411 98.41044423 98.78167277 99.32730398 99.62627999 99.11552931
 95.41819269 96.63901139 94.54368787 97.12733886 97.86730448 98.18122929
 98.6670653  98.66955677 96.87819219 98.67204823 98.68450557 99.06569998
 97.87477888 98.29085383 99.01587064 93.37020704 97.91215088 92.81710143
 96.86324339 96.11829484 97.117373   94.81276628 98.22358422 98.81904477
 98.03921569 98.73931784 98.73184344 98.6446421  98.88133144 98.33819169
 98.7268605  97.69290181 91.09051499 97.05259486 96.38488178 97.43628074
 93.1559409  98.7492837  97.1024242  97.3715026  98.6745397  97.50604181
 98.49017116 95.73710043 98.77668984 98.21610982 99.81563146 97.56085407
 98.4652565  95.76699803 97.43877221 97.54341381 99.21020505 98.70194584
 99.82559733 99.13795251]
Accuracy th:0.7 is [86.63826395 97.27931833 92.86942223 98.22607569 98.84395944 97.58826021
 98.18122929 95.25624735 98.04669009 96.95542766 98.6147445  98.98099011
 99.35969305 95.11672522 97.33662207 96.86324339 96.25781698 97.64805541
 98.82651917 98.37058076 98.6894885  99.29242345 99.59887386 98.98597304
 95.45058176 96.56426738 94.44652067 97.06754366 97.83242395 98.13139996
 98.60477863 98.6745397  96.55181005 98.5250517  98.48269676 98.85641677
 97.62812368 98.29334529 98.86887411 93.00894437 97.87228742 93.56454145
 97.28679273 96.63402845 97.09245833 94.72556494 98.22607569 98.82153624
 97.97194608 98.74679224 98.58982983 98.58982983 98.87634851 98.38303809
 98.7119117  97.61068341 90.70184618 96.96041059 96.29768044 97.3864514
 92.88437103 98.70194584 97.07501806 97.32416474 98.7866557  97.50853327
 98.57986397 95.7694895  98.77668984 98.05665595 99.81563146 97.53344794
 98.39300396 95.73460896 97.3939258  97.63559808 99.24010265 98.66955677
 99.82559733 99.15539278]
Avg Prec: is [96.22678689 28.93160025 69.56160104 71.79738852 75.67845732 63.61255023
 77.36345804 46.34767825 57.65082341 52.88296651 33.94628847 54.95487741
 18.97541083 28.40031837 30.07787632 58.20642344 28.64396433 39.35218576
 41.93831188 33.42864652 64.56642192 54.09881596 91.95065381 84.61293889
 24.07319472 32.68546105 33.48450814 41.55777033 23.05909263 34.22442194
 76.30629827 34.92919027 54.58250137 61.6366945  72.56139394 81.42698514
 56.85953445 75.99920396 88.85740374 43.95977088 32.23938495 52.77973314
 44.47006506 39.40362622 35.18874819 52.37997232 29.04591254 26.64409096
 37.59391662 40.08159709 62.3156679  32.42329037 18.57313733 74.7074331
 24.87989723 34.91590223 57.16679686 54.22433114 35.0428251  56.28701171
 67.5488835  85.32100571 62.95958013 48.17153178 60.74896259 34.57229406
 58.8322546  23.14107692 41.12799257 64.62136949  9.43666235 74.05066082
 55.16655402 42.06879906 65.07041325 54.43890026 10.98228616 53.8329466
  2.35751277 19.00436753]
Accuracy th:0.5 is [45.29984802 97.22450607 71.36806438 96.96290206 97.90716795 76.93898398
 77.05109998 75.83775569 78.44881282 96.41976231 78.68301069 98.5325261
 99.34972718 78.66806189 78.53601415 96.31262924 96.21047911 78.01280614
 98.78167277 98.34068316 79.55502404 79.3183347  98.31327703 78.12243067
 78.64563869 96.52938685 94.3393876  78.03522934 97.81747515 78.63068989
 97.52597354 98.67204823 96.39983058 98.18870369 88.25771732 78.24700401
 78.18970028 92.04474674 97.0276802  75.21488901 78.38652615 92.37362035
 77.36004186 77.01871092 97.03764606 94.02795426 98.18621222 98.77668984
 95.60754416 88.15805865 85.80860553 98.55993223 98.87385704 77.38993946
 98.6969629  78.04768667 71.80656252 94.35184493 96.16314124 96.78102499
 90.13379176 97.04761193 91.5912998  77.9704512  98.32075143 78.82502429
 98.13139996 77.14079278 79.33079204 97.53593941 79.79918778 96.07843137
 79.03679896 95.44559882 77.07352318 83.79300894 88.21037945 78.68301069
 79.86894885 99.15040985]
Accuracy th:0.7 is [45.51909709 97.22450607 71.36806438 96.96290206 97.90716795 76.93898398
 77.05109998 75.88509355 78.44881282 96.41976231 78.68301069 98.5325261
 99.34972718 79.05922216 78.53601415 96.31262924 96.21047911 78.01280614
 98.78167277 98.34068316 79.90632085 79.3183347  98.31327703 78.14485388
 78.9894611  96.52938685 94.3393876  78.03522934 97.81747515 78.63068989
 97.52597354 98.67204823 96.39983058 98.18870369 88.46201759 78.24700401
 78.18970028 92.27894461 97.0276802  75.21488901 78.82004136 92.37362035
 77.36004186 77.01871092 97.03764606 94.02795426 98.18621222 98.77668984
 96.69880659 88.44956026 85.9381618  98.55993223 98.87385704 77.38993946
 98.6969629  78.04768667 71.80656252 94.60597454 96.16314124 96.78102499
 90.13379176 97.04761193 91.73829634 77.9704512  98.32075143 78.82502429
 98.13139996 77.14079278 79.33079204 97.53593941 79.79918778 96.07843137
 79.04427336 95.44559882 77.07352318 83.87024441 88.35488452 78.68301069
 79.86894885 99.15040985]
Avg Prec: is [53.73565837  3.70589365 14.83077766  4.54467904  1.4810436   4.28958023
 14.44751053  8.64460569  8.49703819  5.2665535   2.57891214  5.04560768
  2.38752358  5.84385933  2.95591922  3.6726547  22.31394631  6.4956953
  1.54984593  2.71734075  3.49717561  1.54475708  1.15673149  5.11719817
  5.59046598  8.47654517  7.80789982  4.57640446  3.89061834  5.23291528
  2.19263536  0.84251879  2.9026323   1.14300839  1.66775566  2.17337044
  1.96159462  2.24887832  2.19449739  6.25334962  1.70669263  6.05617997
  2.15906322  2.69182458  2.33743805  4.90434126  1.68647012  1.01553287
  1.42727832  1.1359129   1.19318917  1.01251023  0.73991481  2.25328389
  0.87394653  1.85896839 10.08024342  3.02349765  3.80920005  2.76448288
  7.85179497  2.00004139  3.25574075  2.50816578  1.35198851  1.94095195
  1.52275596  3.52646697  1.08748784  2.23171351  0.19432365  3.29214234
  1.61071901  3.97818027  3.18831618  2.30524961  0.54864611  1.46324918
  0.12199858  0.6097452 ]
mAP score regular 48.92, mAP score EMA 4.28
Train_data_mAP: current_mAP = 48.22, highest_mAP = 48.22
Val_data_mAP: current_mAP = 48.92, highest_mAP = 48.92
tensor([0.1323, 0.0395, 0.1488, 0.0421, 0.0166, 0.0327, 0.0096, 0.0236, 0.0458,
        0.0224, 0.0075, 0.0108, 0.0061, 0.0365, 0.0461, 0.0363, 0.0550, 0.0320,
        0.0233, 0.0392, 0.0035, 0.0075, 0.0122, 0.0091, 0.0255, 0.0156, 0.0759,
        0.0139, 0.0096, 0.0168, 0.0151, 0.0086, 0.0809, 0.0021, 0.0192, 0.0378,
        0.0145, 0.0254, 0.0432, 0.1080, 0.2371, 0.4898, 0.3441, 0.4117, 0.7683,
        0.6510, 0.0116, 0.0156, 0.0162, 0.0238, 0.0037, 0.0089, 0.0129, 0.0235,
        0.0086, 0.0260, 0.3285, 0.0678, 0.1461, 0.0134, 0.7303, 0.0417, 0.2284,
        0.0615, 0.0517, 0.0339, 0.0819, 0.0529, 0.2544, 0.1290, 0.0199, 0.0381,
        0.4851, 0.1706, 0.8140, 0.9936, 0.9971, 0.9883, 0.1676, 0.0816],
       device='cuda:0')
Sum Train Loss:  tensor([4.9685e+00, 2.4263e-01, 2.9236e+00, 4.9891e-01, 1.1129e-01, 4.7501e-01,
        6.2208e-02, 3.7077e-01, 3.0640e-01, 2.4766e-01, 7.2518e-02, 2.0795e-02,
        2.8902e-03, 5.7559e-01, 4.4657e-01, 3.4902e-01, 6.0060e-01, 3.5028e-01,
        9.9007e-02, 1.3908e-01, 5.8183e-03, 2.2314e-02, 5.8450e-03, 3.0209e-02,
        4.4019e-01, 1.8314e-01, 9.9706e-01, 1.1381e-01, 6.4717e-02, 9.5990e-02,
        1.7954e-01, 1.2834e-02, 4.2714e-01, 7.8206e-03, 8.4679e-02, 2.8244e-01,
        1.7191e-01, 1.7155e-01, 3.3639e-02, 3.7206e+00, 4.9398e+00, 1.0738e+01,
        4.7752e+00, 5.5823e+00, 8.9456e+00, 1.1860e+01, 7.1312e-02, 2.1516e-01,
        2.5565e-01, 1.7565e-01, 1.2563e-02, 1.0814e-01, 6.9980e-02, 2.6564e-01,
        3.3321e-02, 3.5800e-01, 7.1020e+00, 1.2884e+00, 2.9347e+00, 1.1919e-01,
        1.9284e+01, 1.4604e-01, 1.8454e+00, 7.7458e-01, 2.0741e-01, 3.0079e-01,
        6.1158e-01, 9.4717e-01, 1.5604e+00, 3.1360e-01, 3.1992e-03, 1.6852e-01,
        1.4807e+00, 1.7119e+00, 9.5074e+00, 3.5398e+00, 3.5730e+00, 1.7943e+00,
        3.5258e-02, 1.3931e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [11/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 127.7
Sum Train Loss:  tensor([4.8159e+00, 5.1419e-01, 3.8878e+00, 2.0600e-01, 1.7461e-01, 1.4348e-01,
        8.1474e-02, 3.6767e-01, 2.6734e-01, 2.6683e-01, 2.0034e-02, 4.4289e-02,
        4.2192e-02, 6.7540e-01, 2.6802e-01, 4.9655e-01, 9.6975e-01, 6.1654e-01,
        6.7072e-02, 2.8742e-01, 3.5781e-02, 5.0062e-03, 6.8962e-03, 1.6343e-02,
        4.1729e-01, 3.3153e-01, 1.6191e+00, 1.7527e-01, 2.0435e-01, 4.9926e-02,
        8.7939e-02, 1.0393e-02, 4.5694e-01, 1.9228e-02, 4.8806e-02, 9.7540e-02,
        1.5325e-01, 2.7391e-01, 2.1640e-01, 3.4820e+00, 2.2432e+00, 1.2915e+01,
        2.2307e+00, 4.2686e+00, 7.8008e+00, 1.1222e+01, 1.8780e-02, 2.6869e-02,
        8.0583e-02, 3.0582e-02, 6.4595e-03, 4.7817e-02, 6.2012e-02, 6.3447e-02,
        6.8151e-02, 1.1273e-01, 9.4056e+00, 7.7961e-01, 1.7894e+00, 8.2119e-02,
        1.5067e+01, 9.8545e-02, 1.5915e+00, 6.6006e-01, 3.6834e-01, 1.3896e-01,
        6.1054e-01, 1.0600e+00, 1.3358e+00, 2.2991e-01, 4.5450e-03, 4.1768e-01,
        1.2217e+00, 2.4366e+00, 1.7371e+01, 1.0101e+01, 4.3331e+00, 1.5044e+01,
        4.2378e-02, 2.4923e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [11/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 147.6
Sum Train Loss:  tensor([5.2467e+00, 5.9456e-01, 3.1057e+00, 3.3390e-01, 1.1248e-01, 3.1692e-01,
        5.2364e-02, 4.4486e-01, 2.9693e-01, 9.4608e-02, 4.7167e-02, 3.8587e-02,
        3.2247e-02, 5.1616e-01, 4.5250e-01, 2.2661e-01, 1.0669e+00, 1.9102e-01,
        5.2029e-02, 7.7142e-02, 1.4924e-02, 6.5585e-03, 9.1439e-03, 6.5850e-02,
        5.6034e-01, 1.8377e-01, 1.2906e+00, 9.9131e-02, 1.4474e-01, 1.1828e-01,
        4.9858e-02, 4.2092e-02, 3.5792e-01, 2.8148e-02, 1.1207e-01, 2.7207e-01,
        1.0813e-01, 4.2075e-02, 2.6042e-01, 2.7529e+00, 1.5271e+00, 8.8516e+00,
        4.4443e+00, 3.7959e+00, 8.5927e+00, 8.0951e+00, 1.0489e-01, 7.3273e-02,
        1.9885e-01, 5.0457e-02, 9.9210e-03, 5.6371e-02, 3.4448e-02, 7.0644e-02,
        9.9820e-02, 1.5520e-01, 1.0610e+01, 6.4039e-01, 2.2785e+00, 7.5843e-02,
        1.7879e+01, 6.6219e-01, 5.7209e+00, 9.4347e-01, 3.4479e-01, 5.7363e-01,
        9.0831e-01, 7.9846e-01, 8.9508e-01, 3.5332e-01, 2.6964e-03, 4.4930e-01,
        1.9034e+00, 3.0712e+00, 5.0404e+00, 6.1757e+00, 5.2742e+00, 4.4644e+00,
        1.3654e+00, 1.6369e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [11/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 126.6
Sum Train Loss:  tensor([4.8324e+00, 6.6572e-01, 3.7345e+00, 2.7830e-01, 1.2558e-01, 1.6328e-01,
        8.6262e-02, 3.3647e-01, 2.1744e-01, 2.0434e-01, 3.0361e-02, 8.2876e-02,
        5.6842e-03, 6.1312e-01, 3.4200e-01, 3.7717e-01, 8.5855e-01, 4.2927e-01,
        1.5969e-01, 7.3385e-01, 1.9443e-02, 4.0317e-02, 1.2813e-01, 3.6921e-02,
        3.6044e-01, 2.0090e-01, 1.8235e+00, 3.0532e-01, 5.6412e-02, 6.9970e-02,
        2.7404e-02, 6.2526e-02, 1.2301e+00, 7.4794e-03, 3.0341e-01, 3.9180e-01,
        1.9840e-01, 7.3928e-02, 2.4270e-01, 2.1349e+00, 2.4811e+00, 1.0880e+01,
        3.0844e+00, 4.9677e+00, 6.9128e+00, 9.3769e+00, 1.2878e-01, 8.0804e-02,
        5.5199e-02, 1.1255e-01, 4.9069e-03, 3.8958e-02, 8.0393e-02, 2.3622e-01,
        1.2786e-01, 2.3897e-01, 8.8821e+00, 5.0998e-01, 2.4426e+00, 1.4678e-01,
        1.1508e+01, 5.8524e-02, 2.6820e+00, 6.9306e-01, 5.6098e-01, 2.2927e-01,
        2.1997e-01, 7.6397e-01, 7.1302e-01, 4.6471e-01, 9.3129e-02, 1.6796e-01,
        3.2304e+00, 2.6062e+00, 8.0256e+00, 1.0804e+01, 3.1178e+00, 2.9055e+00,
        2.1869e-02, 9.7311e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [11/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 122.6
Sum Train Loss:  tensor([4.6733e+00, 3.9834e-01, 3.4753e+00, 2.2312e-01, 1.5272e-01, 1.7714e-01,
        1.0574e-01, 2.8626e-01, 5.0474e-01, 1.4208e-01, 1.4421e-02, 5.2608e-02,
        9.7627e-03, 5.8228e-01, 5.4596e-01, 3.8653e-01, 5.7538e-01, 4.8910e-01,
        7.9543e-02, 4.1289e-01, 5.2366e-02, 1.0034e-02, 2.5680e-02, 1.6997e-02,
        6.1422e-01, 1.9759e-01, 1.7863e+00, 1.8426e-01, 8.5659e-02, 1.7363e-01,
        6.7431e-02, 4.4538e-02, 4.3801e-01, 1.3799e-02, 1.1838e-01, 1.3771e-01,
        1.7720e-01, 1.7002e-01, 1.4487e-01, 2.2082e+00, 1.9791e+00, 1.3658e+01,
        5.0831e+00, 1.3316e+01, 1.5703e+01, 1.4503e+01, 1.1066e-01, 2.2201e-01,
        1.4350e-01, 2.3986e-01, 1.0046e-02, 3.7117e-02, 1.3404e-01, 4.0869e-01,
        1.3489e-01, 2.7569e-01, 1.2838e+01, 1.2112e+00, 8.6567e-01, 1.1758e-01,
        2.5662e+01, 1.3475e-01, 1.9637e+00, 5.3084e-01, 1.5277e-01, 4.2291e-01,
        2.8370e-01, 7.4576e-01, 1.0421e+00, 4.2405e-01, 3.9284e-02, 2.2659e-01,
        2.8245e+00, 3.0727e+00, 5.4891e+00, 6.0293e+00, 7.7610e+00, 2.6908e+00,
        2.0485e-02, 2.8647e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [11/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 160.7
Sum Train Loss:  tensor([5.7469e+00, 5.6508e-01, 3.7904e+00, 2.6910e-01, 1.2183e-01, 1.8349e-01,
        5.1245e-02, 3.8681e-01, 1.7711e-01, 2.8402e-01, 5.4759e-02, 3.7497e-02,
        5.9352e-02, 5.7154e-01, 1.6818e+00, 1.1929e-01, 6.9902e-01, 4.2398e-01,
        2.8972e-02, 5.7229e-01, 3.5087e-02, 1.3334e-02, 1.0558e-02, 8.9673e-03,
        3.7411e-01, 3.4905e-01, 2.5411e+00, 1.5969e-01, 1.4057e-01, 2.5436e-01,
        1.0318e-01, 1.6276e-02, 1.3141e+00, 1.7367e-02, 1.7354e-01, 3.7345e-01,
        1.0702e-01, 1.5335e-01, 7.1675e-01, 2.2014e+00, 1.4640e+00, 1.0923e+01,
        4.3391e+00, 3.9233e+00, 6.6244e+00, 6.8436e+00, 1.0377e-01, 1.6908e-02,
        6.5178e-02, 7.2979e-02, 8.1948e-03, 1.6953e-02, 1.9170e-02, 2.0543e-01,
        5.1591e-02, 4.0188e-01, 7.6508e+00, 4.8000e-01, 3.7983e+00, 5.5130e-02,
        1.3295e+01, 3.8278e-01, 1.8259e+00, 5.5021e-01, 1.7467e-01, 2.3656e-01,
        2.8727e-01, 1.0505e+00, 1.7035e+00, 1.1374e+00, 5.2781e-03, 2.7389e-01,
        2.7676e+00, 3.0761e+00, 1.0239e+01, 1.1644e+01, 1.6610e+00, 3.5311e+00,
        3.6275e-02, 1.0256e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [11/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 125.9
Sum Train Loss:  tensor([5.1581e+00, 2.6073e-01, 3.1861e+00, 3.0210e-01, 2.8477e-02, 3.6311e-01,
        7.2218e-02, 3.3505e-01, 3.1936e-01, 2.2377e-01, 9.9607e-02, 1.1775e-02,
        2.4167e-03, 7.9390e-01, 6.3469e-01, 3.1693e-01, 5.9732e-01, 1.9455e-01,
        1.8133e-02, 3.1363e-01, 6.6708e-02, 5.2124e-02, 1.2217e-01, 4.7932e-02,
        3.5192e-01, 3.3871e-01, 1.9727e+00, 7.9265e-02, 1.0963e-01, 1.5082e-01,
        9.9127e-02, 7.6681e-03, 1.4871e+00, 1.9067e-02, 1.2435e-01, 1.6258e-01,
        1.1987e-01, 1.2670e-01, 3.2125e-01, 2.2650e+00, 1.6637e+00, 5.6607e+00,
        2.2609e+00, 4.6210e+00, 7.2445e+00, 6.8594e+00, 3.1224e-02, 2.5068e-02,
        3.1566e-02, 1.8095e-02, 8.1870e-03, 3.3342e-02, 1.0396e-01, 2.2603e-01,
        5.1966e-02, 1.7285e-01, 8.7670e+00, 4.4471e-01, 1.8589e+00, 9.5773e-02,
        1.6536e+01, 5.0998e-02, 2.8793e+00, 5.9928e-01, 6.6527e-02, 3.8643e-01,
        2.8965e-01, 1.0774e+00, 1.1672e+00, 6.1671e-01, 1.3654e-03, 3.3967e-01,
        1.2737e+00, 2.2366e+00, 7.5847e+00, 9.9530e+00, 4.6652e+00, 8.5068e+00,
        2.3220e-02, 6.3272e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [11/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 119.8
Sum_Val Meta Model:  tensor([7.0531e+00, 2.7602e+00, 1.1505e+01, 1.9448e+00, 2.4102e-02, 3.9706e-01,
        4.4541e-02, 4.0475e-01, 2.6671e-01, 2.4948e-01, 4.5878e-02, 1.2195e-01,
        4.1560e-02, 5.7750e-01, 4.0004e-01, 3.3595e-01, 2.5077e+01, 6.1188e-02,
        3.1746e-02, 4.7049e-02, 3.6154e-03, 3.8978e-03, 7.4814e-03, 9.8361e-03,
        7.4958e-01, 2.8664e-01, 2.1991e+00, 5.7000e-02, 9.8540e-02, 1.8422e-01,
        1.7667e-02, 4.0716e-03, 7.5769e-01, 1.2045e-03, 3.5561e-02, 7.6609e-02,
        3.9944e-02, 1.1789e-01, 8.3444e-02, 5.2788e+00, 2.6558e+00, 1.1198e+01,
        2.8413e+00, 6.2644e+00, 1.3255e+01, 1.0837e+01, 2.4673e-02, 8.2975e-02,
        1.8673e-02, 1.2682e-01, 1.5596e-03, 5.6394e-03, 9.1311e-02, 2.4815e-02,
        1.1436e-02, 4.7179e-02, 8.4652e+00, 4.2343e-01, 1.9451e+00, 3.3462e-02,
        8.5854e+00, 6.9837e-01, 1.1791e+00, 4.2199e-01, 4.7186e-02, 4.8140e-02,
        8.5269e-02, 4.0553e-01, 2.2233e+00, 2.0577e+00, 3.4012e-03, 5.3821e-01,
        6.7364e+00, 1.9213e+00, 1.1467e+01, 8.0509e+00, 3.2194e-01, 7.1160e+00,
        8.1949e-03, 3.3071e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([6.5831e+00, 2.0501e+00, 7.8780e+00, 1.1057e+00, 9.2137e-03, 4.0787e-01,
        4.1553e-02, 4.3721e-01, 2.4889e-01, 2.1955e-01, 5.1867e-02, 1.2164e-01,
        6.2730e-02, 6.2325e-01, 4.0200e-01, 1.1348e+00, 1.5942e+01, 1.4605e-01,
        2.1474e-02, 8.8064e-02, 5.8271e-03, 5.7440e-03, 3.1414e-03, 1.8218e-02,
        7.0082e-01, 3.0009e-01, 2.0904e+00, 8.5853e-02, 1.3182e-01, 2.3049e-01,
        7.3875e-03, 2.3049e-03, 6.9886e-01, 2.0447e-03, 3.1208e-02, 5.8271e-02,
        9.8026e-02, 5.9589e-02, 3.2914e-02, 4.7290e+00, 2.7154e+00, 1.1517e+01,
        2.5125e+00, 4.8591e+00, 1.2752e+01, 9.4477e+00, 2.0397e-02, 8.0324e-02,
        1.0957e-02, 9.9887e-02, 5.0795e-04, 3.3503e-03, 7.0655e-02, 1.1462e-02,
        7.4599e-03, 2.5631e-02, 8.0433e+00, 3.9640e-01, 1.6343e+00, 2.9855e-02,
        7.3154e+00, 2.1994e-01, 1.1331e+00, 4.2227e-01, 2.5864e-02, 6.3340e-02,
        4.8898e-02, 4.6473e-01, 2.4652e+00, 1.6633e+00, 6.1026e-03, 6.0525e-01,
        5.1939e+00, 1.9736e+00, 1.3547e+01, 8.5160e+00, 4.6556e-01, 6.3931e+00,
        9.5042e-03, 5.1221e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.9748e+01, 5.1923e+01, 5.2944e+01, 2.6269e+01, 5.5382e-01, 1.2455e+01,
        4.3511e+00, 1.8523e+01, 5.4386e+00, 9.8149e+00, 6.9535e+00, 1.1227e+01,
        1.0350e+01, 1.7052e+01, 8.7172e+00, 3.1289e+01, 2.8965e+02, 4.5689e+00,
        9.2126e-01, 2.2491e+00, 1.6890e+00, 7.6923e-01, 2.5792e-01, 1.9962e+00,
        2.7498e+01, 1.9182e+01, 2.7527e+01, 6.1859e+00, 1.3770e+01, 1.3708e+01,
        4.9007e-01, 2.6840e-01, 8.6405e+00, 9.7357e-01, 1.6238e+00, 1.5406e+00,
        6.7731e+00, 2.3473e+00, 7.6231e-01, 4.3798e+01, 1.1451e+01, 2.3514e+01,
        7.3025e+00, 1.1802e+01, 1.6596e+01, 1.4513e+01, 1.7548e+00, 5.1451e+00,
        6.7681e-01, 4.2035e+00, 1.3781e-01, 3.7489e-01, 5.4858e+00, 4.8709e-01,
        8.6569e-01, 9.8737e-01, 2.4481e+01, 5.8425e+00, 1.1183e+01, 2.2308e+00,
        1.0017e+01, 5.2723e+00, 4.9599e+00, 6.8659e+00, 5.0047e-01, 1.8676e+00,
        5.9682e-01, 8.7902e+00, 9.6902e+00, 1.2895e+01, 3.0743e-01, 1.5872e+01,
        1.0708e+01, 1.1568e+01, 1.6642e+01, 8.5709e+00, 4.6689e-01, 6.4686e+00,
        5.6692e-02, 6.2779e-01], device='cuda:0')
Outer loop valEpocw Maximum [11/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 171.7
model_train val_loss valEpocw [11/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 151.7
model_train val_loss valEpocw [11/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1076.6
Sum_Val Meta Model:  tensor([1.1136e+01, 9.7471e-01, 1.2775e+01, 5.0675e-01, 1.9881e-02, 2.2689e+00,
        2.5663e+00, 2.5633e+00, 2.8521e+00, 1.5032e+00, 3.0413e-01, 5.5357e+00,
        1.2176e-01, 3.0595e-01, 1.0475e+00, 9.4186e-01, 8.9828e-01, 9.2557e-01,
        4.3885e-01, 2.2324e+00, 1.5085e-03, 7.6098e-04, 1.3853e-02, 9.1550e-02,
        9.9510e-01, 6.4769e-01, 3.2337e+00, 3.9893e-01, 3.0215e-01, 3.4690e-03,
        5.4338e-03, 2.3534e-03, 2.4488e-02, 1.0215e-03, 2.6269e-03, 4.3005e-03,
        1.1144e-01, 7.9331e-03, 6.3437e-03, 2.5583e-01, 5.6210e-02, 2.1407e+00,
        6.6745e-02, 1.5251e-01, 2.9059e-01, 2.5240e+00, 1.9923e-02, 1.2276e-02,
        3.8232e-03, 2.7417e-01, 4.8852e-04, 2.1995e-03, 1.0566e-03, 2.1789e-03,
        2.4510e-03, 1.7488e-01, 4.1965e+00, 6.1745e-01, 1.5133e+00, 7.6104e-02,
        1.5141e+00, 2.2494e-02, 6.2093e-01, 1.6872e-01, 2.8340e-02, 4.6416e-02,
        5.5098e-02, 1.4115e+00, 6.0797e-02, 6.9792e-01, 2.5639e-03, 3.2803e-02,
        2.2752e+00, 1.1886e+00, 3.6469e+00, 4.3727e-01, 1.4243e-01, 5.9965e-01,
        8.4200e-03, 1.5992e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.0530e+01, 1.0619e+00, 8.3854e+00, 5.5558e-01, 1.3916e-01, 1.8294e+00,
        1.3437e+00, 1.9835e+00, 2.1630e+00, 1.2432e+00, 2.3776e-01, 1.9225e+00,
        1.3402e-01, 5.4439e-01, 1.1224e+00, 2.2509e-01, 8.1514e-01, 7.9879e-01,
        1.3596e-01, 1.3895e+00, 9.7174e-03, 3.8458e-03, 1.3021e-02, 8.7143e-02,
        1.0468e+00, 5.5585e-01, 3.0352e+00, 4.0320e-01, 2.7889e-01, 5.5128e-02,
        1.5993e-02, 4.0158e-03, 5.9831e-02, 9.2102e-03, 1.1298e-02, 1.2853e-02,
        7.6717e-02, 1.6926e-01, 9.8503e-03, 2.8604e-01, 1.0184e-01, 1.4257e+00,
        1.6127e-01, 4.3621e-01, 2.2119e-01, 9.4146e-01, 1.6748e-02, 9.8708e-03,
        1.0392e-02, 2.0879e-01, 6.8421e-04, 2.9605e-03, 8.8466e-03, 4.0838e-02,
        5.7731e-03, 4.7545e-02, 2.8873e+00, 4.4302e-01, 1.0720e+00, 5.3352e-03,
        2.1699e+00, 1.7327e-02, 2.7346e-01, 6.2978e-02, 1.3446e-02, 1.2704e-02,
        3.0556e-02, 1.3308e+00, 5.4922e-02, 3.4197e-01, 5.7187e-04, 1.6369e-02,
        1.2271e+00, 7.2409e-01, 5.1324e+00, 2.3130e-01, 1.4070e-01, 1.5714e-01,
        1.2045e-03, 1.2384e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.4983e+01, 1.7633e+01, 4.2787e+01, 8.2433e+00, 4.9335e+00, 3.1292e+01,
        5.1730e+01, 4.2311e+01, 2.9583e+01, 3.2042e+01, 1.5194e+01, 7.6270e+01,
        8.9328e+00, 1.0956e+01, 1.5189e+01, 3.5166e+00, 9.9663e+00, 1.4017e+01,
        2.3194e+00, 1.6678e+01, 1.0281e+00, 2.4023e-01, 5.1670e-01, 5.0483e+00,
        2.3776e+01, 1.8315e+01, 2.9198e+01, 1.3789e+01, 1.2442e+01, 1.6909e+00,
        5.1633e-01, 2.0655e-01, 5.5054e-01, 1.4943e+00, 3.5025e-01, 2.4504e-01,
        2.7324e+00, 3.6610e+00, 1.4728e-01, 2.4701e+00, 4.3476e-01, 3.0956e+00,
        4.8514e-01, 1.0974e+00, 3.1244e-01, 1.5605e+00, 6.8880e-01, 3.2204e-01,
        3.1141e-01, 4.7098e+00, 7.2424e-02, 1.6614e-01, 3.4830e-01, 1.0144e+00,
        3.0350e-01, 1.0364e+00, 8.4861e+00, 4.2768e+00, 6.6849e+00, 1.7909e-01,
        3.1730e+00, 2.8174e-01, 1.0821e+00, 6.7000e-01, 1.7880e-01, 2.1862e-01,
        2.8763e-01, 1.8804e+01, 2.2524e-01, 2.4121e+00, 1.5264e-02, 3.0425e-01,
        2.9866e+00, 3.5534e+00, 7.1114e+00, 2.3444e-01, 1.4150e-01, 1.6024e-01,
        5.7112e-03, 1.1653e-01], device='cuda:0')
Outer loop valEpocw Maximum [11/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 81.1
model_train val_loss valEpocw [11/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 62.7
model_train val_loss valEpocw [11/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 694.5
Sum_Val Meta Model:  tensor([1.0512e+01, 1.5247e-01, 1.8091e+00, 1.0673e-01, 1.6279e-02, 8.8406e-02,
        1.4380e-02, 3.0933e-01, 1.3106e-01, 5.4528e-02, 1.2927e-02, 1.5824e-02,
        3.0009e-03, 7.7522e-01, 1.3874e-01, 1.2868e-01, 3.7921e-01, 6.3370e-02,
        4.4245e-02, 1.0349e-01, 4.8309e-03, 1.6690e-02, 1.4062e-01, 2.5399e-02,
        1.8248e-01, 3.4032e-02, 1.5855e+00, 1.2579e-01, 2.2484e-02, 7.3512e-02,
        9.1819e-02, 7.0102e-02, 7.3096e-01, 8.1204e-04, 5.4100e-02, 1.1186e-01,
        2.9849e-01, 4.4088e-01, 1.8217e-02, 3.0330e+00, 7.4579e+00, 3.4867e+01,
        2.9795e+01, 3.6032e+01, 3.9953e+01, 4.6445e+01, 2.4894e-01, 2.5429e-01,
        1.5433e+00, 5.3383e-01, 2.5661e-01, 5.4120e-01, 1.1469e+00, 4.9979e-01,
        7.9004e-01, 3.3163e+00, 5.2870e+01, 1.1908e+00, 1.2658e+00, 4.8220e-02,
        6.6297e+01, 7.8666e-02, 2.5738e+00, 9.5028e-01, 8.4724e-01, 5.2401e-02,
        8.9071e-01, 6.3323e-01, 1.0535e+00, 3.1284e-01, 1.1730e-02, 2.9794e-01,
        2.0890e+00, 5.6237e+00, 1.6928e+00, 1.7661e+01, 9.7112e+00, 1.3158e+00,
        5.4618e-02, 7.5744e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([2.8022e+00, 2.0265e-02, 5.4227e-01, 8.6942e-03, 1.7031e-03, 3.8473e-03,
        1.4842e-03, 3.0625e-01, 1.6047e-02, 3.4893e-03, 7.3431e-04, 7.1603e-04,
        5.4602e-04, 5.8006e-01, 1.4447e-02, 6.2378e-02, 1.3909e-01, 1.1196e-02,
        6.0754e-03, 5.5754e-03, 3.8766e-04, 3.3919e-04, 3.4652e-04, 8.4668e-04,
        1.2558e-01, 1.8271e-02, 1.6710e+00, 1.1110e-01, 8.9553e-03, 9.8721e-03,
        8.8264e-03, 8.7485e-02, 7.2368e-01, 2.4800e-04, 9.1391e-03, 2.4835e-02,
        2.2523e-01, 3.4624e-01, 8.0052e-03, 4.1764e+00, 6.4257e+00, 2.9270e+01,
        2.5775e+01, 3.2533e+01, 3.1557e+01, 4.1888e+01, 1.5762e-01, 1.3408e-01,
        9.0495e-01, 2.9524e-01, 1.2083e-01, 5.4345e-01, 1.1530e+00, 6.0004e-01,
        6.3523e-01, 2.4432e+00, 3.1654e+01, 1.1033e+00, 1.2259e+00, 1.2458e-02,
        6.0849e+01, 1.0310e-02, 2.0228e+00, 7.4560e-01, 6.6115e-01, 1.3161e-01,
        6.9278e-01, 5.9856e-01, 1.1988e+00, 7.0560e-01, 2.9743e-03, 2.5867e-01,
        2.2760e+00, 6.4642e+00, 2.6227e+00, 1.5191e+01, 9.8030e+00, 7.2147e-01,
        9.4964e-03, 4.2973e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.3450e+01, 5.2617e-01, 3.5717e+00, 2.0709e-01, 1.1187e-01, 1.1310e-01,
        1.7520e-01, 1.4087e+01, 3.6104e-01, 1.5805e-01, 9.8620e-02, 6.7913e-02,
        9.4380e-02, 1.6218e+01, 3.1433e-01, 1.7068e+00, 2.3341e+00, 3.5361e-01,
        2.1628e-01, 1.2763e-01, 1.3300e-01, 4.0466e-02, 2.2452e-02, 9.1694e-02,
        4.7408e+00, 1.3884e+00, 2.6513e+01, 8.9333e+00, 1.0862e+00, 4.8617e-01,
        5.7654e-01, 8.2863e+00, 8.8250e+00, 9.9911e-02, 4.4313e-01, 6.0175e-01,
        1.3457e+01, 1.1228e+01, 1.9417e-01, 4.4111e+01, 2.8285e+01, 5.6944e+01,
        6.2683e+01, 6.7816e+01, 4.0415e+01, 6.0426e+01, 1.0910e+01, 7.0686e+00,
        3.7384e+01, 1.0201e+01, 2.3243e+01, 4.3838e+01, 6.5604e+01, 2.4343e+01,
        6.1298e+01, 7.4787e+01, 1.0069e+02, 1.5810e+01, 9.4676e+00, 7.8094e-01,
        8.0226e+01, 2.7475e-01, 9.0871e+00, 1.1961e+01, 1.2902e+01, 3.7743e+00,
        8.7703e+00, 1.3243e+01, 6.0366e+00, 6.8004e+00, 1.4850e-01, 8.0466e+00,
        5.9347e+00, 3.9965e+01, 3.4672e+00, 1.5289e+01, 9.8276e+00, 7.2897e-01,
        6.1422e-02, 5.4986e-01], device='cuda:0')
Outer loop valEpocw Maximum [11/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 393.2
model_train val_loss valEpocw [11/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 325.5
model_train val_loss valEpocw [11/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1234.6
Sum_Val Meta Model:  tensor([1.4795e+01, 7.4009e-02, 3.3121e+00, 3.6453e-02, 7.6227e-03, 3.6469e-01,
        5.1141e-02, 3.6435e-01, 5.4523e-02, 4.1398e-01, 3.6789e-02, 5.0227e-02,
        9.3581e-04, 5.7394e-01, 7.1972e-01, 5.4917e-02, 9.4298e-02, 2.0152e-02,
        1.1794e-02, 2.0924e-02, 1.8548e-03, 2.8513e-03, 6.9424e-03, 6.0670e-03,
        6.0968e-01, 1.9571e-02, 2.0978e+00, 1.9685e-02, 1.3139e-01, 6.4554e-03,
        9.5268e-03, 1.2036e-03, 2.3221e-01, 5.8316e-03, 2.9208e-02, 2.0449e-01,
        3.8924e-03, 2.0678e-02, 2.3073e-01, 2.6645e+00, 3.7676e+00, 1.4471e+01,
        3.4044e+00, 4.4827e+00, 7.2691e+00, 1.0788e+01, 6.6747e-03, 9.4919e-03,
        2.5481e-02, 1.0648e-02, 4.3699e-03, 6.6182e-03, 5.1255e-03, 9.8067e-02,
        8.4795e-03, 4.2258e-02, 7.0527e+00, 2.8002e-01, 2.2809e+00, 1.9148e-02,
        3.0916e+01, 2.1797e-02, 1.0055e+00, 2.6888e-01, 1.6813e-01, 6.6174e-02,
        3.3323e-01, 3.3797e+00, 2.9750e-01, 3.5363e-01, 2.2348e-03, 5.1718e-02,
        2.3567e+00, 1.7998e+00, 3.4603e+02, 1.3564e+01, 1.4369e+00, 1.0242e+01,
        1.0325e-02, 8.8778e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.3371e+00, 1.0286e-01, 2.9439e+00, 2.3167e-02, 1.7993e-02, 2.8409e-01,
        8.1052e-02, 3.1787e-01, 5.7214e-02, 3.3731e-01, 2.2883e-02, 9.5367e-02,
        7.1450e-03, 5.0374e-01, 6.8065e-01, 5.2064e-02, 6.3513e-02, 2.0217e-02,
        3.1147e-03, 4.3570e-03, 1.6704e-03, 3.7287e-04, 2.3100e-03, 3.7905e-03,
        5.7682e-01, 4.3342e-02, 1.3995e+00, 3.5907e-02, 1.3898e-01, 3.7827e-03,
        3.2713e-03, 2.0693e-03, 2.3706e-02, 5.9268e-04, 5.7224e-03, 1.0194e-02,
        8.3349e-03, 8.1564e-03, 6.9098e-03, 2.8363e-01, 3.0629e-01, 8.8280e-01,
        3.8336e-01, 6.3441e-01, 6.3369e-01, 1.0725e+00, 3.6146e-03, 2.3765e-03,
        2.6489e-03, 2.1676e-03, 1.9217e-04, 8.0730e-04, 1.2814e-03, 4.2064e-03,
        2.3230e-03, 3.1783e-02, 1.3387e+00, 3.5131e-02, 1.5088e+00, 3.3013e-03,
        7.4376e+00, 1.1863e-02, 2.4022e-01, 3.4014e-02, 1.1318e-02, 2.7047e-02,
        2.9357e-02, 4.4568e-01, 7.4029e-02, 1.1477e-01, 6.3697e-04, 1.2489e-02,
        1.6957e-01, 1.0941e+00, 5.3834e+01, 3.2583e+00, 9.0366e-02, 6.4128e+00,
        1.1143e-03, 4.3166e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.0560e+01, 2.8492e+00, 2.1669e+01, 6.1881e-01, 1.1972e+00, 9.5616e+00,
        9.8265e+00, 1.5008e+01, 1.3920e+00, 1.7198e+01, 3.4025e+00, 9.7310e+00,
        1.3184e+00, 1.4757e+01, 1.6141e+01, 1.4701e+00, 1.2831e+00, 6.8861e-01,
        1.2951e-01, 1.1767e-01, 5.7375e-01, 5.1327e-02, 2.0806e-01, 4.5967e-01,
        2.3465e+01, 3.3591e+00, 1.9732e+01, 2.8023e+00, 1.7886e+01, 2.3701e-01,
        2.4593e-01, 2.5347e-01, 2.8641e-01, 2.7403e-01, 2.9818e-01, 2.1468e-01,
        6.4828e-01, 3.3811e-01, 1.5568e-01, 2.7424e+00, 1.1746e+00, 1.7419e+00,
        1.0143e+00, 1.4491e+00, 8.2602e-01, 1.6428e+00, 3.2508e-01, 1.6348e-01,
        1.7755e-01, 1.0321e-01, 5.7973e-02, 9.3631e-02, 1.0747e-01, 1.9328e-01,
        2.9589e-01, 1.3229e+00, 4.0339e+00, 5.5145e-01, 1.0077e+01, 2.6237e-01,
        9.9399e+00, 3.1767e-01, 1.0042e+00, 5.2388e-01, 1.9180e-01, 8.4324e-01,
        3.2627e-01, 6.8394e+00, 3.4793e-01, 1.0257e+00, 3.4391e-02, 3.9857e-01,
        3.8787e-01, 6.4292e+00, 7.0372e+01, 3.2752e+00, 9.0566e-02, 6.4686e+00,
        9.0044e-03, 4.8197e-02], device='cuda:0')
Outer loop valEpocw Maximum [11/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 493.8
model_train val_loss valEpocw [11/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 93.7
model_train val_loss valEpocw [11/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 377.6
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [86.75820226 97.29413628 92.91431635 97.94593146 98.46493098 97.53536141
 98.26147342 95.17549737 98.01537506 96.94935491 98.57214215 98.91570522
 99.44445121 95.3606803  97.44886149 97.19301665 96.33776392 97.59871347
 98.77194479 98.38574091 98.579452   99.26535983 99.53826099 98.9132686
 95.25346913 96.76660859 94.32755449 96.94691829 98.04826939 98.26512835
 98.25050864 98.58188862 96.99686895 98.42838172 98.66838854 98.81945883
 97.44764318 98.3833043  98.88159257 93.29199206 97.97638918 93.77078739
 97.34286863 96.63746787 97.40987561 95.51906044 98.17862843 98.63183928
 98.1055299  98.71346597 98.71102935 98.59163509 99.01560654 98.33700856
 98.71955751 97.53292479 92.0018031  96.85920006 96.50345391 97.32824892
 94.47740646 98.56605061 97.15037585 97.31484753 98.77316309 97.53536141
 98.59041678 95.96252482 98.83286022 98.24198048 99.81603538 97.35627003
 98.65498715 95.89186292 97.64988243 98.06532571 99.27632461 98.88281088
 99.84405648 99.15814866]
Accuracy th:0.7 is [85.90416783 97.25758702 92.24302823 97.77902316 98.3552832  97.27342503
 98.12380453 94.87579342 97.9093822  96.78000999 98.5368112  98.87428272
 99.42495827 95.31925781 97.37332635 97.13575614 96.33289068 97.53657972
 98.69640964 98.32969871 98.54777598 99.26657814 99.49562018 98.84260669
 95.2205748  96.6837636  94.34217419 96.92620704 98.02877645 98.20543122
 98.08116373 98.57457877 96.76051705 98.28218467 98.64524068 98.76707155
 97.20398143 98.37355783 98.79265603 93.07635141 97.91791036 93.29930191
 97.28438981 96.48761589 97.17961526 95.15356782 98.1323327  98.61965619
 98.0628891  98.66351531 98.60747311 98.56970553 99.00220514 98.23954387
 98.70859273 97.49515722 91.06979691 96.61432    96.40355259 97.09433365
 93.91454782 98.40157893 96.94691829 97.20641805 98.72321244 97.4634812
 98.52462811 95.95521497 98.77681802 98.28218467 99.81603538 97.13331953
 98.54412105 95.6810955  97.5475445  97.98369903 99.25439505 98.78412787
 99.84405648 99.14840219]
Avg Prec: is [95.93393296 29.72725303 69.24601476 63.44203272 75.20259213 62.67287769
 71.84913025 45.77691636 53.60654233 49.82613828 26.47516611 51.56578673
 20.47879213 25.42156634 30.44683046 52.22467814 25.7453937  35.35946242
 42.18373413 34.25637008 55.7173944  43.6431198  89.2724254  78.88270445
 25.10202138 28.3332939  36.63322419 35.22154128 21.15523396 36.22391203
 70.17253982 34.5989812  55.01581424 60.73321655 71.13315173 77.67308443
 54.04562959 73.86935401 84.93146763 44.38374833 36.72311239 60.34804524
 54.02871735 49.43320658 52.47704064 64.63408803 34.42649659 29.16726007
 38.77007582 42.07218525 58.03865106 33.43874232 18.87169329 70.54115454
 22.73756132 35.05245735 64.58049233 54.75124369 37.69719229 53.96670369
 79.45392081 80.57467583 63.15623741 45.42012025 56.10335889 39.78322219
 54.21237853 23.66889443 46.81265808 64.56298748 10.26622177 70.39691427
 67.61851137 45.08024177 67.23214177 70.18276297 35.9476144  67.00934571
  6.27234244 20.11273513]
Accuracy th:0.5 is [45.42951475 97.2137279  72.59170819 97.02489005 97.26733349 77.40524604
 77.86211182 76.60116227 78.68081529 96.43644692 78.98661079 98.52097319
 99.41399349 80.27558144 78.5699492  96.56680596 96.29512311 78.28121002
 98.65376884 98.30776915 80.52898966 79.52875818 98.38695922 78.47004788
 80.50218686 96.65086926 94.0778012  78.00465394 98.01293844 78.91594888
 97.30875598 98.57457877 96.36213009 98.02024829 87.23577929 78.54923795
 78.24100584 90.62998745 97.11504489 75.51808579 79.53728634 92.05906361
 77.82678086 77.39549957 96.9627563  93.87434364 98.02877645 98.57336046
 95.28392685 88.40657399 87.22115959 98.55508583 98.99976852 78.01074548
 98.70615611 78.32506914 72.89384876 93.3675272  96.24273583 96.9067141
 89.79300934 97.17717864 91.92383134 78.33116068 98.42838172 78.71249132
 98.20786784 77.41621082 79.41545546 97.55729097 79.98318734 95.99054592
 79.09260365 95.45083515 77.66596411 82.81697348 86.44997015 78.93666013
 80.03801123 99.14718388]
Accuracy th:0.7 is [45.5062682  97.2137279  72.59170819 97.02489005 97.26733349 77.40524604
 77.86211182 76.78025365 78.68081529 96.4742145  78.98661079 98.52097319
 99.41399349 80.72635567 78.5699492  96.56680596 96.29512311 78.28121002
 98.65376884 98.30776915 80.94443294 79.52875818 98.38695922 78.59187875
 81.01143992 96.65086926 94.0778012  78.00465394 98.01293844 78.91594888
 97.30875598 98.57457877 96.36213009 98.02024829 87.45872979 78.54923795
 78.24100584 90.89801538 97.11504489 75.51808579 80.1586238  92.05906361
 77.82678086 77.39549957 96.9627563  93.87434364 98.02877645 98.57336046
 96.58264397 89.04374947 87.39781435 98.55508583 98.99976852 78.01074548
 98.70615611 78.32506914 72.89384876 93.7720057  96.24273583 96.9067141
 89.79300934 97.17717864 92.10292272 78.33116068 98.42838172 78.71249132
 98.20786784 77.41621082 79.41545546 97.55972759 79.98318734 95.99054592
 79.13402614 95.45083515 77.66596411 82.89738185 86.58885735 78.93666013
 80.03801123 99.14718388]
Avg Prec: is [55.69396661  3.1180669  11.25381735  3.32880489  2.31009384  3.72630029
  3.31617664  5.54346848  2.42969483  3.83153266  1.64485469  1.60526528
  0.5999911   5.16218847  2.61044655  3.22235464  3.60011201  2.64598939
  1.40754639  1.70942435  1.95441097  0.89337537  1.8758505   2.36585095
  5.04753899  3.52233054  6.62960908  3.26051413  2.16093445  1.86463527
  2.55136596  1.34674114  3.68119428  1.62437926  2.37302856  2.35527138
  3.03977738  2.55407741  2.81282869  7.52617101  2.25229683  8.30065247
  3.36286061  4.01249474  3.14791791  6.42598649  2.02354343  1.58288734
  2.15141639  1.63725337  1.82341212  1.64389219  1.07063389  2.96874392
  1.35538715  2.70661992 11.30520707  3.73283085  3.95631765  2.91667944
 10.84046791  2.19463381  3.79798105  2.92641477  1.6264397   2.50070744
  1.78778784  4.12979347  1.2525989   2.3189026   0.16353105  3.46138478
  1.87370039  4.74300084  3.97129295  3.04668224  0.83450788  1.86810978
  0.16395664  0.7836198 ]
mAP score regular 49.55, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [87.58751277 97.2892842  93.18832997 98.20116102 98.94361811 97.59822608
 98.37556369 95.37334629 98.05665595 97.01522286 98.64215063 99.01836211
 99.36467598 95.18897775 97.43628074 97.16720233 96.24037671 97.68293594
 98.87883997 98.38054663 98.7418093  99.31235518 99.63375439 99.18778185
 95.44310736 96.72621272 94.21730573 97.14976206 97.88225328 98.06911329
 98.6521165  98.67204823 96.93549593 98.66208237 98.93863517 99.14044398
 97.86232155 98.31576849 99.05822558 93.1708897  97.91464235 93.41007051
 97.11488153 96.52689538 97.0276802  94.92986521 98.29334529 98.8016045
 98.07409622 98.75177517 98.75426664 98.61225303 98.88880584 98.4204101
 98.71440317 97.69041034 91.14283579 96.99529113 96.36744151 97.304233
 93.10860303 98.7866557  97.23447193 97.43378927 98.7717069  97.52597354
 98.5250517  95.7769639  98.78416424 98.25597329 99.81563146 97.45372101
 98.45279916 95.78942123 97.45372101 97.51600767 99.24508558 98.7119117
 99.82559733 99.15788425]
Accuracy th:0.7 is [87.5924957  97.26436953 92.62525849 98.04419862 98.83648504 97.3341306
 98.29085383 95.15409722 98.03423275 96.85576899 98.55993223 98.97102424
 99.37962479 95.13167402 97.35406234 97.12235593 96.31013778 97.60071754
 98.84645091 98.36061489 98.78416424 99.31235518 99.61133119 99.08812318
 95.47051349 96.60163939 94.571094   97.12235593 97.82494955 98.23355009
 98.48518823 98.67204823 96.82587139 98.5175773  98.88133144 99.11802078
 97.60570048 98.28088796 98.97351571 93.14597504 97.88723622 93.39512171
 97.3266562  96.62157112 97.07501806 94.87754441 98.27590503 98.81157037
 98.02177542 98.73184344 98.60228717 98.56740663 98.88133144 98.4503077
 98.7044373  97.64307248 90.81645365 96.90808979 96.34750978 97.10491566
 93.2157361  98.64215063 97.0501034  97.34409647 98.7268605  97.52099061
 98.51259436 95.7769639  98.76174104 98.22109276 99.81563146 97.416349
 98.4353589  95.7395919  97.43129781 97.64058101 99.25255998 98.6745397
 99.82559733 99.15040985]
Avg Prec: is [96.37214575 28.76290224 70.20465884 71.83122721 76.40776013 63.5241818
 77.52335807 46.30916508 59.52622951 53.67355328 35.58149953 54.88128382
 21.24423777 28.99449063 32.36306865 58.24836577 29.67673333 39.43720293
 44.28148238 32.71471996 64.52206422 52.36467575 91.70970927 85.02414746
 25.01089122 34.07878547 33.55843417 42.54670852 24.92136875 35.52996307
 76.38857944 36.73905452 55.29325253 63.85582307 74.35486008 82.31239698
 57.90436562 76.59926224 89.32456106 43.98211631 33.17583318 53.04675889
 46.91661807 40.93932274 35.82796059 53.85664296 32.63998448 28.27868983
 39.79844747 40.55887605 63.13391365 33.08252837 20.34855576 75.72416989
 27.36320599 36.29392333 57.42806031 52.80125146 36.55023505 57.00568996
 68.68451955 86.10877317 64.79764382 48.89889224 59.68855332 35.56608494
 58.29746074 24.38662289 40.92888404 65.52110698  9.81011987 72.89164067
 56.14409588 42.19760628 66.37832104 54.11328636 14.19214406 53.8395697
  2.06725353 19.1651295 ]
Accuracy th:0.5 is [45.32974562 97.22450607 71.22106784 96.96290206 97.90716795 76.79198744
 76.89413758 75.68577622 78.31676508 96.41976231 78.52604828 98.5325261
 99.34972718 78.57836909 78.39400055 96.31262924 96.21047911 77.87079254
 98.78167277 98.34068316 79.47529711 79.1613723  98.31327703 77.99287441
 78.55096295 96.52938685 94.3393876  77.87826694 97.81747515 78.48369335
 97.52597354 98.67204823 96.39983058 98.18870369 88.32249545 78.09502454
 78.03772081 92.08710168 97.0276802  75.102773   78.28188455 92.37362035
 77.21304532 76.87669731 97.03764606 94.02795426 98.18621222 98.77668984
 96.10832897 88.20539652 85.80113113 98.55993223 98.87385704 77.23297705
 98.6969629  77.8957072  71.68946359 94.36430226 96.16314124 96.78102499
 90.13379176 97.04761193 91.7856342  77.82345467 98.32075143 78.68301069
 98.13139996 77.01372798 79.1837955  97.53593941 79.64222538 96.07843137
 78.88980243 95.44559882 76.92652665 83.79799188 88.35986745 78.53601415
 79.71198645 99.15040985]
Accuracy th:0.7 is [45.52158856 97.22450607 71.22106784 96.96290206 97.90716795 76.79198744
 76.89413758 75.74307995 78.31676508 96.41976231 78.52604828 98.5325261
 99.34972718 78.9745123  78.39400055 96.31262924 96.21047911 77.87079254
 98.78167277 98.34068316 79.83406832 79.1613723  98.31327703 78.02277201
 78.88731096 96.52938685 94.3393876  77.87826694 97.81747515 78.48369335
 97.52597354 98.67204823 96.39983058 98.18870369 88.54423599 78.09502454
 78.03772081 92.31631662 97.0276802  75.102773   78.71789122 92.37362035
 77.21304532 76.87669731 97.03764606 94.02795426 98.18621222 98.77668984
 97.04512046 88.49938959 85.9456362  98.55993223 98.87385704 77.23297705
 98.6969629  77.8957072  71.68946359 94.6159404  96.16314124 96.78102499
 90.13379176 97.04761193 91.92764781 77.82345467 98.32075143 78.68301069
 98.13139996 77.01372798 79.1837955  97.53593941 79.64222538 96.07843137
 78.89727683 95.44559882 76.92652665 83.86277001 88.53925306 78.53601415
 79.71198645 99.15040985]
Avg Prec: is [53.81065934  3.71796961 14.84507761  4.54330545  1.48443707  4.25954558
 14.31805568  8.69570903  8.33991143  5.26681211  2.45411345  4.80310205
  2.36642441  5.82977029  2.98973023  3.64110244 23.31225081  6.47069468
  1.55466735  2.73722734  3.50122708  1.53507741  1.23744129  5.14199102
  5.60033312  8.70479078  7.80138808  4.57342511  3.88559264  5.35141196
  2.19273541  0.84567843  2.88769406  1.14649646  1.67124984  2.20794446
  1.984899    2.30847015  2.19202911  6.19340876  1.70811104  6.02084607
  2.16944259  2.68704618  2.34040928  4.8522285   1.66795007  1.01655013
  1.4083688   1.13340765  1.1950448   0.9742073   0.76283791  2.26125437
  0.91623531  1.86418239 10.0789558   2.989827    3.89204136  2.7926001
  7.82146508  2.08144826  3.18187958  2.50709259  1.34774546  1.92716743
  1.52398404  3.51873403  1.07632011  2.23507926  0.19443161  3.26252022
  1.6078483   3.98367766  3.19928307  2.29576843  0.56774134  1.44440197
  0.1221018   0.60699695]
mAP score regular 49.70, mAP score EMA 4.30
Train_data_mAP: current_mAP = 49.55, highest_mAP = 49.55
Val_data_mAP: current_mAP = 49.70, highest_mAP = 49.70
tensor([0.1157, 0.0350, 0.1365, 0.0361, 0.0145, 0.0288, 0.0077, 0.0201, 0.0394,
        0.0191, 0.0064, 0.0092, 0.0051, 0.0323, 0.0416, 0.0337, 0.0483, 0.0284,
        0.0233, 0.0366, 0.0027, 0.0069, 0.0106, 0.0078, 0.0233, 0.0123, 0.0675,
        0.0120, 0.0072, 0.0154, 0.0130, 0.0078, 0.0722, 0.0017, 0.0156, 0.0353,
        0.0123, 0.0224, 0.0368, 0.0975, 0.2325, 0.4904, 0.3972, 0.4497, 0.8004,
        0.6723, 0.0106, 0.0143, 0.0144, 0.0208, 0.0031, 0.0085, 0.0117, 0.0187,
        0.0074, 0.0233, 0.3318, 0.0592, 0.1399, 0.0120, 0.7473, 0.0359, 0.2230,
        0.0583, 0.0500, 0.0291, 0.0783, 0.0495, 0.2511, 0.1211, 0.0178, 0.0303,
        0.4625, 0.1664, 0.8291, 0.9952, 0.9987, 0.9927, 0.2102, 0.0830],
       device='cuda:0')
Sum Train Loss:  tensor([4.7475e+00, 6.5558e-01, 2.8495e+00, 3.5427e-01, 4.7158e-02, 3.6098e-01,
        1.4003e-02, 2.1623e-01, 2.5285e-01, 3.9933e-01, 4.8393e-02, 5.8071e-02,
        7.4853e-03, 9.1899e-01, 6.5447e-01, 2.5564e-01, 8.0428e-01, 4.2755e-01,
        1.0714e-01, 1.7954e-01, 2.7616e-02, 7.4041e-03, 1.8973e-02, 4.5253e-02,
        4.6709e-01, 1.2263e-01, 1.5769e+00, 1.4310e-01, 5.2371e-02, 2.3274e-01,
        4.5076e-02, 1.1375e-01, 8.7184e-01, 2.0866e-03, 8.2369e-02, 8.2319e-02,
        1.6724e-01, 6.5831e-02, 3.9776e-02, 2.6226e+00, 9.6680e-01, 9.3640e+00,
        1.9544e+00, 1.8328e+00, 5.9697e+00, 7.0297e+00, 8.2112e-02, 5.9010e-02,
        1.3821e-01, 2.6967e-02, 2.2794e-02, 4.7516e-02, 1.4627e-02, 3.2694e-02,
        8.3646e-02, 2.6852e-01, 9.8625e+00, 7.9326e-01, 1.6773e+00, 1.0087e-01,
        9.5833e+00, 8.7459e-01, 8.7859e-01, 4.3991e-01, 1.0072e-01, 3.9510e-01,
        3.6235e-01, 8.6123e-01, 9.7102e-01, 3.7854e-01, 3.3004e-03, 7.5462e-01,
        3.6945e+00, 2.3320e+00, 4.8922e+00, 6.4339e+00, 6.5223e+00, 7.2064e+00,
        1.4578e-02, 1.0083e+00], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [12/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 108.2
Sum Train Loss:  tensor([4.9425e+00, 3.7022e-01, 4.5935e+00, 1.4579e-01, 1.2583e-01, 1.7078e-01,
        6.9015e-02, 2.7320e-01, 1.4150e-01, 3.0934e-01, 7.2704e-02, 8.1421e-02,
        3.2064e-02, 6.4358e-01, 5.3880e-01, 1.9107e-01, 8.0921e-01, 4.0079e-01,
        1.1206e-01, 2.4848e-01, 2.4462e-02, 2.4130e-02, 1.1033e-02, 1.7363e-02,
        5.2408e-01, 1.5953e-01, 1.8955e+00, 4.2997e-02, 8.9548e-02, 9.6933e-02,
        1.9382e-02, 4.1970e-02, 7.1379e-01, 6.3670e-03, 5.0677e-02, 7.3925e-02,
        1.1881e-01, 1.7886e-01, 8.0636e-02, 2.1324e+00, 2.2679e+00, 1.2034e+01,
        2.6784e+00, 5.0197e+00, 4.5877e+00, 1.2123e+01, 1.0688e-01, 1.0747e-01,
        2.6088e-01, 1.5830e-01, 1.0059e-02, 4.6446e-02, 1.1955e-01, 1.2595e-01,
        7.2796e-02, 2.1161e-01, 1.0882e+01, 4.8110e-01, 1.3422e+00, 1.0892e-01,
        1.6454e+01, 1.7473e-01, 6.3827e-01, 2.3509e-01, 4.6303e-02, 1.6826e-01,
        1.1257e-01, 1.3518e+00, 7.1687e-01, 2.8303e-01, 3.7814e-03, 1.6952e-01,
        7.0759e-01, 9.2673e-01, 1.1483e+01, 9.4263e+00, 3.8653e+00, 3.2767e+00,
        2.6467e-02, 3.9698e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [12/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 123.8
Sum Train Loss:  tensor([3.8790e+00, 4.6964e-01, 2.4213e+00, 5.4596e-01, 6.5039e-02, 2.3095e-01,
        2.3527e-02, 2.2524e-01, 2.6902e-01, 1.6327e-01, 9.5913e-03, 2.6283e-02,
        2.9944e-03, 4.5166e-01, 2.5761e-01, 3.8023e-01, 1.0938e+00, 1.1963e-01,
        1.0604e-01, 3.4439e-01, 1.5848e-02, 1.0926e-02, 6.7316e-03, 4.6176e-02,
        3.0324e-01, 2.7210e-01, 2.7598e+00, 7.3678e-02, 7.4977e-02, 4.5966e-02,
        2.5351e-02, 4.4115e-02, 3.3063e-01, 4.0036e-03, 3.2559e-02, 4.9630e-02,
        1.7443e-01, 1.4344e-01, 1.3422e-01, 2.2371e+00, 4.8659e+00, 1.1316e+01,
        4.6354e+00, 5.3840e+00, 1.0525e+01, 1.2817e+01, 6.9600e-02, 5.5891e-02,
        6.1724e-02, 1.4940e-01, 1.7358e-02, 4.9247e-02, 1.8611e-02, 1.1347e-01,
        3.9890e-02, 2.8114e-01, 9.2846e+00, 8.7737e-01, 2.7301e+00, 1.6833e-01,
        1.7490e+01, 2.2983e-01, 3.3288e+00, 8.8312e-01, 2.4297e-01, 2.0849e-01,
        3.7339e-01, 1.5271e+00, 1.5909e+00, 8.1285e-01, 3.3097e-03, 2.0942e-01,
        8.1145e+00, 3.7629e+00, 4.9204e+00, 1.3827e+01, 2.1220e+00, 5.3566e+00,
        7.9270e-01, 4.8002e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [12/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 147.6
Sum Train Loss:  tensor([4.5306e+00, 7.7794e-01, 3.0089e+00, 6.3620e-01, 5.1437e-02, 2.4085e-01,
        4.9579e-02, 2.6324e-01, 4.9051e-01, 1.1534e-01, 9.4784e-02, 1.1097e-02,
        1.6261e-02, 4.1444e-01, 3.2224e-01, 3.5769e-01, 1.2150e+00, 8.8715e-02,
        1.2941e-01, 1.6430e-01, 8.7125e-03, 1.5131e-02, 7.4621e-03, 1.6169e-02,
        7.3397e-01, 2.5482e-01, 8.3222e-01, 1.4558e-01, 8.4573e-02, 3.0859e-02,
        8.9524e-02, 8.5021e-02, 1.1008e+00, 8.2280e-03, 8.3785e-02, 3.9257e-01,
        2.0769e-01, 3.2920e-01, 5.1034e-02, 1.6810e+00, 1.4492e+00, 1.0873e+01,
        2.4593e+00, 3.3353e+00, 4.3024e+00, 1.1892e+01, 4.1280e-02, 7.7497e-02,
        4.4236e-02, 4.9640e-02, 6.9976e-03, 4.8163e-02, 6.8328e-02, 1.3995e-01,
        3.6642e-02, 1.5992e-01, 6.3569e+00, 3.0665e-01, 1.4645e+00, 8.2136e-02,
        1.1171e+01, 1.1892e-01, 2.7078e+00, 7.7044e-01, 1.8152e-01, 1.6143e-01,
        2.3910e-01, 3.5658e-01, 8.4027e-01, 8.7184e-01, 5.2939e-03, 1.2491e-01,
        3.4378e+00, 1.4013e+00, 1.8948e+01, 4.7736e+00, 9.0974e+00, 2.3622e+00,
        4.7492e-02, 3.4918e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [12/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 120.3
Sum Train Loss:  tensor([6.0568e+00, 3.8686e-01, 3.8123e+00, 3.2180e-01, 2.5214e-01, 2.9739e-01,
        1.0567e-01, 4.0046e-01, 5.3414e-01, 1.7753e-01, 2.5695e-02, 7.7025e-02,
        1.2041e-02, 6.4158e-01, 3.0872e-01, 2.3480e-01, 5.2843e-01, 3.8274e-01,
        2.5564e-01, 2.9452e-01, 1.7339e-02, 3.5907e-02, 1.3538e-02, 6.1703e-02,
        4.0370e-01, 1.7416e-01, 1.0715e+00, 1.4469e-01, 5.2165e-02, 5.7072e-02,
        1.2249e-01, 3.3958e-02, 6.0710e-01, 1.5182e-02, 7.8241e-02, 3.5268e-02,
        1.2281e-01, 9.1693e-02, 5.3840e-02, 1.3285e+00, 8.3870e-01, 8.2339e+00,
        6.3769e-01, 8.6967e-01, 2.2490e+00, 6.1152e+00, 3.1162e-02, 6.0793e-02,
        6.4536e-02, 3.6005e-02, 1.8127e-03, 1.1715e-02, 9.0459e-03, 2.5246e-02,
        6.8581e-02, 1.0864e-01, 6.5744e+00, 7.5502e-01, 1.0467e+00, 2.0575e-01,
        1.2347e+01, 4.5635e-01, 2.4817e+00, 4.3006e-01, 1.6505e-01, 1.7820e-01,
        2.1423e-01, 8.0040e-01, 1.1523e+00, 3.6099e-01, 8.2205e-02, 2.4477e-01,
        7.4533e-01, 2.8030e+00, 6.7525e+00, 1.4733e+00, 5.1455e+00, 7.3494e-01,
        3.6332e-02, 6.0869e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [12/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 84.8
Sum Train Loss:  tensor([4.0634e+00, 2.5549e-01, 2.6452e+00, 2.8500e-01, 5.6119e-02, 2.6985e-01,
        1.7933e-02, 3.1085e-01, 2.7216e-01, 1.5491e-01, 9.0952e-03, 4.3638e-02,
        1.5098e-02, 4.9080e-01, 2.6641e-01, 4.7482e-01, 1.2072e+00, 1.9795e-01,
        1.4404e-01, 3.8787e-01, 1.8902e-02, 2.6479e-02, 4.2898e-02, 7.5236e-02,
        4.1452e-01, 1.1194e-01, 1.3281e+00, 1.7471e-01, 6.2215e-02, 1.0842e-01,
        3.0181e-02, 1.1112e-02, 7.6479e-01, 1.5399e-02, 3.3008e-02, 6.7699e-02,
        1.4398e-01, 7.6704e-02, 2.3722e-01, 1.4391e+00, 2.1592e+00, 9.9472e+00,
        3.4616e+00, 6.0422e+00, 9.7787e+00, 9.3456e+00, 2.4862e-02, 2.9877e-02,
        6.4571e-02, 4.2273e-02, 8.7314e-03, 5.9479e-02, 5.4006e-02, 1.2939e-01,
        3.7190e-02, 4.1485e-01, 9.3834e+00, 4.6027e-01, 1.8051e+00, 1.1289e-01,
        1.4140e+01, 1.8142e-01, 3.2329e+00, 6.8393e-01, 1.7085e-01, 4.0007e-01,
        1.9849e-01, 9.5327e-01, 2.0208e+00, 1.1368e+00, 9.5529e-02, 2.6450e-01,
        3.5497e+00, 4.9828e+00, 6.4670e+00, 1.2961e+01, 1.4330e+00, 1.0222e+01,
        1.8288e+00, 2.4291e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [12/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 135.3
Sum Train Loss:  tensor([4.6812e+00, 4.5171e-01, 2.9665e+00, 1.8203e-01, 4.9482e-02, 1.8352e-01,
        5.7694e-02, 3.2620e-01, 9.2622e-02, 1.6744e-01, 1.5918e-02, 1.4846e-02,
        2.8633e-03, 7.3820e-01, 8.1093e-01, 3.6986e-01, 9.7852e-01, 3.9438e-01,
        4.7708e-02, 3.2591e-01, 4.8317e-03, 9.3520e-03, 6.0223e-03, 3.3094e-02,
        2.2843e-01, 1.0113e-01, 1.4104e+00, 1.3934e-01, 1.6687e-02, 4.8842e-02,
        4.9525e-02, 6.3564e-02, 7.5193e-01, 4.4363e-03, 1.6461e-01, 3.5391e-01,
        6.9154e-02, 5.6147e-02, 2.3599e-01, 2.1771e+00, 1.8921e+00, 1.0549e+01,
        3.7497e+00, 7.5264e+00, 9.5539e+00, 1.3940e+01, 1.2428e-01, 7.6437e-02,
        6.7411e-02, 1.3607e-01, 4.1285e-02, 6.7502e-02, 3.7168e-02, 7.4003e-02,
        3.8640e-02, 2.2302e-01, 6.3315e+00, 4.6632e-01, 8.1942e-01, 5.7914e-02,
        1.3988e+01, 4.9115e-01, 1.5405e+00, 9.1833e-01, 1.3291e-01, 3.3029e-01,
        1.5937e-01, 8.6945e-01, 2.7331e+00, 1.0636e+00, 9.5758e-03, 3.3292e-01,
        2.9889e+00, 3.2257e+00, 2.4181e+00, 8.4875e+00, 4.0653e+00, 1.2807e+00,
        4.9199e-02, 2.7089e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [12/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 119.9
Sum_Val Meta Model:  tensor([6.2358e+00, 2.4964e+00, 1.0222e+01, 1.4758e+00, 1.6010e-02, 3.3907e-01,
        3.1198e-02, 3.6015e-01, 2.6362e-01, 1.9684e-01, 3.6425e-02, 1.1142e-01,
        3.3415e-02, 5.5296e-01, 3.6551e-01, 2.5016e-01, 2.1618e+01, 8.5839e-02,
        3.8230e-02, 4.9198e-02, 3.6874e-03, 5.2330e-03, 5.9904e-03, 6.0936e-03,
        6.5997e-01, 2.1575e-01, 2.0112e+00, 4.3362e-02, 7.7991e-02, 1.7827e-01,
        1.7930e-02, 3.7160e-03, 6.4206e-01, 1.5715e-03, 1.9870e-02, 4.1532e-02,
        3.3164e-02, 1.1306e-01, 6.8452e-02, 4.9144e+00, 2.4467e+00, 1.1674e+01,
        2.6806e+00, 6.7962e+00, 1.3197e+01, 1.1467e+01, 1.5653e-02, 8.5582e-02,
        1.2089e-02, 1.0184e-01, 1.4883e-03, 5.9092e-03, 8.6979e-02, 3.2635e-02,
        1.0359e-02, 4.6316e-02, 8.3718e+00, 3.4301e-01, 2.1594e+00, 5.9622e-02,
        1.1086e+01, 6.0085e-01, 1.3312e+00, 3.4622e-01, 4.2984e-02, 4.2160e-02,
        9.1664e-02, 4.2728e-01, 2.7368e+00, 2.6087e+00, 3.7757e-03, 4.6059e-01,
        7.0854e+00, 1.9393e+00, 1.2041e+01, 8.4190e+00, 3.6735e-01, 9.0803e+00,
        2.0520e-02, 4.1828e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.5141e+00, 1.8055e+00, 6.7085e+00, 6.8728e-01, 6.2218e-03, 3.4459e-01,
        2.4796e-02, 4.0314e-01, 2.1037e-01, 1.8880e-01, 4.0795e-02, 1.0732e-01,
        4.8269e-02, 5.2089e-01, 3.2353e-01, 7.7793e-01, 1.3408e+01, 1.5368e-01,
        2.8519e-02, 8.3970e-02, 5.6767e-03, 8.1271e-03, 2.0751e-03, 8.8156e-03,
        6.1100e-01, 2.1135e-01, 1.9051e+00, 6.0160e-02, 1.0175e-01, 1.8270e-01,
        1.3158e-02, 2.8185e-03, 5.9599e-01, 2.3509e-03, 1.5221e-02, 2.1758e-02,
        7.6658e-02, 8.1467e-02, 2.4161e-02, 4.7672e+00, 2.5873e+00, 1.4072e+01,
        2.8362e+00, 5.7661e+00, 1.4076e+01, 1.0117e+01, 9.7579e-03, 7.7985e-02,
        3.6648e-03, 7.0547e-02, 2.9231e-04, 2.3995e-03, 8.9416e-02, 5.6030e-03,
        4.1273e-03, 1.3659e-02, 8.2827e+00, 3.6306e-01, 1.8102e+00, 7.3866e-02,
        1.2650e+01, 2.0530e-01, 9.9088e-01, 3.4005e-01, 2.5106e-02, 5.1138e-02,
        6.5121e-02, 5.2674e-01, 2.4989e+00, 1.9883e+00, 5.4402e-03, 5.3720e-01,
        5.3707e+00, 1.8837e+00, 1.3475e+01, 6.9240e+00, 5.1335e-01, 1.0769e+01,
        2.8837e-02, 7.9318e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.7650e+01, 5.1575e+01, 4.9160e+01, 1.9037e+01, 4.2860e-01, 1.1968e+01,
        3.2407e+00, 2.0062e+01, 5.3380e+00, 9.8593e+00, 6.4226e+00, 1.1643e+01,
        9.4923e+00, 1.6140e+01, 7.7833e+00, 2.3097e+01, 2.7742e+02, 5.4080e+00,
        1.2255e+00, 2.2913e+00, 2.0997e+00, 1.1852e+00, 1.9501e-01, 1.1251e+00,
        2.6186e+01, 1.7114e+01, 2.8227e+01, 5.0014e+00, 1.4080e+01, 1.1858e+01,
        1.0157e+00, 3.6199e-01, 8.2594e+00, 1.3675e+00, 9.7804e-01, 6.1661e-01,
        6.2319e+00, 3.6411e+00, 6.5716e-01, 4.8896e+01, 1.1131e+01, 2.8695e+01,
        7.1405e+00, 1.2823e+01, 1.7587e+01, 1.5047e+01, 9.2489e-01, 5.4554e+00,
        2.5492e-01, 3.3953e+00, 9.4293e-02, 2.8092e-01, 7.6342e+00, 2.9883e-01,
        5.5958e-01, 5.8696e-01, 2.4962e+01, 6.1371e+00, 1.2942e+01, 6.1699e+00,
        1.6929e+01, 5.7243e+00, 4.4427e+00, 5.8288e+00, 5.0203e-01, 1.7546e+00,
        8.3135e-01, 1.0642e+01, 9.9522e+00, 1.6419e+01, 3.0596e-01, 1.7710e+01,
        1.1612e+01, 1.1322e+01, 1.6253e+01, 6.9571e+00, 5.1402e-01, 1.0848e+01,
        1.3720e-01, 9.5523e-01], device='cuda:0')
Outer loop valEpocw Maximum [12/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 172.2
model_train val_loss valEpocw [12/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 159.3
model_train val_loss valEpocw [12/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1070.1
Sum_Val Meta Model:  tensor([1.0949e+01, 9.0497e-01, 1.2471e+01, 5.2792e-01, 2.0903e-02, 2.3201e+00,
        1.9997e+00, 2.4316e+00, 2.5714e+00, 1.5523e+00, 3.5534e-01, 5.4643e+00,
        1.3461e-01, 2.9324e-01, 1.0255e+00, 1.2140e+00, 1.0301e+00, 9.8263e-01,
        4.3819e-01, 2.8306e+00, 1.3899e-03, 9.7301e-04, 1.4136e-02, 8.3107e-02,
        1.0987e+00, 6.4521e-01, 3.2252e+00, 4.6817e-01, 2.8017e-01, 2.5764e-03,
        5.7933e-03, 2.1919e-03, 2.2818e-02, 9.1701e-04, 2.3648e-03, 4.6474e-03,
        1.0647e-01, 7.7201e-03, 5.4803e-03, 2.7582e-01, 7.3181e-02, 2.1002e+00,
        9.0161e-02, 1.9632e-01, 3.4800e-01, 2.6537e+00, 1.9647e-02, 1.4066e-02,
        4.7040e-03, 2.5320e-01, 4.9346e-04, 2.6527e-03, 1.4275e-03, 3.1534e-03,
        2.9611e-03, 1.7868e-01, 4.2996e+00, 6.1767e-01, 1.4854e+00, 7.4817e-02,
        1.8111e+00, 2.8323e-02, 5.5448e-01, 1.5986e-01, 3.2303e-02, 5.1843e-02,
        6.4674e-02, 1.3373e+00, 6.8159e-02, 6.6522e-01, 2.8294e-03, 3.3759e-02,
        2.2132e+00, 1.0851e+00, 2.3566e+00, 4.7754e-01, 1.7958e-01, 6.3814e-01,
        1.2289e-02, 1.8377e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.0069e+01, 1.0744e+00, 8.3312e+00, 8.2152e-01, 1.1663e-01, 2.0088e+00,
        1.1797e+00, 1.8454e+00, 2.1540e+00, 1.2670e+00, 2.5327e-01, 1.8260e+00,
        1.2401e-01, 4.9903e-01, 1.1615e+00, 2.1334e-01, 8.1871e-01, 7.5747e-01,
        2.0897e-01, 1.3452e+00, 1.0861e-02, 1.2973e-02, 5.2624e-03, 1.0407e-01,
        9.8575e-01, 5.2739e-01, 3.1103e+00, 4.2724e-01, 2.7884e-01, 1.7819e-02,
        1.1701e-02, 3.2772e-03, 5.5874e-02, 1.5233e-02, 1.1450e-02, 8.0609e-03,
        7.5297e-02, 5.7765e-02, 4.3345e-03, 1.5241e-01, 2.6451e-02, 1.3026e+00,
        3.0112e-02, 1.2700e-01, 1.7365e-01, 7.1201e-01, 9.5865e-03, 6.9808e-03,
        3.6600e-03, 2.4293e-01, 4.1197e-04, 2.5167e-03, 3.3373e-03, 1.2369e-02,
        5.0264e-03, 8.2087e-02, 2.3728e+00, 4.1767e-01, 9.4669e-01, 2.0520e-02,
        9.7319e-01, 2.6029e-02, 1.7291e-01, 2.4294e-02, 1.0707e-02, 1.4840e-02,
        2.5019e-02, 1.2945e+00, 1.4028e-02, 4.5679e-01, 4.8908e-04, 1.9584e-02,
        1.9275e+00, 4.9480e-01, 4.3665e+00, 1.6588e-01, 1.9699e-01, 2.9522e-01,
        3.5434e-03, 1.6552e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.3492e+01, 1.8086e+01, 4.5651e+01, 1.2584e+01, 4.0105e+00, 3.4022e+01,
        4.6909e+01, 4.1582e+01, 3.0340e+01, 3.3088e+01, 1.5365e+01, 7.3179e+01,
        8.0235e+00, 1.0066e+01, 1.6072e+01, 3.1655e+00, 1.0580e+01, 1.2929e+01,
        3.2283e+00, 1.5177e+01, 1.1182e+00, 7.4013e-01, 2.0725e-01, 5.8724e+00,
        2.2880e+01, 1.7600e+01, 3.0873e+01, 1.4076e+01, 1.3119e+01, 5.1199e-01,
        3.6751e-01, 1.5679e-01, 5.0894e-01, 2.3544e+00, 3.7493e-01, 1.4935e-01,
        2.6412e+00, 1.2709e+00, 6.5644e-02, 1.3590e+00, 1.0973e-01, 2.8267e+00,
        8.4041e-02, 3.0195e-01, 2.4093e-01, 1.1559e+00, 3.6067e-01, 2.1507e-01,
        1.0190e-01, 5.4814e+00, 4.1837e-02, 1.2742e-01, 1.2812e-01, 3.0864e-01,
        2.5644e-01, 1.7476e+00, 6.7706e+00, 4.0496e+00, 5.9010e+00, 6.7197e-01,
        1.4144e+00, 4.0986e-01, 6.6899e-01, 2.4908e-01, 1.3656e-01, 2.5583e-01,
        2.2929e-01, 1.8430e+01, 5.1525e-02, 3.0363e+00, 1.2639e-02, 3.8171e-01,
        4.7187e+00, 2.3286e+00, 6.0298e+00, 1.6797e-01, 1.9782e-01, 3.0050e-01,
        1.4065e-02, 1.4811e-01], device='cuda:0')
Outer loop valEpocw Maximum [12/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 80.0
model_train val_loss valEpocw [12/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 58.9
model_train val_loss valEpocw [12/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 683.9
Sum_Val Meta Model:  tensor([1.1212e+01, 1.7738e-01, 1.8258e+00, 1.2535e-01, 2.6788e-02, 1.1736e-01,
        1.9118e-02, 3.7049e-01, 1.3913e-01, 6.8411e-02, 2.3032e-02, 1.9947e-02,
        5.0484e-03, 8.5741e-01, 1.7632e-01, 1.8090e-01, 4.4817e-01, 7.9612e-02,
        5.4844e-02, 1.2623e-01, 5.2670e-03, 1.6924e-02, 1.8049e-01, 2.6125e-02,
        2.2016e-01, 4.5587e-02, 1.7730e+00, 1.6906e-01, 2.3368e-02, 9.4646e-02,
        1.2239e-01, 1.0238e-01, 8.6170e-01, 1.4760e-03, 6.8375e-02, 1.4796e-01,
        3.8593e-01, 5.6712e-01, 2.5184e-02, 3.2108e+00, 8.0714e+00, 3.4552e+01,
        3.2650e+01, 3.6490e+01, 3.9874e+01, 4.6345e+01, 3.2222e-01, 3.2418e-01,
        1.9457e+00, 6.2038e-01, 3.2387e-01, 7.0328e-01, 1.3068e+00, 5.8540e-01,
        1.0436e+00, 3.8901e+00, 5.1251e+01, 1.3046e+00, 1.3974e+00, 7.7163e-02,
        6.8264e+01, 1.1653e-01, 2.6383e+00, 1.2097e+00, 1.0725e+00, 6.5099e-02,
        9.6247e-01, 6.7129e-01, 1.3315e+00, 3.6819e-01, 1.4737e-02, 3.0966e-01,
        2.0984e+00, 6.3609e+00, 1.6751e+00, 1.9530e+01, 9.9847e+00, 1.7723e+00,
        8.6685e-02, 1.1081e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([3.0670e+00, 3.0047e-02, 5.5208e-01, 1.7561e-02, 1.6045e-03, 5.7201e-03,
        3.0574e-03, 3.5525e-01, 1.8068e-02, 6.8098e-03, 8.2613e-04, 1.2218e-03,
        9.5925e-04, 6.5425e-01, 2.0586e-02, 5.0278e-02, 1.8127e-01, 2.3921e-02,
        9.9989e-03, 7.4888e-03, 7.8406e-04, 6.7133e-04, 5.2840e-04, 1.0713e-03,
        1.4608e-01, 2.2749e-02, 1.8096e+00, 1.3205e-01, 9.3394e-03, 7.5127e-03,
        1.3660e-02, 1.1877e-01, 7.3476e-01, 5.6544e-04, 2.0466e-02, 4.2408e-02,
        2.4526e-01, 4.7932e-01, 8.0715e-03, 4.1389e+00, 6.0357e+00, 2.6858e+01,
        3.0894e+01, 3.4474e+01, 3.3719e+01, 4.8038e+01, 1.8245e-01, 1.6494e-01,
        1.1557e+00, 3.8574e-01, 1.5757e-01, 6.9892e-01, 1.5634e+00, 5.3439e-01,
        7.1831e-01, 2.9498e+00, 3.7214e+01, 1.1785e+00, 1.4312e+00, 5.4562e-02,
        7.4341e+01, 2.0394e-02, 1.7556e+00, 8.8284e-01, 7.7720e-01, 1.7003e-01,
        8.4463e-01, 8.6593e-01, 1.5702e+00, 4.4903e-01, 5.4509e-03, 2.8527e-01,
        1.4082e+00, 6.1074e+00, 3.0598e+00, 1.7885e+01, 9.4925e+00, 6.6918e+00,
        2.7323e-02, 7.1237e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.3809e+01, 7.0438e-01, 3.7043e+00, 3.7366e-01, 8.7223e-02, 1.4715e-01,
        2.7841e-01, 1.4204e+01, 3.8668e-01, 2.6425e-01, 8.5340e-02, 9.1483e-02,
        1.2730e-01, 1.6492e+01, 4.1478e-01, 1.2096e+00, 2.9902e+00, 6.5686e-01,
        2.8094e-01, 1.4936e-01, 1.9810e-01, 6.6621e-02, 2.9558e-02, 9.5634e-02,
        4.9804e+00, 1.4565e+00, 2.6574e+01, 8.3480e+00, 9.3137e-01, 3.1447e-01,
        7.7244e-01, 8.7067e+00, 8.1700e+00, 1.6729e-01, 9.2982e-01, 9.5386e-01,
        1.2326e+01, 1.3532e+01, 1.7533e-01, 4.1131e+01, 2.4834e+01, 5.3591e+01,
        6.9863e+01, 6.9786e+01, 4.3416e+01, 6.9437e+01, 9.6121e+00, 7.1353e+00,
        3.8471e+01, 1.1771e+01, 2.3947e+01, 4.4594e+01, 7.4697e+01, 1.9229e+01,
        5.6045e+01, 7.9434e+01, 1.1428e+02, 1.5574e+01, 1.0610e+01, 2.8541e+00,
        9.9829e+01, 4.6486e-01, 7.5259e+00, 1.2421e+01, 1.3089e+01, 4.3326e+00,
        9.9313e+00, 1.7440e+01, 6.7292e+00, 3.7061e+00, 2.2228e-01, 8.1670e+00,
        3.6262e+00, 3.4322e+01, 4.1094e+00, 1.8015e+01, 9.5135e+00, 6.7663e+00,
        1.3420e-01, 7.9873e-01], device='cuda:0')
Outer loop valEpocw Maximum [12/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 405.8
model_train val_loss valEpocw [12/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 368.1
model_train val_loss valEpocw [12/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1296.6
Sum_Val Meta Model:  tensor([1.8335e+01, 1.0154e-01, 3.2271e+00, 4.5552e-02, 1.2187e-02, 5.1063e-01,
        8.1798e-02, 4.8373e-01, 8.0753e-02, 5.8666e-01, 6.1434e-02, 9.0914e-02,
        1.6737e-03, 7.6198e-01, 8.4053e-01, 7.5455e-02, 1.0889e-01, 2.9588e-02,
        1.6290e-02, 2.8602e-02, 2.6374e-03, 3.4747e-03, 1.0707e-02, 9.0330e-03,
        7.9950e-01, 3.1894e-02, 2.4966e+00, 2.9636e-02, 2.1856e-01, 8.9667e-03,
        1.3066e-02, 1.9021e-03, 2.7062e-01, 8.0990e-03, 4.4599e-02, 2.8715e-01,
        6.1496e-03, 2.4861e-02, 2.3163e-01, 2.6659e+00, 3.6833e+00, 1.3364e+01,
        3.5522e+00, 4.3044e+00, 6.6923e+00, 9.8093e+00, 1.0000e-02, 1.3571e-02,
        3.7405e-02, 1.5129e-02, 6.3478e-03, 9.3370e-03, 6.6932e-03, 1.3387e-01,
        1.1722e-02, 5.4185e-02, 6.6798e+00, 3.5326e-01, 2.3303e+00, 2.7108e-02,
        2.8289e+01, 2.8138e-02, 1.0736e+00, 3.3815e-01, 2.0759e-01, 7.8886e-02,
        3.7994e-01, 3.5792e+00, 3.3659e-01, 3.7610e-01, 3.2957e-03, 6.1370e-02,
        2.1316e+00, 1.8420e+00, 3.3643e+02, 1.2690e+01, 1.4008e+00, 1.0161e+01,
        1.3755e-02, 1.0606e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.8428e+00, 1.3136e-01, 3.3096e+00, 8.9231e-02, 4.0113e-02, 4.7268e-01,
        1.2998e-01, 4.8225e-01, 1.1730e-01, 5.8325e-01, 4.0044e-02, 1.5381e-01,
        1.2410e-02, 7.9751e-01, 9.2143e-01, 4.3654e-02, 1.0717e-01, 4.3374e-02,
        1.0420e-02, 1.3396e-02, 4.4607e-03, 1.2133e-03, 4.7688e-03, 1.0706e-02,
        7.3498e-01, 6.1551e-02, 1.9157e+00, 4.0019e-02, 2.3607e-01, 2.9531e-03,
        2.8801e-03, 2.4236e-03, 5.3765e-02, 1.5688e-03, 8.8680e-03, 1.2909e-02,
        1.2245e-02, 3.6209e-03, 1.3894e-02, 2.8445e-01, 2.1876e-01, 7.4247e-01,
        9.3062e-02, 2.3680e-01, 3.3281e-01, 8.2001e-01, 3.8522e-03, 3.0988e-03,
        1.8202e-03, 3.9433e-03, 2.0927e-04, 9.6079e-04, 9.7277e-04, 4.9396e-03,
        3.9634e-03, 3.8726e-02, 1.6294e+00, 8.5520e-02, 1.6316e+00, 2.4124e-02,
        8.0795e+00, 2.8410e-02, 4.3064e-01, 3.3011e-02, 4.1025e-02, 4.5995e-02,
        1.1578e-01, 5.1168e-01, 3.5596e-02, 4.6648e-02, 1.1727e-03, 3.3703e-02,
        1.2479e-01, 1.0491e+00, 6.9219e+01, 4.3015e+00, 1.2587e-01, 5.4649e+00,
        6.3041e-03, 8.0267e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.7483e+01, 2.7400e+00, 2.1779e+01, 1.7060e+00, 1.7694e+00, 1.0991e+01,
        9.2285e+00, 1.5780e+01, 2.1910e+00, 1.9960e+01, 3.4551e+00, 9.4690e+00,
        1.3329e+00, 1.7439e+01, 1.7161e+01, 9.3454e-01, 1.7322e+00, 1.0711e+00,
        2.7658e-01, 2.6380e-01, 7.8411e-01, 1.0052e-01, 2.9082e-01, 8.1541e-01,
        2.2130e+01, 3.1271e+00, 2.2501e+01, 1.9599e+00, 1.8483e+01, 1.2085e-01,
        1.4549e-01, 1.7558e-01, 5.2353e-01, 3.8366e-01, 3.5615e-01, 2.2853e-01,
        6.2440e-01, 1.0841e-01, 2.3982e-01, 2.4724e+00, 7.7629e-01, 1.4965e+00,
        2.2722e-01, 5.2793e-01, 4.3858e-01, 1.2771e+00, 2.1434e-01, 1.4255e-01,
        7.5257e-02, 1.3312e-01, 3.5226e-02, 7.0133e-02, 5.3263e-02, 1.5701e-01,
        3.0848e-01, 1.1200e+00, 4.7446e+00, 1.0605e+00, 1.0259e+01, 1.2406e+00,
        1.1120e+01, 5.4904e-01, 1.6823e+00, 3.8951e-01, 5.3508e-01, 1.0541e+00,
        1.0565e+00, 6.5809e+00, 1.3710e-01, 3.3913e-01, 4.0459e-02, 8.0720e-01,
        2.8208e-01, 5.3265e+00, 9.4389e+01, 4.3368e+00, 1.2625e-01, 5.5352e+00,
        3.4239e-02, 7.0179e-02], device='cuda:0')
Outer loop valEpocw Maximum [12/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 483.3
model_train val_loss valEpocw [12/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 112.3
model_train val_loss valEpocw [12/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 411.1
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [86.90927255 97.31484753 93.00812612 98.0494877  98.51000841 97.52926987
 98.19081152 95.19620862 98.01659337 96.87991131 98.56361399 98.93519816
 99.43714136 95.37651832 97.43911502 97.17230541 96.36091178 97.64257258
 98.84017008 98.39061415 98.53559289 99.26657814 99.5260779  98.8852475
 95.22788465 96.74833396 94.42501919 96.91889719 98.0360863  98.19568475
 98.31629732 98.57214215 97.02610836 98.52706473 98.67082516 98.843825
 97.44520656 98.28218467 98.92667    93.33341455 97.96664271 93.83657607
 97.37698127 96.68010867 97.40256576 95.46301824 98.15060733 98.64645899
 98.10187498 98.69762795 98.74757861 98.60016325 99.00342345 98.3138607
 98.71955751 97.53536141 92.04688052 96.85798175 96.54243979 97.51099524
 94.60167396 98.65133222 97.25271378 97.27707996 98.79021942 97.53048818
 98.5782337  95.9588699  98.73905045 98.2724382  99.81603538 97.36236157
 98.67448009 95.94912343 97.68399508 98.17619181 99.27997953 98.90961367
 99.84405648 99.16180358]
Accuracy th:0.7 is [86.32813928 97.2405307  92.60973916 98.02877645 98.40036062 97.32581231
 98.17741012 95.14382135 97.93131175 96.88600285 98.53802951 98.87915596
 99.42739489 95.3326592  97.36967142 96.94691829 96.35847516 97.55850928
 98.78900111 98.33335364 98.55508583 99.28485277 99.51389481 98.75367016
 95.22666634 96.67158051 94.28004045 96.85310851 98.01659337 98.16279041
 98.17862843 98.57457877 96.89209439 98.3552832  98.59650833 98.70981104
 97.20032651 98.11527637 98.75245185 93.08487957 97.91791036 93.10680913
 97.12844629 96.45472156 97.33068554 95.50931397 98.10796652 98.63183928
 98.01415675 98.67326178 98.65498715 98.59894494 98.99855021 98.1055299
 98.71468428 97.51343185 91.30858542 96.51929192 96.43401031 97.44276995
 93.674541   98.50026194 97.01148865 97.14793923 98.76341663 97.45860796
 98.59407171 95.95521497 98.68422656 98.07507218 99.81603538 97.23687577
 98.480769   95.69327859 97.4634812  97.9240019  99.26048659 98.85478978
 99.84405648 99.15083881]
Avg Prec: is [96.05982478 31.55008305 70.0535658  65.16114617 76.18414066 62.89876326
 70.85559101 46.9972837  54.88676341 50.39891823 26.83228312 52.43218065
 21.35779431 26.26055312 30.71288004 51.69127838 26.81576326 37.05797166
 44.21519477 35.5342625  55.79137678 45.70619214 88.66166591 79.27405498
 24.79902428 29.53750131 36.70675031 34.81655259 21.44984594 34.66891931
 70.3669119  34.7485161  56.93060927 60.4008558  71.18093764 77.91696205
 54.40504971 74.15719648 86.20941454 45.74139738 36.75740002 61.99572427
 56.22960435 50.82936919 53.47003886 65.58157362 32.48910699 30.08210919
 39.44661644 40.85169708 57.42997566 32.67001439 17.68555655 69.88957395
 21.44977981 34.62374629 65.02562367 55.21281886 38.07188254 54.9488573
 80.17484187 80.87535908 65.02355957 46.0556857  56.39070306 38.41962312
 56.02682782 22.74897152 48.69069984 65.51263988  8.87597814 70.76734209
 70.05476056 46.37493342 70.2726088  74.15274323 38.17917895 69.22335265
  5.78813202 21.61276113]
Accuracy th:0.5 is [45.57936672 97.2137279  72.65871517 97.02489005 97.26733349 77.57946419
 77.91449909 76.82654938 78.80630109 96.432792   79.17057541 98.52097319
 99.41399349 80.39984893 78.68568853 96.56680596 96.29512311 78.3896395
 98.65376884 98.30776915 80.61670789 79.67617354 98.38695922 78.53339993
 80.94199632 96.65086926 94.0778012  78.1106468  98.01293844 78.98295586
 97.30875598 98.57457877 96.36213009 98.02024829 87.51233538 78.64548434
 78.42984369 90.89070552 97.11504489 75.61920542 79.66764537 92.05906361
 77.89866108 77.47225302 96.9627563  93.87434364 98.02877645 98.57336046
 95.77125035 88.54180626 87.31375105 98.55508583 98.99976852 78.04851305
 98.70615611 78.40425921 73.03639088 93.45402712 96.24273583 96.9067141
 89.79300934 97.17717864 92.1419086  78.42253384 98.42838172 78.90132917
 98.20786784 77.52220368 79.49464553 97.55972759 80.09649005 95.99054592
 79.21565283 95.45083515 77.74759079 82.89738185 86.78378675 79.07432902
 80.15375056 99.14718388]
Accuracy th:0.7 is [45.55865547 97.2137279  72.65871517 97.02489005 97.26733349 77.57946419
 77.91449909 76.97152812 78.80630109 96.47299619 79.17057541 98.52097319
 99.41399349 80.82503868 78.68568853 96.56680596 96.29512311 78.3896395
 98.65376884 98.30776915 81.05895396 79.67617354 98.38695922 78.67228713
 81.4549043  96.65086926 94.0778012  78.1106468  98.01293844 78.98295586
 97.30875598 98.57457877 96.36213009 98.02024829 87.68046198 78.64548434
 78.42984369 91.16360668 97.11504489 75.61920542 80.32187717 92.05906361
 77.89866108 77.47225302 96.9627563  93.87434364 98.02877645 98.57336046
 96.8933127  89.1460874  87.48918751 98.55508583 98.99976852 78.04851305
 98.70615611 78.40425921 73.03639088 93.88896334 96.24273583 96.9067141
 89.79300934 97.17717864 92.31490844 78.42253384 98.42838172 78.90132917
 98.20786784 77.52220368 79.49464553 97.55972759 80.09649005 95.99054592
 79.25707533 95.45083515 77.74759079 82.98144516 86.92023733 79.07432902
 80.15375056 99.14718388]
Avg Prec: is [55.66171411  3.09904311 11.09039995  3.27864849  2.31135984  3.72006539
  3.3107831   5.64251332  2.43350595  3.72062736  1.52290503  1.56712463
  0.5893288   5.1152241   2.58995066  3.19639566  3.6626546   2.7322264
  1.33958675  1.80804992  1.94273686  0.8611414   1.83160478  2.49958269
  5.08782566  3.56385509  6.44536722  3.29602558  2.07767471  1.97425147
  2.60906033  1.31447137  3.77113738  1.66216349  2.36521712  2.36508677
  3.07392234  2.60642347  2.8373241   7.51386089  2.26140779  8.20799003
  3.34821576  4.00934845  3.22240871  6.50170761  2.01675619  1.55945334
  2.06766571  1.56312002  1.88393751  1.63568839  1.17464632  2.98842738
  1.27692175  2.62948762 11.18985102  3.69165434  3.96988696  2.81980948
 10.90556699  2.18635329  3.81903965  3.01876264  1.59528966  2.48673736
  1.84374262  4.13419939  1.35362431  2.50864974  0.19246448  3.44960214
  1.92593737  4.54088285  3.89567659  3.18101959  0.78867925  1.83927805
  0.12202299  0.75518253]
mAP score regular 50.14, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [87.67222264 97.34658794 93.1634153  98.19119516 98.93863517 97.70535914
 98.30580263 95.26870469 98.11146822 96.84331166 98.61225303 98.98597304
 99.34474425 95.19894362 97.42133194 97.1771682  96.18307298 97.68791888
 98.92617784 98.40795276 98.66457383 99.31484665 99.63624586 99.14293545
 95.38829509 96.69631512 94.4814012  97.12733886 97.86979595 98.24351596
 98.6595909  98.67204823 96.89314099 98.72187757 98.92617784 99.13795251
 97.89471062 98.15631462 99.08812318 93.32785211 97.93457408 93.47734011
 97.30174154 96.58669058 96.81590552 94.67324414 98.26593916 98.76672397
 98.05416449 98.7268605  98.83648504 98.57986397 98.88133144 98.36808929
 98.7492837  97.71034208 91.21010539 97.08996686 96.36744151 97.51600767
 93.09365423 98.75177517 97.24692927 97.3490794  98.7268605  97.55836261
 98.5624237  95.75204923 98.7642325  98.23355009 99.81563146 97.44873807
 98.43037596 95.90153723 97.36651967 97.60570048 99.21269651 98.6894885
 99.82559733 99.14542691]
Accuracy th:0.7 is [87.75942397 97.28679273 92.9690809  98.27092209 98.81157037 97.45870394
 98.34566609 95.37832922 98.05416449 96.94795326 98.55993223 98.96354984
 99.37464185 95.13665695 97.3565538  96.89064953 96.33754391 97.64805541
 98.92119491 98.35812343 98.78914717 99.32232105 99.61133119 99.03829384
 95.45058176 96.59167352 94.48389267 97.06256073 97.82245808 98.15133169
 98.54498343 98.67204823 96.95542766 98.58982983 98.83150211 99.02832798
 97.70037621 98.02924982 98.99593891 93.15843237 97.89221915 93.1634153
 97.22699753 96.56177592 97.07003513 94.89000174 98.27839649 98.81157037
 97.97942048 98.71689464 98.76672397 98.5997957  98.87634851 98.22109276
 98.71440317 97.66051274 90.90614645 96.90061539 96.38737325 97.46866981
 92.83205023 98.7567581  97.042629   97.14976206 98.71440317 97.56085407
 98.6072701  95.77447243 98.7268605  98.01180955 99.81563146 97.53593941
 98.37058076 95.7022199  97.20955727 97.61566634 99.27000025 98.67703117
 99.82559733 99.15040985]
Avg Prec: is [96.41999913 31.08494846 70.53760021 72.04756575 76.60878704 64.98124216
 77.32434524 48.10691819 60.83045083 53.25013265 33.96182407 54.95387291
 21.71445363 29.5095192  33.01125653 58.87964296 29.8957931  40.22124705
 45.9787459  34.20852454 64.98011289 53.96248846 92.07246904 86.64445374
 24.93624998 33.45449586 33.68551051 41.22951551 26.1138915  37.00635788
 75.97949502 35.77278582 55.35397135 64.23919288 73.68885177 82.13495918
 58.58129136 77.04533473 89.95811184 45.04169222 34.76815885 52.71683614
 46.48827609 40.6050733  34.2156631  53.78858168 31.92438631 26.52983465
 40.74139255 39.54539888 64.86586472 32.34878197 20.68350557 74.3072503
 25.33273579 35.67884656 57.53105851 55.57576623 36.07283751 58.90286722
 67.24959598 86.30888318 64.97423084 49.63876469 59.88836139 36.63209347
 59.80682543 23.68817305 41.16687363 64.65804152  8.97841202 73.7397837
 54.95777613 43.34381811 64.16052812 53.93507534 15.58069552 51.5921029
  2.69579653 19.00062111]
Accuracy th:0.5 is [45.29984802 97.22450607 71.11642624 96.96290206 97.90716795 76.66741411
 76.76956424 75.56369435 78.20215761 96.41976231 78.39649201 98.5325261
 99.34972718 78.54348855 78.27441015 96.31262924 96.21047911 77.74123627
 98.78167277 98.34068316 79.41799337 79.03181603 98.31327703 77.9181304
 78.49116775 96.52938685 94.3393876  77.75867653 97.81747515 78.35912001
 97.52597354 98.67204823 96.39983058 98.18870369 88.47198346 77.96546827
 77.9181304  92.14689688 97.0276802  74.9831826  78.21959788 92.37362035
 77.09843785 76.75710691 97.03764606 94.02795426 98.18621222 98.77668984
 96.52689538 88.26519172 85.85096046 98.55993223 98.87385704 77.10342078
 98.6969629  77.77113387 71.58980492 94.3767596  96.16314124 96.78102499
 90.13379176 97.04761193 91.98495154 77.7088472  98.32075143 78.55843735
 98.13139996 76.88417171 79.05423923 97.53593941 79.51266911 96.07843137
 78.76522909 95.44559882 76.81191918 83.78802601 88.56915066 78.41642375
 79.58243018 99.15040985]
Accuracy th:0.7 is [45.52158856 97.22450607 71.11642624 96.96290206 97.90716795 76.66741411
 76.76956424 75.63096395 78.20215761 96.41976231 78.39649201 98.5325261
 99.34972718 78.94461469 78.27441015 96.31262924 96.21047911 77.74123627
 98.78167277 98.34068316 79.78673045 79.03181603 98.31327703 77.948028
 78.85492189 96.52938685 94.3393876  77.75867653 97.81747515 78.35912001
 97.52597354 98.67204823 96.39983058 98.18870369 88.63642026 77.96546827
 77.9181304  92.34621422 97.0276802  74.9831826  78.66307895 92.37362035
 77.09843785 76.75710691 97.03764606 94.02795426 98.18621222 98.77668984
 97.28679273 88.52928719 86.03034606 98.55993223 98.87385704 77.10342078
 98.6969629  77.77113387 71.58980492 94.63338067 96.16314124 96.78102499
 90.13379176 97.04761193 92.10454194 77.7088472  98.32075143 78.55843735
 98.13139996 76.88417171 79.05423923 97.53593941 79.51266911 96.07843137
 78.77270349 95.44559882 76.81191918 83.86027855 88.71365573 78.41642375
 79.58243018 99.15040985]
Avg Prec: is [53.77916705  3.70178181 14.87080218  4.53974299  1.47836537  4.25213361
 14.13003727  8.68211912  8.19703546  5.27089809  2.3880041   4.71154629
  2.35612202  5.84538253  2.99821833  3.67949612 23.95077564  6.47827792
  1.56333598  2.7668475   3.50982131  1.52038235  1.13584823  5.15998702
  5.60960251  8.85314251  7.81944775  4.53432795  3.86728461  5.45291197
  2.22198915  0.85154263  3.01674205  1.10831334  1.66220332  2.20637558
  1.98964859  2.28573591  2.24286562  6.17917195  1.72428914  6.00792473
  2.16810261  2.69697511  2.3746484   4.8379879   1.69091795  1.02381699
  1.35836594  1.15822281  1.19275971  0.97289707  0.73442155  2.30738959
  0.86140765  1.84964216  9.97284589  2.87687356  3.7898166   2.74802363
  7.80754468  2.05354208  3.10486233  2.49696763  1.33909998  1.82413703
  1.52838153  3.43078795  1.09095355  2.26258441  0.19195085  3.2547195
  1.59148062  3.89302743  3.54542878  2.31238849  0.59665588  1.50229732
  0.12836038  0.58909599]
mAP score regular 49.88, mAP score EMA 4.30
Train_data_mAP: current_mAP = 50.14, highest_mAP = 50.14
Val_data_mAP: current_mAP = 49.88, highest_mAP = 49.88
tensor([0.1180, 0.0366, 0.1367, 0.0394, 0.0150, 0.0312, 0.0084, 0.0210, 0.0413,
        0.0207, 0.0069, 0.0103, 0.0056, 0.0338, 0.0420, 0.0358, 0.0497, 0.0284,
        0.0281, 0.0403, 0.0030, 0.0075, 0.0106, 0.0085, 0.0248, 0.0133, 0.0682,
        0.0129, 0.0078, 0.0165, 0.0133, 0.0086, 0.0721, 0.0019, 0.0149, 0.0354,
        0.0124, 0.0229, 0.0356, 0.0938, 0.2431, 0.4857, 0.4339, 0.4682, 0.8138,
        0.6829, 0.0118, 0.0152, 0.0161, 0.0216, 0.0035, 0.0097, 0.0126, 0.0186,
        0.0079, 0.0253, 0.3246, 0.0599, 0.1411, 0.0128, 0.7488, 0.0368, 0.2295,
        0.0615, 0.0549, 0.0296, 0.0849, 0.0493, 0.2912, 0.1288, 0.0200, 0.0300,
        0.4662, 0.1798, 0.8256, 0.9957, 0.9990, 0.9933, 0.2788, 0.0935],
       device='cuda:0')
Sum Train Loss:  tensor([3.4682e+00, 5.2821e-01, 4.5581e+00, 2.3520e-01, 9.2570e-02, 2.6259e-01,
        3.9227e-02, 3.4184e-01, 4.9529e-01, 2.7618e-01, 5.1064e-02, 2.0572e-02,
        5.3014e-02, 6.8860e-01, 1.1533e+00, 3.7909e-01, 7.6908e-01, 2.1442e-01,
        7.2230e-02, 7.6114e-02, 1.0027e-02, 1.0757e-01, 9.8878e-03, 1.8041e-02,
        5.6358e-01, 3.7394e-01, 1.6233e+00, 2.4533e-01, 6.9167e-02, 1.7893e-01,
        1.3250e-01, 6.1008e-02, 9.1446e-01, 1.2116e-02, 9.8446e-02, 1.2913e-01,
        1.0882e-01, 2.2308e-01, 5.3817e-02, 4.1952e+00, 2.3804e+00, 1.2035e+01,
        2.9110e+00, 8.2237e+00, 4.5276e+00, 1.0022e+01, 2.2039e-02, 3.4309e-02,
        1.1633e-01, 2.1943e-02, 1.6632e-02, 6.0848e-02, 3.7466e-02, 2.2932e-01,
        1.2102e-02, 1.6823e-01, 8.1738e+00, 5.8716e-01, 1.1833e+00, 1.4238e-01,
        1.3384e+01, 7.1712e-02, 2.2930e+00, 2.9773e-01, 2.6781e-01, 3.4117e-01,
        5.6903e-01, 1.3316e+00, 2.8604e-01, 5.0141e-01, 2.9132e-03, 1.2947e-01,
        1.9669e+00, 3.3253e+00, 7.6137e+00, 2.7401e+00, 7.4366e+00, 2.7517e+00,
        3.4501e-02, 7.3971e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [13/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 119.2
Sum Train Loss:  tensor([4.1419e+00, 3.4492e-01, 3.5278e+00, 4.5832e-01, 1.0152e-01, 2.7460e-01,
        9.8745e-02, 3.4470e-01, 7.4433e-01, 2.4022e-01, 6.5753e-02, 4.0501e-02,
        4.0292e-02, 1.5003e+00, 6.6947e-01, 4.3445e-01, 7.5564e-01, 2.3905e-01,
        4.2083e-02, 2.9864e-01, 6.1072e-03, 2.4880e-02, 3.6792e-03, 1.3404e-02,
        5.5640e-01, 1.7562e-01, 1.8041e+00, 1.0180e-01, 5.0097e-02, 1.7351e-01,
        8.6139e-02, 4.9490e-02, 5.8267e-01, 5.3166e-03, 1.4047e-02, 4.5131e-02,
        1.9395e-01, 1.9179e-01, 1.8425e-01, 1.6354e+00, 6.6572e-01, 1.0216e+01,
        2.5865e+00, 6.0900e+00, 5.9178e+00, 5.9834e+00, 1.4812e-01, 5.5183e-02,
        6.4738e-02, 3.7188e-02, 1.4030e-02, 2.8893e-02, 5.2488e-02, 1.7028e-01,
        8.8972e-02, 1.0615e-01, 7.8389e+00, 5.5940e-01, 1.2200e+00, 3.8720e-02,
        1.2003e+01, 1.8448e-01, 1.5873e+00, 8.9853e-01, 2.2388e-01, 4.1227e-01,
        2.7649e-01, 1.0565e+00, 1.0173e+00, 8.9373e-01, 6.0835e-02, 3.2411e-01,
        1.2523e+00, 1.6093e+00, 1.4366e+01, 1.0274e+01, 6.2096e-01, 2.8218e+00,
        1.6772e+00, 3.4240e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [13/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 114.0
Sum Train Loss:  tensor([4.2102e+00, 3.3659e-01, 3.4796e+00, 4.7118e-01, 2.7332e-01, 4.7827e-01,
        3.4747e-02, 3.0742e-01, 1.8213e-01, 1.8821e-01, 7.0478e-02, 1.8784e-02,
        9.3695e-03, 4.1114e-01, 1.2540e-01, 5.0861e-01, 7.5027e-01, 1.5343e-01,
        2.1468e-01, 3.5664e-01, 1.5350e-02, 1.9262e-02, 3.5335e-02, 3.4060e-02,
        5.9414e-01, 2.2387e-01, 1.9264e+00, 1.5039e-01, 1.0833e-01, 9.6569e-02,
        7.1818e-02, 7.9398e-02, 7.5400e-01, 4.3476e-03, 9.5367e-02, 4.2906e-02,
        1.2077e-01, 2.1972e-01, 1.8328e-01, 2.5362e+00, 1.1642e+00, 1.1480e+01,
        2.8417e+00, 3.0746e+00, 2.7452e+00, 7.0837e+00, 3.0851e-02, 3.6588e-02,
        8.2928e-02, 2.5356e-02, 7.1306e-03, 4.4282e-02, 7.3309e-02, 8.2489e-02,
        4.6578e-02, 1.3627e-01, 8.5106e+00, 1.1361e+00, 2.0670e+00, 5.0463e-02,
        7.9076e+00, 8.3330e-02, 2.7554e+00, 1.0720e+00, 2.5550e-01, 3.4341e-01,
        6.5047e-01, 9.4309e-01, 1.8338e+00, 1.0867e+00, 2.1762e-03, 4.3039e-01,
        9.2966e-01, 2.4190e+00, 1.2782e+01, 1.5714e+01, 6.6367e-01, 3.7023e+00,
        5.8329e-02, 6.3700e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [13/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 114.9
Sum Train Loss:  tensor([4.5753e+00, 4.8215e-01, 3.8594e+00, 4.0393e-01, 1.1018e-01, 2.8749e-01,
        6.6555e-02, 2.2369e-01, 2.1227e-01, 3.6396e-01, 4.8045e-02, 5.4164e-02,
        2.5860e-03, 7.6761e-01, 2.3102e-01, 5.6858e-01, 9.8757e-01, 3.5404e-01,
        1.0960e-01, 2.9383e-01, 2.8695e-02, 4.2902e-03, 3.8549e-02, 3.7926e-02,
        4.8532e-01, 2.6851e-01, 1.9448e+00, 1.3363e-01, 9.6857e-02, 1.4196e-01,
        8.7389e-02, 1.3710e-02, 8.1592e-01, 1.9036e-02, 3.3194e-02, 6.6815e-02,
        1.0029e-01, 1.2833e-01, 4.4316e-02, 2.0988e+00, 2.2682e+00, 1.1120e+01,
        3.5199e+00, 5.7732e+00, 5.1686e+00, 9.3774e+00, 1.3539e-01, 1.6946e-01,
        4.0192e-02, 2.9404e-01, 1.0447e-02, 5.2280e-02, 2.2109e-02, 8.0938e-02,
        5.8776e-02, 1.3360e-01, 8.3503e+00, 4.7183e-01, 9.3738e-01, 1.6245e-01,
        1.3131e+01, 2.6004e-01, 1.5408e+00, 5.7320e-01, 3.5100e-01, 5.2977e-01,
        1.2393e+00, 1.0111e+00, 2.0673e+00, 5.2981e-01, 4.7942e-03, 2.3606e-01,
        1.6039e+00, 2.5641e+00, 7.3608e+00, 1.6288e+01, 7.2414e+00, 6.7112e+00,
        5.6549e-02, 5.1945e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [13/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 132.6
Sum Train Loss:  tensor([5.3647e+00, 4.7475e-01, 3.2775e+00, 3.6197e-01, 1.0298e-01, 6.2397e-01,
        1.0328e-01, 2.5549e-01, 1.4970e-01, 2.6619e-01, 3.9243e-02, 1.5075e-01,
        3.8277e-02, 5.3550e-01, 5.8645e-01, 5.5071e-01, 6.8701e-01, 3.4248e-01,
        7.4642e-02, 2.5565e-01, 9.2514e-03, 1.0511e-02, 7.5828e-03, 1.7112e-02,
        7.0482e-01, 1.8582e-01, 9.6530e-01, 1.9573e-01, 4.1598e-02, 2.5149e-01,
        5.0561e-02, 3.2216e-02, 3.1559e-01, 6.6939e-03, 1.9687e-01, 2.5045e-01,
        1.4486e-01, 5.3243e-02, 2.1575e-01, 3.3589e+00, 5.6013e+00, 8.2174e+00,
        3.6276e+00, 9.2220e+00, 1.0027e+01, 9.8534e+00, 6.4807e-02, 2.2760e-01,
        7.1184e-02, 1.7324e-01, 3.3989e-02, 3.6468e-02, 5.1132e-02, 2.9252e-01,
        4.3811e-02, 1.0938e-01, 6.2920e+00, 6.4099e-01, 3.7649e+00, 4.1655e-02,
        1.4921e+01, 2.6356e-01, 1.9912e+00, 4.2370e-01, 8.7828e-02, 5.0896e-01,
        1.8410e-01, 8.7738e-01, 1.9326e+00, 8.7355e-01, 2.3635e-02, 3.2702e-01,
        4.1140e+00, 3.8100e+00, 1.5800e+01, 6.9607e+00, 4.9784e-01, 1.4620e+00,
        3.2682e-02, 7.0591e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [13/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 134.8
Sum Train Loss:  tensor([5.1280e+00, 3.1248e-01, 2.9021e+00, 3.4612e-01, 5.6216e-02, 1.9608e-01,
        2.6951e-02, 2.4859e-01, 5.6530e-01, 1.8673e-01, 1.1008e-01, 1.5097e-01,
        4.4240e-03, 6.2618e-01, 3.3657e-01, 3.4452e-01, 2.4637e-01, 2.6700e-01,
        2.5175e-01, 1.3491e-01, 3.7684e-02, 1.7299e-02, 5.5075e-03, 2.8255e-02,
        3.1990e-01, 1.2815e-01, 1.6078e+00, 1.8773e-01, 1.6673e-01, 2.2032e-02,
        5.8615e-02, 2.6410e-02, 5.6587e-01, 5.1146e-03, 2.3834e-02, 4.0848e-02,
        6.8199e-02, 1.3935e-01, 7.7964e-02, 1.2637e+00, 8.1095e-01, 7.9949e+00,
        3.2055e+00, 4.9576e+00, 4.1234e+00, 5.0184e+00, 1.1899e-01, 9.2465e-02,
        8.6724e-02, 2.7040e-02, 1.9264e-02, 8.5864e-02, 6.0583e-03, 1.5191e-01,
        5.9621e-02, 1.1447e-01, 5.1662e+00, 3.8807e-01, 1.0564e+00, 1.7930e-01,
        1.6460e+01, 1.8243e-01, 2.8985e+00, 6.9506e-01, 3.9304e-01, 3.5717e-01,
        5.5012e-01, 9.5578e-01, 1.3255e+00, 7.8857e-01, 2.0846e-03, 3.6686e-01,
        2.2226e+00, 2.2326e+00, 7.0883e+00, 1.4137e+00, 1.0017e+00, 7.1606e+00,
        1.9009e-02, 4.4337e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [13/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 97.5
Sum Train Loss:  tensor([5.0147e+00, 5.3436e-01, 3.0981e+00, 3.3820e-01, 1.7316e-01, 1.6628e-01,
        8.5466e-02, 2.4904e-01, 4.0088e-01, 1.8404e-01, 7.3511e-03, 1.1327e-02,
        2.7548e-02, 9.1012e-01, 5.7342e-01, 3.7690e-01, 1.3582e+00, 6.2150e-01,
        1.7446e-01, 4.0337e-01, 2.2878e-02, 4.8822e-02, 4.2220e-02, 8.0298e-03,
        5.8248e-01, 6.9693e-02, 9.5607e-01, 1.8473e-01, 8.7577e-02, 2.0463e-01,
        1.6819e-01, 1.0833e-02, 9.2253e-01, 9.1080e-03, 1.3948e-02, 3.8718e-02,
        6.3294e-02, 8.9409e-02, 1.9100e-01, 1.6475e+00, 2.2646e+00, 1.0829e+01,
        2.0616e+00, 3.3324e+00, 2.4151e+00, 1.3662e+01, 2.6904e-02, 1.3836e-01,
        5.2497e-02, 9.1797e-02, 1.3616e-02, 4.1735e-02, 1.0958e-01, 1.8458e-01,
        2.5531e-02, 3.7331e-01, 7.9560e+00, 9.7992e-01, 1.3707e+00, 1.0754e-01,
        1.7417e+01, 1.4743e-01, 2.3866e+00, 5.1414e-01, 8.7825e-02, 5.0693e-01,
        1.8494e-01, 7.4313e-01, 7.1674e-01, 7.8267e-01, 5.5511e-02, 1.5493e-01,
        9.2214e-01, 3.2142e+00, 4.1277e+00, 3.9850e+00, 7.5604e+00, 4.0762e+00,
        5.7468e-02, 4.2328e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [13/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 114.2
Sum_Val Meta Model:  tensor([6.2619e+00, 2.5131e+00, 1.0908e+01, 1.6444e+00, 2.2865e-02, 3.9372e-01,
        3.3280e-02, 3.8964e-01, 2.4061e-01, 1.9821e-01, 4.4147e-02, 1.1871e-01,
        4.1017e-02, 5.1621e-01, 3.2927e-01, 2.8725e-01, 2.3319e+01, 4.1354e-02,
        5.0210e-02, 5.0181e-02, 2.4236e-03, 2.3890e-03, 6.6700e-03, 1.3288e-02,
        7.6674e-01, 2.4282e-01, 2.0411e+00, 5.3682e-02, 8.1834e-02, 1.9192e-01,
        1.3401e-02, 6.2787e-03, 5.0862e-01, 1.9540e-03, 1.8902e-02, 3.2662e-02,
        4.4294e-02, 9.6840e-02, 8.3694e-02, 4.4321e+00, 2.6565e+00, 1.0604e+01,
        2.6691e+00, 6.7963e+00, 1.2875e+01, 1.1446e+01, 3.4358e-02, 7.5950e-02,
        2.7829e-02, 1.1201e-01, 1.4796e-03, 9.4468e-03, 7.8930e-02, 3.1070e-02,
        1.5316e-02, 4.4972e-02, 8.7069e+00, 3.3858e-01, 2.1319e+00, 5.5027e-02,
        8.8667e+00, 5.0461e-01, 1.4952e+00, 3.8654e-01, 3.7745e-02, 5.3118e-02,
        8.1472e-02, 4.0400e-01, 2.9083e+00, 2.1537e+00, 2.8291e-03, 4.8495e-01,
        6.2246e+00, 1.9467e+00, 1.4307e+01, 8.7324e+00, 3.4723e-01, 8.9709e+00,
        2.9182e-02, 5.7975e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.9537e+00, 1.7482e+00, 7.4531e+00, 8.5107e-01, 6.0190e-03, 4.0903e-01,
        3.5085e-02, 3.6815e-01, 2.1156e-01, 1.9062e-01, 5.3422e-02, 1.2149e-01,
        5.5142e-02, 5.0275e-01, 2.9641e-01, 9.8938e-01, 1.3521e+01, 8.1624e-02,
        4.4285e-02, 1.0473e-01, 2.4462e-03, 2.9897e-03, 2.1623e-03, 2.5302e-02,
        6.8233e-01, 2.1448e-01, 1.7965e+00, 6.5325e-02, 1.0885e-01, 2.1554e-01,
        3.9666e-03, 3.1810e-03, 5.2538e-01, 2.6780e-03, 1.6439e-02, 2.7943e-02,
        8.5688e-02, 6.3954e-02, 2.4832e-02, 3.8888e+00, 2.4705e+00, 1.1482e+01,
        2.1925e+00, 5.3373e+00, 1.3633e+01, 1.0403e+01, 3.3049e-02, 7.6516e-02,
        1.8598e-02, 8.6370e-02, 7.2702e-04, 7.5518e-03, 7.7553e-02, 1.2657e-02,
        1.1878e-02, 2.5096e-02, 8.8932e+00, 3.7640e-01, 1.8371e+00, 6.9986e-02,
        1.0105e+01, 2.5226e-01, 1.1057e+00, 4.6437e-01, 2.4671e-02, 6.4781e-02,
        6.1387e-02, 4.8826e-01, 2.9832e+00, 1.6433e+00, 5.1480e-03, 4.9070e-01,
        4.9466e+00, 1.8238e+00, 1.3233e+01, 1.0777e+01, 3.3143e-01, 9.5944e+00,
        3.9447e-02, 1.0409e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.0438e+01, 4.7822e+01, 5.4518e+01, 2.1604e+01, 4.0195e-01, 1.3120e+01,
        4.2017e+00, 1.7548e+01, 5.1224e+00, 9.2117e+00, 7.6978e+00, 1.1747e+01,
        9.8413e+00, 1.4877e+01, 7.0616e+00, 2.7659e+01, 2.7191e+02, 2.8770e+00,
        1.5750e+00, 2.6002e+00, 8.0972e-01, 3.9913e-01, 2.0460e-01, 2.9650e+00,
        2.7520e+01, 1.6086e+01, 2.6352e+01, 5.0804e+00, 1.3986e+01, 1.3036e+01,
        2.9770e-01, 3.7076e-01, 7.2900e+00, 1.4098e+00, 1.1051e+00, 7.8984e-01,
        6.9180e+00, 2.7881e+00, 6.9835e-01, 4.1469e+01, 1.0164e+01, 2.3639e+01,
        5.0530e+00, 1.1399e+01, 1.6751e+01, 1.5233e+01, 2.8013e+00, 5.0474e+00,
        1.1575e+00, 3.9968e+00, 2.0863e-01, 7.8011e-01, 6.1388e+00, 6.7905e-01,
        1.5073e+00, 9.9040e-01, 2.7399e+01, 6.2873e+00, 1.3022e+01, 5.4884e+00,
        1.3494e+01, 6.8580e+00, 4.8178e+00, 7.5550e+00, 4.4967e-01, 2.1915e+00,
        7.2277e-01, 9.9126e+00, 1.0243e+01, 1.2759e+01, 2.5689e-01, 1.6363e+01,
        1.0611e+01, 1.0145e+01, 1.6029e+01, 1.0824e+01, 3.3174e-01, 9.6588e+00,
        1.4148e-01, 1.1132e+00], device='cuda:0')
Outer loop valEpocw Maximum [13/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 172.8
model_train val_loss valEpocw [13/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 156.3
model_train val_loss valEpocw [13/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1053.6
Sum_Val Meta Model:  tensor([1.0373e+01, 8.6197e-01, 1.1429e+01, 4.6395e-01, 1.7078e-02, 2.0291e+00,
        1.7832e+00, 2.2854e+00, 2.2938e+00, 1.3308e+00, 2.7187e-01, 4.9769e+00,
        1.1565e-01, 2.7038e-01, 8.9407e-01, 1.1153e+00, 8.3424e-01, 7.2722e-01,
        3.6093e-01, 2.6175e+00, 1.2007e-03, 1.0128e-03, 5.7227e-03, 8.3955e-02,
        9.3583e-01, 5.5835e-01, 2.7891e+00, 4.0486e-01, 2.1551e-01, 2.6533e-03,
        3.5327e-03, 1.7966e-03, 1.9630e-02, 5.5035e-04, 1.9212e-03, 3.8338e-03,
        9.1932e-02, 5.9566e-03, 4.5687e-03, 2.0924e-01, 5.2169e-02, 2.0594e+00,
        7.6307e-02, 1.7167e-01, 3.0277e-01, 2.3760e+00, 1.7887e-02, 1.1041e-02,
        3.6043e-03, 2.1956e-01, 4.2216e-04, 1.9923e-03, 1.0707e-03, 2.4099e-03,
        2.2202e-03, 1.5247e-01, 4.0678e+00, 5.0963e-01, 1.4150e+00, 6.8109e-02,
        1.5366e+00, 2.0520e-02, 5.3695e-01, 1.4424e-01, 2.8135e-02, 3.5394e-02,
        5.6005e-02, 1.1364e+00, 6.0568e-02, 7.0265e-01, 2.1712e-03, 2.7865e-02,
        2.3545e+00, 1.0760e+00, 4.4195e+00, 4.1704e-01, 1.5437e-01, 5.8956e-01,
        1.3000e-02, 1.7386e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([8.9358e+00, 1.0497e+00, 8.5680e+00, 6.7070e-01, 1.3906e-01, 1.8283e+00,
        1.3132e+00, 1.7283e+00, 1.8620e+00, 1.0342e+00, 2.3378e-01, 1.5870e+00,
        1.1663e-01, 3.8189e-01, 1.0126e+00, 1.5919e-01, 7.7461e-01, 7.5078e-01,
        2.1096e-01, 1.2246e+00, 4.3722e-03, 3.2234e-03, 7.0833e-03, 8.5682e-02,
        8.8599e-01, 4.5089e-01, 2.6788e+00, 3.8770e-01, 2.2101e-01, 5.0273e-02,
        4.1619e-03, 3.0434e-03, 5.4966e-02, 8.8380e-03, 7.5675e-03, 6.6873e-03,
        8.0393e-02, 8.1786e-02, 1.1866e-02, 4.5361e-01, 1.5690e-01, 1.7369e+00,
        1.9649e-01, 4.8823e-01, 6.6232e-01, 1.1520e+00, 3.2804e-02, 2.4559e-02,
        1.5378e-02, 1.4277e-01, 9.4112e-04, 1.0420e-02, 7.3690e-03, 3.4852e-02,
        1.0926e-02, 7.2673e-02, 3.7871e+00, 4.0029e-01, 8.3184e-01, 1.2564e-02,
        2.6829e+00, 1.3223e-02, 2.0047e-01, 5.3413e-02, 9.1108e-03, 1.4906e-02,
        2.0647e-02, 1.2174e+00, 4.0967e-02, 4.1509e-01, 4.0347e-04, 2.3738e-02,
        9.9278e-01, 6.8967e-01, 4.1705e+00, 1.7720e-01, 1.2729e-01, 1.4674e+00,
        5.7292e-03, 2.8723e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.0749e+01, 2.0241e+01, 5.0141e+01, 1.1215e+01, 5.5679e+00, 3.5568e+01,
        6.1456e+01, 4.3426e+01, 2.9245e+01, 3.1547e+01, 1.7670e+01, 7.3830e+01,
        9.0889e+00, 8.9764e+00, 1.5945e+01, 2.6318e+00, 1.1222e+01, 1.4833e+01,
        3.4736e+00, 1.4671e+01, 5.5772e-01, 2.1203e-01, 3.2725e-01, 5.5585e+00,
        2.4690e+01, 1.7571e+01, 3.0750e+01, 1.4774e+01, 1.2917e+01, 1.6698e+00,
        1.6528e-01, 1.7018e-01, 5.5795e-01, 1.7315e+00, 3.0091e-01, 1.3934e-01,
        3.4947e+00, 2.1381e+00, 2.1481e-01, 4.5811e+00, 6.8953e-01, 3.8732e+00,
        4.9779e-01, 1.1840e+00, 8.8602e-01, 1.8369e+00, 1.4246e+00, 8.9861e-01,
        4.8834e-01, 3.6770e+00, 1.1035e-01, 5.9879e-01, 3.2563e-01, 1.0129e+00,
        6.7350e-01, 1.8078e+00, 1.1230e+01, 4.4595e+00, 5.3970e+00, 4.8908e-01,
        3.8433e+00, 2.3759e-01, 7.7964e-01, 6.3060e-01, 1.2259e-01, 3.0059e-01,
        1.9283e-01, 1.9375e+01, 1.4034e-01, 2.9385e+00, 1.1728e-02, 5.3899e-01,
        2.4342e+00, 3.2728e+00, 5.5995e+00, 1.7879e-01, 1.2760e-01, 1.4868e+00,
        1.9324e-02, 2.5569e-01], device='cuda:0')
Outer loop valEpocw Maximum [13/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 75.2
model_train val_loss valEpocw [13/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 61.2
model_train val_loss valEpocw [13/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 728.1
Sum_Val Meta Model:  tensor([1.0267e+01, 2.0467e-01, 2.0010e+00, 1.8824e-01, 3.5794e-02, 1.7781e-01,
        3.1466e-02, 3.6922e-01, 1.5613e-01, 8.0712e-02, 2.6077e-02, 2.7940e-02,
        7.6745e-03, 7.8452e-01, 2.1593e-01, 1.9414e-01, 4.9290e-01, 9.9713e-02,
        9.9737e-02, 1.6612e-01, 8.8748e-03, 2.6076e-02, 1.7208e-01, 3.7537e-02,
        2.3479e-01, 5.1973e-02, 1.5395e+00, 1.6341e-01, 3.1206e-02, 1.3331e-01,
        1.0576e-01, 1.0764e-01, 9.6682e-01, 2.1649e-03, 9.4105e-02, 2.0283e-01,
        3.3176e-01, 4.9410e-01, 4.5641e-02, 2.9840e+00, 7.6085e+00, 3.4832e+01,
        3.2840e+01, 3.4673e+01, 3.9244e+01, 4.7130e+01, 3.3720e-01, 3.2260e-01,
        1.9585e+00, 5.8803e-01, 2.1917e-01, 6.8322e-01, 1.3976e+00, 5.5362e-01,
        8.8547e-01, 3.7961e+00, 5.1234e+01, 1.1567e+00, 1.3757e+00, 9.5014e-02,
        6.0826e+01, 1.6397e-01, 2.7338e+00, 9.8438e-01, 9.2168e-01, 9.1293e-02,
        9.7630e-01, 6.2929e-01, 1.4761e+00, 5.6550e-01, 2.3897e-02, 2.9527e-01,
        2.2825e+00, 6.5397e+00, 1.9041e+00, 2.9102e+01, 9.1805e+00, 2.1767e+00,
        1.5627e-01, 1.5208e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([3.1131e+00, 2.7635e-02, 5.4448e-01, 1.5678e-02, 2.6698e-03, 6.3010e-03,
        1.1697e-03, 3.5747e-01, 1.0854e-02, 3.5556e-03, 1.3975e-03, 9.9424e-04,
        4.1086e-04, 6.2169e-01, 1.6762e-02, 4.1421e-02, 1.2172e-01, 1.0061e-02,
        1.2219e-02, 5.6919e-03, 3.9726e-04, 2.7270e-04, 3.9453e-04, 9.7805e-04,
        1.2067e-01, 1.7697e-02, 1.7350e+00, 1.1667e-01, 6.4949e-03, 1.3472e-02,
        1.5267e-02, 1.1544e-01, 7.8217e-01, 6.0700e-04, 1.7357e-02, 2.3576e-02,
        2.0952e-01, 3.9995e-01, 1.0313e-02, 4.9177e+00, 6.9898e+00, 3.1020e+01,
        2.8762e+01, 3.4153e+01, 3.3828e+01, 4.6262e+01, 2.0389e-01, 1.6861e-01,
        1.2292e+00, 3.2830e-01, 1.3770e-01, 6.4254e-01, 1.3131e+00, 7.9619e-01,
        6.3758e-01, 2.7965e+00, 2.8592e+01, 1.0560e+00, 1.3065e+00, 3.1383e-02,
        6.1176e+01, 1.6409e-02, 1.8844e+00, 8.1717e-01, 7.6357e-01, 1.8029e-01,
        7.9224e-01, 6.9568e-01, 1.6579e+00, 5.8996e-01, 3.0969e-03, 3.0500e-01,
        2.4028e+00, 6.9002e+00, 4.2909e+00, 1.8258e+01, 1.0153e+01, 1.4785e+00,
        3.1923e-02, 8.2880e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.4841e+01, 6.9623e-01, 3.7847e+00, 3.3701e-01, 1.5510e-01, 1.6869e-01,
        1.1634e-01, 1.4979e+01, 2.4604e-01, 1.4395e-01, 1.5622e-01, 7.5662e-02,
        5.9651e-02, 1.6875e+01, 3.5797e-01, 1.0354e+00, 2.1121e+00, 2.9162e-01,
        3.3495e-01, 1.1502e-01, 1.1052e-01, 2.9556e-02, 2.4158e-02, 9.0506e-02,
        4.5486e+00, 1.2444e+00, 2.7831e+01, 7.8733e+00, 7.1023e-01, 5.8542e-01,
        9.3432e-01, 8.4986e+00, 8.9951e+00, 1.9491e-01, 8.6682e-01, 5.2757e-01,
        1.1621e+01, 1.2291e+01, 2.4840e-01, 5.4033e+01, 2.9741e+01, 6.2695e+01,
        6.0399e+01, 7.1334e+01, 4.2477e+01, 6.6424e+01, 1.1143e+01, 7.7574e+00,
        4.1178e+01, 1.0482e+01, 2.3476e+01, 4.1985e+01, 6.5304e+01, 3.0431e+01,
        5.4573e+01, 8.1395e+01, 8.9358e+01, 1.5347e+01, 9.8004e+00, 1.7677e+00,
        8.1662e+01, 3.9036e-01, 8.0964e+00, 1.2346e+01, 1.2770e+01, 4.8994e+00,
        8.9814e+00, 1.4692e+01, 6.4402e+00, 4.9832e+00, 1.2794e-01, 9.3084e+00,
        6.1453e+00, 3.8012e+01, 5.6414e+00, 1.8366e+01, 1.0169e+01, 1.4920e+00,
        1.2721e-01, 8.8105e-01], device='cuda:0')
Outer loop valEpocw Maximum [13/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 405.5
model_train val_loss valEpocw [13/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 346.2
model_train val_loss valEpocw [13/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1270.7
Sum_Val Meta Model:  tensor([1.6615e+01, 7.0943e-02, 3.1055e+00, 2.7469e-02, 7.6492e-03, 4.7718e-01,
        6.9428e-02, 4.5704e-01, 6.9495e-02, 5.8323e-01, 5.5184e-02, 8.3106e-02,
        8.9891e-04, 7.1399e-01, 7.8586e-01, 5.2047e-02, 7.8188e-02, 1.8988e-02,
        8.6196e-03, 1.8568e-02, 1.3474e-03, 2.5049e-03, 5.7900e-03, 6.2777e-03,
        7.4185e-01, 2.1344e-02, 2.4682e+00, 2.1492e-02, 1.9809e-01, 5.3651e-03,
        1.1108e-02, 1.2677e-03, 1.6004e-01, 6.9797e-03, 2.7469e-02, 2.4806e-01,
        3.3147e-03, 1.5664e-02, 1.5554e-01, 2.5531e+00, 3.4986e+00, 1.3275e+01,
        3.9212e+00, 4.0782e+00, 7.6351e+00, 1.0581e+01, 6.0845e-03, 9.2753e-03,
        2.8550e-02, 8.8267e-03, 2.5239e-03, 5.8057e-03, 5.2956e-03, 1.1689e-01,
        9.1868e-03, 4.7871e-02, 5.8029e+00, 3.0136e-01, 2.3529e+00, 2.2383e-02,
        3.0551e+01, 1.5764e-02, 8.2094e-01, 2.6854e-01, 1.6196e-01, 4.3551e-02,
        3.1013e-01, 3.1531e+00, 2.6382e-01, 2.7331e-01, 1.3779e-03, 4.7498e-02,
        2.2603e+00, 1.9715e+00, 3.6331e+02, 1.3194e+01, 1.2984e+00, 1.0430e+01,
        8.4340e-03, 9.1083e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.0028e+00, 2.0189e-01, 3.5478e+00, 1.2218e-01, 2.1031e-02, 3.9290e-01,
        9.1833e-02, 3.9222e-01, 1.4360e-01, 4.4604e-01, 3.6960e-02, 1.5424e-01,
        7.3283e-03, 6.6170e-01, 8.2565e-01, 5.7822e-02, 9.3130e-02, 2.4006e-02,
        1.2682e-02, 1.0154e-02, 2.5988e-03, 5.1586e-04, 4.3451e-03, 1.0695e-02,
        7.0861e-01, 4.8443e-02, 1.7818e+00, 2.7635e-02, 2.1506e-01, 6.7159e-03,
        3.7406e-03, 3.6727e-03, 2.1965e-02, 1.3433e-03, 4.4688e-03, 6.5636e-03,
        1.3269e-02, 7.4802e-03, 9.9566e-03, 3.6754e-01, 3.9877e-01, 1.4179e+00,
        3.7070e-01, 6.0994e-01, 6.6197e-01, 9.2766e-01, 1.0780e-02, 6.5848e-03,
        7.7151e-03, 8.7817e-03, 5.3609e-04, 2.4350e-03, 1.9846e-03, 1.1311e-02,
        1.0061e-02, 3.8092e-02, 2.0213e+00, 5.2989e-02, 1.7789e+00, 1.6464e-02,
        7.9644e+00, 9.1568e-03, 2.5008e-01, 4.8541e-02, 1.5564e-02, 4.0380e-02,
        4.3742e-02, 5.1561e-01, 8.3441e-02, 1.0727e-01, 6.6947e-04, 1.7879e-02,
        1.7243e-01, 1.2605e+00, 7.1018e+01, 3.0385e+00, 8.7626e-02, 5.9115e+00,
        6.8232e-03, 8.5301e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.4181e+01, 4.5211e+00, 2.3808e+01, 2.5678e+00, 1.0159e+00, 9.8609e+00,
        7.6727e+00, 1.4578e+01, 2.8700e+00, 1.6032e+01, 3.5703e+00, 1.0222e+01,
        8.9884e-01, 1.5684e+01, 1.6809e+01, 1.3147e+00, 1.5914e+00, 6.3541e-01,
        3.4392e-01, 2.0808e-01, 5.4299e-01, 5.0651e-02, 2.9098e-01, 8.6699e-01,
        2.4226e+01, 2.8211e+00, 2.2860e+01, 1.5282e+00, 1.9527e+01, 3.0666e-01,
        2.0985e-01, 2.9052e-01, 2.3813e-01, 3.7117e-01, 2.0962e-01, 1.2058e-01,
        7.7105e-01, 2.5186e-01, 2.0351e-01, 3.4507e+00, 1.4428e+00, 2.8876e+00,
        8.0493e-01, 1.3564e+00, 8.4100e-01, 1.3998e+00, 6.6299e-01, 3.3239e-01,
        3.4804e-01, 3.2124e-01, 9.1855e-02, 1.8140e-01, 1.1269e-01, 3.7231e-01,
        8.8234e-01, 1.2069e+00, 5.8547e+00, 7.0833e-01, 1.0625e+01, 8.4854e-01,
        1.0776e+01, 1.9231e-01, 9.8435e-01, 6.3624e-01, 1.9941e-01, 1.0216e+00,
        3.9480e-01, 7.0692e+00, 2.9633e-01, 8.1069e-01, 2.5064e-02, 4.6323e-01,
        3.7949e-01, 6.3696e+00, 9.4866e+01, 3.0573e+00, 8.7783e-02, 5.9677e+00,
        3.2665e-02, 7.3745e-02], device='cuda:0')
Outer loop valEpocw Maximum [13/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 510.1
model_train val_loss valEpocw [13/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 114.4
model_train val_loss valEpocw [13/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 412.5
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.14075121 97.36845311 93.03980215 98.06776233 98.54412105 97.57191067
 98.23223401 95.21935649 97.99466381 96.88843947 98.60503649 98.89377566
 99.43470474 95.34849722 97.4634812  97.17596033 96.36456671 97.56581913
 98.75123354 98.397924   98.57336046 99.25804998 99.53216944 99.01682484
 95.26443391 96.78975646 94.41649103 96.99199571 98.03852292 98.27121989
 98.08725527 98.58310693 96.89574932 98.50513517 98.70737442 98.80849405
 97.56703744 98.36868459 98.91083198 93.1311753  97.97273425 94.0924209
 97.49272061 96.64721434 97.46591781 95.47276471 98.19933968 98.61600127
 98.05923417 98.74270538 98.75732508 98.62209281 99.00951499 98.32726209
 98.73417722 97.57191067 92.31978168 96.91280564 96.55705949 97.48053752
 94.97082151 98.54412105 97.29413628 97.37454466 98.77925464 97.56947406
 98.6208745  95.9734896  98.8572264  98.30655085 99.81603538 97.3806362
 98.71712089 95.92110233 97.68765    98.22248754 99.29825416 98.98758543
 99.84405648 99.16302189]
Accuracy th:0.7 is [86.30864634 97.28926305 92.74375312 98.01293844 98.45518451 97.50003046
 98.0775088  94.9696032  97.90816389 96.80803109 98.56605061 98.84138838
 99.42495827 95.31925781 97.35748833 97.05291115 96.36700333 97.50490369
 98.80727574 98.35040996 98.42107187 99.21540917 99.48831033 98.8852475
 95.2205748  96.6983833  94.25445596 96.90549579 98.0214666  98.23345232
 97.80948088 98.579452   96.5168553  98.33457195 98.579452   98.64767729
 97.33799539 98.36015643 98.81336728 93.35047088 97.95567793 93.97059003
 97.32703062 96.62406647 97.31362922 95.12432841 98.16888196 98.64158575
 98.08481865 98.70615611 98.62818435 98.57579708 99.00220514 98.31873393
 98.71590258 97.50368538 91.94454259 96.60701015 96.46203141 97.33677709
 94.6223852  98.30655085 97.01270696 97.2405307  98.73052229 97.47566428
 98.60381818 95.95521497 98.75854339 98.21517769 99.81603538 97.09189703
 98.63183928 95.70058844 97.50855862 98.01050182 99.26901475 98.89255735
 99.84405648 99.15083881]
Avg Prec: is [96.21709232 31.3470954  70.36848597 65.12458167 75.9393158  63.11292584
 71.32077702 47.28669788 54.08617597 49.29568856 27.17271645 51.25120837
 20.82470151 26.66064276 31.80223161 52.28710567 26.57808905 37.20416528
 42.68599616 36.70073687 58.12826928 45.28515115 88.57638341 81.24432555
 25.11073823 30.45830513 37.43631747 36.49778029 22.10582707 37.78801046
 70.19937044 33.8372186  55.71721519 60.01477336 71.95466809 77.96967228
 56.05669733 73.7627608  85.4754954  46.30815455 38.07899588 64.63334768
 57.29284569 52.47564003 54.31031934 63.80088433 34.17505808 29.85363089
 39.27427984 43.0804945  59.87111758 34.69680883 19.12628115 71.32347595
 23.58029519 37.91258449 67.26064326 55.76505703 39.76330622 54.79408789
 82.75524749 81.16962778 66.49570131 47.38497557 56.96241784 40.26259144
 56.45267358 24.24039887 48.76688309 64.43504136 12.88014769 71.61768845
 69.97955962 46.83722765 68.81402973 75.6361171  43.78855682 71.66513003
  5.36772378 21.65703477]
Accuracy th:0.5 is [45.50748651 97.2137279  72.29444086 97.02489005 97.26733349 77.10310547
 77.48199949 76.45740183 78.38842119 96.44253847 78.70152654 98.52097319
 99.41399349 80.24634203 78.26293539 96.56680596 96.29512311 78.0692243
 98.65376884 98.30776915 80.41812356 79.25585702 98.38695922 78.17765378
 80.6045248  96.65086926 94.0778012  77.64890779 98.01293844 78.59431537
 97.30875598 98.57457877 96.36213009 98.02024829 87.21994128 78.23978753
 77.98272438 90.80298729 97.11504489 75.17452273 79.24732886 92.05906361
 77.52220368 77.0519365  96.9627563  93.87434364 98.02877645 98.57336046
 96.13430636 88.63683435 87.36857494 98.55508583 98.99976852 77.63794301
 98.70615611 77.96932299 72.71597568 93.33463286 96.24273583 96.9067141
 89.79300934 97.17717864 92.34292955 78.06556938 98.42838172 78.50781545
 98.20786784 77.12868995 79.0718924  97.55972759 79.65424398 95.99054592
 78.83310389 95.45083515 77.3029081  82.52945261 86.45362508 78.61502662
 79.70663125 99.14718388]
Accuracy th:0.7 is [45.5757118  97.2137279  72.29444086 97.02489005 97.26733349 77.10310547
 77.48199949 76.60725381 78.38842119 96.4742145  78.70152654 98.52097319
 99.41399349 80.68005994 78.26293539 96.56680596 96.29512311 78.0692243
 98.65376884 98.30776915 80.83234853 79.25585702 98.38695922 78.35187193
 81.16982006 96.65086926 94.0778012  77.64890779 98.01293844 78.59431537
 97.30875598 98.57457877 96.36213009 98.02024829 87.40756082 78.23978753
 77.98272438 91.04786735 97.11504489 75.17452273 79.94420146 92.05906361
 77.52220368 77.0519365  96.9627563  93.87434364 98.02877645 98.57336046
 97.18327018 89.30568585 87.53182832 98.55508583 98.99976852 77.63794301
 98.70615611 77.96932299 72.71597568 93.77322401 96.24273583 96.9067141
 89.79300934 97.17717864 92.54273218 78.06556938 98.42838172 78.50781545
 98.20786784 77.12868995 79.0718924  97.55972759 79.65424398 95.99054592
 78.88792778 95.45083515 77.3029081  82.60376945 86.60591367 78.61502662
 79.70663125 99.14718388]
Avg Prec: is [55.78624877  3.00565441 11.08010664  3.29313655  2.23586632  3.91983782
  3.28020161  5.55532907  2.4446188   3.79919586  1.68998261  1.65195093
  0.64426986  5.13729859  2.58919507  3.19290932  3.61950619  2.72532609
  1.33363917  1.72318898  1.9775596   0.92013533  1.86456388  2.42756024
  4.93715068  3.56872242  6.57496616  3.32698116  1.97482876  1.85070216
  2.51370771  1.32776723  3.67822047  1.68878531  2.32710438  2.31568057
  3.1252563   2.48419998  2.78629398  7.4043501   2.29560393  8.17001197
  3.43095351  4.03774482  3.22809685  6.42371756  2.05603791  1.47842925
  2.09433438  1.67382682  1.83875646  1.58596876  1.06308919  2.96837661
  1.39795846  2.73200892 11.09717776  3.65336249  3.95829641  2.80704974
 10.81334941  2.23710822  3.81445033  3.02195947  1.56513632  2.47665491
  1.86903224  4.23815826  1.25121895  2.40025884  0.18992039  3.43358803
  1.89730619  4.55617126  3.99133395  3.21363646  0.8273033   1.86656417
  0.13494778  0.73902504]
mAP score regular 50.87, mAP score EMA 3.75
starting validation
Accuracy th:0.5 is [87.94877544 97.34409647 93.05628223 98.18621222 98.90375464 97.68293594
 98.38054663 95.32600842 98.06413035 96.97535939 98.6371677  98.97849864
 99.36716745 95.17652042 97.43877221 97.22699753 96.21047911 97.63310661
 98.91870344 98.35812343 98.84395944 99.28494905 99.61631412 99.20522211
 95.45307322 96.74116152 94.5636196  97.1696938  97.85484715 98.12143409
 98.45529063 98.65460797 96.83832872 98.70692877 98.88880584 99.00590478
 97.98191195 98.29832823 99.03081944 92.8943369  97.89471062 93.21075317
 97.21952313 96.18058151 96.84331166 94.5561452  98.29334529 98.77419837
 98.01180955 98.67204823 98.79911304 98.6595909  98.87883997 98.36808929
 98.72935197 97.75020555 90.81147071 97.05259486 96.39235618 97.46119541
 92.95911503 98.67952263 97.23198047 97.41385754 98.75924957 97.54092234
 98.54996637 95.79440417 98.75177517 98.20863542 99.81563146 97.60320901
 98.33819169 95.81682737 97.2743354  97.53843087 99.23511971 98.68699704
 99.82559733 99.14542691]
Accuracy th:0.7 is [87.73201784 97.30174154 93.12853477 98.24849889 98.89129731 97.61068341
 98.28088796 95.25624735 98.05167302 96.88566659 98.60228717 98.96105838
 99.35471012 95.12419962 97.3565538  97.042629   96.39235618 97.54341381
 98.93116077 98.36808929 98.71689464 99.22764531 99.59139946 99.14542691
 95.43812442 96.62157112 94.49385853 97.08498393 97.82744101 98.22856716
 98.24102449 98.67952263 96.54931858 98.57488103 98.7941301  98.92368637
 97.72529088 98.26344769 98.89628024 93.33532651 97.97942048 93.62433665
 97.3490794  96.60413085 96.98034233 94.67324414 98.31825996 98.8016045
 98.05416449 98.74430077 98.62471037 98.59730423 98.87385704 98.47023943
 98.72187757 97.64805541 91.12788699 96.92553006 96.34501831 97.35406234
 93.2531081  98.4428333  97.11986446 97.36651967 98.73682637 97.56334554
 98.58235543 95.77198097 98.75426664 98.23853302 99.81563146 97.3117074
 98.4652565  95.74457483 97.266861   97.55587114 99.27000025 98.6670653
 99.82559733 99.15788425]
Avg Prec: is [96.48588386 30.08856539 70.69454224 72.92490035 76.02819831 64.56828699
 77.68654448 47.83801317 58.83012246 52.91401934 34.32110372 53.81379716
 20.86531601 30.51991038 33.53525888 59.34274339 31.04602618 41.06629892
 46.43869227 33.62942895 67.6483285  54.47364654 91.56470592 86.49763266
 24.23277904 33.88130631 33.59058304 42.53634974 25.03287391 36.10634344
 76.08565223 36.02450185 53.7229716  64.01855095 73.86656    81.40427837
 59.28747201 76.88571826 89.27414483 45.31224018 36.13360711 54.91085702
 47.06799817 39.89928186 33.50856415 48.67571645 33.51752841 26.38122875
 39.93474226 40.09174    63.76064697 35.26734405 21.59348276 75.84078863
 26.55590352 38.90751001 57.86347828 55.19174706 36.92810529 58.03463144
 68.76176389 85.98477627 65.54252389 50.49102314 59.90651616 37.48978007
 59.46852842 24.64046615 40.65650983 65.54607468  9.51075273 74.36156641
 56.52515336 43.25393458 62.42355162 51.89194085 16.38561268 52.29741885
  2.71251999 21.14087163]
Accuracy th:0.5 is [45.31728829 97.22450607 70.98936144 96.96290206 97.90716795 76.53536637
 76.62755064 75.43413808 78.06014401 96.41976231 78.25447841 98.5325261
 99.34972718 78.45877868 78.14236241 96.31262924 96.21047911 77.59423973
 98.78167277 98.34068316 79.35819817 78.88980243 98.31327703 77.81847173
 78.42140668 96.52938685 94.3393876  77.61666293 97.81747515 78.21212348
 97.52597354 98.67204823 96.39983058 98.18870369 88.59157386 77.83342053
 77.7761168  92.19174328 97.0276802  74.84615193 78.11495627 92.37362035
 76.95144131 76.61011037 97.03764606 94.02795426 98.18621222 98.77668984
 96.90061539 88.29259785 85.89829833 98.55993223 98.87385704 76.95642425
 98.6969629  77.62413733 71.48765478 94.38672547 96.16314124 96.78102499
 90.13379176 97.04761193 92.16682861 77.57679946 98.32075143 78.43137255
 98.13139996 76.74714104 78.91222563 97.53593941 79.36567257 96.07843137
 78.62321549 95.44559882 76.68485437 83.81792361 88.74355333 78.26942721
 79.43543364 99.15040985]
Accuracy th:0.7 is [45.51909709 97.22450607 70.98936144 96.96290206 97.90716795 76.53536637
 76.62755064 75.51386501 78.06014401 96.41976231 78.25447841 98.5325261
 99.34972718 78.85741336 78.14236241 96.31262924 96.21047911 77.59423973
 98.78167277 98.34068316 79.70202058 78.88980243 98.31327703 77.87577547
 78.82751576 96.52938685 94.3393876  77.61666293 97.81747515 78.21212348
 97.52597354 98.67204823 96.39983058 98.18870369 88.7609936  77.83342053
 77.7761168  92.41597528 97.0276802  74.84615193 78.56092882 92.37362035
 76.95144131 76.61011037 97.03764606 94.02795426 98.18621222 98.77668984
 97.50853327 88.59157386 86.04031193 98.55993223 98.87385704 76.95642425
 98.6969629  77.62413733 71.48765478 94.63587214 96.16314124 96.78102499
 90.13379176 97.04761193 92.31382515 77.57679946 98.32075143 78.43137255
 98.13139996 76.74714104 78.91222563 97.53593941 79.36567257 96.07843137
 78.63318135 95.44559882 76.68485437 83.89017615 88.91297307 78.26942721
 79.43543364 99.15040985]
Avg Prec: is [53.86162484  3.71910749 14.87242895  4.53517219  1.47260521  4.24472704
 13.92293127  8.65965013  8.1853124   5.26743345  2.33178676  4.73576392
  2.3384228   5.85327958  2.97382426  3.69042878 24.03744497  6.49927866
  1.56379605  2.74875686  3.51209292  1.51088186  1.1337494   5.18418554
  5.63236431  9.0604106   7.82377914  4.54794242  3.90603924  5.57600401
  2.23314143  0.85939671  3.03191014  1.1089393   1.66112236  2.2193693
  1.98622826  2.20615134  2.24155856  6.18063667  1.72546316  5.99643382
  2.17248156  2.70174965  2.36508682  4.8539649   1.69188457  1.02503364
  1.37244425  1.15835763  1.2028077   0.9764693   0.73107748  2.30940817
  0.85045426  1.83921089 10.00666719  2.89554498  3.84347774  2.79938457
  7.81577318  2.05994252  3.10629551  2.49309916  1.34189347  1.82844991
  1.49943535  3.43485396  1.09009486  2.25896813  0.19169822  3.24841487
  1.58940798  3.92075374  3.53577856  2.30695945  0.59699888  1.5054435
  0.12915025  0.59015526]
mAP score regular 50.04, mAP score EMA 4.30
Train_data_mAP: current_mAP = 50.87, highest_mAP = 50.87
Val_data_mAP: current_mAP = 50.04, highest_mAP = 50.04
tensor([0.1227, 0.0405, 0.1462, 0.0428, 0.0176, 0.0353, 0.0097, 0.0235, 0.0459,
        0.0243, 0.0085, 0.0128, 0.0067, 0.0374, 0.0454, 0.0397, 0.0541, 0.0331,
        0.0335, 0.0462, 0.0037, 0.0085, 0.0126, 0.0104, 0.0264, 0.0147, 0.0707,
        0.0150, 0.0090, 0.0189, 0.0155, 0.0106, 0.0769, 0.0025, 0.0165, 0.0418,
        0.0142, 0.0256, 0.0382, 0.0942, 0.2456, 0.4753, 0.4861, 0.4642, 0.8215,
        0.6844, 0.0137, 0.0176, 0.0190, 0.0246, 0.0048, 0.0121, 0.0152, 0.0232,
        0.0096, 0.0287, 0.3403, 0.0652, 0.1570, 0.0166, 0.7435, 0.0416, 0.2359,
        0.0650, 0.0647, 0.0336, 0.0969, 0.0552, 0.3339, 0.1379, 0.0236, 0.0339,
        0.4752, 0.1946, 0.8224, 0.9953, 0.9991, 0.9932, 0.3095, 0.1045],
       device='cuda:0')
Sum Train Loss:  tensor([5.4691e+00, 4.6142e-01, 4.4712e+00, 6.4668e-01, 1.3098e-01, 6.0821e-01,
        1.4187e-01, 4.0317e-01, 1.0541e-01, 1.7366e-01, 2.9726e-02, 6.1618e-02,
        3.9918e-03, 1.0402e+00, 1.0576e+00, 1.8495e-01, 8.0816e-01, 2.0152e-01,
        8.3078e-02, 3.7963e-01, 1.8258e-02, 3.4756e-03, 4.7464e-02, 5.0225e-02,
        4.9479e-01, 1.8415e-01, 1.2603e+00, 1.2509e-01, 1.0609e-01, 1.3489e-01,
        2.8808e-02, 3.0100e-02, 9.9670e-01, 1.1351e-02, 1.1673e-01, 3.2135e-01,
        1.0680e-01, 5.5970e-01, 6.3963e-02, 1.8992e+00, 1.4971e+00, 8.3388e+00,
        3.0583e+00, 3.7530e+00, 2.1122e+00, 9.3776e+00, 9.8612e-02, 2.2963e-01,
        1.1433e-01, 6.4388e-02, 3.6284e-03, 1.3041e-02, 6.2750e-02, 1.9375e-01,
        8.4725e-02, 3.2694e-01, 1.0077e+01, 7.8233e-01, 2.0208e+00, 4.9711e-02,
        1.1276e+01, 1.0423e-01, 1.4575e+00, 4.7736e-01, 2.0373e-01, 2.7785e-01,
        3.0149e-01, 1.3633e+00, 1.3454e+00, 4.0450e-01, 8.5021e-02, 1.0430e-01,
        1.9898e+00, 3.4723e+00, 1.1730e+01, 1.2816e+00, 6.3811e+00, 1.5303e+00,
        6.6556e-02, 2.0915e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [14/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 109.3
Sum Train Loss:  tensor([4.9230e+00, 1.4005e-01, 2.3126e+00, 1.3285e-01, 3.2841e-02, 4.8198e-01,
        4.3490e-02, 3.3852e-01, 1.8808e-01, 2.3854e-01, 1.7089e-02, 3.3820e-02,
        4.7342e-03, 6.2539e-01, 3.6091e-01, 3.2850e-01, 5.6698e-01, 2.8709e-01,
        1.1954e-01, 4.1221e-01, 4.2319e-02, 9.0805e-03, 3.7429e-02, 4.4542e-02,
        6.0608e-01, 1.0573e-01, 1.4481e+00, 1.2389e-01, 9.3758e-02, 1.6846e-01,
        8.6031e-02, 1.9238e-02, 1.1610e+00, 1.6797e-02, 1.2616e-01, 4.8776e-01,
        1.0918e-01, 9.9983e-02, 4.0172e-01, 1.7836e+00, 2.4212e+00, 1.0554e+01,
        6.5735e+00, 4.5976e+00, 1.1293e+01, 1.1098e+01, 6.3812e-02, 1.1490e-01,
        1.3832e-01, 1.6661e-01, 5.1174e-03, 6.0502e-02, 3.5446e-02, 1.5571e-01,
        1.1149e-01, 2.5540e-01, 4.7222e+00, 6.3838e-01, 1.6530e+00, 1.2577e-01,
        1.1805e+01, 5.1355e-01, 1.2587e+00, 6.5630e-01, 1.9408e-01, 1.5678e-01,
        5.2155e-01, 7.2564e-01, 2.5211e+00, 9.4771e-01, 6.8847e-03, 3.4843e-01,
        2.4280e+00, 1.6988e+00, 2.8048e+00, 3.2510e+00, 2.2613e+00, 7.0066e-01,
        9.9855e-02, 3.6478e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [14/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 106.6
Sum Train Loss:  tensor([3.9642e+00, 6.4538e-01, 4.9181e+00, 1.4008e-01, 1.0066e-01, 3.9730e-01,
        4.6120e-02, 5.4230e-01, 6.6599e-01, 2.6072e-01, 1.0681e-02, 6.6903e-02,
        6.2693e-03, 6.0906e-01, 1.8517e-01, 1.3602e-01, 3.7031e-01, 3.5198e-01,
        1.3760e-01, 2.3271e-01, 1.3939e-02, 2.4193e-03, 8.1660e-03, 7.5334e-02,
        5.3712e-01, 2.9154e-01, 1.4477e+00, 1.5321e-01, 5.2805e-02, 1.1077e-01,
        4.2956e-02, 3.1800e-02, 1.3387e+00, 9.3550e-03, 8.9194e-02, 2.3042e-01,
        1.7757e-01, 4.6722e-02, 3.4266e-01, 2.3137e+00, 3.0462e+00, 8.0135e+00,
        3.6913e+00, 6.5237e+00, 9.0081e+00, 5.6171e+00, 1.2842e-01, 1.2013e-01,
        5.9141e-02, 1.5132e-01, 2.6589e-02, 3.0838e-02, 4.9526e-02, 4.6623e-02,
        1.5965e-01, 1.3619e-01, 1.0082e+01, 7.6335e-01, 1.7370e+00, 1.6155e-01,
        7.4686e+00, 3.4957e-01, 3.3289e+00, 5.6928e-01, 5.4625e-02, 3.8886e-01,
        1.1717e-01, 1.6402e+00, 8.4837e-01, 8.1016e-01, 1.0939e-02, 6.3125e-01,
        5.0458e+00, 1.7499e+00, 6.7334e+00, 4.9828e+00, 7.8084e+00, 5.7171e-01,
        7.3795e-02, 6.0264e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [14/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 114.4
Sum Train Loss:  tensor([4.0759e+00, 3.9030e-01, 2.6690e+00, 5.4319e-01, 5.1514e-02, 2.2476e-01,
        1.0928e-01, 3.7889e-01, 1.6222e-01, 1.6931e-01, 1.1172e-02, 1.4412e-02,
        3.5199e-03, 5.8041e-01, 3.6486e-01, 3.2770e-01, 9.6418e-01, 2.6724e-01,
        2.9420e-01, 1.7084e-01, 2.0087e-02, 5.5925e-02, 1.1700e-02, 4.7829e-02,
        6.4822e-01, 5.6599e-01, 1.4576e+00, 2.1308e-01, 1.5419e-01, 5.6327e-02,
        1.3806e-01, 1.3463e-02, 8.6288e-01, 1.2444e-02, 4.6478e-02, 1.7264e-01,
        1.9119e-01, 1.2810e-01, 2.3540e-01, 2.4119e+00, 1.1902e+00, 9.3362e+00,
        3.9338e+00, 2.7455e+00, 2.8408e+00, 6.0878e+00, 6.4466e-02, 2.3708e-02,
        1.5518e-01, 2.1245e-01, 2.8408e-02, 3.6752e-02, 1.3964e-01, 1.7577e-01,
        2.3317e-02, 1.5535e-01, 7.3252e+00, 8.7484e-01, 2.1685e+00, 2.2569e-01,
        1.2481e+01, 5.1496e-02, 2.5443e+00, 6.1803e-01, 1.2027e-01, 2.8114e-01,
        1.8843e-01, 8.1400e-01, 3.8158e-01, 8.3280e-01, 5.7899e-02, 1.8349e-01,
        6.8611e+00, 2.1223e+00, 6.3260e+00, 5.2987e+00, 1.3646e+01, 6.1589e+00,
        4.8858e-02, 3.2476e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [14/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 116.3
Sum Train Loss:  tensor([5.3529e+00, 4.0655e-01, 3.5722e+00, 4.2771e-01, 5.8534e-02, 7.5030e-01,
        1.0866e-01, 5.0500e-01, 3.3090e-01, 2.9232e-01, 1.2942e-01, 1.4403e-01,
        5.7571e-02, 6.8674e-01, 5.4002e-01, 7.9484e-01, 1.3232e+00, 1.6422e-01,
        9.6287e-02, 1.3372e-01, 3.2747e-02, 3.8262e-02, 9.3834e-02, 3.1081e-02,
        6.0648e-01, 2.2640e-01, 1.3442e+00, 1.4206e-01, 1.5656e-01, 7.6020e-02,
        8.5088e-02, 4.2787e-02, 1.0242e+00, 5.9535e-03, 1.2556e-01, 2.4497e-01,
        1.4188e-01, 2.2333e-01, 7.0888e-02, 2.2360e+00, 2.6151e+00, 1.0741e+01,
        2.7653e+00, 5.6492e+00, 6.9075e+00, 1.1157e+01, 7.0683e-02, 6.0987e-02,
        9.8729e-02, 5.3787e-02, 1.5525e-02, 5.5893e-02, 9.2546e-02, 2.5309e-01,
        1.1723e-02, 2.4118e-01, 7.4276e+00, 8.0270e-01, 8.1713e-01, 1.8532e-01,
        5.6360e+00, 1.5628e-01, 1.5791e+00, 9.5679e-01, 3.6466e-02, 1.3685e-01,
        7.1854e-02, 6.5609e-01, 1.1716e+00, 7.3384e-01, 5.5266e-03, 2.5330e-01,
        3.3690e+00, 1.7517e+00, 5.4715e+00, 3.1485e+00, 9.8432e-01, 6.8207e+00,
        2.9743e-02, 1.7085e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [14/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 106.0
Sum Train Loss:  tensor([4.0110e+00, 8.4623e-01, 4.7505e+00, 6.5232e-01, 5.5596e-02, 5.0366e-01,
        8.1686e-02, 7.1288e-01, 3.6545e-01, 2.1577e-01, 1.0290e-01, 2.0436e-02,
        8.9267e-02, 1.2454e+00, 4.6689e-01, 3.9414e-01, 5.6322e-01, 2.1090e-01,
        2.3195e-01, 4.5426e-01, 1.7767e-02, 2.7502e-02, 1.4074e-01, 5.8145e-02,
        7.9377e-01, 2.1590e-01, 3.0242e+00, 9.9147e-02, 6.5080e-02, 1.2209e-01,
        5.4584e-02, 2.5051e-02, 7.2481e-01, 8.7500e-03, 4.3585e-02, 6.1082e-02,
        1.2023e-01, 2.1436e-01, 6.8284e-02, 2.1130e+00, 1.3703e+00, 7.0445e+00,
        2.1593e+00, 2.2860e+00, 2.6444e+00, 5.0360e+00, 8.7623e-02, 5.0076e-02,
        3.8946e-02, 3.5085e-01, 4.2673e-03, 3.4185e-02, 1.2489e-02, 4.7792e-02,
        5.1962e-02, 1.4363e-01, 1.0104e+01, 8.8863e-01, 2.5805e+00, 6.8431e-02,
        2.0519e+01, 2.4008e-01, 2.3574e+00, 8.9066e-01, 2.6261e-01, 4.0570e-01,
        6.8226e-01, 6.7618e-01, 4.2881e+00, 1.4121e+00, 5.1077e-03, 3.2978e-01,
        4.2932e+00, 3.1320e+00, 6.1071e+00, 4.5589e+00, 3.4769e+00, 1.3519e+00,
        1.5863e+00, 1.1592e+00], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [14/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 116.7
Sum Train Loss:  tensor([4.2583e+00, 6.2968e-01, 2.4801e+00, 4.4502e-01, 1.6304e-01, 2.9663e-01,
        3.6025e-02, 2.6830e-01, 5.2154e-01, 1.4024e-01, 4.8356e-02, 6.8036e-02,
        5.4455e-03, 8.2519e-01, 4.3065e-01, 6.9779e-01, 9.2643e-01, 1.2037e-01,
        1.8372e-01, 6.1142e-01, 5.5721e-03, 1.0849e-01, 6.7641e-03, 6.6629e-03,
        6.8175e-01, 3.1578e-01, 1.8949e+00, 3.0093e-01, 1.2616e-01, 6.3836e-02,
        1.3143e-01, 8.4888e-02, 2.8934e-01, 1.0775e-02, 2.4943e-02, 3.7098e-02,
        1.4050e-01, 2.2365e-01, 3.9405e-02, 1.8640e+00, 8.3949e-01, 6.2946e+00,
        7.7954e+00, 6.2313e+00, 1.3101e+01, 1.4803e+01, 1.5075e-01, 3.4867e-02,
        2.8152e-01, 1.8565e-01, 1.3532e-02, 1.0031e-01, 7.1610e-02, 1.6050e-01,
        5.5404e-02, 4.6395e-01, 5.8396e+00, 5.2221e-01, 1.4561e+00, 1.4321e-01,
        1.1493e+01, 1.4574e-01, 5.9166e-01, 5.5141e-01, 9.6516e-02, 2.7619e-01,
        3.0438e-01, 1.0825e+00, 2.0857e+00, 8.8971e-01, 4.2770e-03, 2.9484e-01,
        4.7410e+00, 3.2067e+00, 8.6043e+00, 1.4698e+01, 1.0495e+00, 1.5785e+00,
        2.4160e+00, 6.3269e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [14/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 132.8
Sum_Val Meta Model:  tensor([7.1550e+00, 2.7510e+00, 1.2552e+01, 1.7793e+00, 1.6594e-02, 4.6685e-01,
        4.4082e-02, 4.1094e-01, 2.6801e-01, 2.2745e-01, 5.0341e-02, 1.4333e-01,
        4.8700e-02, 5.8119e-01, 3.6013e-01, 3.1592e-01, 2.6101e+01, 1.1633e-01,
        3.8052e-02, 6.0084e-02, 2.9727e-03, 2.1086e-03, 4.0889e-03, 1.2853e-02,
        8.0002e-01, 2.6224e-01, 2.0360e+00, 6.8528e-02, 9.4450e-02, 2.1278e-01,
        2.4978e-02, 4.2342e-03, 6.7782e-01, 2.9762e-03, 3.0144e-02, 5.2481e-02,
        5.7491e-02, 1.3347e-01, 7.7607e-02, 4.4671e+00, 2.8878e+00, 1.0039e+01,
        3.3973e+00, 8.4447e+00, 1.4901e+01, 1.0682e+01, 1.7781e-02, 9.1620e-02,
        1.9096e-02, 1.2623e-01, 2.0836e-03, 8.7615e-03, 1.0875e-01, 3.7079e-02,
        8.8664e-03, 3.4322e-02, 9.2145e+00, 4.0239e-01, 2.1420e+00, 6.2342e-02,
        8.5825e+00, 4.5544e-01, 1.4253e+00, 3.7571e-01, 5.1876e-02, 6.0102e-02,
        1.1644e-01, 4.9844e-01, 2.9162e+00, 2.7389e+00, 4.7549e-03, 5.7509e-01,
        7.3646e+00, 2.3315e+00, 1.3498e+01, 7.8445e+00, 5.4675e-01, 8.5042e+00,
        4.2557e-02, 3.2906e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([6.3499e+00, 1.8730e+00, 8.4821e+00, 9.1640e-01, 3.8632e-03, 4.5806e-01,
        4.3020e-02, 3.8879e-01, 2.1776e-01, 2.2522e-01, 5.3814e-02, 1.3981e-01,
        6.1320e-02, 5.1261e-01, 3.6784e-01, 1.2570e+00, 1.6478e+01, 2.5868e-01,
        3.0890e-02, 8.7896e-02, 3.9923e-03, 2.1316e-03, 1.6205e-03, 2.6533e-02,
        7.0753e-01, 2.4172e-01, 1.7575e+00, 1.1275e-01, 1.4704e-01, 2.1623e-01,
        2.2841e-02, 4.2155e-03, 6.2785e-01, 5.3173e-03, 2.3197e-02, 3.7930e-02,
        1.1832e-01, 1.1904e-01, 3.2666e-02, 4.0643e+00, 2.9381e+00, 1.1910e+01,
        3.0466e+00, 7.5618e+00, 1.8105e+01, 1.1097e+01, 1.4736e-02, 9.5465e-02,
        9.9043e-03, 1.0265e-01, 5.8293e-04, 4.0961e-03, 1.0447e-01, 1.0976e-02,
        5.7087e-03, 1.4680e-02, 8.6946e+00, 4.6019e-01, 1.9650e+00, 1.3268e-01,
        1.0157e+01, 2.0695e-01, 1.0339e+00, 4.1617e-01, 3.3096e-02, 7.6685e-02,
        7.0927e-02, 6.1521e-01, 3.3510e+00, 2.0395e+00, 8.2750e-03, 6.0012e-01,
        6.6340e+00, 2.5323e+00, 1.5057e+01, 8.1795e+00, 5.1020e-01, 9.0368e+00,
        5.1969e-02, 3.2324e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.1734e+01, 4.6206e+01, 5.7999e+01, 2.1405e+01, 2.1980e-01, 1.2991e+01,
        4.4242e+00, 1.6516e+01, 4.7429e+00, 9.2849e+00, 6.3557e+00, 1.0882e+01,
        9.0919e+00, 1.3689e+01, 8.1054e+00, 3.1686e+01, 3.0483e+02, 7.8260e+00,
        9.2131e-01, 1.9022e+00, 1.0712e+00, 2.5199e-01, 1.2856e-01, 2.5599e+00,
        2.6758e+01, 1.6422e+01, 2.4849e+01, 7.5274e+00, 1.6295e+01, 1.1422e+01,
        1.4740e+00, 3.9866e-01, 8.1662e+00, 2.1270e+00, 1.4073e+00, 9.0835e-01,
        8.3347e+00, 4.6472e+00, 8.5473e-01, 4.3132e+01, 1.1962e+01, 2.5059e+01,
        6.2678e+00, 1.6291e+01, 2.2039e+01, 1.6214e+01, 1.0769e+00, 5.4199e+00,
        5.2217e-01, 4.1672e+00, 1.2110e-01, 3.3972e-01, 6.8504e+00, 4.7342e-01,
        5.9650e-01, 5.1105e-01, 2.5549e+01, 7.0537e+00, 1.2515e+01, 8.0124e+00,
        1.3663e+01, 4.9770e+00, 4.3822e+00, 6.4065e+00, 5.1188e-01, 2.2823e+00,
        7.3213e-01, 1.1141e+01, 1.0035e+01, 1.4790e+01, 3.5135e-01, 1.7702e+01,
        1.3960e+01, 1.3011e+01, 1.8310e+01, 8.2182e+00, 5.1066e-01, 9.0984e+00,
        1.6792e-01, 3.0931e-01], device='cuda:0')
Outer loop valEpocw Maximum [14/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 183.1
model_train val_loss valEpocw [14/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 173.4
model_train val_loss valEpocw [14/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1121.1
Sum_Val Meta Model:  tensor([8.9732e+00, 7.2355e-01, 1.0170e+01, 4.0039e-01, 1.0512e-02, 1.7354e+00,
        1.5462e+00, 1.7298e+00, 1.9111e+00, 1.1393e+00, 1.9207e-01, 3.8733e+00,
        8.9123e-02, 2.2574e-01, 7.4442e-01, 8.7556e-01, 7.5145e-01, 6.9398e-01,
        5.0534e-01, 2.4209e+00, 8.4696e-04, 7.6230e-04, 8.0373e-03, 6.1281e-02,
        7.6554e-01, 4.0836e-01, 2.4061e+00, 3.0197e-01, 1.6931e-01, 2.2526e-03,
        3.4378e-03, 1.7395e-03, 1.8817e-02, 6.9303e-04, 1.5658e-03, 3.7790e-03,
        7.2432e-02, 6.6898e-03, 3.3981e-03, 1.9780e-01, 4.9708e-02, 1.9476e+00,
        8.0652e-02, 1.7671e-01, 3.4874e-01, 2.9380e+00, 1.5109e-02, 9.3101e-03,
        2.8857e-03, 1.9349e-01, 3.9063e-04, 1.7555e-03, 9.5922e-04, 2.3093e-03,
        2.0734e-03, 1.0233e-01, 4.1322e+00, 4.4856e-01, 1.4642e+00, 7.1133e-02,
        1.5184e+00, 1.2981e-02, 5.9453e-01, 1.3838e-01, 2.7590e-02, 3.4831e-02,
        5.7998e-02, 9.4741e-01, 6.3744e-02, 6.3875e-01, 1.7728e-03, 1.9332e-02,
        2.3787e+00, 9.9816e-01, 3.3590e+00, 3.9261e-01, 1.7322e-01, 5.9758e-01,
        1.5241e-02, 1.5415e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([8.4489e+00, 8.4347e-01, 7.8919e+00, 4.9449e-01, 4.4649e-02, 1.5350e+00,
        8.8613e-01, 1.2519e+00, 1.6683e+00, 8.2514e-01, 1.5604e-01, 1.1958e+00,
        1.0055e-01, 3.1924e-01, 8.0344e-01, 1.5140e-01, 6.7435e-01, 5.9849e-01,
        1.2569e-01, 1.2113e+00, 3.9276e-03, 1.8337e-03, 5.3821e-03, 5.4579e-02,
        7.3507e-01, 3.7078e-01, 2.4997e+00, 2.7526e-01, 1.5912e-01, 2.9744e-02,
        1.3303e-02, 2.8372e-03, 6.6379e-02, 1.3546e-02, 1.1363e-02, 9.7301e-03,
        6.6723e-02, 6.0734e-02, 9.9936e-03, 1.7578e-01, 3.9187e-02, 1.2917e+00,
        5.1150e-02, 1.5391e-01, 1.4382e-01, 1.5571e+00, 9.8592e-03, 1.1233e-02,
        3.9681e-03, 1.5247e-01, 3.9635e-04, 2.2600e-03, 7.6272e-03, 1.5207e-02,
        2.5352e-03, 4.0399e-02, 3.1905e+00, 3.0771e-01, 7.2012e-01, 2.1786e-02,
        1.4530e+00, 2.5317e-02, 1.6581e-01, 6.7465e-02, 1.5816e-02, 1.0879e-02,
        2.5437e-02, 1.0141e+00, 4.6944e-02, 4.0514e-01, 3.8977e-04, 7.3454e-03,
        1.3243e+00, 5.7741e-01, 5.1711e+00, 3.0402e-01, 3.9852e-01, 1.3059e+00,
        5.7359e-03, 7.0171e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.4581e+01, 1.9256e+01, 4.7295e+01, 1.0157e+01, 2.2054e+00, 3.4998e+01,
        5.3663e+01, 3.9716e+01, 2.9417e+01, 3.0565e+01, 1.5058e+01, 6.7116e+01,
        1.0176e+01, 8.7992e+00, 1.5430e+01, 2.8601e+00, 1.0974e+01, 1.3851e+01,
        2.1866e+00, 1.5938e+01, 7.1631e-01, 1.6127e-01, 3.0401e-01, 4.2812e+00,
        2.3678e+01, 1.8719e+01, 3.1912e+01, 1.3552e+01, 1.2145e+01, 1.2548e+00,
        6.7178e-01, 2.0111e-01, 7.8673e-01, 3.7060e+00, 5.7133e-01, 2.1557e-01,
        3.7047e+00, 1.9009e+00, 2.2555e-01, 1.9877e+00, 1.8451e-01, 2.9562e+00,
        1.1610e-01, 3.5629e-01, 1.8181e-01, 2.4433e+00, 5.2311e-01, 5.0002e-01,
        1.5681e-01, 4.6873e+00, 5.5740e-02, 1.5583e-01, 4.1264e-01, 5.3612e-01,
        1.9648e-01, 1.1881e+00, 9.2839e+00, 3.8933e+00, 4.3990e+00, 9.5463e-01,
        2.0385e+00, 5.4843e-01, 6.6840e-01, 9.4297e-01, 2.2887e-01, 2.5637e-01,
        2.5739e-01, 1.8997e+01, 1.4942e-01, 3.0214e+00, 1.3795e-02, 1.9525e-01,
        3.2165e+00, 2.8657e+00, 6.7466e+00, 3.0587e-01, 3.9901e-01, 1.3164e+00,
        1.8482e-02, 6.3402e-02], device='cuda:0')
Outer loop valEpocw Maximum [14/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 67.1
model_train val_loss valEpocw [14/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 53.8
model_train val_loss valEpocw [14/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 690.4
Sum_Val Meta Model:  tensor([9.7860e+00, 1.1769e-01, 1.5925e+00, 9.7873e-02, 1.5589e-02, 8.3110e-02,
        1.2635e-02, 2.7402e-01, 9.7682e-02, 4.9042e-02, 1.4464e-02, 1.6209e-02,
        3.3850e-03, 6.3247e-01, 1.2197e-01, 1.2099e-01, 3.4772e-01, 6.3940e-02,
        6.5317e-02, 1.1405e-01, 3.0734e-03, 1.3952e-02, 1.2667e-01, 2.3926e-02,
        1.5338e-01, 2.9243e-02, 1.2721e+00, 1.1302e-01, 9.6722e-03, 7.4949e-02,
        7.8165e-02, 8.6902e-02, 6.9293e-01, 1.1018e-03, 5.4410e-02, 1.1621e-01,
        2.8386e-01, 3.8979e-01, 1.8262e-02, 2.4387e+00, 6.9697e+00, 3.2057e+01,
        3.8851e+01, 3.6352e+01, 4.0499e+01, 4.5311e+01, 2.3910e-01, 2.1827e-01,
        1.4137e+00, 4.0289e-01, 1.9467e-01, 5.3804e-01, 1.0437e+00, 3.9716e-01,
        8.0591e-01, 2.7568e+00, 5.0283e+01, 8.4361e-01, 1.3456e+00, 5.8057e-02,
        6.8805e+01, 6.9983e-02, 2.5700e+00, 8.7761e-01, 8.8993e-01, 5.3327e-02,
        7.3579e-01, 5.2388e-01, 1.4861e+00, 3.0860e-01, 1.1482e-02, 2.3331e-01,
        2.2889e+00, 6.2667e+00, 1.5175e+00, 1.8875e+01, 9.6793e+00, 1.6708e+00,
        9.7371e-02, 9.3382e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([2.7018e+00, 2.4465e-02, 5.0501e-01, 1.6413e-02, 1.2166e-03, 6.2954e-03,
        1.2382e-03, 2.6192e-01, 9.5867e-03, 2.4737e-03, 5.0746e-04, 7.8672e-04,
        2.2757e-04, 4.8692e-01, 1.3912e-02, 6.3322e-02, 8.9245e-02, 2.3767e-02,
        9.4579e-03, 6.2610e-03, 1.7941e-04, 1.3738e-04, 1.9505e-04, 6.9539e-04,
        9.0628e-02, 2.1449e-02, 1.4625e+00, 9.4254e-02, 8.9748e-03, 6.1671e-03,
        6.8257e-03, 8.6163e-02, 5.9297e-01, 7.1677e-04, 1.2617e-02, 2.9475e-02,
        1.7716e-01, 3.0051e-01, 1.0390e-02, 3.3458e+00, 5.2385e+00, 2.8219e+01,
        3.4567e+01, 3.5248e+01, 4.5173e+01, 4.2622e+01, 1.3226e-01, 1.2512e-01,
        8.3464e-01, 2.2975e-01, 1.1355e-01, 5.3638e-01, 1.0366e+00, 5.5145e-01,
        5.7649e-01, 2.2000e+00, 2.9873e+01, 9.3068e-01, 1.4980e+00, 5.0371e-02,
        7.2744e+01, 2.0739e-02, 2.1283e+00, 7.9826e-01, 6.8502e-01, 1.1381e-01,
        7.7276e-01, 6.0129e-01, 1.9032e+00, 5.1210e-01, 2.6379e-03, 2.4241e-01,
        1.4487e+00, 6.2871e+00, 3.4116e+00, 2.8535e+01, 1.0781e+01, 2.5559e+00,
        3.1832e-02, 2.6198e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.4836e+01, 7.6897e-01, 3.7308e+00, 4.5696e-01, 8.6098e-02, 2.0735e-01,
        1.6872e-01, 1.4262e+01, 2.5824e-01, 1.2738e-01, 7.5179e-02, 7.4040e-02,
        4.3142e-02, 1.6438e+01, 3.7720e-01, 1.9005e+00, 1.8150e+00, 8.2459e-01,
        2.8664e-01, 1.5169e-01, 7.3456e-02, 1.8538e-02, 1.4640e-02, 8.3914e-02,
        4.4075e+00, 1.9682e+00, 2.7389e+01, 8.1065e+00, 1.2298e+00, 3.6368e-01,
        5.2543e-01, 8.2367e+00, 8.0292e+00, 3.4000e-01, 8.3063e-01, 7.8168e-01,
        1.2576e+01, 1.1674e+01, 3.2233e-01, 4.2817e+01, 2.4661e+01, 5.8257e+01,
        6.5916e+01, 7.0860e+01, 5.4264e+01, 6.1003e+01, 9.4175e+00, 7.6146e+00,
        3.7461e+01, 9.5301e+00, 2.2539e+01, 4.2688e+01, 6.6015e+01, 2.5390e+01,
        6.3968e+01, 7.8283e+01, 9.3263e+01, 1.5870e+01, 1.0643e+01, 3.3023e+00,
        9.6111e+01, 6.0034e-01, 9.5837e+00, 1.4732e+01, 1.2746e+01, 3.7289e+00,
        1.0046e+01, 1.5279e+01, 6.8256e+00, 4.6213e+00, 1.4188e-01, 8.7437e+00,
        3.7107e+00, 3.6746e+01, 4.3769e+00, 2.8644e+01, 1.0789e+01, 2.5699e+00,
        1.2563e-01, 2.8470e-01], device='cuda:0')
Outer loop valEpocw Maximum [14/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 397.2
model_train val_loss valEpocw [14/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 373.8
model_train val_loss valEpocw [14/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1288.0
Sum_Val Meta Model:  tensor([1.8684e+01, 1.0011e-01, 4.0134e+00, 5.6132e-02, 1.1359e-02, 6.1063e-01,
        9.5184e-02, 5.7692e-01, 8.4346e-02, 6.7552e-01, 8.0730e-02, 1.1388e-01,
        2.0561e-03, 8.4670e-01, 9.0887e-01, 7.6843e-02, 1.0933e-01, 3.1906e-02,
        2.1754e-02, 3.7996e-02, 2.2460e-03, 5.0629e-03, 1.0775e-02, 1.4866e-02,
        8.4224e-01, 3.1943e-02, 2.6096e+00, 4.1426e-02, 3.1247e-01, 1.2770e-02,
        1.5967e-02, 3.1309e-03, 3.0712e-01, 1.3483e-02, 4.3288e-02, 3.0812e-01,
        8.4598e-03, 2.3367e-02, 2.2549e-01, 3.2770e+00, 4.0688e+00, 1.5241e+01,
        4.7775e+00, 4.9265e+00, 8.6252e+00, 1.0746e+01, 1.1546e-02, 1.5167e-02,
        4.5850e-02, 1.6449e-02, 7.7284e-03, 1.3025e-02, 1.2400e-02, 1.5623e-01,
        1.4641e-02, 6.8589e-02, 8.0094e+00, 3.6621e-01, 2.6786e+00, 3.8456e-02,
        3.3393e+01, 2.9397e-02, 1.1083e+00, 4.1049e-01, 2.6648e-01, 9.0108e-02,
        4.6161e-01, 3.9610e+00, 5.2600e-01, 4.9138e-01, 3.9742e-03, 8.4821e-02,
        2.9141e+00, 2.1026e+00, 3.2882e+02, 1.1244e+01, 1.7715e+00, 9.1467e+00,
        2.1734e-02, 1.5093e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.6083e+00, 1.6043e-01, 2.6961e+00, 9.2795e-02, 1.9017e-02, 4.9186e-01,
        1.4964e-01, 4.7308e-01, 1.2522e-01, 6.0619e-01, 5.1389e-02, 2.2095e-01,
        6.1456e-03, 7.6858e-01, 1.0640e+00, 8.1372e-02, 6.0059e-02, 5.4148e-02,
        5.5550e-03, 8.8404e-03, 2.7056e-03, 5.5223e-04, 5.8392e-03, 3.0673e-02,
        7.6041e-01, 6.2885e-02, 2.2260e+00, 5.1716e-02, 3.0334e-01, 4.3232e-03,
        8.1281e-03, 2.7226e-03, 3.4273e-02, 2.3396e-03, 6.7728e-03, 1.1080e-02,
        1.8792e-02, 7.4466e-03, 1.1798e-02, 3.1951e-01, 1.9259e-01, 6.6268e-01,
        2.6035e-01, 3.1899e-01, 1.7594e-01, 9.9482e-01, 4.0450e-03, 5.5239e-03,
        2.7882e-03, 2.9203e-03, 3.2437e-04, 1.0336e-03, 1.9016e-03, 6.0798e-03,
        2.4898e-03, 3.7986e-02, 1.4173e+00, 6.7853e-02, 2.1124e+00, 2.5223e-02,
        8.0657e+00, 2.6515e-02, 2.1944e-01, 7.0246e-02, 1.7173e-02, 4.3687e-02,
        3.6501e-02, 6.3509e-01, 8.9169e-02, 5.8532e-02, 1.0212e-03, 1.6076e-02,
        4.8165e-02, 1.2404e+00, 5.0063e+01, 3.8452e+00, 7.2096e-02, 4.5958e+00,
        9.3998e-03, 2.4586e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.5589e+01, 3.2762e+00, 1.6593e+01, 1.6314e+00, 7.0438e-01, 1.0030e+01,
        9.3608e+00, 1.4224e+01, 2.1075e+00, 1.7738e+01, 3.6495e+00, 1.0412e+01,
        5.2361e-01, 1.5856e+01, 1.8713e+01, 1.5792e+00, 9.2974e-01, 1.1493e+00,
        1.1425e-01, 1.5520e-01, 4.1337e-01, 3.7689e-02, 2.8247e-01, 1.9518e+00,
        2.1950e+01, 2.8595e+00, 2.5547e+01, 2.0875e+00, 1.8752e+01, 1.5781e-01,
        3.5863e-01, 1.5751e-01, 3.1154e-01, 4.3723e-01, 2.5330e-01, 1.6646e-01,
        8.3308e-01, 2.0406e-01, 1.9928e-01, 2.7910e+00, 7.0161e-01, 1.3747e+00,
        5.3479e-01, 6.9550e-01, 2.2308e-01, 1.5527e+00, 1.9586e-01, 2.2395e-01,
        9.3737e-02, 8.6622e-02, 3.6688e-02, 5.8489e-02, 8.4790e-02, 1.6602e-01,
        1.6439e-01, 9.8240e-01, 3.7961e+00, 7.9376e-01, 1.1312e+01, 9.8061e-01,
        1.1097e+01, 4.6329e-01, 8.5274e-01, 8.1975e-01, 1.9928e-01, 8.7595e-01,
        3.0372e-01, 7.9101e+00, 2.7317e-01, 3.9081e-01, 3.0198e-02, 3.3296e-01,
        1.0691e-01, 5.8334e+00, 6.8020e+01, 3.8769e+00, 7.2255e-02, 4.6490e+00,
        3.7297e-02, 1.7535e-02], device='cuda:0')
Outer loop valEpocw Maximum [14/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 492.1
model_train val_loss valEpocw [14/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 92.1
model_train val_loss valEpocw [14/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 374.3
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [86.84957542 97.39769252 93.00203458 98.1055299  98.4259451  97.57678391
 98.26878328 95.21326495 98.04826939 96.95910138 98.59650833 98.94372632
 99.42130335 95.34971552 97.50003046 97.33434047 96.40111597 97.70226971
 98.83529684 98.43081834 98.64645899 99.2336838  99.51145819 98.97296573
 95.26443391 96.8372705  94.42014595 96.98103093 98.07507218 98.25903681
 98.35162827 98.58798017 96.9347352  98.54655767 98.67082516 98.82189544
 97.60724163 98.40036062 98.93885308 93.37483705 98.01537506 94.10825891
 97.45373473 96.75808043 97.47566428 95.99420085 98.17253688 98.64036744
 98.13476931 98.72199413 98.78412787 98.61965619 99.01316992 98.29802268
 98.7183392  97.5755656  92.38678866 96.98590417 96.49248913 97.47566428
 95.19133539 98.64767729 97.39891083 97.2405307  98.78169126 97.62186133
 98.54412105 95.96617975 98.843825   98.32726209 99.81603538 97.35748833
 98.67813501 95.91379247 97.99222719 97.84115691 99.33724004 99.09235999
 99.84405648 99.15083881]
Accuracy th:0.7 is [83.78431062 97.32946723 92.24059161 97.99588212 98.20908615 97.57922053
 98.08725527 95.02320878 97.94593146 96.79219308 98.5514309  98.91814184
 99.4152118  95.32047612 97.3940376  97.24784055 96.33898222 97.63648104
 98.76828986 98.36381136 98.49173378 99.20078946 99.47369062 98.93885308
 95.22788465 96.74102411 94.24227288 96.97006615 98.02512153 98.20664953
 98.23223401 98.57701539 96.87503807 98.51853657 98.55630414 98.63914913
 97.3806362  98.24198048 98.76707155 93.03858384 97.94105822 93.55880167
 97.45129811 96.56558765 97.22103776 95.49713088 98.13111439 98.64402237
 98.08238204 98.69153641 98.70615611 98.56605061 99.02047977 98.27852975
 98.70981104 97.51221355 91.59732459 96.75686212 96.51929192 97.56581913
 94.42258257 98.54899429 97.16865048 97.34774186 98.80605743 97.47322767
 98.62331112 95.95277835 98.83042361 98.19690306 99.81603538 96.97737601
 98.56848723 95.93572203 97.86917801 98.29680438 99.30921894 99.07530366
 99.84405648 99.14718388]
Avg Prec: is [96.25886152 33.7789978  71.02287439 66.43134985 76.30931955 64.24837746
 72.29617559 46.94855302 56.53425482 50.41608632 29.75184966 52.18025845
 22.10589454 26.81364648 33.13707249 56.03352331 27.91844548 39.5709539
 45.07988752 37.26830476 58.8608104  44.10743806 88.9827303  81.18081271
 25.75973011 30.84372383 37.78424037 37.71869686 23.38764178 36.20545984
 71.72332032 36.92628448 56.58284167 60.47675099 71.83762391 78.63718394
 57.16333248 75.17248861 86.56075946 45.75362645 38.72298599 64.72452371
 59.04820018 53.4687406  60.98982269 71.58895616 35.09426777 31.32262685
 40.37807117 42.67544929 60.23356786 34.26802929 21.01724731 69.67250041
 23.09772681 37.70749879 67.95893502 57.70538789 39.55608885 56.93814578
 84.15555414 82.03101378 67.25410735 45.97678588 56.95577889 42.02537217
 57.14057162 24.11976689 49.32004894 65.47976916  9.96223159 72.79983455
 71.47952484 48.25324447 74.11831667 78.87907133 50.07088508 76.40572657
  5.36949531 23.79434611]
Accuracy th:0.5 is [45.58545827 97.2137279  72.48693364 97.02489005 97.26733349 77.34676722
 77.70129506 76.55486653 78.58091398 96.44863001 78.89645594 98.52097319
 99.41399349 80.36329967 78.39207612 96.56680596 96.29512311 78.17643547
 98.65376884 98.30776915 80.64351068 79.39718083 98.38695922 78.32994237
 80.74706692 96.65086926 94.0778012  77.92424556 98.01293844 78.80873771
 97.30875598 98.57457877 96.36213009 98.02024829 87.67558875 78.41522399
 78.19958334 90.91994493 97.11504489 75.40356477 79.68104677 92.05906361
 77.62697823 77.23955605 96.9627563  93.87434364 98.02877645 98.57336046
 96.47786942 88.52840487 87.45385656 98.55508583 98.99976852 77.85480196
 98.70615611 78.11795665 72.71353907 93.41869617 96.24273583 96.9067141
 89.79300934 97.17717864 92.40871822 78.22394951 98.42838172 78.62964632
 98.20786784 77.26270391 79.30337106 97.55972759 79.86135646 95.99054592
 78.92935028 95.45083515 77.45154177 82.96926207 87.0981104  78.8635616
 79.91374374 99.14718388]
Accuracy th:0.7 is [45.67073988 97.2137279  72.48693364 97.02489005 97.26733349 77.34676722
 77.70129506 76.73883115 78.58091398 96.4742145  78.89645594 98.52097319
 99.41399349 80.81285559 78.39207612 96.56680596 96.29512311 78.17643547
 98.65376884 98.30776915 81.07966521 79.39718083 98.38695922 78.5565478
 81.33063681 96.65086926 94.0778012  77.92424556 98.01293844 78.80873771
 97.30875598 98.57457877 96.36213009 98.02024829 87.83153227 78.41522399
 78.19958334 91.16117006 97.11504489 75.40356477 80.26096173 92.05906361
 77.62697823 77.23955605 96.9627563  93.87434364 98.02877645 98.57336046
 97.38550944 89.12293954 87.60980008 98.55508583 98.99976852 77.85480196
 98.70615611 78.11795665 72.71353907 93.79271695 96.24273583 96.9067141
 89.79300934 97.17717864 92.57440821 78.22394951 98.42838172 78.62964632
 98.20786784 77.26270391 79.30337106 97.55972759 79.86135646 95.99054592
 79.00732204 95.45083515 77.45154177 83.05210706 87.24918069 78.8635616
 79.91374374 99.14718388]
Avg Prec: is [56.14739166  3.22987658 11.16389006  3.39411223  2.23130184  3.77235976
  3.3244348   5.64484981  2.47444551  3.74698337  1.5898145   1.62026895
  0.62921531  5.19458072  2.70328801  3.10168722  3.6026473   2.67944434
  1.37405906  1.7234252   1.95363198  0.86004415  1.73976475  2.52408189
  5.27448364  3.61357968  6.6877188   3.3669166   2.07278222  1.85309046
  2.65472699  1.35605966  3.68324496  1.62574108  2.41690054  2.4385421
  3.08365462  2.58853349  2.89336719  7.48104881  2.28132809  8.35088672
  3.40923238  4.10374841  3.20178882  6.36867938  2.14975472  1.5312533
  2.17553184  1.58403192  1.77399037  1.54090515  1.05522671  2.89053828
  1.27673411  2.7074405  11.43521065  3.73502935  3.85986364  2.76098646
 10.75539723  2.17104345  3.82276214  2.93245471  1.525223    2.52692922
  1.79821351  4.29452042  1.25751679  2.48346696  0.20607666  3.44894875
  1.97754918  4.64949198  3.96384143  3.08196337  0.90402246  1.85115366
  0.12867273  0.76321493]
mAP score regular 52.02, mAP score EMA 3.78
starting validation
Accuracy th:0.5 is [88.09078905 97.38395994 93.22570197 98.25099036 98.80409597 97.65802128
 98.4652565  95.39826096 98.13887436 96.98781673 98.62720183 98.98846451
 99.35720158 95.18150335 97.43378927 97.33163914 96.34750978 97.69290181
 98.94610957 98.38802103 98.87883997 99.27000025 99.62129706 99.19525625
 95.42068416 96.75860179 94.51379027 97.18713407 97.89471062 98.17873782
 98.7119117  98.69447144 96.83334579 98.65460797 98.87385704 99.13047811
 98.00682662 98.34566609 99.07566584 93.35027531 97.94204848 93.51720358
 97.1995914  96.58918205 97.10491566 94.93484815 98.28836236 98.73184344
 98.10399382 98.74679224 98.86139971 98.6222189  98.83399357 98.36310636
 98.73184344 97.73276528 91.17024192 97.2070658  96.36245858 97.49856741
 93.33283504 98.76672397 97.229489   97.15972793 98.69197997 97.61566634
 98.5101029  95.81184443 98.7567581  98.27092209 99.81563146 97.58826021
 98.4428333  95.48795376 97.304233   96.47208312 99.21269651 98.60477863
 99.82559733 99.15290131]
Accuracy th:0.7 is [86.66317861 97.341605   92.81460996 98.25099036 98.66208237 97.66300421
 98.32075143 95.33099135 98.09402795 96.81839699 98.58982983 99.00341331
 99.35969305 95.11921668 97.416349   97.21952313 96.30515484 97.72778235
 98.88382291 98.37805516 98.7866557  99.20023918 99.59139946 99.17283305
 95.45556469 96.71624685 94.48389267 97.1098986  97.85235568 98.18621222
 98.55744077 98.66955677 96.86822632 98.67952263 98.7343349  98.94860104
 97.80003488 98.19119516 98.98846451 93.10611157 97.90965942 93.44744251
 97.35406234 96.67140045 97.0874754  94.94232255 98.29085383 98.80658744
 98.04669009 98.7343349  98.76174104 98.60976157 98.88133144 98.37307223
 98.70194584 97.67546154 90.99334778 97.0276802  96.40979645 97.58078581
 93.1260433  98.73184344 97.16720233 97.43129781 98.7941301  97.52348207
 98.61972743 95.7844383  98.7567581  98.17624636 99.81563146 97.341605
 98.46774796 95.9414007  97.47365274 97.117373   99.26252585 98.69197997
 99.82559733 99.15290131]
Avg Prec: is [96.48094273 32.41200098 70.86028579 73.04561312 76.01779394 64.61110602
 78.97766543 48.03470837 61.2291324  54.19098026 34.88513543 55.93650836
 20.43235606 30.64729582 33.52123714 60.74373941 30.4655108  42.53459351
 48.03063123 35.85946183 68.73552213 54.81260292 92.19550121 86.63152012
 25.30682886 35.40584681 34.29598252 42.68668216 26.34819719 35.99747606
 75.78789906 38.3979196  54.50586562 62.565703   73.20750722 82.06916103
 59.62308959 77.61835602 89.41831829 44.90666452 34.05970469 53.92659029
 47.9613691  41.55034514 35.77445332 54.64873203 33.67864967 26.32981192
 40.22833005 40.64131961 65.56652539 33.17762867 20.95247257 74.81124002
 28.42519383 38.57028042 58.0808542  57.61192045 37.79437158 60.78233827
 68.7211985  86.96594847 65.89866684 49.54049553 60.77396271 37.109553
 60.14977985 25.4075099  43.0913686  66.43168025  9.16200455 75.0220797
 57.3061868  43.99606832 66.16798232 53.87168797 18.20042389 53.93983803
  2.3543383  21.51491061]
Accuracy th:0.5 is [45.31977975 97.22450607 70.87475397 96.96290206 97.90716795 76.4207589
 76.51294317 75.32949647 77.95051947 96.41976231 78.13987094 98.5325261
 99.34972718 78.42638962 78.04270374 96.31262924 96.21047911 77.48959813
 98.78167277 98.34068316 79.3258091  78.77519496 98.31327703 77.81847173
 78.38652615 96.52938685 94.3393876  77.51202133 97.81747515 78.10249894
 97.52597354 98.67204823 96.39983058 98.18870369 88.65386053 77.71881307
 77.6714752  92.23409821 97.0276802  74.75645913 78.05765254 92.37362035
 76.84679971 76.50546877 97.03764606 94.02795426 98.18621222 98.77668984
 97.14228766 88.34990159 85.97304233 98.55993223 98.87385704 76.84181678
 98.6969629  77.51949573 71.40792785 94.38672547 96.16314124 96.78102499
 90.13379176 97.04761193 92.40351795 77.47714079 98.32075143 78.32174801
 98.13139996 76.64249944 78.79761816 97.53593941 79.2510651  96.07843137
 78.51359095 95.44559882 76.5702469  83.85778708 88.9329048  78.15481974
 79.32082617 99.15040985]
Accuracy th:0.7 is [45.48919949 97.22450607 70.87475397 96.96290206 97.90716795 76.4207589
 76.51294317 75.41669781 77.95051947 96.41976231 78.13987094 98.5325261
 99.34972718 78.82751576 78.04270374 96.31262924 96.21047911 77.48959813
 98.78167277 98.34068316 79.66464858 78.77519496 98.31327703 77.910656
 78.80509256 96.52938685 94.3393876  77.51202133 97.81747515 78.10249894
 97.52597354 98.67204823 96.39983058 98.18870369 88.843212   77.71881307
 77.6714752  92.46082169 97.0276802  74.75645913 78.51109948 92.37362035
 76.84679971 76.50546877 97.03764606 94.02795426 98.18621222 98.77668984
 97.62064928 88.61648853 86.0801754  98.55993223 98.87385704 76.84181678
 98.6969629  77.51949573 71.40792785 94.64832947 96.16314124 96.78102499
 90.13379176 97.04761193 92.56048035 77.47714079 98.32075143 78.32174801
 98.13139996 76.64249944 78.79761816 97.53593941 79.2510651  96.07843137
 78.52355682 95.44559882 76.5702469  83.93003961 89.09734161 78.15481974
 79.32082617 99.15040985]
Avg Prec: is [53.95419897  3.75717576 14.92910546  4.5330664   1.44801029  4.249939
 13.84339879  8.68670123  8.11526603  5.2839913   2.3403442   4.74236237
  2.30508278  5.86768357  2.95345599  3.65570593 24.05669935  6.45726285
  1.55040315  2.72974508  3.51285527  1.50858412  1.20132062  5.20149014
  5.63814827  9.11307527  7.84251676  4.62349989  3.98617177  5.5181758
  2.224756    0.85639494  3.09240893  1.09933472  1.67197097  2.23791629
  1.98458609  2.19385117  2.25080432  6.17255413  1.72800411  6.01944517
  2.19454279  2.71433623  2.37637492  4.83397778  1.6603723   1.00181072
  1.39279505  1.13768502  1.16182952  0.96012266  0.73116652  2.30752108
  0.84708406  1.8231039  10.04763223  2.93564734  3.84226654  2.84436268
  7.82140559  2.06850245  3.09698908  2.49746815  1.34227257  1.83298991
  1.49831436  3.46266665  1.08466057  2.2513453   0.19219077  3.24967392
  1.58627228  3.94296231  3.55001146  2.31561607  0.59686366  1.49205856
  0.12692816  0.5794695 ]
mAP score regular 50.75, mAP score EMA 4.31
Train_data_mAP: current_mAP = 52.02, highest_mAP = 52.02
Val_data_mAP: current_mAP = 50.75, highest_mAP = 50.75
tensor([0.1079, 0.0319, 0.1359, 0.0360, 0.0146, 0.0295, 0.0075, 0.0189, 0.0399,
        0.0195, 0.0067, 0.0108, 0.0055, 0.0310, 0.0383, 0.0341, 0.0454, 0.0279,
        0.0312, 0.0391, 0.0026, 0.0072, 0.0108, 0.0082, 0.0225, 0.0118, 0.0636,
        0.0125, 0.0076, 0.0147, 0.0126, 0.0086, 0.0707, 0.0019, 0.0136, 0.0378,
        0.0114, 0.0211, 0.0327, 0.0825, 0.2257, 0.4653, 0.5249, 0.4877, 0.8518,
        0.6899, 0.0112, 0.0141, 0.0159, 0.0198, 0.0041, 0.0100, 0.0122, 0.0182,
        0.0076, 0.0243, 0.3367, 0.0568, 0.1625, 0.0136, 0.7449, 0.0351, 0.2261,
        0.0553, 0.0570, 0.0290, 0.0870, 0.0451, 0.3722, 0.1318, 0.0192, 0.0298,
        0.4618, 0.1839, 0.8295, 0.9967, 0.9995, 0.9957, 0.3502, 0.1078],
       device='cuda:0')
Sum Train Loss:  tensor([3.9667e+00, 4.7030e-01, 2.6706e+00, 3.2833e-01, 8.0045e-02, 4.0581e-01,
        9.4172e-02, 3.3359e-01, 4.7794e-01, 3.6678e-01, 2.2569e-02, 1.0966e-01,
        3.4053e-03, 7.4910e-01, 3.3029e-01, 2.7931e-01, 6.9643e-01, 1.1783e-01,
        1.1891e-01, 7.4214e-02, 2.4837e-02, 1.5090e-02, 2.2205e-03, 1.7107e-02,
        5.2982e-01, 1.5926e-01, 1.4064e+00, 1.8891e-01, 7.6079e-02, 1.4285e-02,
        1.5069e-01, 4.5144e-02, 7.1566e-01, 3.9114e-03, 2.4925e-02, 4.3203e-02,
        1.2854e-01, 1.1219e-01, 3.9998e-01, 2.2725e+00, 1.2418e+00, 5.9847e+00,
        5.1701e+00, 5.9144e+00, 5.1348e+00, 6.9528e+00, 1.1258e-01, 1.6550e-01,
        8.2452e-02, 3.7843e-02, 4.4750e-02, 5.2810e-02, 5.7054e-02, 7.9385e-02,
        3.4995e-02, 1.1057e-01, 7.3883e+00, 1.1128e+00, 2.1672e+00, 2.2978e-01,
        9.5578e+00, 1.2473e-01, 1.7830e+00, 6.8564e-01, 1.1126e-01, 2.4780e-01,
        5.2260e-01, 4.4249e-01, 6.3039e-01, 1.1268e+00, 4.0690e-02, 2.4191e-01,
        1.0787e+00, 3.4897e+00, 8.5432e+00, 8.2900e+00, 8.5393e+00, 9.2965e+00,
        1.8542e-01, 9.0915e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [15/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 115.9
Sum Train Loss:  tensor([3.2492e+00, 2.7805e-01, 2.5534e+00, 2.0224e-01, 5.5082e-02, 3.8773e-01,
        1.8845e-02, 2.4966e-01, 1.5995e-01, 2.0169e-01, 2.3170e-02, 1.2692e-01,
        2.3803e-02, 1.0318e+00, 2.9336e-01, 4.6894e-01, 8.4536e-01, 3.2124e-01,
        8.5483e-02, 3.2148e-01, 1.8325e-02, 2.8010e-02, 5.1848e-02, 1.0402e-02,
        4.6907e-01, 9.4166e-02, 1.0705e+00, 1.4433e-01, 6.1327e-02, 4.7008e-02,
        3.4835e-02, 2.7703e-02, 1.0855e+00, 6.8867e-03, 3.6518e-02, 8.8164e-02,
        3.1300e-02, 4.8908e-02, 1.1347e-01, 2.0758e+00, 2.1802e+00, 1.0374e+01,
        3.6233e+00, 5.3514e+00, 3.7089e+00, 1.2896e+01, 5.5544e-02, 4.3000e-02,
        7.4933e-02, 1.2318e-01, 1.1758e-02, 8.8773e-02, 2.0463e-02, 9.4587e-02,
        4.7127e-02, 1.3531e-01, 8.2964e+00, 5.1235e-01, 2.2356e+00, 1.1274e-01,
        8.7206e+00, 7.6860e-02, 1.2248e+00, 4.6511e-01, 2.2220e-01, 2.0575e-01,
        2.6056e-01, 6.1851e-01, 7.0939e-01, 1.1258e+00, 1.0586e-02, 1.5229e-01,
        2.1602e+00, 2.6785e+00, 4.3729e+00, 4.1502e+00, 7.8505e-01, 1.3243e+01,
        5.4429e-02, 2.8167e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [15/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 107.9
Sum Train Loss:  tensor([3.4018e+00, 2.1561e-01, 2.6157e+00, 1.3871e-01, 2.0460e-01, 4.3752e-01,
        8.0621e-02, 3.2175e-01, 2.1349e-01, 8.8628e-02, 2.0947e-02, 1.2234e-02,
        3.1783e-03, 8.8378e-01, 3.5830e-01, 2.8875e-01, 8.1472e-01, 2.6140e-01,
        1.1367e-01, 4.9845e-01, 1.2125e-02, 2.1644e-02, 3.4169e-02, 1.8302e-02,
        3.8967e-01, 1.9141e-01, 1.1166e+00, 1.8583e-01, 1.3695e-01, 9.6723e-02,
        7.2712e-02, 1.1648e-02, 3.5267e-01, 8.4380e-03, 5.3658e-02, 1.0342e-01,
        2.7570e-01, 1.5458e-01, 2.5869e-01, 1.4831e+00, 3.7294e+00, 9.3999e+00,
        5.9184e+00, 2.5765e+00, 1.6837e+00, 7.2956e+00, 9.7456e-02, 3.1202e-02,
        1.1164e-01, 8.8901e-02, 3.6514e-03, 4.8652e-02, 1.7094e-01, 1.3377e-01,
        1.2241e-02, 6.2556e-02, 5.3926e+00, 5.9981e-01, 6.1127e-01, 8.9612e-02,
        1.1818e+01, 5.4449e-01, 1.2360e+00, 1.6778e-01, 7.6218e-02, 2.0026e-01,
        3.4641e-01, 4.9743e-01, 1.1682e+00, 4.5009e-01, 4.2743e-02, 2.8779e-01,
        3.6057e+00, 1.0824e+00, 3.1060e+00, 3.3426e+00, 3.3350e+00, 9.7116e+00,
        4.6961e-02, 1.2973e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [15/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 95.2
Sum Train Loss:  tensor([3.7622e+00, 2.8578e-01, 1.8405e+00, 2.7592e-01, 2.1478e-01, 1.2467e-01,
        2.0299e-02, 2.5527e-01, 4.0077e-01, 1.2435e-01, 1.2320e-02, 7.2705e-02,
        1.6068e-02, 4.3087e-01, 4.1182e-01, 6.8345e-01, 3.8721e-01, 3.7848e-01,
        3.2732e-01, 2.0932e-01, 4.9416e-02, 7.0194e-03, 2.3928e-02, 5.1417e-02,
        2.7469e-01, 1.8951e-01, 1.0005e+00, 1.6167e-01, 4.7113e-02, 8.0884e-02,
        9.0859e-02, 7.2280e-02, 9.4030e-01, 5.1948e-03, 6.1493e-02, 5.1942e-02,
        1.3605e-01, 7.1852e-02, 4.1061e-01, 1.5484e+00, 6.8624e-01, 5.7935e+00,
        3.5023e+00, 3.6415e+00, 4.7792e+00, 7.5017e+00, 7.2682e-02, 3.4161e-02,
        4.6597e-02, 9.0353e-02, 2.3126e-02, 2.3006e-02, 5.2286e-02, 4.7488e-02,
        1.1660e-02, 6.4639e-02, 8.1243e+00, 5.8952e-01, 2.7639e+00, 2.4165e-01,
        7.7283e+00, 2.4531e-01, 2.2020e+00, 3.4077e-01, 2.7487e-01, 2.6912e-01,
        4.7084e-01, 5.8513e-01, 2.9546e+00, 8.3275e-01, 7.9875e-03, 2.0710e-01,
        3.2075e+00, 2.9777e+00, 8.4063e+00, 1.0349e+01, 6.0003e-01, 2.4391e+00,
        5.8286e-02, 4.7742e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [15/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 98.2
Sum Train Loss:  tensor([3.5752e+00, 4.5439e-01, 2.1295e+00, 3.0132e-01, 1.2053e-01, 2.1843e-01,
        5.7961e-02, 2.3139e-01, 9.5177e-02, 2.5088e-01, 4.6192e-02, 2.8787e-02,
        5.7056e-03, 6.8736e-01, 6.3374e-01, 4.7029e-01, 5.8172e-01, 4.2486e-01,
        1.2231e-01, 1.2145e-01, 2.1381e-02, 1.9592e-02, 2.8878e-02, 3.7374e-02,
        5.3131e-01, 5.5638e-02, 1.0713e+00, 4.0335e-02, 3.9847e-02, 8.0237e-02,
        9.4991e-02, 4.6180e-02, 1.5931e+00, 2.1240e-02, 1.0857e-01, 5.2732e-01,
        1.7807e-01, 6.5707e-02, 1.1182e-01, 2.1473e+00, 7.0803e-01, 6.8069e+00,
        3.2428e+00, 5.3260e+00, 7.3674e+00, 5.1336e+00, 4.9187e-02, 2.9994e-02,
        1.2912e-01, 1.8410e-02, 7.1113e-03, 5.2753e-02, 8.5773e-02, 1.0093e-01,
        3.4929e-02, 2.5190e-01, 7.6673e+00, 5.4492e-01, 1.9145e+00, 1.5699e-01,
        8.0249e+00, 1.1229e-01, 4.9816e+00, 4.0413e-01, 5.2185e-01, 9.0892e-02,
        7.3195e-01, 8.4605e-01, 9.7607e-01, 8.3300e-01, 8.4991e-02, 1.5210e-01,
        5.2565e+00, 2.6707e+00, 3.7789e+00, 6.3370e+00, 1.6423e+00, 6.9075e-01,
        8.9401e-01, 2.0125e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [15/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 96.2
Sum Train Loss:  tensor([4.0218e+00, 1.1735e+00, 2.1990e+00, 4.7114e-01, 7.7153e-02, 2.3166e-01,
        9.3345e-02, 3.1590e-01, 1.9644e-01, 2.2547e-01, 1.5624e-02, 8.1847e-02,
        3.6624e-02, 5.5267e-01, 3.5016e-01, 7.7736e-01, 6.4084e-01, 2.4216e-01,
        3.0105e-01, 2.5341e-01, 2.2792e-02, 1.3136e-02, 1.0218e-02, 5.0286e-02,
        7.5951e-01, 1.5531e-01, 1.1972e+00, 1.8648e-01, 4.2170e-02, 9.8639e-02,
        9.1234e-02, 9.7924e-03, 9.4548e-01, 2.8257e-03, 4.2578e-02, 2.2102e-02,
        9.8888e-02, 2.1764e-01, 2.0546e-01, 1.9578e+00, 2.6793e+00, 7.9544e+00,
        2.4539e+00, 3.3627e+00, 4.9680e+00, 5.0411e+00, 6.8433e-02, 5.6368e-02,
        2.3156e-01, 6.4631e-02, 4.8691e-03, 1.4593e-02, 1.4321e-01, 9.8356e-02,
        2.7372e-02, 1.8235e-01, 1.2073e+01, 5.5878e-01, 1.9268e+00, 1.0380e-01,
        7.7094e+00, 4.3626e-01, 5.5368e+00, 2.0764e-01, 3.0299e-01, 2.6584e-01,
        2.4817e-01, 6.7328e-01, 6.4787e-01, 3.3803e-01, 2.2320e-03, 2.3478e-01,
        1.8089e+00, 4.4768e+00, 1.0450e+01, 6.9784e+00, 4.5647e+00, 4.4877e+00,
        5.8889e-02, 5.5672e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [15/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 110.4
Sum Train Loss:  tensor([5.6727e+00, 1.6756e-01, 3.3380e+00, 2.2860e-01, 6.6454e-02, 4.1697e-01,
        5.2083e-02, 4.2763e-01, 3.8196e-01, 1.7671e-01, 1.2644e-01, 3.0206e-02,
        4.7726e-03, 6.0247e-01, 2.4099e-01, 1.8457e-01, 5.8184e-01, 4.5811e-01,
        1.6370e-01, 1.0171e-01, 1.2036e-02, 3.1991e-03, 9.1562e-03, 1.1319e-02,
        3.8669e-01, 2.0067e-01, 1.2479e+00, 1.5764e-01, 6.0222e-02, 1.4731e-01,
        3.0385e-02, 4.2424e-02, 7.9615e-01, 2.2177e-02, 9.4187e-02, 1.0252e-01,
        2.1320e-01, 7.7699e-02, 2.7547e-02, 2.6657e+00, 2.7579e+00, 6.3345e+00,
        3.5615e+00, 8.1423e+00, 5.8576e+00, 8.2875e+00, 1.2232e-01, 7.9757e-02,
        4.9574e-02, 8.8024e-02, 1.7529e-02, 3.5629e-02, 1.4909e-02, 8.6131e-02,
        2.1638e-02, 1.5218e-01, 6.0899e+00, 1.0239e+00, 1.8181e+00, 1.4958e-01,
        5.5091e+00, 6.0975e-02, 3.0487e+00, 3.0296e-01, 1.3849e-01, 4.3095e-01,
        2.0301e-01, 1.0270e+00, 3.0098e+00, 7.4253e-01, 8.9106e-02, 2.6044e-01,
        3.9202e+00, 2.6137e+00, 6.7784e+00, 7.3812e+00, 3.3583e-01, 1.3536e+00,
        8.3012e-02, 1.0204e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [15/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 101.8
Sum_Val Meta Model:  tensor([6.2301e+00, 2.2859e+00, 1.0301e+01, 1.5050e+00, 1.8303e-02, 3.5524e-01,
        3.2516e-02, 3.1045e-01, 2.3678e-01, 1.8289e-01, 4.1223e-02, 1.2166e-01,
        4.2291e-02, 4.5960e-01, 2.7725e-01, 3.3801e-01, 2.2877e+01, 5.9535e-02,
        4.7252e-02, 5.0221e-02, 2.8836e-03, 3.3269e-03, 4.8438e-03, 8.5641e-03,
        6.8200e-01, 2.3209e-01, 1.8772e+00, 5.6480e-02, 8.7616e-02, 1.8359e-01,
        1.5540e-02, 7.6503e-03, 6.2196e-01, 1.4299e-03, 1.9404e-02, 5.5199e-02,
        4.0407e-02, 9.2543e-02, 3.8440e-02, 4.2684e+00, 2.6466e+00, 1.0130e+01,
        4.7859e+00, 8.3187e+00, 1.3697e+01, 1.1155e+01, 1.9081e-02, 7.7966e-02,
        1.9532e-02, 9.9559e-02, 3.4897e-03, 1.2810e-02, 8.3815e-02, 3.0957e-02,
        1.2225e-02, 6.3824e-02, 1.0171e+01, 3.7663e-01, 2.2948e+00, 4.8873e-02,
        1.1454e+01, 4.5848e-01, 1.6324e+00, 3.0991e-01, 5.9699e-02, 6.2527e-02,
        1.2560e-01, 3.8134e-01, 3.2296e+00, 2.4677e+00, 2.7749e-03, 5.4381e-01,
        6.2438e+00, 1.9198e+00, 1.1253e+01, 8.4630e+00, 4.2427e-01, 9.5536e+00,
        5.9031e-02, 8.0220e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.4938e+00, 1.6125e+00, 7.7054e+00, 7.5702e-01, 5.5177e-03, 3.5234e-01,
        2.6931e-02, 3.1149e-01, 1.8102e-01, 1.6637e-01, 5.2773e-02, 1.2372e-01,
        5.1650e-02, 4.3914e-01, 2.8307e-01, 1.2195e+00, 1.5006e+01, 1.0754e-01,
        2.8911e-02, 7.4019e-02, 4.4595e-03, 3.3321e-03, 1.4921e-03, 8.8349e-03,
        6.3438e-01, 2.1599e-01, 1.6954e+00, 8.2722e-02, 1.1935e-01, 2.0234e-01,
        7.7932e-03, 7.2453e-03, 6.3193e-01, 2.4390e-03, 1.4220e-02, 3.7863e-02,
        1.0702e-01, 6.7055e-02, 4.1300e-02, 3.9695e+00, 2.7520e+00, 1.1804e+01,
        4.2352e+00, 7.6874e+00, 1.2696e+01, 9.5699e+00, 1.2024e-02, 7.7901e-02,
        7.4756e-03, 6.8524e-02, 1.1006e-03, 6.5582e-03, 8.1606e-02, 8.3072e-03,
        7.7709e-03, 3.6195e-02, 1.0177e+01, 3.5555e-01, 2.0549e+00, 8.7902e-02,
        9.7634e+00, 1.4567e-01, 1.5538e+00, 3.1642e-01, 3.3116e-02, 8.1322e-02,
        7.9039e-02, 4.5285e-01, 3.7081e+00, 2.4297e+00, 2.6770e-03, 5.5265e-01,
        5.8196e+00, 2.0573e+00, 1.3727e+01, 7.0315e+00, 3.3200e-01, 2.0328e+01,
        1.0316e-01, 1.7427e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.0897e+01, 5.0603e+01, 5.6689e+01, 2.1024e+01, 3.7748e-01, 1.1962e+01,
        3.5736e+00, 1.6480e+01, 4.5376e+00, 8.5216e+00, 7.9203e+00, 1.1483e+01,
        9.3918e+00, 1.4175e+01, 7.3910e+00, 3.5748e+01, 3.3067e+02, 3.8507e+00,
        9.2808e-01, 1.8939e+00, 1.6970e+00, 4.6214e-01, 1.3787e-01, 1.0756e+00,
        2.8219e+01, 1.8239e+01, 2.6658e+01, 6.5989e+00, 1.5667e+01, 1.3757e+01,
        6.2009e-01, 8.4512e-01, 8.9384e+00, 1.2916e+00, 1.0480e+00, 1.0011e+00,
        9.3735e+00, 3.1815e+00, 1.2611e+00, 4.8142e+01, 1.2194e+01, 2.5370e+01,
        8.0692e+00, 1.5762e+01, 1.4904e+01, 1.3871e+01, 1.0717e+00, 5.5431e+00,
        4.6989e-01, 3.4543e+00, 2.7001e-01, 6.5379e-01, 6.6714e+00, 4.5663e-01,
        1.0206e+00, 1.4905e+00, 3.0224e+01, 6.2544e+00, 1.2645e+01, 6.4647e+00,
        1.3107e+01, 4.1484e+00, 6.8720e+00, 5.7186e+00, 5.8101e-01, 2.8024e+00,
        9.0819e-01, 1.0037e+01, 9.9635e+00, 1.8441e+01, 1.3928e-01, 1.8570e+01,
        1.2603e+01, 1.1190e+01, 1.6549e+01, 7.0547e+00, 3.3217e-01, 2.0417e+01,
        2.9460e-01, 1.6163e+00], device='cuda:0')
Outer loop valEpocw Maximum [15/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 176.8
model_train val_loss valEpocw [15/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 172.3
model_train val_loss valEpocw [15/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1164.5
Sum_Val Meta Model:  tensor([9.4111e+00, 6.7472e-01, 1.0884e+01, 3.8227e-01, 8.1241e-03, 1.7868e+00,
        1.3694e+00, 1.6987e+00, 1.8608e+00, 1.1582e+00, 2.0558e-01, 4.2036e+00,
        1.0495e-01, 1.9551e-01, 7.7510e-01, 1.1989e+00, 7.2773e-01, 6.5867e-01,
        5.2706e-01, 2.1668e+00, 6.1617e-04, 8.3440e-04, 1.1690e-02, 7.6713e-02,
        7.7501e-01, 4.3879e-01, 2.5646e+00, 3.2907e-01, 1.7499e-01, 1.7942e-03,
        4.7851e-03, 1.9088e-03, 1.8374e-02, 6.1065e-04, 1.1678e-03, 3.4529e-03,
        9.0239e-02, 5.7975e-03, 3.5152e-03, 2.0127e-01, 5.5429e-02, 1.9910e+00,
        1.0050e-01, 2.1944e-01, 3.6637e-01, 2.9012e+00, 1.4276e-02, 9.8570e-03,
        2.8563e-03, 1.8443e-01, 4.6153e-04, 1.9527e-03, 8.7356e-04, 3.1600e-03,
        2.0898e-03, 1.0088e-01, 4.2591e+00, 5.1355e-01, 1.4194e+00, 9.3610e-02,
        1.6090e+00, 1.7956e-02, 7.2400e-01, 1.4983e-01, 3.0073e-02, 3.6345e-02,
        6.6325e-02, 8.9820e-01, 6.7483e-02, 6.4261e-01, 1.5748e-03, 2.1019e-02,
        2.0638e+00, 1.2423e+00, 5.1828e+00, 4.4331e-01, 1.7303e-01, 1.0010e+00,
        1.6554e-02, 1.6906e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([8.0296e+00, 8.3357e-01, 7.0558e+00, 4.9833e-01, 5.0931e-02, 1.5085e+00,
        9.0409e-01, 1.2096e+00, 1.5595e+00, 7.1033e-01, 1.6933e-01, 1.1354e+00,
        9.5544e-02, 2.7383e-01, 9.1217e-01, 1.4726e-01, 6.2475e-01, 6.2108e-01,
        1.5273e-01, 1.1250e+00, 5.0664e-03, 4.6721e-03, 2.5581e-03, 6.5409e-02,
        7.4740e-01, 3.6292e-01, 2.6218e+00, 3.0213e-01, 1.8978e-01, 2.9395e-02,
        6.1730e-03, 5.3812e-03, 2.9190e-02, 7.3844e-03, 6.8645e-03, 8.5162e-03,
        5.6550e-02, 5.1234e-02, 3.1324e-03, 1.1299e-01, 4.8151e-02, 1.3216e+00,
        1.3934e-01, 4.3866e-01, 6.1189e-01, 5.0650e-01, 8.6260e-03, 1.0961e-02,
        5.9556e-03, 1.5000e-01, 1.6499e-03, 7.6922e-03, 4.1983e-03, 1.1558e-02,
        4.1214e-03, 4.0425e-02, 3.5032e+00, 2.9260e-01, 9.2843e-01, 1.0798e-02,
        2.0478e+00, 4.0199e-02, 6.0640e-01, 2.7292e-02, 1.6725e-02, 2.1493e-02,
        3.7726e-02, 9.8103e-01, 2.8090e-02, 3.8702e-01, 2.7092e-04, 1.3055e-02,
        2.3397e+00, 4.9788e-01, 4.3453e+00, 2.0403e-01, 2.2501e-01, 1.4662e+00,
        1.0226e-02, 3.6415e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.2650e+01, 1.9938e+01, 4.3259e+01, 1.0409e+01, 2.5188e+00, 3.4602e+01,
        5.6081e+01, 3.9695e+01, 2.7766e+01, 2.7444e+01, 1.6391e+01, 6.1187e+01,
        9.2420e+00, 7.8369e+00, 1.6850e+01, 2.7234e+00, 1.0415e+01, 1.4292e+01,
        2.5100e+00, 1.6311e+01, 9.4391e-01, 3.8385e-01, 1.4038e-01, 4.7472e+00,
        2.4052e+01, 1.7874e+01, 3.3327e+01, 1.4194e+01, 1.3455e+01, 1.2897e+00,
        3.0920e-01, 3.7574e-01, 3.2954e-01, 1.9328e+00, 3.2920e-01, 1.7299e-01,
        3.0252e+00, 1.6167e+00, 7.2732e-02, 1.3114e+00, 2.2411e-01, 2.9605e+00,
        3.0172e-01, 9.7080e-01, 7.6833e-01, 7.9680e-01, 4.6009e-01, 4.8340e-01,
        2.3051e-01, 4.7938e+00, 2.1135e-01, 5.1063e-01, 2.2435e-01, 4.1537e-01,
        3.1540e-01, 1.1948e+00, 1.0261e+01, 3.7787e+00, 5.3364e+00, 4.6947e-01,
        2.9254e+00, 8.5183e-01, 2.4915e+00, 3.7525e-01, 2.4803e-01, 5.1092e-01,
        3.7548e-01, 1.8482e+01, 8.2092e-02, 2.7780e+00, 9.3945e-03, 3.3529e-01,
        5.9400e+00, 2.3987e+00, 5.6265e+00, 2.0525e-01, 2.2524e-01, 1.4767e+00,
        3.3327e-02, 3.1833e-01], device='cuda:0')
Outer loop valEpocw Maximum [15/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 71.3
model_train val_loss valEpocw [15/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 53.6
model_train val_loss valEpocw [15/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 682.4
Sum_Val Meta Model:  tensor([7.4941e+00, 8.3995e-02, 1.3488e+00, 6.9159e-02, 8.7480e-03, 6.4339e-02,
        1.0944e-02, 2.0136e-01, 6.6123e-02, 4.1635e-02, 8.8826e-03, 1.0238e-02,
        2.0050e-03, 5.1784e-01, 9.5232e-02, 8.7720e-02, 2.5091e-01, 5.4503e-02,
        4.4367e-02, 7.3154e-02, 1.8996e-03, 1.1902e-02, 7.6599e-02, 1.8880e-02,
        1.4167e-01, 1.8683e-02, 1.2508e+00, 9.7009e-02, 7.8346e-03, 4.4186e-02,
        5.2221e-02, 6.4074e-02, 5.7693e-01, 5.6664e-04, 3.4529e-02, 9.1080e-02,
        2.0254e-01, 3.4854e-01, 8.5260e-03, 2.1001e+00, 6.0183e+00, 3.1904e+01,
        3.6941e+01, 3.8664e+01, 3.9864e+01, 4.8576e+01, 1.8722e-01, 2.0183e-01,
        1.0970e+00, 3.8914e-01, 1.6547e-01, 4.5768e-01, 1.0441e+00, 3.0159e-01,
        5.9563e-01, 2.8213e+00, 5.1906e+01, 7.6549e-01, 1.2860e+00, 2.8232e-02,
        6.3881e+01, 3.4798e-02, 2.3616e+00, 7.5796e-01, 7.8624e-01, 3.1996e-02,
        7.7233e-01, 4.0999e-01, 1.7692e+00, 2.2939e-01, 4.6769e-03, 2.2226e-01,
        2.0969e+00, 6.6961e+00, 1.2144e+00, 2.8428e+01, 1.0882e+01, 1.0705e+00,
        5.0643e-02, 5.2764e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([2.4815e+00, 1.3373e-02, 4.6387e-01, 1.1303e-02, 1.4997e-03, 3.1672e-03,
        8.7072e-04, 2.0490e-01, 1.0854e-02, 2.1421e-03, 9.3921e-04, 6.2104e-04,
        1.9551e-04, 4.3246e-01, 1.1517e-02, 4.5047e-02, 5.5272e-02, 7.9568e-03,
        4.5438e-03, 2.8732e-03, 2.0885e-04, 1.8956e-04, 1.2340e-04, 3.6058e-04,
        8.9312e-02, 1.6339e-02, 1.3943e+00, 8.9633e-02, 4.6454e-03, 7.4940e-03,
        1.0737e-02, 6.4869e-02, 6.0688e-01, 2.6431e-04, 1.3071e-02, 4.1536e-02,
        1.1823e-01, 2.6732e-01, 3.4357e-03, 2.5605e+00, 4.8775e+00, 2.6645e+01,
        2.9179e+01, 3.5647e+01, 3.6344e+01, 4.5057e+01, 1.0271e-01, 1.1973e-01,
        6.9952e-01, 2.3880e-01, 1.0289e-01, 4.2192e-01, 8.3014e-01, 3.7136e-01,
        4.1278e-01, 1.7160e+00, 2.7658e+01, 8.5575e-01, 1.6371e+00, 1.8150e-02,
        6.9871e+01, 2.0475e-02, 1.9578e+00, 5.9001e-01, 6.0604e-01, 1.2985e-01,
        6.8372e-01, 5.0001e-01, 1.8017e+00, 3.2238e-01, 1.3108e-03, 2.3181e-01,
        1.6564e+00, 5.8663e+00, 2.5899e+00, 2.4947e+01, 9.3032e+00, 8.1184e+00,
        5.7746e-02, 1.0526e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.5599e+01, 5.0624e-01, 3.7405e+00, 3.8240e-01, 1.3037e-01, 1.2296e-01,
        1.5087e-01, 1.4049e+01, 3.3556e-01, 1.3694e-01, 1.7937e-01, 6.9943e-02,
        4.5873e-02, 1.7651e+01, 3.5643e-01, 1.5482e+00, 1.2934e+00, 3.0843e-01,
        1.5139e-01, 8.1475e-02, 1.1947e-01, 2.9619e-02, 1.1596e-02, 5.2206e-02,
        4.9332e+00, 1.7556e+00, 2.8202e+01, 9.5303e+00, 7.4599e-01, 5.5781e-01,
        1.0196e+00, 7.7291e+00, 8.8141e+00, 1.6335e-01, 1.0099e+00, 1.1662e+00,
        1.0603e+01, 1.2723e+01, 1.2818e-01, 3.7527e+01, 2.4170e+01, 5.4665e+01,
        5.3310e+01, 6.7923e+01, 4.2724e+01, 6.4092e+01, 9.1206e+00, 8.4222e+00,
        3.8343e+01, 1.1905e+01, 2.3661e+01, 4.0159e+01, 6.7498e+01, 2.1488e+01,
        5.7718e+01, 7.2163e+01, 8.9605e+01, 1.7201e+01, 1.1276e+01, 1.4809e+00,
        9.2110e+01, 6.7853e-01, 9.3629e+00, 1.2064e+01, 1.2953e+01, 5.0852e+00,
        9.4849e+00, 1.4692e+01, 5.9393e+00, 3.0491e+00, 8.5377e-02, 9.7515e+00,
        4.4557e+00, 3.5345e+01, 3.2228e+00, 2.5017e+01, 9.3069e+00, 8.1466e+00,
        2.3099e-01, 1.1886e+00], device='cuda:0')
Outer loop valEpocw Maximum [15/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 400.6
model_train val_loss valEpocw [15/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 351.3
model_train val_loss valEpocw [15/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1232.8
Sum_Val Meta Model:  tensor([1.8451e+01, 7.5708e-02, 3.9504e+00, 3.7209e-02, 9.4003e-03, 5.6322e-01,
        8.5560e-02, 5.3501e-01, 6.8006e-02, 7.1551e-01, 7.0072e-02, 1.0719e-01,
        1.3609e-03, 7.0274e-01, 8.6120e-01, 6.0617e-02, 9.3365e-02, 2.8045e-02,
        1.5677e-02, 2.3207e-02, 1.7895e-03, 4.8703e-03, 5.9046e-03, 1.2168e-02,
        8.0599e-01, 2.2487e-02, 2.5030e+00, 2.7281e-02, 3.4288e-01, 9.3949e-03,
        1.2681e-02, 1.8496e-03, 3.1826e-01, 2.3386e-02, 4.8806e-02, 4.0300e-01,
        4.0771e-03, 2.6500e-02, 2.6112e-01, 2.7260e+00, 3.6647e+00, 1.4586e+01,
        4.5970e+00, 4.7443e+00, 8.8363e+00, 1.1690e+01, 7.3497e-03, 1.2864e-02,
        3.3324e-02, 1.2008e-02, 5.6855e-03, 8.5744e-03, 5.1577e-03, 1.3526e-01,
        1.3145e-02, 5.5820e-02, 7.4048e+00, 3.1003e-01, 2.5760e+00, 2.9057e-02,
        3.3718e+01, 1.7383e-02, 9.7765e-01, 5.3008e-01, 3.0967e-01, 8.9423e-02,
        5.3026e-01, 3.9824e+00, 3.8305e-01, 3.8258e-01, 1.9782e-03, 5.5727e-02,
        2.2783e+00, 2.1181e+00, 3.4900e+02, 1.3064e+01, 1.4653e+00, 1.0177e+01,
        1.2512e-02, 1.1218e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.7534e+00, 1.4415e-01, 3.4190e+00, 1.0619e-01, 3.0130e-02, 4.4323e-01,
        1.2504e-01, 4.7845e-01, 1.1979e-01, 4.8087e-01, 5.1144e-02, 2.0044e-01,
        1.0202e-02, 6.8325e-01, 9.7720e-01, 8.4150e-02, 4.9538e-02, 2.9719e-02,
        5.2034e-03, 7.8804e-03, 2.8042e-03, 4.8129e-04, 3.0798e-03, 9.5844e-03,
        6.8569e-01, 6.1800e-02, 2.1112e+00, 3.9593e-02, 3.1706e-01, 4.8131e-03,
        4.2334e-03, 6.6667e-03, 2.3867e-02, 9.8782e-04, 5.7866e-03, 1.3829e-02,
        1.6227e-02, 8.5578e-03, 1.0100e-02, 3.2170e-01, 1.7096e-01, 7.5710e-01,
        1.8122e-01, 3.7022e-01, 7.5137e-01, 1.1356e+00, 3.4547e-03, 4.3121e-03,
        1.9735e-03, 5.7434e-03, 8.0084e-04, 1.7550e-03, 1.0105e-03, 5.3774e-03,
        3.3068e-03, 2.3388e-02, 1.9962e+00, 9.2826e-02, 2.2034e+00, 2.0325e-02,
        7.4930e+00, 4.1257e-02, 8.2839e-01, 3.6061e-02, 3.8454e-02, 4.8812e-02,
        1.2947e-01, 5.1274e-01, 3.4528e-02, 3.4601e-02, 5.1230e-04, 2.9651e-02,
        5.4495e-02, 1.1866e+00, 5.1303e+01, 7.1704e+00, 6.2545e-02, 3.6805e+00,
        2.1160e-02, 1.5255e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.8082e+01, 3.1187e+00, 2.1433e+01, 2.0933e+00, 1.1851e+00, 9.5566e+00,
        8.2288e+00, 1.5741e+01, 2.1163e+00, 1.5312e+01, 3.9458e+00, 9.9635e+00,
        9.0911e-01, 1.5590e+01, 1.8371e+01, 1.7379e+00, 7.8494e-01, 6.2537e-01,
        1.0581e-01, 1.4294e-01, 4.7335e-01, 3.3520e-02, 1.5026e-01, 6.5029e-01,
        2.1078e+01, 2.8130e+00, 2.4214e+01, 1.7022e+00, 1.9656e+01, 1.8971e-01,
        1.9304e-01, 4.0607e-01, 2.1825e-01, 1.8933e-01, 2.2382e-01, 2.1016e-01,
        7.7086e-01, 2.3642e-01, 1.7927e-01, 2.9466e+00, 6.4318e-01, 1.5697e+00,
        3.6545e-01, 7.7634e-01, 9.3755e-01, 1.7771e+00, 1.7815e-01, 1.8027e-01,
        6.6737e-02, 1.7826e-01, 8.6555e-02, 1.0363e-01, 4.6796e-02, 1.4961e-01,
        2.3022e-01, 6.2531e-01, 5.5655e+00, 1.1828e+00, 1.1575e+01, 8.0121e-01,
        1.0360e+01, 7.2446e-01, 3.3261e+00, 4.1408e-01, 4.5532e-01, 1.0339e+00,
        1.0831e+00, 6.5832e+00, 1.0197e-01, 2.2243e-01, 1.5963e-02, 6.3139e-01,
        1.2588e-01, 5.6454e+00, 6.8559e+01, 7.2239e+00, 6.2653e-02, 3.7163e+00,
        8.6821e-02, 1.1077e-01], device='cuda:0')
Outer loop valEpocw Maximum [15/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 512.0
model_train val_loss valEpocw [15/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 97.3
model_train val_loss valEpocw [15/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 383.1
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.63538456 97.35870664 93.047112   98.0775088  98.55386752 97.5755656
 98.23345232 95.28392685 98.01902998 96.9761577  98.62331112 98.9132686
 99.42495827 95.35824369 97.47079105 97.34286863 96.39867935 97.63891765
 98.84504331 98.39183246 98.6756984  99.30434571 99.51633143 98.9693108
 95.21204664 96.78610153 94.41892764 96.96762954 98.06532571 98.26756497
 98.34919165 98.51122672 97.11748151 98.5234098  98.66473362 98.89499397
 97.69861478 98.43081834 98.97540235 93.32366808 98.06045248 94.23739964
 97.588967   96.78731984 97.67424861 96.16110915 98.16766365 98.6769167
 98.17984674 98.75245185 98.62452943 98.6208745  99.01560654 98.38695922
 98.72808567 97.66572045 92.61826732 96.96153799 96.52294685 97.54023465
 95.33753244 98.66838854 97.19667158 97.36479819 98.83042361 97.59993177
 98.60990972 95.96617975 98.84017008 98.30411423 99.81603538 97.45617134
 98.6903181  95.95643328 97.99100888 98.31264239 99.39450055 98.98514882
 99.84527479 99.1642402 ]
Accuracy th:0.7 is [86.29646325 97.27951658 92.79126716 97.94349484 98.38695922 97.44276995
 98.04217785 95.01468062 97.88867095 96.81655925 98.58554355 98.88890243
 99.41643011 95.3180395  97.39647421 97.27220672 96.34263715 97.53657972
 98.76585324 98.3418818  98.57092384 99.25317674 99.47125401 98.8292053
 95.23641281 96.6983833  94.25811089 96.9481366  98.02999476 98.22005093
 98.07507218 98.60381818 96.75564382 98.37599444 98.5648323  98.75367016
 97.4915023  98.32604379 98.83407853 93.02761906 97.94105822 93.81708313
 97.50246708 96.75077058 97.770495   96.02222195 98.12380453 98.66351531
 98.05192432 98.76098001 98.7317406  98.61600127 99.00829668 98.28462129
 98.71346597 97.59749516 92.19307757 96.82996065 96.50589052 97.46713612
 95.01955386 98.65864207 97.43424179 97.16134063 98.81702221 97.51343185
 98.62452943 95.95765159 98.72321244 98.05192432 99.81603538 97.1442843
 98.52462811 95.91379247 97.84603014 98.52706473 99.35795129 99.10576138
 99.84405648 99.1642402 ]
Avg Prec: is [96.35397266 33.10118942 70.82415992 65.55993979 77.14215891 63.88710606
 73.25314308 47.3973277  56.28548568 51.20001249 29.86224206 50.85856476
 20.05458101 28.03518526 32.91045504 55.98351362 28.48345368 38.59894849
 45.33549013 36.95928448 58.91700465 46.76559573 88.79642593 81.48644048
 26.1277236  30.31074546 37.83392301 36.85481776 23.31838523 37.30279216
 72.62738571 35.78529283 58.18634947 61.54497056 71.70391441 79.22910204
 58.01891451 74.67418904 87.09625908 46.06389366 40.72417429 65.89430318
 62.50203663 56.26695372 65.03040433 73.30494332 36.54814353 32.57896419
 40.69341882 44.76392956 58.21650773 35.25607474 21.02983377 71.85061234
 24.74586889 40.77378609 69.56248424 58.38831934 40.30740149 56.65291661
 85.32109078 82.52573611 68.77415962 47.98640397 59.82029148 42.35584554
 60.02649687 24.44600158 50.45091194 66.16979724 10.03671109 71.8807859
 72.19733153 48.53277339 76.20362864 80.81889469 56.94919131 76.5772743
 10.04708565 23.65906854]
Accuracy th:0.5 is [45.71459899 97.2137279  72.1750466  97.02489005 97.26733349 77.093359
 77.39671788 76.39404978 78.31044943 96.44863001 78.62355478 98.52097319
 99.41399349 80.28167298 78.21907628 96.56680596 96.29512311 77.91084417
 98.65376884 98.30776915 80.47051084 79.12671629 98.38695922 78.06800599
 80.84087669 96.65086926 94.0778012  77.59773882 98.01293844 78.51390699
 97.30875598 98.57457877 96.36213009 98.02024829 87.56959589 78.09359048
 77.92424556 90.99791669 97.11504489 75.19401567 79.38499775 92.05906361
 77.37844325 77.0519365  96.9627563  93.87434364 98.02877645 98.57336046
 96.78366492 88.61977802 87.47213119 98.55508583 98.99976852 77.56971772
 98.70615611 77.95470328 72.64044054 93.38823845 96.24273583 96.9067141
 89.79300934 97.17717864 92.57684482 77.99490747 98.42838172 78.50294222
 98.20786784 77.08726746 79.03290652 97.55972759 79.56408913 95.99054592
 78.70274485 95.45083515 77.2882884  82.85230443 87.0286668  78.58578721
 79.6432792  99.14718388]
Accuracy th:0.7 is [45.73165532 97.2137279  72.1750466  97.02489005 97.26733349 77.093359
 77.39671788 76.62552844 78.31044943 96.4754328  78.62355478 98.52097319
 99.41399349 80.72513736 78.21907628 96.56680596 96.29512311 77.91084417
 98.65376884 98.30776915 80.9115386  79.12671629 98.38695922 78.29826635
 81.37815085 96.65086926 94.0778012  77.59773882 98.01293844 78.51390699
 97.30875598 98.57457877 96.36213009 98.02024829 87.76087036 78.09359048
 77.92424556 91.26107138 97.11504489 75.19401567 80.08430697 92.05906361
 77.37844325 77.0519365  96.9627563  93.87434364 98.02877645 98.57336046
 97.52196002 89.24111548 87.62563809 98.55508583 98.99976852 77.56971772
 98.70615611 77.95470328 72.64044054 93.85606901 96.24273583 96.9067141
 89.79300934 97.17717864 92.7400982  77.99490747 98.42838172 78.50294222
 98.20786784 77.08726746 79.03290652 97.55972759 79.56408913 95.99054592
 78.75756874 95.45083515 77.2882884  82.91443818 87.18217371 78.58578721
 79.6432792  99.14718388]
Avg Prec: is [55.49282391  3.09239982 11.20250744  3.36200998  2.22762965  3.8574172
  3.28480975  5.52863286  2.51265639  3.73883512  1.64924687  1.60702864
  0.59969044  5.03206319  2.56526544  3.18370053  3.55679509  2.70016426
  1.37541852  1.8205423   2.05159075  0.8161415   1.74516955  2.50352291
  5.00435552  3.57705664  6.50387735  3.31202413  2.07406292  1.86199374
  2.55247706  1.32156896  3.77150827  1.62340779  2.30068136  2.44752741
  3.05711433  2.57577033  2.83761384  7.4701925   2.30060495  8.36162306
  3.32819949  3.94358131  3.22305616  6.45866209  2.01745416  1.56640104
  2.10696137  1.59892506  1.83894718  1.61474784  1.0942264   2.97856827
  1.30036735  2.63308643 11.13842551  3.69912703  4.0125679   2.85199106
 10.89025097  2.19076167  3.99014674  3.04216522  1.62707018  2.51412078
  1.84655211  4.17972782  1.26292093  2.3415364   0.20406749  3.45762114
  1.97397877  4.49568716  3.96693069  3.2321747   0.8613084   1.83124673
  0.14830291  0.77532664]
mAP score regular 52.81, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [88.15805865 97.3939258  93.06873957 98.29085383 98.95109251 97.71034208
 98.43785036 95.37085482 98.10150235 97.09245833 98.64962503 99.01836211
 99.37962479 95.17652042 97.44624661 97.39641727 96.28273164 97.68791888
 98.95607544 98.40047836 98.90126317 99.33726985 99.61880559 99.20023918
 95.30109375 96.78351646 94.54617934 97.16720233 97.91215088 98.19368662
 98.67952263 98.57488103 97.00027406 98.67952263 98.95358397 99.17283305
 98.01180955 98.41044423 99.08563171 93.41007051 97.98689489 93.43000224
 97.26935247 96.41727085 96.61907965 94.79532601 98.35064903 98.7492837
 98.12641702 98.64713357 98.72935197 98.60228717 98.88631437 98.42539303
 98.7418093  97.72778235 90.92607818 97.09245833 96.26778284 97.59573461
 93.06126517 98.80658744 97.0202058  97.41884047 98.70692877 97.52348207
 98.53003463 95.83925057 98.80658744 98.20863542 99.81563146 97.57829434
 98.39051249 95.6872711  97.4238234  97.03764606 99.20522211 98.24102449
 99.82310586 99.12798665]
Accuracy th:0.7 is [87.90143758 97.33163914 93.1260433  98.23604156 98.88133144 97.49358447
 98.28088796 95.28116202 98.03672422 96.88317513 98.6371677  99.00590478
 99.36467598 95.11423375 97.33662207 97.23198047 96.28771458 97.62314074
 98.91122904 98.39051249 98.88880584 99.27498318 99.58143359 99.09808905
 95.49791963 96.64897725 94.53123054 97.15972793 97.84238981 98.25099036
 98.44781623 98.7044373  96.80843112 98.61225303 98.83648504 99.06071704
 97.90716795 98.25597329 98.99095598 93.0737225  97.91464235 93.59194758
 97.42133194 96.74365299 96.97037646 95.07686175 98.31078556 98.82901064
 97.99686075 98.74430077 98.77419837 98.62720183 98.88382291 98.38303809
 98.71440317 97.72279941 91.25246032 97.09993273 96.39235618 97.51351621
 93.19082144 98.77419837 97.34409647 97.23447193 98.7492837  97.58576874
 98.6222189  95.7844383  98.75426664 97.96945462 99.81563146 97.46866981
 98.37307223 95.9339263  97.44375514 97.44624661 99.24508558 98.56740663
 99.82559733 99.15539278]
Avg Prec: is [96.5282888  32.28007073 71.12666904 73.18065489 77.8742701  65.1579381
 79.41378466 47.84818515 61.72234697 55.52080609 36.37590695 55.79999724
 22.41814807 30.87968864 33.66961963 62.09329122 30.38263927 42.53872059
 47.60747913 35.61969155 69.39986616 55.31688064 92.40615328 87.60943184
 25.70792969 34.96260841 34.22736236 43.64654887 26.86324948 38.1436837
 77.54084372 37.99311771 56.16867735 64.37481546 74.52425361 82.44123698
 60.45232493 77.81531233 89.82548662 45.43513082 37.0873249  54.90716754
 50.13552226 43.51307051 35.90895507 55.57830537 37.21101434 27.73287644
 40.88750653 40.58685071 63.1212901  34.1397186  22.13547175 75.87579798
 27.68906497 40.16387049 58.46765306 56.00376584 37.33449055 60.49339506
 68.34535771 86.90822788 66.99018625 50.12305901 60.49299475 38.53184039
 61.15080255 24.96394484 41.8133442  64.77571638  8.72298881 74.49951827
 53.50728517 44.97983545 65.80856168 53.32608259 14.07699656 55.80131793
  2.21218665 20.66145242]
Accuracy th:0.5 is [45.26496749 97.22450607 70.7377233  96.96290206 97.90716795 76.26877943
 76.3609637  75.22236341 77.80850587 96.41976231 77.99287441 98.5325261
 99.34972718 78.36659441 77.8957072  96.31262924 96.21047911 77.33761866
 98.78167277 98.34068316 79.27847124 78.62321549 98.31327703 77.84836934
 78.33420535 96.52938685 94.3393876  77.37000772 97.81747515 77.96048534
 97.52597354 98.67204823 96.39983058 98.18870369 88.7460448  77.56683359
 77.52946159 92.25901288 97.0276802  74.62441139 77.98290854 92.37362035
 76.70478611 76.36345517 97.03764606 94.02795426 98.18621222 98.77668984
 97.36901114 88.35986745 85.9755338  98.55993223 98.87385704 76.68983731
 98.6969629  77.37249919 71.28584598 94.38921693 96.16314124 96.78102499
 90.13379176 97.04761193 92.61778409 77.33512719 98.32075143 78.18970028
 98.13139996 76.50048584 78.65062162 97.53593941 79.09908563 96.07843137
 78.38403468 95.44559882 76.4431821  83.84283828 89.10979894 78.02277201
 79.1688467  99.15040985]
Accuracy th:0.7 is [45.48670803 97.22450607 70.7377233  96.96290206 97.90716795 76.26877943
 76.3609637  75.32451354 77.80850587 96.41976231 77.99287441 98.5325261
 99.34972718 78.77021202 77.8957072  96.31262924 96.21047911 77.33761866
 98.78167277 98.34068316 79.57246431 78.62321549 98.31327703 77.96795974
 78.77021202 96.52938685 94.3393876  77.37000772 97.81747515 77.96048534
 97.52597354 98.67204823 96.39983058 98.18870369 88.92792187 77.56683359
 77.52946159 92.49570222 97.0276802  74.62441139 78.44133842 92.37362035
 76.70478611 76.36345517 97.03764606 94.02795426 98.18621222 98.77668984
 97.74522261 88.65884346 86.15741087 98.55993223 98.87385704 76.68983731
 98.6969629  77.37249919 71.28584598 94.67324414 96.16314124 96.78102499
 90.13379176 97.04761193 92.78222089 77.33512719 98.32075143 78.18970028
 98.13139996 76.50048584 78.65062162 97.53593941 79.09908563 96.07843137
 78.39150908 95.44559882 76.4431821  83.92007375 89.30911628 78.02277201
 79.1688467  99.15040985]
Avg Prec: is [54.06588388  3.71355678 14.85471764  4.54105422  1.48535953  4.25787798
 13.61744039  8.63324917  8.06188763  5.2359056   2.33300876  4.77710633
  2.09961998  5.84316519  2.91854248  3.66787595 24.14248823  6.45949559
  1.55508578  2.71904335  3.53134327  1.50693809  1.23710189  5.23246301
  5.65180188  9.31157662  7.90415966  4.61463298  3.92408871  5.61092467
  2.2520176   0.86104574  2.9665512   1.1558171   1.69893029  2.27356605
  1.99047449  2.16705466  2.19067111  6.23874308  1.71721698  6.05359269
  2.16414189  2.69734487  2.33971979  4.91379028  1.70983175  1.02493252
  1.43741295  1.17707665  1.20040509  1.05445199  0.74310342  2.26032103
  0.88964818  1.86590045 10.15522594  3.0516609   3.9636014   2.84893797
  7.85329091  2.00303922  3.32449439  2.50690843  1.35221126  1.92029647
  1.53367922  3.51740172  1.07938832  2.21789465  0.19495322  3.24898707
  1.58596466  4.01717325  3.20435591  2.31006645  0.54990488  1.46574282
  0.12134593  0.60983898]
mAP score regular 51.12, mAP score EMA 4.31
Train_data_mAP: current_mAP = 52.81, highest_mAP = 52.81
Val_data_mAP: current_mAP = 51.12, highest_mAP = 51.12
tensor([0.0957, 0.0271, 0.1237, 0.0283, 0.0117, 0.0246, 0.0060, 0.0150, 0.0338,
        0.0154, 0.0052, 0.0087, 0.0044, 0.0251, 0.0317, 0.0292, 0.0404, 0.0248,
        0.0281, 0.0334, 0.0019, 0.0059, 0.0093, 0.0066, 0.0186, 0.0100, 0.0580,
        0.0100, 0.0065, 0.0116, 0.0102, 0.0068, 0.0651, 0.0015, 0.0117, 0.0341,
        0.0091, 0.0184, 0.0270, 0.0739, 0.2084, 0.4775, 0.5424, 0.5083, 0.8653,
        0.6976, 0.0092, 0.0117, 0.0138, 0.0164, 0.0036, 0.0084, 0.0104, 0.0159,
        0.0061, 0.0209, 0.3197, 0.0473, 0.1637, 0.0117, 0.7500, 0.0309, 0.2135,
        0.0504, 0.0509, 0.0237, 0.0787, 0.0395, 0.3856, 0.1300, 0.0158, 0.0255,
        0.4440, 0.1727, 0.8473, 0.9976, 0.9997, 0.9972, 0.3181, 0.1003],
       device='cuda:0')
Sum Train Loss:  tensor([2.9185e+00, 1.2490e-01, 2.6072e+00, 3.7108e-01, 2.6812e-02, 9.3816e-02,
        2.9032e-02, 1.6190e-01, 2.2428e-01, 1.3305e-01, 2.6823e-02, 4.0227e-02,
        3.4110e-02, 2.3635e-01, 1.7368e-01, 4.6355e-01, 4.0894e-01, 2.9294e-01,
        2.6925e-01, 2.1745e-01, 1.1918e-02, 4.2523e-02, 2.1336e-02, 2.1062e-02,
        2.9848e-01, 4.3054e-02, 8.5522e-01, 1.1282e-01, 7.3781e-02, 3.1284e-02,
        1.2041e-02, 2.5814e-02, 3.7405e-01, 7.4054e-03, 1.1741e-01, 1.0365e-01,
        5.4031e-02, 9.4214e-02, 1.1474e-01, 2.4628e+00, 2.1910e+00, 5.8327e+00,
        5.8099e+00, 4.5241e+00, 1.0621e+01, 8.5355e+00, 8.0604e-02, 5.8197e-02,
        1.4396e-01, 4.4653e-02, 1.6849e-02, 3.4257e-02, 2.7924e-02, 1.1608e-01,
        6.6222e-02, 1.6034e-01, 6.7088e+00, 4.4148e-01, 1.3832e+00, 7.9276e-02,
        1.3062e+01, 1.3317e-01, 1.5076e+00, 6.6403e-01, 1.1158e-01, 3.1731e-01,
        8.7891e-01, 6.7847e-01, 1.9144e+00, 4.6502e-01, 2.6297e-03, 3.1699e-01,
        2.4180e+00, 2.1418e+00, 1.5611e+01, 8.5304e+00, 5.8971e-01, 5.5941e+00,
        1.8446e-01, 4.0816e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [16/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 116.1
Sum Train Loss:  tensor([3.2260e+00, 7.1508e-01, 3.4739e+00, 4.4654e-01, 8.2601e-02, 1.9958e-01,
        7.9586e-03, 2.5693e-01, 5.1824e-01, 2.2532e-01, 6.4518e-02, 4.3042e-02,
        7.8431e-03, 3.9829e-01, 2.6463e-01, 2.4172e-01, 1.1457e+00, 3.2805e-01,
        1.5997e-01, 4.3086e-01, 1.8803e-02, 1.5519e-02, 3.1696e-02, 1.7516e-02,
        3.2564e-01, 2.0853e-01, 1.8669e+00, 1.2119e-01, 9.5872e-02, 1.3879e-01,
        7.7488e-02, 4.9937e-02, 5.4089e-01, 1.3864e-02, 2.4535e-02, 1.0240e-01,
        7.2687e-02, 6.2006e-02, 1.1687e-01, 1.2893e+00, 9.0860e-01, 1.2470e+01,
        2.3849e+00, 4.8348e+00, 3.0616e+00, 5.8982e+00, 3.9194e-02, 7.1508e-02,
        4.1027e-02, 4.9460e-02, 3.7361e-03, 1.1046e-01, 1.0951e-02, 2.2538e-02,
        2.2581e-02, 9.3226e-02, 6.6751e+00, 4.6589e-01, 3.4707e+00, 1.1527e-01,
        9.7779e+00, 5.4410e-02, 2.6378e+00, 3.9769e-01, 2.8340e-01, 2.4211e-01,
        2.0183e-01, 5.3889e-01, 1.0962e+00, 4.4698e-01, 3.9008e-03, 2.7280e-01,
        1.7185e+00, 2.2809e+00, 1.3799e+01, 7.4220e+00, 4.6632e+00, 1.2500e+00,
        1.2584e-01, 1.9555e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [16/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 105.6
Sum Train Loss:  tensor([3.8739e+00, 3.5607e-01, 2.3280e+00, 6.8508e-02, 4.5186e-02, 1.7841e-01,
        1.4770e-02, 1.4450e-01, 1.2787e-01, 5.9289e-02, 3.9104e-02, 1.4691e-02,
        4.2302e-03, 5.5114e-01, 3.3710e-01, 2.8787e-01, 5.2216e-01, 1.8191e-01,
        1.9597e-01, 8.9422e-02, 8.6474e-03, 2.3372e-03, 2.7672e-02, 8.1721e-03,
        1.6767e-01, 1.5530e-01, 6.1795e-01, 1.5365e-01, 1.8985e-02, 1.2826e-01,
        3.0905e-02, 8.7729e-03, 9.1187e-01, 6.0866e-03, 7.1032e-02, 4.7216e-01,
        1.2801e-01, 1.7470e-01, 1.1930e-01, 1.6923e+00, 2.8144e+00, 1.4539e+01,
        3.2667e+00, 7.0701e+00, 9.9326e+00, 1.5781e+01, 1.3067e-01, 1.3037e-01,
        1.6505e-01, 8.5369e-02, 9.4350e-03, 1.4837e-02, 1.3554e-02, 1.2447e-01,
        3.1451e-02, 2.3540e-01, 6.5473e+00, 5.1360e-01, 3.1479e+00, 7.7524e-02,
        1.1408e+01, 1.9893e-01, 2.0830e+00, 5.4990e-01, 2.2887e-01, 2.5054e-01,
        8.7673e-01, 6.1749e-01, 3.4879e+00, 1.2932e+00, 3.9799e-03, 9.5694e-02,
        1.0501e+00, 3.3734e+00, 4.1847e+00, 4.2939e+00, 7.6643e+00, 2.0864e+01,
        8.3489e-02, 8.4020e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [16/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 141.6
Sum Train Loss:  tensor([3.4728e+00, 2.2906e-01, 2.0058e+00, 1.3397e-01, 1.0310e-01, 2.8115e-01,
        1.0031e-01, 2.7888e-01, 3.6291e-01, 1.2377e-01, 1.4982e-02, 1.4694e-02,
        1.6603e-02, 6.2783e-01, 3.5330e-01, 3.3292e-01, 1.8426e-01, 1.8130e-01,
        1.9234e-01, 3.3953e-01, 3.2651e-02, 3.0571e-02, 1.7520e-02, 3.0229e-02,
        2.5853e-01, 1.4180e-01, 9.9108e-01, 9.8668e-02, 9.1061e-02, 3.8343e-02,
        1.2300e-01, 1.3469e-02, 4.3883e-01, 8.8760e-03, 2.6301e-02, 3.9505e-02,
        9.0473e-02, 1.1153e-01, 1.1058e-01, 2.1903e+00, 4.2680e-01, 1.3040e+01,
        2.0932e+00, 3.3864e+00, 5.9200e+00, 8.4476e+00, 5.9014e-02, 2.2751e-02,
        1.7706e-01, 2.5393e-02, 1.7285e-02, 5.9559e-02, 6.0984e-02, 7.1399e-02,
        1.6754e-02, 6.5440e-02, 8.2611e+00, 4.1258e-01, 2.1096e+00, 9.7798e-02,
        8.5806e+00, 2.5041e-02, 2.1554e+00, 9.4550e-01, 5.3819e-01, 1.1281e-01,
        1.3933e+00, 1.2532e+00, 1.6848e+00, 1.4221e+00, 6.4273e-03, 2.1759e-01,
        3.6133e+00, 2.5636e+00, 5.2287e+00, 7.4252e+00, 1.2870e+00, 2.5811e+00,
        3.4934e-02, 5.2726e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [16/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 100.6
Sum Train Loss:  tensor([4.1949e+00, 4.1617e-01, 2.9473e+00, 1.2834e-01, 1.1382e-01, 4.1221e-01,
        4.2153e-02, 2.0581e-01, 2.7702e-01, 1.9333e-01, 5.3292e-02, 3.9711e-02,
        2.5914e-03, 4.2882e-01, 4.6977e-01, 3.9416e-01, 1.1864e+00, 1.8707e-01,
        2.2718e-01, 6.9260e-02, 4.2046e-03, 5.8102e-02, 1.7180e-02, 1.6600e-02,
        3.6259e-01, 1.9734e-01, 8.2059e-01, 1.4335e-01, 8.5035e-02, 7.3352e-02,
        3.3976e-02, 2.3675e-02, 4.0555e-01, 4.3745e-03, 4.5790e-02, 5.8738e-02,
        1.1752e-01, 1.6701e-01, 4.2996e-02, 1.5510e+00, 1.3437e+00, 8.8715e+00,
        4.1715e+00, 4.1945e+00, 4.0249e+00, 1.2467e+01, 6.0037e-02, 2.5683e-02,
        8.5488e-02, 1.2360e-01, 1.9055e-02, 7.2094e-02, 4.4007e-02, 8.3231e-02,
        8.2833e-02, 1.3165e-01, 1.2402e+01, 4.9981e-01, 2.6310e+00, 1.7192e-01,
        2.0246e+01, 9.1339e-02, 1.4302e+00, 3.4074e-01, 9.1454e-02, 2.7094e-01,
        1.6568e-01, 7.8688e-01, 5.9125e-01, 9.4582e-01, 7.0640e-02, 1.5368e-01,
        1.4282e+00, 4.8882e+00, 5.2031e+00, 1.6607e+00, 5.4540e+00, 5.6008e-01,
        5.0173e-02, 3.6102e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [16/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 112.5
Sum Train Loss:  tensor([3.6099e+00, 2.6529e-01, 2.6547e+00, 2.0831e-01, 4.3358e-02, 1.1385e-01,
        4.7255e-02, 1.2148e-01, 3.4902e-01, 1.1293e-01, 2.5606e-02, 5.4662e-02,
        1.0225e-02, 4.3019e-01, 4.2289e-01, 2.1263e-01, 4.7837e-01, 1.2138e-01,
        6.6500e-02, 2.0929e-01, 1.7173e-02, 5.1937e-02, 1.5643e-02, 6.4136e-02,
        4.8411e-01, 6.4236e-02, 1.3856e+00, 6.6540e-02, 2.2187e-02, 7.9793e-02,
        5.4043e-02, 2.4827e-02, 3.0220e-01, 3.2828e-03, 3.1330e-02, 1.3952e-01,
        4.6325e-02, 7.7863e-02, 1.5832e-01, 1.7978e+00, 1.4444e+00, 8.8569e+00,
        4.4163e+00, 3.2536e+00, 4.2003e+00, 1.3195e+01, 1.0939e-01, 2.6646e-02,
        9.5947e-02, 8.3422e-02, 3.9550e-03, 7.4811e-02, 1.6659e-02, 7.8949e-02,
        4.9273e-02, 1.6376e-01, 9.1325e+00, 7.3404e-01, 2.4449e+00, 1.3157e-01,
        7.6435e+00, 1.2686e-01, 2.6221e+00, 4.6697e-01, 2.4136e-01, 3.5743e-01,
        5.0290e-01, 6.1000e-01, 4.2728e+00, 8.3745e-01, 3.4529e-03, 1.4441e-01,
        3.6735e+00, 3.1885e+00, 1.4945e+01, 2.1433e+00, 2.3561e+00, 5.2719e+00,
        5.6809e-02, 3.3729e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [16/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 112.8
Sum Train Loss:  tensor([3.6227e+00, 3.9961e-01, 2.0998e+00, 1.3915e-01, 2.4118e-02, 5.0379e-01,
        8.0003e-02, 1.7243e-01, 8.0709e-02, 2.2279e-01, 5.4518e-02, 8.1053e-02,
        3.8895e-03, 4.7939e-01, 3.7934e-01, 5.4688e-01, 3.0361e-01, 1.6276e-01,
        7.7791e-02, 2.8637e-01, 1.2696e-02, 2.2324e-02, 1.5766e-02, 4.7721e-02,
        3.2907e-01, 1.0751e-01, 1.7801e+00, 2.6595e-01, 4.9955e-02, 4.4662e-02,
        4.0427e-02, 7.6372e-02, 4.0525e-01, 2.8414e-03, 8.9764e-02, 7.8407e-02,
        5.0923e-02, 4.8422e-02, 6.1694e-02, 1.1164e+00, 2.2666e+00, 9.2450e+00,
        2.0480e+00, 6.3296e+00, 3.7922e+00, 6.6787e+00, 4.7874e-02, 1.7707e-02,
        2.0898e-02, 1.3671e-01, 1.8581e-02, 2.2741e-02, 4.0154e-02, 4.0230e-02,
        8.3445e-02, 1.5040e-01, 5.4802e+00, 6.1429e-01, 3.6400e+00, 8.2635e-02,
        9.3260e+00, 8.4531e-02, 2.2705e+00, 3.6585e-01, 3.4070e-01, 1.5177e-01,
        3.1943e-01, 4.7076e-01, 1.6664e+00, 1.1369e+00, 5.5028e-02, 1.2146e-01,
        1.1472e+00, 4.5717e+00, 1.0006e+01, 3.8416e+00, 1.6702e+00, 7.6850e+00,
        4.1741e-02, 8.1279e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [16/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 100.5
Sum_Val Meta Model:  tensor([4.9664e+00, 1.9157e+00, 9.9300e+00, 1.2309e+00, 1.7164e-02, 2.9923e-01,
        3.2054e-02, 2.5448e-01, 2.2731e-01, 1.4287e-01, 3.4492e-02, 9.5843e-02,
        3.0681e-02, 3.6934e-01, 2.1912e-01, 2.1651e-01, 2.2261e+01, 6.4033e-02,
        1.9989e-02, 6.2997e-02, 2.3846e-03, 2.1412e-03, 4.1298e-03, 1.3657e-02,
        5.6622e-01, 1.8628e-01, 1.6242e+00, 4.4670e-02, 7.1492e-02, 1.4628e-01,
        1.6923e-02, 4.4073e-03, 5.0938e-01, 7.0093e-04, 1.6956e-02, 5.9177e-02,
        3.1551e-02, 8.8241e-02, 2.8319e-02, 4.2418e+00, 2.2300e+00, 1.1532e+01,
        3.7430e+00, 8.1839e+00, 1.5382e+01, 1.1879e+01, 1.3732e-02, 6.6044e-02,
        1.2301e-02, 7.1873e-02, 1.4272e-03, 3.5470e-03, 7.9487e-02, 1.7069e-02,
        7.1222e-03, 3.2922e-02, 9.6598e+00, 3.2472e-01, 2.3702e+00, 4.7534e-02,
        8.9236e+00, 4.7768e-01, 1.5076e+00, 2.7779e-01, 2.6828e-02, 5.1232e-02,
        5.4564e-02, 3.2673e-01, 3.9461e+00, 2.2441e+00, 3.1449e-03, 4.1742e-01,
        6.2219e+00, 1.7434e+00, 1.3841e+01, 5.8399e+00, 3.8734e-01, 1.0287e+01,
        3.1158e-02, 9.4006e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.4024e+00, 1.4190e+00, 7.1945e+00, 7.2760e-01, 4.0765e-03, 2.9229e-01,
        2.2722e-02, 2.5618e-01, 1.7364e-01, 1.3362e-01, 3.7587e-02, 1.0019e-01,
        3.6728e-02, 3.8977e-01, 2.3203e-01, 5.0078e-01, 1.5207e+01, 1.0810e-01,
        1.5287e-02, 8.6653e-02, 3.1188e-03, 1.9496e-03, 1.7348e-03, 2.0244e-02,
        5.1278e-01, 1.8310e-01, 1.5361e+00, 7.6300e-02, 1.0253e-01, 1.8129e-01,
        7.7520e-03, 2.7103e-03, 5.1011e-01, 1.2150e-03, 2.4129e-02, 6.4628e-02,
        7.0559e-02, 6.6883e-02, 3.8050e-02, 3.6307e+00, 2.4798e+00, 1.2682e+01,
        4.0755e+00, 6.8511e+00, 1.7620e+01, 1.0837e+01, 1.0298e-02, 6.1197e-02,
        7.2885e-03, 5.4110e-02, 3.8580e-04, 1.7240e-03, 7.8865e-02, 4.5463e-03,
        6.8561e-03, 1.2167e-02, 8.0586e+00, 3.1988e-01, 2.0424e+00, 8.5784e-02,
        1.0150e+01, 2.0038e-01, 1.2031e+00, 3.2015e-01, 1.6013e-02, 8.0323e-02,
        3.4694e-02, 4.3798e-01, 3.7105e+00, 1.8264e+00, 3.9100e-03, 4.4757e-01,
        5.3163e+00, 1.8386e+00, 1.4469e+01, 1.1403e+01, 2.6495e-01, 9.2376e+00,
        3.0758e-02, 1.8829e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.6472e+01, 5.2413e+01, 5.8145e+01, 2.5682e+01, 3.4777e-01, 1.1904e+01,
        3.8102e+00, 1.7034e+01, 5.1424e+00, 8.6785e+00, 7.2226e+00, 1.1523e+01,
        8.3790e+00, 1.5513e+01, 7.3146e+00, 1.7140e+01, 3.7657e+02, 4.3582e+00,
        5.4465e-01, 2.5937e+00, 1.6160e+00, 3.2993e-01, 1.8720e-01, 3.0770e+00,
        2.7565e+01, 1.8282e+01, 2.6465e+01, 7.6130e+00, 1.5800e+01, 1.5562e+01,
        7.6225e-01, 4.0024e-01, 7.8333e+00, 8.3488e-01, 2.0702e+00, 1.8958e+00,
        7.7375e+00, 3.6344e+00, 1.4105e+00, 4.9152e+01, 1.1897e+01, 2.6560e+01,
        7.5144e+00, 1.3478e+01, 2.0363e+01, 1.5535e+01, 1.1224e+00, 5.2506e+00,
        5.2837e-01, 3.2915e+00, 1.0789e-01, 2.0514e-01, 7.6105e+00, 2.8648e-01,
        1.1208e+00, 5.8148e-01, 2.5205e+01, 6.7629e+00, 1.2475e+01, 7.3350e+00,
        1.3534e+01, 6.4890e+00, 5.6359e+00, 6.3544e+00, 3.1430e-01, 3.3912e+00,
        4.4074e-01, 1.1092e+01, 9.6224e+00, 1.4046e+01, 2.4759e-01, 1.7546e+01,
        1.1975e+01, 1.0648e+01, 1.7077e+01, 1.1431e+01, 2.6503e-01, 9.2635e+00,
        9.6703e-02, 1.8781e+00], device='cuda:0')
Outer loop valEpocw Maximum [16/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 172.4
model_train val_loss valEpocw [16/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 165.8
model_train val_loss valEpocw [16/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1201.6
Sum_Val Meta Model:  tensor([8.6675e+00, 6.5757e-01, 1.0291e+01, 3.5186e-01, 9.4433e-03, 1.6432e+00,
        1.5088e+00, 1.5552e+00, 1.8096e+00, 1.1024e+00, 2.2606e-01, 4.2112e+00,
        1.0173e-01, 2.0029e-01, 6.9926e-01, 1.0912e+00, 7.5247e-01, 7.7348e-01,
        5.9822e-01, 2.4644e+00, 9.2555e-04, 1.3596e-03, 1.2447e-02, 6.3010e-02,
        7.4925e-01, 4.4981e-01, 2.7426e+00, 3.1923e-01, 1.9740e-01, 2.5547e-03,
        4.3865e-03, 2.3677e-03, 2.4773e-02, 8.7177e-04, 1.9602e-03, 4.8723e-03,
        6.9817e-02, 7.1577e-03, 3.4106e-03, 2.0221e-01, 6.2356e-02, 2.2191e+00,
        1.1142e-01, 2.1901e-01, 3.9013e-01, 3.0401e+00, 1.6201e-02, 1.2113e-02,
        4.1636e-03, 1.8301e-01, 6.4516e-04, 3.3041e-03, 1.7065e-03, 2.0570e-03,
        4.0439e-03, 9.9206e-02, 4.0117e+00, 4.4990e-01, 1.5722e+00, 9.9217e-02,
        1.8689e+00, 2.0630e-02, 6.3526e-01, 1.8786e-01, 4.5353e-02, 6.1037e-02,
        9.3731e-02, 9.8293e-01, 9.0646e-02, 7.0651e-01, 1.9916e-03, 3.1316e-02,
        1.8071e+00, 1.1082e+00, 3.6964e+00, 4.2782e-01, 2.2261e-01, 9.6958e-01,
        2.1935e-02, 2.3947e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([7.3881e+00, 7.5446e-01, 7.6843e+00, 4.2364e-01, 4.6988e-02, 1.7330e+00,
        7.6093e-01, 1.1920e+00, 1.3713e+00, 8.3401e-01, 1.4653e-01, 1.3898e+00,
        1.0154e-01, 3.5488e-01, 8.4382e-01, 1.9922e-01, 5.7839e-01, 7.5817e-01,
        1.0229e-01, 1.0434e+00, 4.7580e-03, 2.2007e-03, 4.0078e-03, 5.9378e-02,
        6.8237e-01, 4.3619e-01, 2.6757e+00, 2.9133e-01, 1.8292e-01, 2.8716e-02,
        7.1154e-03, 2.2658e-03, 9.4397e-02, 9.1226e-03, 2.4615e-02, 2.6404e-02,
        4.2652e-02, 4.9782e-02, 6.6201e-03, 8.1101e-02, 3.0931e-02, 1.7286e+00,
        2.4038e-02, 1.1296e-01, 1.1863e-01, 1.0804e+00, 1.0525e-02, 6.6039e-03,
        4.9865e-03, 1.9932e-01, 5.5236e-04, 2.0465e-03, 4.4460e-03, 6.6362e-03,
        5.6374e-03, 4.1758e-02, 3.1057e+00, 2.8330e-01, 1.1784e+00, 1.8545e-02,
        8.7352e-01, 8.8322e-03, 3.3290e-01, 3.5270e-02, 8.3513e-03, 1.8975e-02,
        1.4661e-02, 1.0691e+00, 5.7746e-02, 4.3040e-01, 4.4644e-04, 1.2559e-02,
        4.5110e-01, 7.1596e-01, 5.6072e+00, 3.9584e-02, 1.9653e-01, 6.1224e-01,
        3.3157e-03, 3.3185e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.8119e+01, 1.7964e+01, 4.6373e+01, 9.3714e+00, 2.3420e+00, 3.9026e+01,
        4.7503e+01, 3.9592e+01, 2.5066e+01, 3.0888e+01, 1.3514e+01, 7.2983e+01,
        9.7917e+00, 1.0021e+01, 1.6414e+01, 3.6629e+00, 9.2374e+00, 1.6365e+01,
        1.5875e+00, 1.4938e+01, 8.8433e-01, 1.7080e-01, 2.0801e-01, 4.6456e+00,
        2.2391e+01, 2.0504e+01, 3.2321e+01, 1.3665e+01, 1.2072e+01, 1.2453e+00,
        3.4506e-01, 1.5499e-01, 1.0364e+00, 2.2473e+00, 1.1514e+00, 5.2733e-01,
        2.2778e+00, 1.5072e+00, 1.4732e-01, 9.3111e-01, 1.5174e-01, 3.8838e+00,
        4.9333e-02, 2.3726e-01, 1.4631e-01, 1.6722e+00, 5.5647e-01, 2.7864e-01,
        1.7826e-01, 5.9612e+00, 6.3969e-02, 1.2503e-01, 2.1576e-01, 2.1813e-01,
        4.1836e-01, 1.1482e+00, 9.1181e+00, 3.7453e+00, 6.4820e+00, 7.5348e-01,
        1.2318e+00, 1.8549e-01, 1.3579e+00, 4.5845e-01, 1.1463e-01, 4.4657e-01,
        1.3847e-01, 1.9027e+01, 1.6311e-01, 2.9801e+00, 1.6076e-02, 3.1950e-01,
        1.1464e+00, 3.4338e+00, 7.2369e+00, 3.9812e-02, 1.9672e-01, 6.1600e-01,
        9.5431e-03, 2.7208e-01], device='cuda:0')
Outer loop valEpocw Maximum [16/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 69.1
model_train val_loss valEpocw [16/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 50.9
model_train val_loss valEpocw [16/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 678.0
Sum_Val Meta Model:  tensor([6.9636e+00, 7.1866e-02, 1.2935e+00, 6.4139e-02, 8.1718e-03, 5.3796e-02,
        7.2581e-03, 1.4900e-01, 6.3990e-02, 2.5955e-02, 7.3509e-03, 9.6468e-03,
        1.9895e-03, 3.9399e-01, 8.2828e-02, 9.2348e-02, 2.7615e-01, 4.1686e-02,
        5.7643e-02, 8.5056e-02, 9.4208e-04, 9.5601e-03, 7.5753e-02, 1.3938e-02,
        1.0387e-01, 1.4350e-02, 9.3872e-01, 5.8193e-02, 6.5557e-03, 3.5354e-02,
        4.4075e-02, 3.6096e-02, 5.2786e-01, 4.4098e-04, 2.8935e-02, 9.4012e-02,
        1.3978e-01, 2.3390e-01, 1.4344e-02, 1.6949e+00, 6.4933e+00, 3.3519e+01,
        4.4906e+01, 4.2576e+01, 4.4566e+01, 4.8262e+01, 1.2571e-01, 1.3311e-01,
        7.4559e-01, 2.5944e-01, 1.1473e-01, 3.1710e-01, 5.8829e-01, 2.1142e-01,
        3.9797e-01, 2.1110e+00, 4.7509e+01, 5.6157e-01, 1.3420e+00, 3.3091e-02,
        6.6248e+01, 4.6939e-02, 2.4264e+00, 6.4930e-01, 7.0914e-01, 3.6711e-02,
        8.1973e-01, 3.2891e-01, 1.6445e+00, 3.2257e-01, 6.0946e-03, 1.2409e-01,
        2.0517e+00, 5.4487e+00, 1.5958e+00, 2.3214e+01, 9.4535e+00, 1.4970e+00,
        1.1539e-01, 8.2426e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.8245e+00, 6.9225e-03, 5.2744e-01, 3.3087e-03, 4.7453e-04, 1.8589e-03,
        8.4925e-04, 1.3302e-01, 5.5521e-03, 1.0700e-03, 1.6028e-04, 2.2140e-04,
        1.3428e-04, 2.6063e-01, 3.3013e-03, 1.3862e-02, 2.8423e-02, 4.8326e-03,
        2.4263e-03, 2.2016e-03, 5.5383e-05, 6.8599e-05, 1.2355e-04, 3.2685e-04,
        5.2999e-02, 6.4443e-03, 1.0309e+00, 4.3981e-02, 3.7676e-03, 2.7060e-03,
        4.0549e-03, 3.8601e-02, 4.9467e-01, 5.1707e-05, 8.3469e-03, 2.1309e-02,
        8.8996e-02, 2.0797e-01, 9.8151e-03, 1.8594e+00, 4.1836e+00, 2.6347e+01,
        3.8272e+01, 3.7013e+01, 4.3593e+01, 5.5129e+01, 7.1480e-02, 7.4788e-02,
        5.0222e-01, 1.4477e-01, 5.9623e-02, 3.3837e-01, 6.3570e-01, 3.0682e-01,
        2.5194e-01, 1.4586e+00, 2.6403e+01, 5.5997e-01, 1.1111e+00, 1.7715e-02,
        7.8229e+01, 5.7172e-03, 1.7123e+00, 4.7675e-01, 5.5596e-01, 9.8662e-02,
        5.6212e-01, 3.7179e-01, 2.1374e+00, 4.9273e-01, 1.5581e-03, 1.2842e-01,
        2.5779e+00, 6.0477e+00, 2.1148e+00, 2.0192e+01, 1.1390e+01, 1.4674e+00,
        2.5980e-02, 9.9458e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.4083e+01, 3.7854e-01, 4.9182e+00, 1.7248e-01, 7.0184e-02, 1.1090e-01,
        2.5528e-01, 1.3897e+01, 2.4453e-01, 1.0935e-01, 5.2247e-02, 4.1338e-02,
        5.5774e-02, 1.4961e+01, 1.4826e-01, 6.4714e-01, 8.3117e-01, 2.8775e-01,
        1.0864e-01, 8.8390e-02, 6.6923e-02, 1.9087e-02, 1.7602e-02, 7.7682e-02,
        4.2282e+00, 1.1478e+00, 2.6687e+01, 7.8907e+00, 1.0037e+00, 3.3542e-01,
        6.1965e-01, 8.1848e+00, 9.4778e+00, 6.2143e-02, 9.8895e-01, 8.0354e-01,
        1.3329e+01, 1.4420e+01, 5.3364e-01, 3.4339e+01, 2.4030e+01, 5.3933e+01,
        6.3199e+01, 6.4022e+01, 4.8834e+01, 7.4076e+01, 9.8622e+00, 8.0759e+00,
        4.0409e+01, 1.0595e+01, 2.2180e+01, 4.6240e+01, 7.2413e+01, 2.6106e+01,
        5.9402e+01, 8.2123e+01, 9.2074e+01, 1.5443e+01, 8.0132e+00, 2.1452e+00,
        9.9320e+01, 2.8567e-01, 8.7376e+00, 1.2492e+01, 1.4130e+01, 5.9254e+00,
        9.1311e+00, 1.4710e+01, 6.7382e+00, 5.3568e+00, 1.6883e-01, 8.0947e+00,
        7.1667e+00, 4.0465e+01, 2.5143e+00, 2.0218e+01, 1.1391e+01, 1.4694e+00,
        8.9493e-02, 1.2807e+00], device='cuda:0')
Outer loop valEpocw Maximum [16/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 405.4
model_train val_loss valEpocw [16/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 371.9
model_train val_loss valEpocw [16/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1278.6
Sum_Val Meta Model:  tensor([1.6391e+01, 8.7943e-02, 3.4233e+00, 4.1329e-02, 7.6312e-03, 5.0993e-01,
        8.3989e-02, 4.9060e-01, 6.8644e-02, 5.9360e-01, 5.8769e-02, 1.0300e-01,
        1.6519e-03, 6.6893e-01, 7.9507e-01, 7.3546e-02, 1.0157e-01, 3.0492e-02,
        1.9397e-02, 3.2710e-02, 1.3295e-03, 4.6386e-03, 6.7742e-03, 1.0774e-02,
        7.3789e-01, 2.7682e-02, 2.4910e+00, 3.0272e-02, 3.2289e-01, 9.9347e-03,
        1.5019e-02, 2.5134e-03, 2.9870e-01, 9.3667e-03, 3.8546e-02, 3.4110e-01,
        6.5247e-03, 1.9522e-02, 2.0271e-01, 2.5027e+00, 3.4795e+00, 1.3662e+01,
        4.7196e+00, 4.4913e+00, 8.4995e+00, 1.1046e+01, 8.5354e-03, 1.3409e-02,
        3.7422e-02, 1.2801e-02, 6.3111e-03, 8.5360e-03, 6.8678e-03, 1.6490e-01,
        1.3032e-02, 5.8021e-02, 6.7329e+00, 2.9887e-01, 2.6659e+00, 3.2319e-02,
        3.0727e+01, 2.5989e-02, 1.0363e+00, 5.6083e-01, 3.1066e-01, 9.3028e-02,
        4.6045e-01, 3.3445e+00, 4.9467e-01, 3.7710e-01, 2.4076e-03, 6.5143e-02,
        2.5508e+00, 2.0809e+00, 3.7287e+02, 1.0750e+01, 1.5133e+00, 1.1262e+01,
        2.4107e-02, 1.2560e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.7008e+00, 9.2638e-02, 3.3210e+00, 6.3446e-02, 2.5862e-02, 4.0478e-01,
        1.2307e-01, 4.2681e-01, 1.7404e-01, 4.6909e-01, 4.1613e-02, 1.6037e-01,
        1.2590e-02, 6.2681e-01, 9.0636e-01, 3.8836e-02, 4.4075e-02, 4.0203e-02,
        4.9089e-03, 1.3875e-02, 2.5523e-03, 5.9531e-04, 3.6599e-03, 2.2458e-02,
        6.8759e-01, 7.6044e-02, 1.9245e+00, 4.9729e-02, 2.8728e-01, 4.3823e-03,
        5.8555e-03, 3.5479e-03, 4.5950e-02, 6.0053e-04, 7.9392e-03, 1.6902e-02,
        1.0794e-02, 7.4207e-03, 2.4589e-02, 3.2601e-01, 1.5002e-01, 9.5551e-01,
        1.8920e-01, 5.7818e-01, 2.9553e-01, 8.2688e-01, 5.2055e-03, 4.6308e-03,
        3.7430e-03, 7.1610e-03, 4.8831e-04, 8.6151e-04, 8.6668e-04, 5.5327e-03,
        4.8225e-03, 2.8083e-02, 1.6863e+00, 9.9675e-02, 2.2375e+00, 2.6245e-02,
        8.7381e+00, 1.4858e-02, 5.1207e-01, 4.0807e-02, 1.8938e-02, 5.8242e-02,
        5.1940e-02, 4.8112e-01, 1.9746e-01, 1.1192e-01, 1.6615e-03, 3.2038e-02,
        3.0005e-01, 1.3261e+00, 4.0589e+01, 4.9410e+00, 7.9818e-02, 5.4723e+00,
        8.8005e-03, 1.6479e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.0045e+01, 2.1905e+00, 2.1245e+01, 1.3954e+00, 1.1297e+00, 9.5135e+00,
        8.5742e+00, 1.5182e+01, 3.3706e+00, 1.6370e+01, 3.4806e+00, 8.4524e+00,
        1.1703e+00, 1.5120e+01, 1.8453e+01, 8.4540e-01, 7.3126e-01, 9.0799e-01,
        1.0342e-01, 2.7929e-01, 4.9961e-01, 4.3636e-02, 1.8866e-01, 1.5867e+00,
        2.3281e+01, 3.6422e+00, 2.3098e+01, 2.3261e+00, 1.8312e+01, 1.8600e-01,
        2.8151e-01, 2.3042e-01, 4.3918e-01, 1.2580e-01, 3.3692e-01, 2.7976e-01,
        5.5138e-01, 2.3218e-01, 4.5316e-01, 3.1894e+00, 5.9882e-01, 1.9765e+00,
        3.6823e-01, 1.1701e+00, 3.6140e-01, 1.2658e+00, 2.8754e-01, 2.0007e-01,
        1.3134e-01, 2.3209e-01, 5.6755e-02, 5.3594e-02, 4.0056e-02, 1.5854e-01,
        3.5901e-01, 7.8200e-01, 4.8178e+00, 1.3726e+00, 1.1609e+01, 1.1376e+00,
        1.2031e+01, 2.8225e-01, 2.1021e+00, 4.7296e-01, 2.2416e-01, 1.3503e+00,
        4.5046e-01, 6.6693e+00, 5.4966e-01, 7.2503e-01, 5.8929e-02, 7.3603e-01,
        7.0172e-01, 6.2892e+00, 5.3683e+01, 4.9726e+00, 7.9923e-02, 5.5130e+00,
        2.9576e-02, 1.1724e-01], device='cuda:0')
Outer loop valEpocw Maximum [16/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 525.3
model_train val_loss valEpocw [16/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 86.3
model_train val_loss valEpocw [16/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 371.9
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.21872297 97.34530525 93.19209074 98.03730461 98.579452   97.57922053
 98.31020577 95.20108186 98.02877645 96.97493939 98.61356465 98.93763478
 99.43226813 95.38626479 97.3660165  97.21981945 96.35116531 97.63769935
 98.79996589 98.38574091 98.66351531 99.30678232 99.53704268 99.03753609
 95.2485959  96.80681278 94.43598397 97.02732667 98.07385388 98.25903681
 98.31507901 98.60138156 97.02489005 98.5234098  98.73783214 98.91570522
 97.59871347 98.43081834 98.9412897  93.22864    98.00319197 94.40674456
 97.59627685 96.94326336 97.71079787 96.15867253 98.22370585 98.67204347
 98.133551   98.73539552 98.77803633 98.59163509 99.01560654 98.37843106
 98.71468428 97.6169881  92.77055591 96.91524226 96.56924258 97.53170649
 95.52393368 98.55386752 97.42815024 97.40012914 98.83773346 97.54510788
 98.64645899 95.97105298 98.89133904 98.36502967 99.81603538 97.47566428
 98.76585324 96.0709543  98.17619181 98.42229018 99.39937379 99.15814866
 99.84405648 99.15936697]
Accuracy th:0.7 is [87.0140471  97.26611518 92.66821798 97.85090338 98.46005775 97.3806362
 98.21883262 94.97569474 97.94105822 96.80437617 98.5514309  98.88159257
 99.42130335 95.32047612 97.31850245 96.94448167 96.30852451 97.56581913
 98.7317406  98.39061415 98.57092384 99.24830351 99.4980568  98.94738125
 95.22788465 96.72762271 94.26054751 96.97859432 98.03974123 98.18350166
 98.12745946 98.57579708 96.73249595 98.35162827 98.64524068 98.83895177
 97.33190385 98.25660019 98.83773346 92.85340091 97.89232587 93.69037902
 97.20519974 96.61432    97.38550944 95.86506012 98.1603538  98.63671252
 98.06410741 98.72199413 98.64767729 98.57336046 99.00951499 98.3565015
 98.71102935 97.52074171 91.81540186 96.83361557 96.44253847 97.52317832
 94.4822797  98.31264239 97.34530525 97.23931239 98.75245185 97.56094589
 98.58676186 95.95643328 98.83773346 98.29558607 99.81603538 97.30266444
 98.82676868 95.84678549 98.01659337 98.17375519 99.33724004 99.1215994
 99.84405648 99.15814866]
Avg Prec: is [96.37882012 33.51994033 71.64883556 65.79458746 77.85961507 64.49022242
 72.51257149 46.81537523 55.45480688 51.56077207 29.43613747 52.61943171
 22.37450923 27.02682136 32.63132492 55.18914117 27.69403654 37.59627194
 45.34174901 36.40469504 59.28811285 47.49141846 89.46620321 81.86359039
 25.79111583 30.56937369 37.89285626 37.95815705 23.66245876 37.34643306
 72.18794141 35.70578736 56.68454961 61.93019166 72.63632478 79.435336
 57.4059749  74.82316677 86.79129874 46.11691738 40.70185511 68.25093912
 64.943093   58.74738193 66.98913478 73.22997049 36.99514233 33.48354498
 39.78589045 43.43052801 59.71060306 35.27263507 20.44226952 70.59472132
 24.01417881 40.25903436 70.47301297 57.24926569 41.64120183 56.86657258
 86.70853708 81.88528215 69.16534336 47.59309228 58.64996624 41.00973597
 59.86794319 23.57468658 52.0788424  67.18524823 10.81423831 72.22288901
 75.17916891 48.99170198 78.60956316 82.52048953 59.29418267 80.40355286
  8.44360791 23.85674832]
Accuracy th:0.5 is [45.56718364 97.2137279  72.17626491 97.02489005 97.26733349 77.15792936
 77.47834456 76.48907786 78.39938597 96.4474117  78.73198426 98.52097319
 99.41399349 80.44370804 78.27146355 96.56680596 96.29512311 78.10699187
 98.65376884 98.30776915 80.743412   79.27413165 98.38695922 78.32994237
 80.95174279 96.65086926 94.0778012  77.75246403 98.01293844 78.61259
 97.30875598 98.57457877 96.36213009 98.02024829 87.7937647  78.32628745
 78.04973136 91.02106456 97.11504489 75.23665647 79.63718766 92.05906361
 77.54778816 77.08970407 96.9627563  93.87434364 98.02877645 98.57336046
 96.95179152 88.83176375 87.65244088 98.55508583 98.99976852 77.68302043
 98.70615611 78.00709056 72.63191238 93.37240043 96.24273583 96.9067141
 89.79300934 97.17717864 92.79492209 78.05216798 98.42838172 78.58944214
 98.20786784 77.25173914 79.12427967 97.55972759 79.69444817 95.99054592
 78.82823065 95.45083515 77.42595729 82.94124097 87.30522289 78.67472375
 79.7541453  99.14718388]
Accuracy th:0.7 is [45.67073988 97.2137279  72.17626491 97.02489005 97.26733349 77.15792936
 77.47834456 76.72177483 78.39938597 96.4754328  78.73198426 98.52097319
 99.41399349 80.92128507 78.27146355 96.56680596 96.29512311 78.10699187
 98.65376884 98.30776915 81.16982006 79.27413165 98.38695922 78.59675199
 81.51703805 96.65086926 94.0778012  77.75246403 98.01293844 78.61259
 97.30875598 98.57457877 96.36213009 98.02024829 87.97285608 78.32628745
 78.04973136 91.29274741 97.11504489 75.23665647 80.30482085 92.05906361
 77.54778816 77.08970407 96.9627563  93.87434364 98.02877645 98.57336046
 97.65110074 89.42020687 87.80472947 98.55508583 98.99976852 77.68302043
 98.70615611 78.00709056 72.63191238 93.78053386 96.24273583 96.9067141
 89.79300934 97.17717864 92.97888671 78.05216798 98.42838172 78.58944214
 98.20786784 77.25173914 79.12427967 97.55972759 79.69444817 95.99054592
 78.89767425 95.45083515 77.42595729 83.00581133 87.46482134 78.67472375
 79.7541453  99.14718388]
Avg Prec: is [55.89287945  3.15090362 11.27503822  3.35816274  2.3121397   3.79060275
  3.4423877   5.69375265  2.58540584  3.79409116  1.62239935  1.5806687
  0.59259588  5.10973678  2.65064078  3.14214341  3.58541155  2.6486324
  1.33710708  1.71075736  2.06883388  0.81730514  1.84901104  2.29933499
  5.07847785  3.67019891  6.59597931  3.36696989  2.07292308  1.86619864
  2.56153335  1.33455917  3.60823936  1.65235017  2.3329055   2.33939938
  2.96075321  2.53734126  2.75191704  7.36388291  2.22092902  8.23577249
  3.35869007  4.0154914   3.29586579  6.25500061  2.06975801  1.49968559
  2.1474319   1.6065334   1.87001355  1.58423763  1.03789933  3.01035244
  1.27787952  2.64530126 11.31222042  3.77328195  3.95256166  2.84601348
 10.79486866  2.14275364  3.8706337   2.98133022  1.68387976  2.42707898
  1.91156831  4.17564427  1.25149848  2.35332227  0.19619456  3.47337583
  1.88819475  4.61166275  4.00659101  3.16453511  0.82328004  1.86458937
  0.13726214  0.74211112]
mAP score regular 53.11, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [87.4878541  97.37399407 93.14348357 98.29583676 98.99095598 97.64805541
 98.44532476 95.30607669 98.08904502 97.06006926 98.64713357 99.02832798
 99.37464185 95.20392655 97.39890874 97.15723647 96.32010365 97.69290181
 98.92368637 98.32573436 98.84146797 99.33726985 99.62877146 99.23262825
 95.44559882 96.76109326 94.5411964  97.17467673 97.88972768 98.22109276
 98.70692877 98.67703117 97.02518873 98.67204823 98.91122904 99.09061464
 97.92959115 98.32324289 99.07068291 93.1708897  97.91464235 93.53713531
 97.39143434 96.62655405 97.0501034  94.73802227 98.35314049 98.80658744
 98.10150235 98.70692877 98.79662157 98.63218477 98.87385704 98.39798689
 98.75924957 97.78259461 91.10297232 97.03266313 96.36744151 97.54092234
 93.2082617  98.6745397  97.27931833 97.46866981 98.7567581  97.39143434
 98.5624237  95.83177617 98.73931784 98.27092209 99.81563146 97.49109301
 98.29832823 95.77447243 97.4238234  97.53843087 99.20771358 98.65709943
 99.82559733 99.06819144]
Accuracy th:0.7 is [88.15058425 97.29177567 93.09863717 98.19119516 98.94361811 97.39641727
 98.4577821  95.27617909 98.11146822 96.87320926 98.5773725  99.00341331
 99.36467598 95.12419962 97.30672447 96.89064953 96.24286818 97.64307248
 98.85641677 98.41791863 98.87634851 99.30238932 99.58392506 99.22266238
 95.45307322 96.66143459 94.49136707 97.192117   97.85235568 98.18122929
 98.50761143 98.6820141  96.68634925 98.5474749  98.87385704 99.06819144
 97.65552981 98.16129756 99.02334504 92.88437103 97.88225328 93.44245958
 97.2743354  96.71624685 97.0725266  94.81774921 98.32573436 98.82651917
 98.06413035 98.73682637 98.6894885  98.59481277 98.88631437 98.43785036
 98.71440317 97.66300421 90.97590752 97.09993273 96.29768044 97.52348207
 92.97406383 98.40795276 97.20457433 97.30672447 98.69197997 97.62812368
 98.5624237  95.7844383  98.7791813  98.19617809 99.81563146 97.51600767
 98.4353589  95.84174203 97.47116127 97.56334554 99.22515385 98.70942024
 99.82559733 99.17532451]
Avg Prec: is [96.49438111 32.55660566 71.21632288 73.00641333 78.00358979 64.74577914
 79.41055978 47.30828534 61.27799066 55.36840508 36.34819085 56.52960001
 21.57890816 29.96361645 33.00992997 60.5913043  29.92709201 41.67465905
 47.59135341 35.84247373 68.01855303 54.66799689 92.326993   87.8152981
 25.07864278 34.11498729 33.95569031 44.59661053 25.81415403 36.34756512
 77.48047661 37.36433086 54.36907127 63.79970063 73.48625381 81.82854989
 59.39853341 77.3789841  89.60891818 43.97341774 34.83871224 53.91733315
 49.44361509 42.2296577  33.74622659 52.87679799 36.73167782 28.59985403
 40.93512291 40.20945396 64.89906055 35.22754439 20.67361957 75.05294054
 26.58622529 40.86314352 57.89188745 55.59470298 36.78673574 60.5978632
 67.304322   85.77745498 66.49810843 50.9878927  60.88862956 38.17958564
 60.78747946 25.30445761 40.93528606 64.96741716  9.18641552 73.80925345
 57.64761347 43.85447203 66.31605263 51.22186511 10.48195658 54.43397289
  2.11426873 22.23201672]
Accuracy th:0.5 is [45.25998455 97.22450607 70.60816703 96.96290206 97.90716795 76.12427436
 76.2114757  75.09280714 77.66898373 96.41976231 77.84836934 98.5325261
 99.34972718 78.27191868 77.76615093 96.31262924 96.21047911 77.18813065
 98.78167277 98.34068316 79.20372723 78.48369335 98.31327703 77.8882328
 78.24949548 96.52938685 94.3393876  77.23048559 97.81747515 77.81099733
 97.52597354 98.67204823 96.39983058 98.18870369 88.83822907 77.42232852
 77.38993946 92.32129955 97.0276802  74.50980392 77.89072427 92.37362035
 76.56028104 76.22891596 97.03764606 94.02795426 98.18621222 98.77668984
 97.49607594 88.37979919 86.0428034  98.55993223 98.87385704 76.55031517
 98.6969629  77.22301119 71.20113611 94.38921693 96.16314124 96.78102499
 90.13379176 97.04761193 92.79716969 77.20557092 98.32075143 78.05017814
 98.13139996 76.3759125  78.50611655 97.53593941 78.94959763 96.07843137
 78.25946134 95.44559882 76.30365996 83.83038095 89.29416748 77.87826694
 79.0193587  99.15040985]
Accuracy th:0.7 is [45.45930189 97.22450607 70.60816703 96.96290206 97.90716795 76.12427436
 76.2114757  75.20492314 77.66898373 96.41976231 77.84836934 98.5325261
 99.34972718 78.67304482 77.76615093 96.31262924 96.21047911 77.18813065
 98.78167277 98.34068316 79.49772031 78.48369335 98.31327703 78.03522934
 78.70045096 96.52938685 94.3393876  77.23048559 97.81747515 77.81099733
 97.52597354 98.67204823 96.39983058 98.18870369 88.99519147 77.42232852
 77.38993946 92.56546329 97.0276802  74.50980392 78.33420535 92.37362035
 76.56028104 76.22891596 97.03764606 94.02795426 98.18621222 98.77668984
 97.80750928 88.693724   86.18730847 98.55993223 98.87385704 76.55031517
 98.6969629  77.22301119 71.20113611 94.67822707 96.16314124 96.78102499
 90.13379176 97.04761193 92.95911503 77.20557092 98.32075143 78.05017814
 98.13139996 76.3759125  78.50611655 97.53593941 78.94959763 96.07843137
 78.26693575 95.44559882 76.30365996 83.90263348 89.45860428 77.87826694
 79.0193587  99.15040985]
Avg Prec: is [54.16198681  3.71975386 14.92239143  4.5417799   1.45560118  4.25519491
 13.36051649  8.66866434  7.97617732  5.26812943  2.33124123  4.81671128
  1.83321991  5.87722895  2.94199395  3.67779138 24.28734172  6.58590604
  1.55204938  2.69709496  3.53640581  1.48017785  1.20301415  5.32013905
  5.6851894   9.58150287  7.9468829   4.6127578   3.91557126  5.79480275
  2.25470414  0.85974133  3.12427094  1.09827129  1.72704685  2.29041045
  2.00696829  2.20722647  2.25057696  6.17960035  1.73040158  6.01722429
  2.19116521  2.71528425  2.37068617  4.84025776  1.69017768  1.00950771
  1.39399192  1.14810249  1.16625888  0.96810874  0.73404167  2.30990863
  0.85323933  1.8247265  10.04373235  2.91296837  3.82796439  2.80596672
  7.82193515  2.06132609  3.18636011  2.50397861  1.34384549  1.83533534
  1.52814915  3.46234532  1.0809639   2.23587297  0.19151415  3.22610222
  1.57337912  3.9391006   3.54257969  2.30832154  0.59916128  1.48338808
  0.12717065  0.57902778]
mAP score regular 50.68, mAP score EMA 4.31
Train_data_mAP: current_mAP = 53.11, highest_mAP = 53.11
Val_data_mAP: current_mAP = 50.68, highest_mAP = 51.12
tensor([0.0904, 0.0238, 0.1194, 0.0251, 0.0102, 0.0220, 0.0055, 0.0137, 0.0301,
        0.0138, 0.0047, 0.0080, 0.0041, 0.0232, 0.0290, 0.0270, 0.0378, 0.0226,
        0.0269, 0.0293, 0.0016, 0.0055, 0.0087, 0.0062, 0.0165, 0.0092, 0.0542,
        0.0090, 0.0063, 0.0104, 0.0096, 0.0063, 0.0625, 0.0014, 0.0104, 0.0306,
        0.0083, 0.0159, 0.0260, 0.0681, 0.1979, 0.4746, 0.5647, 0.5339, 0.8795,
        0.7122, 0.0084, 0.0113, 0.0131, 0.0156, 0.0032, 0.0078, 0.0104, 0.0152,
        0.0055, 0.0198, 0.3065, 0.0427, 0.1631, 0.0104, 0.7535, 0.0277, 0.2060,
        0.0480, 0.0494, 0.0209, 0.0736, 0.0362, 0.4175, 0.1303, 0.0132, 0.0230,
        0.4347, 0.1714, 0.8531, 0.9981, 0.9998, 0.9980, 0.4060, 0.1038],
       device='cuda:0')
Sum Train Loss:  tensor([4.0653e+00, 3.1472e-01, 2.4023e+00, 1.6209e-01, 2.8266e-02, 3.9013e-01,
        8.4155e-02, 1.9328e-01, 1.9902e-01, 8.2434e-02, 5.0636e-02, 2.3585e-02,
        3.6908e-03, 5.9971e-01, 4.7918e-01, 2.8808e-01, 1.0728e+00, 1.2405e-01,
        5.4059e-02, 2.4446e-01, 5.2035e-03, 1.4624e-02, 6.9079e-03, 6.1605e-02,
        3.0099e-01, 6.0512e-02, 7.2468e-01, 1.9609e-01, 2.0975e-02, 9.0460e-02,
        2.6999e-02, 1.7149e-02, 3.8936e-01, 6.1047e-03, 1.7509e-02, 3.4843e-02,
        8.4729e-02, 6.9195e-02, 2.5531e-02, 1.8116e+00, 1.9946e+00, 9.7415e+00,
        2.3671e+00, 6.1613e+00, 2.0542e+00, 9.3265e+00, 8.3160e-02, 8.1667e-02,
        9.8338e-02, 8.7721e-02, 1.0740e-02, 8.4096e-03, 6.0081e-03, 6.3600e-02,
        3.8630e-02, 3.0580e-01, 7.8147e+00, 2.4747e-01, 2.0534e+00, 3.8304e-02,
        9.2269e+00, 4.6960e-02, 9.5312e-01, 6.4733e-01, 3.6896e-01, 1.2604e-01,
        5.9228e-01, 7.4416e-01, 3.1234e+00, 1.0309e+00, 3.5067e-03, 1.2233e-01,
        2.3607e+00, 1.8368e+00, 4.1492e+00, 2.1485e+00, 6.7833e+00, 8.1987e-01,
        9.5176e-02, 4.0433e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [17/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 93.0
Sum Train Loss:  tensor([3.4537e+00, 2.8453e-01, 2.8493e+00, 3.2280e-01, 6.7866e-02, 2.0630e-01,
        4.3950e-02, 3.3585e-01, 1.1431e-01, 2.5603e-01, 4.2259e-02, 1.9469e-02,
        2.2957e-02, 2.4759e-01, 4.0301e-01, 2.9110e-01, 3.8540e-01, 1.1262e-01,
        2.9413e-02, 1.5943e-01, 2.2153e-02, 1.8287e-02, 7.5168e-03, 9.7859e-03,
        3.4810e-01, 1.6398e-01, 9.7256e-01, 8.5008e-02, 1.2195e-02, 4.8124e-02,
        4.6792e-02, 1.9193e-02, 5.9812e-01, 3.5412e-03, 6.7253e-02, 1.7218e-01,
        9.0091e-02, 9.2403e-02, 1.1217e-01, 1.3401e+00, 9.9972e-01, 8.0178e+00,
        9.0499e+00, 4.1416e+00, 9.2016e+00, 8.0020e+00, 3.1260e-02, 1.0091e-01,
        1.5536e-01, 2.0863e-01, 1.6064e-02, 2.7914e-02, 1.2700e-01, 6.2442e-02,
        5.5790e-03, 1.1649e-01, 1.0792e+01, 2.9543e-01, 1.5762e+00, 8.5552e-02,
        9.9606e+00, 1.5647e-01, 9.4241e-01, 2.6680e-01, 3.2729e-01, 2.3036e-01,
        5.0690e-01, 7.0829e-01, 3.1067e-01, 2.1356e-01, 1.7685e-03, 1.0412e-01,
        4.0122e+00, 1.6939e+00, 2.7858e+00, 3.7058e+00, 1.6108e+00, 5.7795e+00,
        1.6550e-01, 4.7960e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [17/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 100.9
Sum Train Loss:  tensor([3.7144e+00, 1.8717e-01, 2.6559e+00, 2.1219e-01, 3.5815e-02, 4.3739e-01,
        2.3539e-02, 3.3571e-01, 5.0414e-01, 1.8459e-01, 3.2871e-02, 1.2533e-01,
        4.2545e-03, 3.6255e-01, 3.4191e-01, 3.1847e-01, 8.4457e-01, 3.2375e-01,
        1.7695e-01, 3.8276e-01, 1.7537e-02, 4.8688e-02, 7.3828e-03, 1.5710e-02,
        3.3278e-01, 9.3388e-02, 1.2835e+00, 1.0668e-01, 9.8255e-02, 7.4723e-02,
        7.1520e-02, 5.0107e-02, 3.7214e-01, 1.3521e-02, 3.4174e-02, 9.3876e-02,
        3.3882e-02, 1.4174e-01, 6.6820e-02, 1.0987e+00, 1.2257e+00, 6.8136e+00,
        2.2878e+00, 2.4699e+00, 4.2641e+00, 4.8985e+00, 5.5833e-02, 5.6241e-02,
        1.8176e-01, 1.2080e-01, 2.9394e-02, 7.5091e-02, 3.0743e-02, 1.7702e-01,
        2.0321e-02, 1.2074e-01, 6.2245e+00, 3.2246e-01, 7.6192e-01, 1.6025e-01,
        9.5413e+00, 5.7853e-02, 1.6604e+00, 4.5380e-01, 7.2686e-02, 1.4965e-01,
        2.2296e-01, 7.4940e-01, 7.8701e-01, 6.8818e-01, 3.4576e-02, 1.2913e-01,
        8.8374e-01, 1.1795e+00, 6.7314e+00, 4.3096e+00, 6.0033e-01, 2.4817e+00,
        2.3262e+00, 5.9110e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [17/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 79.2
Sum Train Loss:  tensor([3.1080e+00, 1.3885e-01, 2.5034e+00, 2.7444e-01, 3.0558e-02, 2.9460e-01,
        2.0311e-02, 2.5568e-01, 1.7006e-01, 1.8187e-01, 1.3854e-02, 1.4928e-02,
        7.5538e-03, 2.8410e-01, 3.3707e-01, 4.1052e-01, 5.7322e-01, 2.2605e-01,
        2.0379e-02, 1.5588e-01, 2.0601e-02, 5.4048e-02, 6.4295e-03, 2.6250e-02,
        2.2582e-01, 6.0889e-02, 9.6900e-01, 1.0710e-01, 9.4371e-02, 3.1634e-02,
        3.4398e-02, 8.6955e-03, 7.1856e-01, 1.1298e-03, 1.1226e-01, 1.9971e-01,
        5.1592e-02, 2.1744e-01, 5.7297e-02, 1.2758e+00, 1.3027e+00, 7.9020e+00,
        4.2263e+00, 3.4122e+00, 4.1839e+00, 5.5863e+00, 4.4073e-02, 2.7617e-02,
        1.2357e-01, 8.4539e-02, 3.4495e-02, 1.6181e-02, 5.7156e-02, 9.9843e-02,
        2.4071e-02, 2.1583e-01, 7.0743e+00, 3.3412e-01, 3.3219e+00, 5.8953e-02,
        6.9049e+00, 2.9891e-02, 1.3953e+00, 4.2114e-01, 5.9252e-02, 1.1056e-01,
        1.2772e-01, 4.5077e-01, 1.5179e+00, 8.3988e-01, 3.8731e-03, 1.5342e-01,
        2.0983e+00, 1.5388e+00, 8.7621e+00, 3.2901e+00, 5.7075e-01, 8.7989e-01,
        1.7673e+00, 5.7403e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [17/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 82.9
Sum Train Loss:  tensor([3.1584e+00, 1.0011e-01, 1.9844e+00, 1.6275e-01, 5.3940e-02, 2.0733e-01,
        2.9456e-02, 1.5405e-01, 3.1352e-01, 1.2207e-01, 6.0069e-02, 8.5068e-02,
        2.8133e-02, 4.6665e-01, 3.2150e-01, 9.3140e-02, 6.5930e-01, 1.5820e-01,
        6.3996e-02, 2.3200e-01, 1.4550e-02, 2.6315e-02, 5.1165e-03, 4.4166e-02,
        3.5913e-01, 1.2721e-01, 1.3044e+00, 6.5947e-02, 1.2390e-01, 6.6700e-02,
        4.0218e-02, 2.6910e-02, 7.5227e-01, 1.3392e-02, 3.3402e-02, 8.4448e-02,
        7.5537e-02, 1.5793e-01, 3.7863e-02, 1.5232e+00, 1.7060e+00, 7.6997e+00,
        2.0565e+00, 5.1898e+00, 4.8794e+00, 5.4093e+00, 4.2495e-02, 1.3802e-02,
        1.6908e-01, 2.3255e-02, 1.7017e-02, 5.6467e-02, 7.9062e-02, 5.5690e-02,
        3.2081e-02, 2.2185e-01, 8.3417e+00, 3.6496e-01, 1.6421e+00, 1.4750e-01,
        9.9676e+00, 2.1121e-01, 9.9651e-01, 1.8077e-01, 4.9536e-02, 1.3405e-01,
        1.5754e-01, 6.1370e-01, 1.6492e+00, 3.4266e-01, 3.7858e-03, 1.1763e-01,
        1.7314e+00, 1.0033e+00, 9.4271e+00, 7.6231e+00, 4.5816e-01, 2.4860e+00,
        2.2596e+00, 6.1054e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [17/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 91.5
Sum Train Loss:  tensor([3.3588e+00, 8.1228e-02, 3.9375e+00, 6.6336e-02, 4.5178e-02, 1.5964e-01,
        3.4015e-02, 2.5951e-01, 1.5072e-01, 4.7735e-02, 8.0653e-03, 4.8429e-02,
        1.7715e-02, 3.0096e-01, 3.1163e-01, 2.6586e-01, 2.6277e-01, 3.3675e-01,
        1.6266e-01, 4.8399e-02, 3.3704e-02, 3.8832e-02, 4.0289e-03, 1.0048e-02,
        1.9007e-01, 1.1886e-01, 7.8329e-01, 1.2957e-01, 4.3584e-02, 2.5398e-02,
        6.2471e-02, 3.4435e-02, 5.9135e-01, 2.8292e-03, 2.7770e-02, 1.2254e-01,
        1.0272e-01, 9.9453e-02, 8.5964e-02, 8.8497e-01, 2.0424e+00, 7.6981e+00,
        3.9650e+00, 3.8860e+00, 7.3934e+00, 5.7112e+00, 5.0239e-02, 8.0992e-02,
        4.6294e-02, 5.7566e-02, 1.6160e-02, 1.3852e-01, 6.9415e-02, 1.1424e-01,
        3.9195e-02, 1.5176e-01, 7.8207e+00, 4.4853e-01, 2.4482e+00, 9.8734e-02,
        8.2075e+00, 3.2530e-01, 1.0954e+00, 2.0908e-01, 5.3373e-02, 2.1131e-01,
        3.1159e-01, 3.0155e-01, 2.6874e+00, 9.2465e-01, 1.3553e-03, 1.6594e-01,
        1.1705e+00, 1.8684e+00, 6.4791e+00, 6.3496e+00, 4.6556e-01, 1.9602e+00,
        1.3884e-01, 5.9888e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [17/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 89.1
Sum Train Loss:  tensor([4.0097e+00, 2.9040e-01, 2.9847e+00, 3.5199e-01, 1.5008e-01, 1.3196e-01,
        8.1158e-02, 4.3911e-01, 6.4589e-01, 2.7376e-01, 1.6689e-02, 1.7898e-02,
        1.8386e-03, 4.3695e-01, 3.8300e-01, 4.8707e-01, 6.2550e-01, 2.9475e-01,
        8.5038e-02, 2.8586e-01, 1.6602e-02, 1.1159e-02, 1.4845e-02, 1.5738e-02,
        4.1746e-01, 8.3221e-02, 1.6906e+00, 1.5338e-01, 6.5291e-02, 2.7600e-02,
        6.9469e-02, 2.8880e-02, 5.5457e-01, 8.9153e-03, 4.1739e-02, 1.3244e-01,
        2.1556e-02, 8.4649e-02, 1.2967e-01, 1.2048e+00, 1.0666e+00, 4.7419e+00,
        6.9912e+00, 7.4939e+00, 5.5820e+00, 9.4469e+00, 3.4540e-02, 1.0033e-01,
        4.9336e-02, 1.1955e-01, 8.2110e-03, 3.2157e-02, 1.8690e-02, 1.3749e-01,
        3.9691e-02, 4.2982e-01, 6.2683e+00, 3.8848e-01, 1.8676e+00, 5.7808e-02,
        1.3862e+01, 4.4005e-02, 1.4765e+00, 3.0964e-01, 3.0257e-01, 1.7828e-01,
        2.9867e-01, 5.9888e-01, 3.8382e+00, 1.0386e+00, 5.9292e-03, 1.6633e-01,
        1.3942e+00, 3.6694e+00, 6.5201e+00, 1.6988e+00, 1.6038e+00, 3.0026e+00,
        4.3507e-02, 5.6313e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [17/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 102.3
Sum_Val Meta Model:  tensor([5.4441e+00, 1.4849e+00, 9.7059e+00, 1.0212e+00, 1.1838e-02, 2.5422e-01,
        2.2996e-02, 2.4235e-01, 1.9933e-01, 1.4461e-01, 3.0381e-02, 9.0861e-02,
        2.8472e-02, 3.5298e-01, 2.5379e-01, 1.9568e-01, 1.9004e+01, 6.8015e-02,
        3.0005e-02, 2.9094e-02, 1.2552e-03, 2.4780e-03, 3.3055e-03, 4.6639e-03,
        4.6066e-01, 1.6915e-01, 1.5748e+00, 3.6866e-02, 7.0427e-02, 1.2771e-01,
        2.2426e-02, 4.4251e-03, 5.0488e-01, 9.2784e-04, 1.9212e-02, 2.5626e-02,
        2.6987e-02, 9.1728e-02, 5.9422e-02, 3.2859e+00, 2.1743e+00, 1.1683e+01,
        5.1458e+00, 1.0277e+01, 1.7119e+01, 1.0508e+01, 1.8325e-02, 5.6093e-02,
        1.3955e-02, 7.4399e-02, 1.2909e-03, 5.8580e-03, 6.7894e-02, 1.9782e-02,
        9.5967e-03, 2.3555e-02, 8.3935e+00, 2.9205e-01, 2.3073e+00, 2.5707e-02,
        8.3990e+00, 3.3016e-01, 1.1715e+00, 2.4784e-01, 2.8660e-02, 4.1085e-02,
        3.3811e-02, 2.5543e-01, 3.5460e+00, 2.4057e+00, 2.4142e-03, 4.6860e-01,
        7.9163e+00, 1.7069e+00, 1.5517e+01, 6.3299e+00, 2.4900e-01, 8.8585e+00,
        4.4310e-02, 6.7142e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.0221e+00, 1.0491e+00, 7.0912e+00, 6.5025e-01, 3.1247e-03, 2.5224e-01,
        1.8568e-02, 2.1644e-01, 1.6639e-01, 1.3096e-01, 3.5716e-02, 9.5415e-02,
        3.5975e-02, 3.4965e-01, 2.4785e-01, 7.5935e-01, 1.1954e+01, 1.3767e-01,
        2.4695e-02, 4.9416e-02, 1.6891e-03, 2.1106e-03, 1.5007e-03, 4.7858e-03,
        4.3750e-01, 1.6125e-01, 1.4268e+00, 5.4104e-02, 8.4430e-02, 1.5269e-01,
        7.4153e-03, 2.2891e-03, 4.8299e-01, 1.4403e-03, 1.8715e-02, 2.3313e-02,
        4.6090e-02, 5.9061e-02, 3.3020e-02, 2.8437e+00, 2.0040e+00, 1.4174e+01,
        4.0066e+00, 7.2229e+00, 1.6745e+01, 9.5830e+00, 1.4599e-02, 5.4062e-02,
        7.8195e-03, 5.0713e-02, 3.8316e-04, 3.5671e-03, 6.7329e-02, 6.1142e-03,
        4.9116e-03, 9.5565e-03, 8.6780e+00, 2.7989e-01, 1.9660e+00, 3.1347e-02,
        1.1580e+01, 1.3194e-01, 9.3734e-01, 3.0952e-01, 2.1558e-02, 6.8158e-02,
        2.4883e-02, 2.8218e-01, 3.6283e+00, 2.1027e+00, 4.0708e-03, 4.1446e-01,
        6.2190e+00, 1.5095e+00, 1.5428e+01, 8.1875e+00, 1.2681e-01, 1.0041e+01,
        4.0383e-02, 1.1117e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.5571e+01, 4.4044e+01, 5.9398e+01, 2.5899e+01, 3.0620e-01, 1.1450e+01,
        3.3798e+00, 1.5801e+01, 5.5334e+00, 9.4985e+00, 7.6121e+00, 1.1862e+01,
        8.7105e+00, 1.5070e+01, 8.5573e+00, 2.8154e+01, 3.1654e+02, 6.0888e+00,
        9.1972e-01, 1.6847e+00, 1.0558e+00, 3.8487e-01, 1.7199e-01, 7.7280e-01,
        2.6509e+01, 1.7565e+01, 2.6313e+01, 5.9851e+00, 1.3496e+01, 1.4658e+01,
        7.7064e-01, 3.6568e-01, 7.7285e+00, 1.0276e+00, 1.8065e+00, 7.6075e-01,
        5.5711e+00, 3.7122e+00, 1.2724e+00, 4.1746e+01, 1.0124e+01, 2.9869e+01,
        7.0949e+00, 1.3528e+01, 1.9039e+01, 1.3455e+01, 1.7312e+00, 4.8008e+00,
        5.9671e-01, 3.2557e+00, 1.1855e-01, 4.5698e-01, 6.4943e+00, 4.0253e-01,
        8.8527e-01, 4.8275e-01, 2.8313e+01, 6.5620e+00, 1.2052e+01, 3.0277e+00,
        1.5368e+01, 4.7711e+00, 4.5504e+00, 6.4421e+00, 4.3630e-01, 3.2561e+00,
        3.3815e-01, 7.7881e+00, 8.6896e+00, 1.6142e+01, 3.0757e-01, 1.8008e+01,
        1.4308e+01, 8.8091e+00, 1.8085e+01, 8.2034e+00, 1.2683e-01, 1.0062e+01,
        9.9472e-02, 1.0713e+00], device='cuda:0')
Outer loop valEpocw Maximum [17/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 170.9
model_train val_loss valEpocw [17/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 160.2
model_train val_loss valEpocw [17/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1116.9
Sum_Val Meta Model:  tensor([9.2803e+00, 6.6310e-01, 1.0527e+01, 3.4044e-01, 8.2750e-03, 1.7064e+00,
        1.7967e+00, 1.6058e+00, 1.7985e+00, 1.1459e+00, 2.5791e-01, 4.6142e+00,
        1.1088e-01, 1.9815e-01, 6.7496e-01, 9.8222e-01, 7.2004e-01, 7.2572e-01,
        5.9734e-01, 2.3704e+00, 5.1997e-04, 6.8611e-04, 8.8065e-03, 6.7793e-02,
        6.7987e-01, 4.6668e-01, 2.4428e+00, 3.5240e-01, 1.8643e-01, 1.3603e-03,
        3.4071e-03, 1.3031e-03, 1.7383e-02, 7.9513e-04, 1.0539e-03, 3.1394e-03,
        8.2430e-02, 5.5104e-03, 3.3443e-03, 1.4379e-01, 4.2675e-02, 2.1807e+00,
        9.8234e-02, 1.8884e-01, 4.0383e-01, 2.7734e+00, 1.3800e-02, 1.0587e-02,
        2.8377e-03, 2.0056e-01, 3.8515e-04, 1.9054e-03, 8.2877e-04, 1.0993e-03,
        2.5681e-03, 1.0638e-01, 3.8462e+00, 4.5743e-01, 1.4800e+00, 7.3034e-02,
        1.7132e+00, 1.4300e-02, 4.3112e-01, 1.3160e-01, 2.3720e-02, 3.6253e-02,
        4.8684e-02, 1.0245e+00, 6.0393e-02, 7.3354e-01, 7.2840e-04, 2.3229e-02,
        2.2368e+00, 1.1226e+00, 4.0402e+00, 4.1342e-01, 1.3114e-01, 5.8202e-01,
        1.8012e-02, 1.3544e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([8.5050e+00, 7.5231e-01, 7.4245e+00, 4.2624e-01, 5.1968e-02, 1.5189e+00,
        1.0172e+00, 1.2650e+00, 1.0834e+00, 8.8006e-01, 1.8550e-01, 1.1223e+00,
        1.1278e-01, 2.8613e-01, 7.5377e-01, 1.6889e-01, 5.7715e-01, 6.5435e-01,
        1.2528e-01, 1.1834e+00, 3.8192e-03, 3.5283e-03, 5.2905e-03, 6.0408e-02,
        6.5342e-01, 4.1312e-01, 2.4354e+00, 3.1397e-01, 1.8909e-01, 3.8712e-02,
        1.0451e-02, 2.5047e-03, 6.9255e-02, 8.7614e-03, 1.8831e-02, 8.8876e-03,
        4.4571e-02, 1.0230e-01, 9.5978e-03, 1.7152e-01, 4.3809e-02, 1.2249e+00,
        1.0096e-01, 1.9689e-01, 2.1257e-01, 4.8551e-01, 1.3562e-02, 1.1332e-02,
        6.2038e-03, 1.7215e-01, 5.8006e-04, 4.3762e-03, 7.6411e-03, 1.3240e-02,
        6.8411e-03, 5.1419e-02, 3.3863e+00, 3.1383e-01, 1.2200e+00, 6.9335e-03,
        3.1205e+00, 1.8398e-02, 6.7663e-02, 3.2238e-02, 1.0018e-02, 1.2351e-02,
        8.0974e-03, 1.0363e+00, 2.6669e-02, 6.0000e-01, 2.8473e-04, 6.0877e-03,
        1.7469e+00, 6.3315e-01, 5.4559e+00, 5.9044e-02, 3.8413e-02, 2.7657e-01,
        3.7095e-03, 2.3054e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.5624e+01, 1.8651e+01, 4.4956e+01, 9.4741e+00, 2.5362e+00, 3.4277e+01,
        6.0008e+01, 4.1646e+01, 2.0190e+01, 3.1602e+01, 1.6572e+01, 5.4026e+01,
        1.0137e+01, 8.3355e+00, 1.4961e+01, 3.2208e+00, 9.3810e+00, 1.4122e+01,
        2.0369e+00, 1.7907e+01, 7.0706e-01, 2.5456e-01, 2.6386e-01, 4.4300e+00,
        2.2703e+01, 1.8930e+01, 2.9491e+01, 1.4110e+01, 1.1449e+01, 1.7118e+00,
        4.7609e-01, 1.6159e-01, 7.1923e-01, 1.8332e+00, 8.9126e-01, 1.8786e-01,
        2.3135e+00, 3.1646e+00, 1.9899e-01, 2.0385e+00, 2.1442e-01, 2.7057e+00,
        2.0301e-01, 4.1830e-01, 2.6277e-01, 7.6258e-01, 7.1483e-01, 4.6543e-01,
        2.0822e-01, 5.1854e+00, 6.4890e-02, 2.6717e-01, 3.4646e-01, 4.1215e-01,
        5.0013e-01, 1.4123e+00, 1.0173e+01, 4.1977e+00, 6.8132e+00, 2.8906e-01,
        4.4094e+00, 3.7670e-01, 2.7726e-01, 4.1205e-01, 1.3832e-01, 2.9224e-01,
        8.2340e-02, 1.8432e+01, 6.9741e-02, 4.0177e+00, 1.0671e-02, 1.4934e-01,
        4.4096e+00, 3.0555e+00, 7.0508e+00, 5.9381e-02, 3.8447e-02, 2.7814e-01,
        8.7767e-03, 1.7765e-01], device='cuda:0')
Outer loop valEpocw Maximum [17/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 69.3
model_train val_loss valEpocw [17/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 53.3
model_train val_loss valEpocw [17/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 675.1
Sum_Val Meta Model:  tensor([5.4285e+00, 5.2719e-02, 1.1636e+00, 4.1265e-02, 5.0626e-03, 4.1033e-02,
        4.5366e-03, 1.1386e-01, 4.6532e-02, 1.9548e-02, 4.2304e-03, 6.5419e-03,
        1.1000e-03, 3.1515e-01, 5.3530e-02, 6.6336e-02, 2.0586e-01, 2.7850e-02,
        3.1168e-02, 5.0971e-02, 4.7730e-04, 5.4982e-03, 5.2322e-02, 6.8333e-03,
        7.7403e-02, 9.5485e-03, 9.1301e-01, 4.1224e-02, 4.5029e-03, 2.3564e-02,
        3.2628e-02, 3.4227e-02, 4.6850e-01, 3.3015e-04, 1.9250e-02, 6.0512e-02,
        1.1465e-01, 1.9719e-01, 6.8605e-03, 1.4446e+00, 5.5088e+00, 3.2761e+01,
        4.4761e+01, 4.4066e+01, 4.3588e+01, 4.7181e+01, 9.5536e-02, 1.0525e-01,
        7.4215e-01, 2.0447e-01, 8.6875e-02, 2.6496e-01, 6.0793e-01, 1.7829e-01,
        2.9229e-01, 1.8253e+00, 4.3378e+01, 4.3658e-01, 1.1318e+00, 1.6482e-02,
        6.9329e+01, 3.5514e-02, 2.1687e+00, 5.4074e-01, 5.8627e-01, 2.0590e-02,
        5.9955e-01, 2.7958e-01, 1.7211e+00, 2.3042e-01, 2.6440e-03, 1.2234e-01,
        1.8175e+00, 5.7793e+00, 1.1294e+00, 2.2158e+01, 9.5174e+00, 9.9989e-01,
        1.0422e-01, 6.6645e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.6768e+00, 1.0773e-02, 3.9436e-01, 4.3305e-03, 3.6784e-04, 1.0542e-03,
        2.5655e-04, 1.2449e-01, 7.0604e-03, 6.7297e-04, 2.7454e-04, 3.5794e-04,
        7.7496e-05, 2.4386e-01, 8.8515e-03, 1.7673e-02, 3.9215e-02, 6.5401e-03,
        3.0037e-03, 1.2635e-03, 3.9732e-05, 6.6852e-05, 1.6675e-04, 2.0288e-04,
        4.5177e-02, 5.8822e-03, 1.0079e+00, 3.0825e-02, 1.6603e-03, 2.4262e-03,
        2.2920e-03, 3.6431e-02, 4.5490e-01, 9.0104e-05, 1.0232e-02, 1.6260e-02,
        7.8987e-02, 1.4170e-01, 8.6845e-03, 2.2454e+00, 4.4908e+00, 2.7279e+01,
        3.4307e+01, 3.9033e+01, 4.5589e+01, 5.6144e+01, 6.3346e-02, 7.1908e-02,
        4.7146e-01, 1.4490e-01, 4.8897e-02, 2.6496e-01, 5.1748e-01, 2.1669e-01,
        2.0893e-01, 1.3602e+00, 2.7493e+01, 4.6443e-01, 1.0641e+00, 4.9988e-03,
        6.9006e+01, 5.6123e-03, 1.7645e+00, 4.3325e-01, 4.8980e-01, 6.4443e-02,
        6.2014e-01, 2.9144e-01, 2.4825e+00, 4.7532e-01, 9.9852e-04, 1.2701e-01,
        1.8333e+00, 6.1355e+00, 1.3459e+00, 2.0636e+01, 1.1650e+01, 2.8136e+00,
        3.0941e-02, 5.6674e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.4076e+01, 7.0392e-01, 3.9632e+00, 2.6267e-01, 6.3840e-02, 7.1898e-02,
        8.6641e-02, 1.5138e+01, 3.4315e-01, 7.6921e-02, 1.0385e-01, 7.4081e-02,
        3.6283e-02, 1.6129e+01, 4.5710e-01, 9.2455e-01, 1.2978e+00, 4.3722e-01,
        1.4940e-01, 5.7659e-02, 6.0978e-02, 2.1540e-02, 2.6731e-02, 5.1574e-02,
        4.2093e+00, 1.2349e+00, 2.8094e+01, 6.7925e+00, 4.8957e-01, 3.5568e-01,
        3.9369e-01, 8.8264e+00, 9.0892e+00, 1.1623e-01, 1.4377e+00, 7.2022e-01,
        1.3678e+01, 1.1734e+01, 5.1159e-01, 4.7640e+01, 2.6375e+01, 5.4657e+01,
        5.4751e+01, 6.6043e+01, 5.0530e+01, 7.5207e+01, 1.0668e+01, 8.8360e+00,
        4.1564e+01, 1.2335e+01, 2.1873e+01, 4.2020e+01, 6.3161e+01, 2.0502e+01,
        5.7838e+01, 8.4808e+01, 1.0169e+02, 1.4539e+01, 7.8670e+00, 7.2818e-01,
        8.7214e+01, 3.1815e-01, 9.0920e+00, 1.2507e+01, 1.3735e+01, 4.5164e+00,
        1.1385e+01, 1.2879e+01, 6.8749e+00, 5.1736e+00, 1.2681e-01, 8.7471e+00,
        5.0222e+00, 4.2781e+01, 1.5773e+00, 2.0656e+01, 1.1651e+01, 2.8162e+00,
        7.7608e-02, 7.1089e-01], device='cuda:0')
Outer loop valEpocw Maximum [17/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 395.6
model_train val_loss valEpocw [17/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 366.1
model_train val_loss valEpocw [17/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1273.8
Sum_Val Meta Model:  tensor([1.4865e+01, 7.5829e-02, 3.3104e+00, 3.5930e-02, 6.6250e-03, 4.4426e-01,
        7.3886e-02, 4.1072e-01, 5.9757e-02, 5.5448e-01, 4.9495e-02, 8.5492e-02,
        1.2085e-03, 6.3002e-01, 6.9917e-01, 6.6321e-02, 9.1952e-02, 2.9691e-02,
        1.6467e-02, 3.0522e-02, 1.2000e-03, 4.9857e-03, 5.6828e-03, 7.3375e-03,
        6.4196e-01, 2.3661e-02, 2.2205e+00, 2.4959e-02, 2.5786e-01, 6.4078e-03,
        1.2272e-02, 2.0221e-03, 2.5838e-01, 7.4685e-03, 2.7534e-02, 2.5285e-01,
        5.0388e-03, 1.3139e-02, 1.8651e-01, 2.4059e+00, 3.6332e+00, 1.4755e+01,
        5.2585e+00, 4.5856e+00, 8.9465e+00, 1.1634e+01, 6.9533e-03, 1.1150e-02,
        3.1112e-02, 1.0434e-02, 4.6890e-03, 7.4297e-03, 5.7952e-03, 1.4625e-01,
        8.2866e-03, 4.8450e-02, 6.1278e+00, 2.7086e-01, 2.5920e+00, 2.7855e-02,
        3.0496e+01, 1.9530e-02, 1.0129e+00, 5.5042e-01, 3.1819e-01, 7.5571e-02,
        5.1090e-01, 2.5850e+00, 5.7658e-01, 4.0879e-01, 2.2476e-03, 5.8369e-02,
        2.7346e+00, 2.0799e+00, 3.7660e+02, 1.0646e+01, 1.6198e+00, 1.0291e+01,
        2.5408e-02, 1.1075e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([4.9819e+00, 1.1573e-01, 2.5314e+00, 7.1247e-02, 1.5070e-02, 3.9916e-01,
        8.3398e-02, 3.9761e-01, 1.9755e-01, 4.1473e-01, 3.6156e-02, 1.6761e-01,
        6.4023e-03, 5.4687e-01, 7.9565e-01, 4.8193e-02, 3.9831e-02, 3.6491e-02,
        3.5573e-03, 3.5479e-03, 1.2055e-03, 4.3331e-04, 2.7872e-03, 5.2390e-03,
        5.9730e-01, 3.5798e-02, 2.0235e+00, 2.0886e-02, 2.8230e-01, 3.3431e-03,
        5.6735e-03, 2.8891e-03, 2.3455e-02, 6.2932e-04, 7.7522e-03, 6.8036e-03,
        6.1331e-03, 8.7941e-03, 1.9053e-02, 2.1805e-01, 2.7571e-01, 7.3913e-01,
        3.6133e-01, 2.9951e-01, 4.6085e-01, 1.5857e+00, 4.3394e-03, 4.7013e-03,
        2.8707e-03, 5.6204e-03, 3.0140e-04, 1.6977e-03, 9.9954e-04, 4.5720e-03,
        3.6176e-03, 1.8938e-02, 1.2440e+00, 3.8711e-02, 2.0005e+00, 6.6956e-03,
        7.0989e+00, 8.2338e-03, 7.1442e-02, 3.2187e-02, 1.9594e-02, 3.0937e-02,
        3.0777e-02, 4.4661e-01, 6.8100e-02, 6.7692e-02, 7.7692e-04, 9.5328e-03,
        3.1337e-02, 1.1934e+00, 5.5088e+01, 4.3814e+00, 4.7442e-02, 7.2030e+00,
        8.3442e-03, 7.7774e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.8340e+01, 3.2103e+00, 1.7144e+01, 1.8168e+00, 8.2857e-01, 1.0712e+01,
        7.1588e+00, 1.6357e+01, 4.3372e+00, 1.6584e+01, 3.6218e+00, 1.0278e+01,
        7.2259e-01, 1.5017e+01, 1.8406e+01, 1.1885e+00, 7.2791e-01, 9.6724e-01,
        8.4257e-02, 8.0307e-02, 3.1232e-01, 3.9522e-02, 1.7124e-01, 4.1677e-01,
        2.3417e+01, 2.0910e+00, 2.6369e+01, 1.2469e+00, 2.1290e+01, 1.7449e-01,
        3.2184e-01, 2.3492e-01, 2.4876e-01, 1.5905e-01, 3.9925e-01, 1.2878e-01,
        3.7749e-01, 3.3895e-01, 3.9612e-01, 2.3603e+00, 1.1226e+00, 1.4769e+00,
        6.6102e-01, 5.9496e-01, 5.5332e-01, 2.3807e+00, 2.9912e-01, 2.4927e-01,
        1.1842e-01, 2.2546e-01, 4.5046e-02, 1.2318e-01, 5.3289e-02, 1.5244e-01,
        3.2559e-01, 5.8686e-01, 3.7130e+00, 5.9747e-01, 1.0831e+01, 3.4766e-01,
        9.5774e+00, 1.8207e-01, 2.9424e-01, 4.0071e-01, 2.4434e-01, 8.4662e-01,
        2.8545e-01, 7.0228e+00, 1.7457e-01, 4.6085e-01, 3.2561e-02, 2.5701e-01,
        7.2282e-02, 5.7850e+00, 7.0599e+01, 4.4015e+00, 4.7478e-02, 7.2369e+00,
        2.3051e-02, 5.7154e-02], device='cuda:0')
Outer loop valEpocw Maximum [17/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 526.7
model_train val_loss valEpocw [17/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 97.1
model_train val_loss valEpocw [17/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 380.5
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.3758848  97.41718546 93.12630207 98.12136792 98.60259987 97.56703744
 98.25538188 95.19499031 98.05192432 96.9627563  98.63183928 98.96443757
 99.42373996 95.36311692 97.51099524 97.27098841 96.41451737 97.74125559
 98.84626162 98.39183246 98.6769167  99.31652879 99.54191591 99.00464176
 95.25712406 96.80803109 94.46765999 97.04194637 98.05436094 98.28462129
 98.40157893 98.5782337  97.03585483 98.55874076 98.72321244 98.86088132
 97.6035867  98.37355783 98.98149389 93.44671727 98.06776233 94.66624432
 97.67668523 97.03341821 97.9934455  96.45472156 98.25538188 98.68422656
 98.15913549 98.75488846 98.77072648 98.6488956  99.01804315 98.40645216
 98.7317406  97.63648104 92.87533047 96.98468586 96.66305235 97.55485435
 95.89917277 98.75001523 97.13941107 97.38429113 98.80971236 97.64257258
 98.6062548  95.9734896  98.91936014 98.34431842 99.81603538 97.46226289
 98.71102935 95.99298254 98.1603538  98.69519134 99.46759908 99.24586689
 99.84161986 99.17520498]
Accuracy th:0.7 is [84.51164094 97.3392137  92.36851403 97.97395256 98.45152959 97.34286863
 98.01172013 94.85264556 98.04095954 96.72274948 98.59407171 98.94738125
 99.41764842 95.31925781 97.44886149 97.17596033 96.34263715 97.66450214
 98.77316309 98.33457195 98.54290274 99.27754291 99.50414834 98.84869824
 95.23763112 96.70934808 94.32268125 96.95179152 98.02268491 98.22492416
 98.1749735  98.579452   96.65330588 98.43812819 98.71346597 98.6756984
 97.30266444 98.42350849 98.9412897  93.26518926 98.01050182 93.99251958
 97.72298096 96.80315786 97.86430477 96.22933444 98.19446644 98.65498715
 98.09578343 98.76463493 98.68666317 98.63183928 99.00220514 98.28949452
 98.71712089 97.56094589 92.12485228 96.77635506 96.54122148 97.44276995
 95.29732825 98.60016325 96.76051705 97.26611518 98.74757861 97.52926987
 98.46980422 95.95521497 98.83164191 98.2297974  99.81603538 97.09920688
 98.52462811 95.76515881 98.02512153 98.70615611 99.4432329  99.2202824
 99.84405648 99.15571204]
Avg Prec: is [96.51610418 34.80935263 72.1416024  66.75965036 77.94143121 64.32803747
 73.61157971 47.34868913 56.65116174 51.88739276 29.45345778 53.46929679
 22.82142263 27.39763773 32.39461268 55.25244556 29.83895749 41.60951577
 47.64132911 37.9342183  60.01280251 47.21995144 89.54863441 82.62524295
 26.37193965 32.52924129 38.78198925 39.12253962 24.48227012 38.02522406
 73.11429337 35.05191622 57.52171422 61.80919817 72.81489597 79.51457578
 58.70010773 74.35103126 87.40277609 47.25878301 42.60837107 70.16809583
 65.89443205 60.70303782 71.00071344 76.73722057 38.12014107 34.02060324
 41.44221936 45.78729049 60.15034512 37.07497084 21.70902884 72.75502689
 26.41148322 42.90427429 71.15784556 58.3274714  43.40212654 58.49437143
 87.71491549 83.46804775 70.21045048 48.73292346 58.97011589 43.20155688
 58.46550308 25.54597273 55.2836892  67.34127247 11.30551764 73.04426978
 76.91943224 49.3935704  79.70013128 85.70970515 65.63035199 82.73237841
  9.47335941 26.68470857]
Accuracy th:0.5 is [45.53916253 97.2137279  72.20915925 97.02489005 97.26733349 76.94959857
 77.27001377 76.34531743 78.20567488 96.45715817 78.50903376 98.52097319
 99.41399349 80.19882799 78.05825952 96.56680596 96.29512311 77.79388653
 98.65376884 98.30776915 80.55335583 79.03899806 98.38695922 78.17643547
 80.90788368 96.65086926 94.0778012  77.6513444  98.01293844 78.38476627
 97.30875598 98.57457877 96.36213009 98.02024829 87.75843374 78.03023842
 77.82190763 91.11487433 97.11504489 75.02101583 79.42398363 92.05906361
 77.31265457 76.9398521  96.9627563  93.87434364 98.02877645 98.57336046
 97.1442843  88.81348911 87.67315213 98.55508583 98.99976852 77.47468964
 98.70615611 77.83530902 72.49668011 93.48570315 96.24273583 96.9067141
 89.79300934 97.17717864 92.92040789 77.83409072 98.42838172 78.24222414
 98.20786784 77.04340834 78.88670947 97.55972759 79.47393428 95.99054592
 78.57725905 95.45083515 77.1908237  82.93758604 87.43680023 78.38598458
 79.49708215 99.14718388]
Accuracy th:0.7 is [45.5342893  97.2137279  72.20915925 97.02489005 97.26733349 76.94959857
 77.27001377 76.55121161 78.20567488 96.4742145  78.50903376 98.52097319
 99.41399349 80.67275009 78.05825952 96.56680596 96.29512311 77.79388653
 98.65376884 98.30776915 80.97367235 79.03899806 98.38695922 78.49806898
 81.53043944 96.65086926 94.0778012  77.6513444  98.01293844 78.38476627
 97.30875598 98.57457877 96.36213009 98.02024829 87.93143358 78.03023842
 77.82190763 91.37315579 97.11504489 75.02101583 80.05263094 92.05906361
 77.31265457 76.9398521  96.9627563  93.87434364 98.02877645 98.57336046
 97.73272743 89.41898856 87.84858859 98.55508583 98.99976852 77.47468964
 98.70615611 77.83530902 72.49668011 93.90601966 96.24273583 96.9067141
 89.79300934 97.17717864 93.06538663 77.83409072 98.42838172 78.24222414
 98.20786784 77.04340834 78.88670947 97.55972759 79.47393428 95.99054592
 78.6540125  95.45083515 77.1908237  83.01677611 87.6268564  78.38598458
 79.49708215 99.14718388]
Avg Prec: is [55.78141011  3.18519321 11.2244394   3.41962993  2.23147661  3.83825611
  3.31468395  5.60091128  2.38898332  3.73278839  1.56726189  1.65940934
  0.64194199  5.10809627  2.71109281  3.1483975   3.64292718  2.71197924
  1.36388547  1.81807832  1.99480179  0.79919148  1.79550286  2.38287044
  5.0787764   3.71180463  6.57389558  3.11564617  2.05323735  1.9035753
  2.6610791   1.32493503  3.75150044  1.66073539  2.36530598  2.38099362
  2.96590931  2.55074992  2.82071351  7.43932979  2.2678441   8.20211526
  3.38614528  4.02580936  3.21117523  6.30530673  2.17573624  1.6155705
  2.15674715  1.66987004  1.80789007  1.67789542  1.04683464  2.98171013
  1.34939243  2.7285683  11.35217106  3.73320037  3.95111344  2.85188074
 10.7658814   2.17762533  3.74279414  2.94888411  1.5921668   2.49910931
  1.75715747  4.22178341  1.34989702  2.44245356  0.18855082  3.40810397
  1.94716966  4.53639898  3.8399277   3.10934021  0.853971    1.9054461
  0.15078138  0.7137008 ]
mAP score regular 54.31, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [88.06836585 97.42133194 93.37518997 98.28337943 98.96853278 97.64556394
 98.4652565  95.41819269 98.05665595 96.99030819 98.65709943 99.00590478
 99.37464185 95.21887535 97.45870394 97.34409647 96.39733911 97.75020555
 98.96105838 98.39051249 98.88880584 99.34474425 99.61631412 99.21518798
 95.44559882 96.74116152 94.5187732  97.26436953 97.87228742 98.15880609
 98.71440317 98.64713357 96.93798739 98.72187757 98.89129731 99.13546105
 97.91713382 98.30580263 99.15788425 93.29048011 97.90716795 93.58696465
 97.06006926 96.49699778 96.80843112 94.64334654 98.36559783 98.77668984
 98.14884022 98.72187757 98.79911304 98.64962503 98.86638264 98.47771383
 98.7866557  97.78757755 91.02324538 97.12235593 96.46211725 97.58327728
 92.74983183 98.79911304 97.07003513 97.50604181 98.71689464 97.51600767
 98.5698981  95.81682737 98.82901064 98.34815756 99.81563146 97.63559808
 98.4204101  95.94389217 97.46119541 97.304233   99.17781598 98.64713357
 99.82559733 99.13795251]
Accuracy th:0.7 is [86.77031168 97.3640282  92.81460996 98.25099036 98.86887411 97.44873807
 98.24351596 95.09928495 98.14136582 96.77355059 98.62720183 99.01836211
 99.35969305 95.12170815 97.4238234  97.1397962  96.28522311 97.71532501
 98.92617784 98.36310636 98.79163864 99.33228692 99.62129706 99.08563171
 95.46553056 96.64150285 94.5187732  97.17467673 97.83989835 98.24102449
 98.5624237  98.68450557 96.77853352 98.63467623 98.96354984 98.97351571
 97.67047861 98.34068316 99.11303785 93.2605825  97.95699728 93.49478038
 97.41385754 96.67389192 96.98781673 94.94481401 98.36061489 98.83399357
 98.10399382 98.79662157 98.71689464 98.64962503 98.88880584 98.45529063
 98.73184344 97.69041034 91.13286992 97.08249246 96.35249271 97.44873807
 93.05379077 98.73184344 96.74863592 97.3939258  98.67952263 97.57829434
 98.52754316 95.78942123 98.79662157 98.22607569 99.81563146 97.45621247
 98.34317463 95.8367591  97.43877221 97.50853327 99.23511971 98.70194584
 99.82559733 99.16535865]
Avg Prec: is [96.53407627 33.72021447 71.67194411 73.49103665 78.54392973 65.02222482
 80.2697765  48.0282408  61.77712964 55.44514031 35.97130432 56.57588693
 22.3576487  31.38986434 33.96551968 61.63269804 31.82757992 44.06382525
 50.71481065 36.47974505 69.86812517 57.17713547 92.20321022 87.63735358
 25.16247927 35.58972995 34.3114681  46.02651985 26.84119036 37.19550979
 77.04104827 36.0198281  55.95316192 65.06856192 74.79781392 82.3470861
 60.23533468 76.78102682 89.97738117 44.73248039 36.74634519 53.4775974
 50.27898823 42.07186781 33.19099813 53.75688447 37.86211777 29.52370646
 42.67349976 43.00657766 64.95456784 36.50820561 23.16100336 76.94697841
 30.60339202 41.88248426 57.93238075 56.4489627  38.24758742 61.01771802
 66.84592155 86.7943396  66.48633958 52.39204558 59.38621551 37.93419652
 60.1836516  25.5049461  44.17824305 67.21662595  8.55825142 74.87352168
 56.13712023 44.7865812  65.61255234 53.46415647 16.15922639 54.95537418
  2.33700969 20.97917415]
Accuracy th:0.5 is [45.27991629 97.22450607 70.48110222 96.96290206 97.90716795 75.98724369
 76.07444503 74.96325087 77.54690186 96.41976231 77.70635573 98.5325261
 99.34972718 78.21959788 77.62912026 96.31262924 96.21047911 77.06106585
 98.78167277 98.34068316 79.16386377 78.34167975 98.31327703 77.9555024
 78.17226001 96.52938685 94.3393876  77.10342078 97.81747515 77.66898373
 97.52597354 98.67204823 96.39983058 98.18870369 88.92293893 77.28529785
 77.24792585 92.37112888 97.0276802  74.37775619 77.80850587 92.37362035
 76.41826743 76.08690236 97.03764606 94.02795426 98.18621222 98.77668984
 97.62812368 88.46700052 86.11256447 98.55993223 98.87385704 76.40830157
 98.6969629  77.08099758 71.08403717 94.38672547 96.16314124 96.78102499
 90.13379176 97.04761193 93.0512993  77.07352318 98.32075143 77.9181304
 98.13139996 76.25383063 78.37905175 97.53593941 78.80758402 96.07843137
 78.13239654 95.44559882 76.17659516 83.83287241 89.47853601 77.74123627
 78.87734509 99.15040985]
Accuracy th:0.7 is [45.47425069 97.22450607 70.48110222 96.96290206 97.90716795 75.98724369
 76.07444503 75.10526447 77.54690186 96.41976231 77.70635573 98.5325261
 99.34972718 78.63318135 77.62912026 96.31262924 96.21047911 77.06106585
 98.78167277 98.34068316 79.43543364 78.34167975 98.31327703 78.12990508
 78.62321549 96.52938685 94.3393876  77.10342078 97.81747515 77.66898373
 97.52597354 98.67204823 96.39983058 98.18870369 89.09734161 77.28529785
 77.24792585 92.60532676 97.0276802  74.37775619 78.26444428 92.37362035
 76.41826743 76.08690236 97.03764606 94.02795426 98.18621222 98.77668984
 97.85235568 88.77095946 86.27949274 98.55993223 98.87385704 76.40830157
 98.6969629  77.08099758 71.08403717 94.69317587 96.16314124 96.78102499
 90.13379176 97.04761193 93.17587264 77.07352318 98.32075143 77.9181304
 98.13139996 76.25383063 78.37905175 97.53593941 78.80758402 96.07843137
 78.14236241 95.44559882 76.17659516 83.90761641 89.71522535 77.74123627
 78.87734509 99.15040985]
Avg Prec: is [54.20756274  3.72614328 14.8286943   4.54740301  1.49369678  4.26579396
 13.15738279  8.6412759   7.95589238  5.24637952  2.32645243  4.85890636
  1.73850608  5.85119073  2.9245185   3.6415422  24.40351342  6.43755947
  1.55640003  2.69463304  3.54916689  1.48754981  1.23956622  5.37798798
  5.68651534  9.68313084  7.97278895  4.60888871  3.94827041  5.92875307
  2.23698081  0.85447252  2.99918029  1.15600579  1.70087889  2.31442991
  1.99412452  2.19573898  2.19424891  6.24860389  1.72095857  6.03315044
  2.16937751  2.70322139  2.34274974  4.85970504  1.69974252  1.03846686
  1.41956752  1.18127391  1.20398957  1.00252699  0.74764535  2.27079498
  0.89104216  1.8840349  10.16068366  3.00115441  3.9408559   2.83390581
  7.87259379  2.06147455  3.39496192  2.50703393  1.35347023  1.91199497
  1.54785547  3.52017197  1.07689179  2.21658127  0.19502736  3.23273719
  1.57001802  4.02888784  3.19918014  2.29820192  0.56096719  1.45490065
  0.12356398  0.60741567]
mAP score regular 51.49, mAP score EMA 4.32
Train_data_mAP: current_mAP = 54.31, highest_mAP = 54.31
Val_data_mAP: current_mAP = 51.49, highest_mAP = 51.49
tensor([0.0878, 0.0224, 0.1160, 0.0239, 0.0094, 0.0219, 0.0054, 0.0137, 0.0291,
        0.0141, 0.0047, 0.0082, 0.0041, 0.0226, 0.0279, 0.0262, 0.0368, 0.0216,
        0.0262, 0.0286, 0.0015, 0.0052, 0.0086, 0.0065, 0.0158, 0.0087, 0.0532,
        0.0084, 0.0064, 0.0099, 0.0093, 0.0060, 0.0611, 0.0015, 0.0098, 0.0299,
        0.0083, 0.0146, 0.0261, 0.0655, 0.1991, 0.4943, 0.5877, 0.5353, 0.8797,
        0.7138, 0.0078, 0.0105, 0.0131, 0.0142, 0.0030, 0.0076, 0.0103, 0.0150,
        0.0055, 0.0199, 0.2979, 0.0416, 0.1577, 0.0100, 0.7637, 0.0269, 0.2084,
        0.0494, 0.0507, 0.0200, 0.0731, 0.0368, 0.4502, 0.1274, 0.0129, 0.0219,
        0.4367, 0.1730, 0.8597, 0.9982, 0.9998, 0.9984, 0.4857, 0.1061],
       device='cuda:0')
Sum Train Loss:  tensor([2.8756e+00, 2.3617e-01, 2.4592e+00, 2.0420e-01, 2.9634e-02, 7.6419e-02,
        1.3743e-02, 2.3792e-01, 1.5108e-01, 1.8553e-01, 1.0385e-02, 1.6360e-02,
        3.6650e-03, 3.3353e-01, 3.6111e-01, 2.8334e-01, 5.0710e-01, 1.4555e-01,
        7.4675e-02, 1.6279e-01, 7.7126e-03, 2.3884e-02, 4.4261e-02, 4.1890e-02,
        2.0116e-01, 1.9370e-01, 1.6054e+00, 7.0651e-02, 4.4337e-02, 6.3476e-02,
        5.7699e-02, 4.8847e-02, 4.5740e-01, 2.1277e-02, 2.4071e-02, 5.4841e-02,
        1.3229e-01, 8.4562e-02, 5.1625e-02, 1.9133e+00, 2.2529e+00, 9.9229e+00,
        4.9530e+00, 5.9197e+00, 6.1820e+00, 5.5053e+00, 8.4306e-02, 1.7633e-01,
        6.2471e-02, 2.0568e-01, 1.5853e-02, 6.5583e-02, 5.5791e-02, 8.4972e-02,
        4.9265e-02, 1.9630e-01, 4.4762e+00, 1.9168e-01, 1.9070e+00, 5.9754e-02,
        9.7955e+00, 2.0419e-02, 1.3857e+00, 2.9310e-01, 2.3824e-01, 1.6108e-01,
        2.7741e-01, 5.4300e-01, 1.2039e+00, 8.6806e-01, 1.8030e-03, 1.5627e-01,
        7.6231e-01, 2.7294e+00, 1.5724e+00, 7.2725e+00, 4.7100e+00, 8.1217e+00,
        1.0076e-01, 9.7879e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [18/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 96.2
Sum Train Loss:  tensor([3.4258e+00, 3.4072e-01, 3.4988e+00, 2.1556e-01, 1.1535e-01, 3.0326e-01,
        1.7772e-02, 3.8566e-01, 1.2271e-01, 1.7270e-01, 4.3526e-02, 1.2271e-02,
        1.6067e-02, 4.9584e-01, 2.5843e-01, 2.5069e-01, 3.7576e-01, 2.3255e-01,
        1.4472e-01, 3.6696e-02, 5.3097e-03, 1.1439e-02, 9.3628e-03, 1.0670e-02,
        4.4917e-01, 6.3856e-02, 9.2868e-01, 7.4878e-02, 3.7775e-02, 6.5194e-02,
        3.6523e-02, 3.2690e-02, 7.8072e-01, 8.0372e-03, 3.5771e-02, 2.3123e-01,
        5.2539e-02, 7.4408e-02, 1.0621e-01, 9.7321e-01, 1.8294e+00, 9.5776e+00,
        5.9761e+00, 3.5940e+00, 1.0438e+01, 9.2104e+00, 3.7735e-02, 1.3969e-02,
        1.3945e-01, 1.9884e-02, 9.7205e-03, 6.4806e-02, 9.3881e-02, 1.5792e-01,
        3.3767e-02, 2.2296e-01, 6.9416e+00, 4.5428e-01, 1.4229e+00, 6.6055e-02,
        1.4313e+01, 1.8179e-01, 1.1997e+00, 2.1550e-01, 8.0298e-02, 1.3793e-01,
        2.5541e-01, 8.6710e-01, 1.7472e+00, 7.7666e-01, 2.3057e-03, 1.1779e-01,
        2.5655e+00, 1.9873e+00, 5.5777e+00, 3.6005e+00, 5.8190e-01, 1.7758e+00,
        5.2070e-02, 6.8398e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [18/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 101.5
Sum Train Loss:  tensor([1.9578e+00, 1.0482e-01, 2.6676e+00, 1.1730e-01, 2.8916e-02, 1.2613e-01,
        2.1721e-02, 2.2071e-01, 4.0700e-02, 8.5923e-02, 6.4101e-02, 8.3350e-02,
        3.1095e-03, 4.0123e-01, 1.4462e-01, 2.3235e-01, 5.4652e-01, 1.9926e-01,
        1.5816e-01, 1.0542e-01, 1.2048e-02, 1.6756e-03, 5.2436e-03, 6.2276e-02,
        3.5198e-01, 1.4179e-01, 1.5668e+00, 6.6414e-02, 1.4980e-02, 5.8299e-02,
        2.1947e-02, 6.9812e-03, 3.6373e-01, 1.1689e-02, 3.9090e-02, 1.3726e-01,
        4.9555e-02, 2.4660e-02, 1.4502e-01, 1.6040e+00, 1.8314e+00, 1.1506e+01,
        6.3526e+00, 6.0831e+00, 6.3471e+00, 7.2532e+00, 5.6024e-02, 4.2654e-02,
        2.2664e-01, 8.7308e-02, 6.2288e-02, 4.5049e-02, 9.9169e-03, 8.1420e-02,
        4.8344e-02, 2.2499e-01, 7.9730e+00, 3.1584e-01, 1.8534e+00, 5.8974e-02,
        1.0318e+01, 8.9299e-02, 2.8876e+00, 4.4977e-01, 5.0285e-01, 2.1132e-01,
        2.9645e-01, 8.1783e-01, 6.3915e-01, 3.6236e-01, 3.0026e-02, 1.2242e-01,
        5.6991e-01, 2.7744e+00, 6.5397e+00, 6.6685e+00, 6.7055e-01, 1.1274e+01,
        3.9163e+00, 6.7411e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [18/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 112.3
Sum Train Loss:  tensor([4.8540e+00, 3.4176e-01, 2.5085e+00, 8.3651e-02, 7.7980e-02, 4.3372e-01,
        6.8650e-03, 2.1538e-01, 2.1784e-01, 8.4299e-02, 3.4693e-02, 4.2842e-02,
        1.6518e-02, 3.5494e-01, 4.2727e-01, 3.8199e-01, 6.4256e-01, 2.6268e-01,
        6.0135e-02, 1.4355e-01, 1.7599e-02, 6.0273e-03, 3.1902e-02, 4.8609e-02,
        4.1076e-01, 7.1532e-02, 9.1028e-01, 1.1853e-01, 6.2034e-02, 4.1254e-02,
        7.4736e-02, 5.8699e-02, 1.2995e-01, 2.3456e-03, 2.6530e-02, 9.7570e-02,
        1.5205e-01, 1.2812e-01, 1.6557e-02, 1.0041e+00, 4.4465e-01, 8.2582e+00,
        2.0778e+00, 2.7509e+00, 1.3758e+00, 7.5633e+00, 3.7287e-02, 2.1012e-02,
        3.3744e-02, 1.7425e-01, 2.4466e-03, 4.0114e-03, 1.5175e-02, 5.4026e-02,
        1.5917e-02, 6.3794e-02, 5.9934e+00, 1.0203e+00, 3.2089e+00, 1.5091e-01,
        8.5407e+00, 5.1719e-02, 1.9653e+00, 7.4143e-01, 6.5171e-01, 2.3130e-01,
        6.6872e-01, 5.5519e-01, 4.2184e-01, 6.1353e-01, 1.4928e-03, 7.7381e-02,
        1.9041e+00, 3.7113e+00, 1.2501e+01, 7.4765e+00, 3.1727e-01, 1.7947e+00,
        6.8997e-02, 1.0700e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [18/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 90.3
Sum Train Loss:  tensor([2.4195e+00, 2.7810e-01, 2.1165e+00, 2.0236e-01, 3.2165e-02, 2.1425e-01,
        4.9219e-02, 1.8815e-01, 1.1321e-01, 1.3985e-01, 1.8450e-02, 9.4572e-03,
        1.4930e-02, 3.2784e-01, 1.8376e-01, 2.8855e-01, 4.5295e-01, 1.5555e-01,
        8.3817e-02, 3.8493e-02, 1.2541e-02, 1.1946e-02, 3.2470e-03, 6.0024e-02,
        1.4924e-01, 5.8926e-02, 7.9597e-01, 7.0480e-02, 1.0069e-01, 1.1156e-01,
        4.8744e-02, 2.5589e-02, 7.3012e-01, 1.8940e-03, 4.3498e-02, 1.8576e-01,
        6.0545e-02, 3.2385e-02, 2.7811e-02, 2.3648e+00, 1.2357e+00, 7.5922e+00,
        2.8059e+00, 3.1070e+00, 3.7145e+00, 8.0089e+00, 4.6291e-02, 2.1525e-02,
        1.0348e-01, 1.9432e-01, 1.2743e-02, 3.0157e-02, 4.8491e-02, 2.0056e-01,
        2.9056e-02, 2.5258e-01, 7.5377e+00, 6.0992e-01, 1.6297e+00, 7.6019e-02,
        7.0212e+00, 6.4986e-02, 2.0182e+00, 3.9711e-01, 1.8791e-01, 1.8285e-01,
        2.6779e-01, 5.2173e-01, 1.2828e+00, 9.8870e-01, 3.1048e-02, 2.9486e-01,
        3.2424e+00, 5.1948e+00, 1.2431e+01, 3.6036e+00, 2.2860e+00, 1.0393e+01,
        5.9623e-02, 7.2300e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [18/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 100.7
Sum Train Loss:  tensor([3.0288e+00, 1.2277e-01, 1.9812e+00, 2.1011e-01, 5.2620e-02, 1.8531e-01,
        1.0524e-02, 1.6218e-01, 1.8908e-01, 1.0878e-01, 2.7414e-02, 1.3064e-02,
        1.7163e-03, 3.1631e-01, 5.3376e-01, 2.0037e-01, 4.5100e-01, 1.6467e-01,
        6.8035e-02, 1.1105e-01, 4.3649e-03, 8.4552e-03, 7.5130e-02, 1.1963e-01,
        2.6883e-01, 8.3725e-02, 1.3495e+00, 1.1296e-01, 6.8912e-02, 6.3268e-02,
        7.8153e-02, 5.6485e-02, 5.5366e-01, 1.4116e-02, 3.7717e-02, 3.5537e-02,
        9.3581e-02, 1.5796e-01, 4.6399e-02, 1.8340e+00, 1.8055e+00, 1.4384e+01,
        8.0189e+00, 4.8787e+00, 8.5447e+00, 7.9256e+00, 3.8360e-02, 1.8896e-02,
        1.4720e-01, 2.8573e-02, 7.6765e-03, 3.9368e-02, 4.8028e-02, 1.0634e-01,
        9.8663e-03, 2.1606e-01, 9.8783e+00, 5.3801e-01, 1.4805e+00, 6.1665e-02,
        1.0909e+01, 8.8967e-02, 1.1084e+00, 4.0257e-01, 3.9339e-01, 1.5468e-01,
        5.6704e-01, 1.1312e+00, 4.8272e-01, 4.9758e-01, 2.5639e-03, 2.6287e-01,
        3.4344e+00, 2.3593e+00, 3.0160e+00, 3.7371e+00, 6.4485e+00, 9.7697e-01,
        5.0699e-02, 9.5793e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [18/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 107.3
Sum Train Loss:  tensor([3.5993e+00, 3.7674e-01, 2.9611e+00, 1.4093e-01, 3.7529e-02, 1.8263e-01,
        2.1933e-02, 2.9352e-01, 1.6641e-01, 2.7888e-01, 5.0011e-02, 1.4704e-01,
        2.2409e-03, 4.8845e-01, 6.4334e-01, 2.5694e-01, 9.6906e-01, 2.2194e-01,
        2.4064e-01, 1.5789e-01, 6.1657e-03, 4.6841e-03, 4.6095e-03, 4.1687e-02,
        3.9537e-01, 1.2824e-01, 1.5058e+00, 1.3569e-01, 5.5048e-02, 1.4988e-01,
        1.9434e-02, 3.7818e-02, 6.7400e-01, 1.3038e-02, 3.8036e-02, 5.2306e-02,
        8.0268e-02, 2.5171e-02, 1.3690e-01, 1.7330e+00, 1.8574e+00, 9.6763e+00,
        4.4359e+00, 4.8469e+00, 5.9054e+00, 1.0629e+01, 7.5410e-02, 1.3132e-01,
        6.2506e-02, 5.4376e-02, 7.0125e-03, 1.1581e-02, 7.7562e-03, 8.4806e-02,
        6.7812e-03, 1.9708e-01, 8.7040e+00, 1.0003e+00, 2.3376e+00, 8.4434e-02,
        1.4055e+01, 1.9340e-01, 2.5382e+00, 4.3450e-01, 3.4530e-01, 2.3712e-01,
        4.6606e-01, 8.0032e-01, 1.6208e+00, 4.4631e-01, 1.5691e-03, 2.9337e-01,
        4.0324e+00, 3.3783e+00, 3.7549e+00, 3.2097e+00, 6.1151e-01, 4.2474e+00,
        1.9812e-01, 3.4098e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [18/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 107.8
Sum_Val Meta Model:  tensor([4.8538e+00, 1.3318e+00, 8.9692e+00, 1.2115e+00, 1.4760e-02, 2.7522e-01,
        2.7068e-02, 2.3003e-01, 1.7978e-01, 1.4822e-01, 2.8790e-02, 8.6435e-02,
        3.0461e-02, 3.4985e-01, 2.5150e-01, 2.0567e-01, 1.8926e+01, 3.9798e-02,
        3.0828e-02, 4.2165e-02, 1.4251e-03, 3.3632e-03, 3.9782e-03, 8.4768e-03,
        4.4389e-01, 1.6501e-01, 1.5079e+00, 2.8029e-02, 7.0585e-02, 1.4372e-01,
        1.7818e-02, 4.5875e-03, 4.6819e-01, 1.5533e-03, 1.1484e-02, 2.2691e-02,
        2.3748e-02, 7.3304e-02, 8.0177e-02, 3.2587e+00, 2.1650e+00, 1.1346e+01,
        4.4751e+00, 1.0539e+01, 1.4768e+01, 1.1086e+01, 1.0464e-02, 5.8276e-02,
        1.1876e-02, 6.5562e-02, 1.3931e-03, 5.3892e-03, 6.3711e-02, 2.2138e-02,
        6.2121e-03, 2.6350e-02, 7.6989e+00, 2.8609e-01, 2.2242e+00, 2.9289e-02,
        8.7664e+00, 3.0635e-01, 1.2172e+00, 2.6230e-01, 1.7826e-02, 1.4779e-02,
        3.0622e-02, 2.6696e-01, 3.6906e+00, 2.2047e+00, 1.9584e-03, 3.6649e-01,
        6.3200e+00, 1.8516e+00, 1.4480e+01, 7.9519e+00, 3.3031e-01, 1.0082e+01,
        2.0820e-02, 3.5587e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([4.2756e+00, 9.0504e-01, 6.1243e+00, 7.1193e-01, 4.8092e-03, 2.7880e-01,
        1.7728e-02, 2.4981e-01, 1.8856e-01, 1.5136e-01, 3.6180e-02, 9.6701e-02,
        3.4986e-02, 3.3999e-01, 2.2956e-01, 6.9830e-01, 1.2179e+01, 9.3271e-02,
        2.3028e-02, 1.0336e-01, 1.6427e-03, 4.5299e-03, 1.6011e-03, 1.2227e-02,
        4.1094e-01, 1.4890e-01, 1.4761e+00, 4.4389e-02, 9.5745e-02, 1.5680e-01,
        1.1753e-02, 3.3564e-03, 5.1508e-01, 1.7173e-03, 1.0887e-02, 1.6374e-02,
        4.7739e-02, 5.5332e-02, 8.2439e-02, 2.9156e+00, 2.1578e+00, 1.3093e+01,
        3.0619e+00, 7.7491e+00, 1.3534e+01, 1.1126e+01, 7.6438e-03, 5.9264e-02,
        6.0767e-03, 5.5523e-02, 2.7774e-04, 2.5873e-03, 6.3290e-02, 8.4262e-03,
        4.0023e-03, 1.3762e-02, 6.8789e+00, 2.7547e-01, 2.0317e+00, 4.4692e-02,
        1.0552e+01, 1.2447e-01, 9.8898e-01, 2.5883e-01, 8.2905e-03, 1.6840e-02,
        1.3470e-02, 3.4680e-01, 4.1459e+00, 2.0132e+00, 5.0576e-03, 3.8094e-01,
        5.9825e+00, 1.8209e+00, 1.7568e+01, 1.0232e+01, 2.4611e-01, 9.2985e+00,
        1.5782e-02, 4.2355e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.8721e+01, 4.0393e+01, 5.2794e+01, 2.9848e+01, 5.1360e-01, 1.2733e+01,
        3.3076e+00, 1.8294e+01, 6.4824e+00, 1.0704e+01, 7.6669e+00, 1.1826e+01,
        8.5088e+00, 1.5059e+01, 8.2171e+00, 2.6679e+01, 3.3060e+02, 4.3210e+00,
        8.8015e-01, 3.6106e+00, 1.0686e+00, 8.6568e-01, 1.8569e-01, 1.8888e+00,
        2.6076e+01, 1.7156e+01, 2.7742e+01, 5.3030e+00, 1.5021e+01, 1.5898e+01,
        1.2695e+00, 5.6378e-01, 8.4247e+00, 1.1517e+00, 1.1122e+00, 5.4751e-01,
        5.7624e+00, 3.7801e+00, 3.1560e+00, 4.4532e+01, 1.0840e+01, 2.6487e+01,
        5.2101e+00, 1.4477e+01, 1.5385e+01, 1.5587e+01, 9.7707e-01, 5.6622e+00,
        4.6214e-01, 3.9176e+00, 9.1598e-02, 3.3961e-01, 6.1493e+00, 5.6006e-01,
        7.3395e-01, 6.9222e-01, 2.3095e+01, 6.6228e+00, 1.2880e+01, 4.4509e+00,
        1.3816e+01, 4.6293e+00, 4.7460e+00, 5.2380e+00, 1.6368e-01, 8.4145e-01,
        1.8428e-01, 9.4294e+00, 9.2089e+00, 1.5802e+01, 3.9202e-01, 1.7364e+01,
        1.3700e+01, 1.0523e+01, 2.0436e+01, 1.0250e+01, 2.4615e-01, 9.3134e+00,
        3.2492e-02, 3.9906e-01], device='cuda:0')
Outer loop valEpocw Maximum [18/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 166.7
model_train val_loss valEpocw [18/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 157.0
model_train val_loss valEpocw [18/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1124.0
Sum_Val Meta Model:  tensor([8.2437e+00, 6.5680e-01, 8.8170e+00, 3.9126e-01, 7.8920e-03, 1.6235e+00,
        1.5423e+00, 1.4486e+00, 1.5842e+00, 1.1138e+00, 2.1806e-01, 4.1682e+00,
        1.0529e-01, 1.9687e-01, 6.5586e-01, 9.4800e-01, 8.9795e-01, 6.8918e-01,
        5.5343e-01, 2.3949e+00, 5.9308e-04, 1.0363e-03, 1.4497e-02, 7.1783e-02,
        6.9442e-01, 4.3577e-01, 2.3886e+00, 3.2956e-01, 2.0471e-01, 1.6067e-03,
        3.3026e-03, 1.7882e-03, 1.6247e-02, 7.8658e-04, 1.2881e-03, 4.2534e-03,
        6.5712e-02, 5.1152e-03, 2.6907e-03, 1.5819e-01, 4.5661e-02, 2.2141e+00,
        8.2503e-02, 1.9496e-01, 3.5524e-01, 3.2108e+00, 1.3036e-02, 1.1707e-02,
        2.8551e-03, 1.6687e-01, 4.1712e-04, 1.8641e-03, 1.0045e-03, 1.5082e-03,
        2.6752e-03, 1.1244e-01, 3.6370e+00, 4.0661e-01, 1.3861e+00, 8.6134e-02,
        1.5015e+00, 1.6943e-02, 5.3755e-01, 1.4911e-01, 3.2639e-02, 4.3196e-02,
        5.7386e-02, 9.9790e-01, 6.1451e-02, 7.7898e-01, 5.8128e-04, 2.7646e-02,
        2.5128e+00, 1.0857e+00, 3.4322e+00, 4.5395e-01, 1.7173e-01, 7.7133e-01,
        2.3758e-02, 1.6561e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([7.3466e+00, 7.9536e-01, 6.8376e+00, 4.4765e-01, 4.2564e-02, 1.4881e+00,
        6.7809e-01, 1.1934e+00, 1.1023e+00, 8.4347e-01, 1.5690e-01, 1.2846e+00,
        9.7694e-02, 3.3341e-01, 7.3057e-01, 1.3923e-01, 6.2091e-01, 6.3619e-01,
        1.3652e-01, 9.2418e-01, 2.6646e-03, 4.1264e-03, 3.2235e-03, 6.8702e-02,
        6.4521e-01, 4.2306e-01, 2.2217e+00, 2.9202e-01, 1.8003e-01, 2.4145e-02,
        9.4326e-03, 2.8249e-03, 5.5379e-02, 1.3985e-02, 1.4558e-02, 1.1981e-02,
        3.3121e-02, 6.7294e-02, 1.0098e-02, 1.4435e-01, 3.5218e-02, 1.5048e+00,
        9.8547e-02, 2.0199e-01, 5.2712e-01, 4.1520e-01, 8.1791e-03, 4.4285e-03,
        4.3976e-03, 1.4918e-01, 5.1149e-04, 3.2671e-03, 8.3636e-03, 1.8310e-02,
        4.1363e-03, 5.0392e-02, 3.2720e+00, 4.1170e-01, 9.0833e-01, 1.1024e-02,
        2.0455e+00, 2.4552e-02, 9.3592e-02, 2.3178e-02, 4.6180e-03, 3.6474e-03,
        7.0314e-03, 1.0294e+00, 5.8310e-02, 4.8759e-01, 3.6723e-04, 9.7109e-03,
        1.7261e+00, 5.3004e-01, 5.0793e+00, 4.6832e-02, 1.8905e-01, 3.1431e-01,
        1.5067e-03, 1.2160e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.1170e+01, 2.1389e+01, 4.4183e+01, 1.0468e+01, 2.3392e+00, 3.4995e+01,
        4.1945e+01, 4.0415e+01, 2.1444e+01, 3.1346e+01, 1.4212e+01, 6.4815e+01,
        9.1953e+00, 9.9690e+00, 1.5391e+01, 2.8105e+00, 1.0547e+01, 1.4966e+01,
        2.3746e+00, 1.5136e+01, 5.2755e-01, 3.2634e-01, 1.6167e-01, 5.0611e+00,
        2.3342e+01, 2.1081e+01, 2.8465e+01, 1.4499e+01, 1.0902e+01, 1.1368e+00,
        4.7547e-01, 1.9385e-01, 5.9278e-01, 2.9198e+00, 7.3577e-01, 2.5919e-01,
        1.7368e+00, 2.2876e+00, 2.1476e-01, 1.7648e+00, 1.7146e-01, 3.2212e+00,
        1.9454e-01, 4.2920e-01, 6.5462e-01, 6.4236e-01, 4.6664e-01, 1.9719e-01,
        1.5188e-01, 4.9121e+00, 6.2214e-02, 2.1406e-01, 4.0537e-01, 5.9238e-01,
        3.0832e-01, 1.4420e+00, 1.0048e+01, 5.6786e+00, 5.2441e+00, 4.7326e-01,
        2.8796e+00, 5.1475e-01, 3.8931e-01, 3.0188e-01, 6.3904e-02, 8.8017e-02,
        7.2358e-02, 1.8568e+01, 1.4959e-01, 3.3439e+00, 1.5050e-02, 2.5406e-01,
        4.5693e+00, 2.5516e+00, 6.4991e+00, 4.7076e-02, 1.8920e-01, 3.1583e-01,
        2.9685e-03, 9.2117e-02], device='cuda:0')
Outer loop valEpocw Maximum [18/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 65.3
model_train val_loss valEpocw [18/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 49.4
model_train val_loss valEpocw [18/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 662.2
Sum_Val Meta Model:  tensor([5.7216e+00, 5.2202e-02, 1.0732e+00, 4.0443e-02, 5.9466e-03, 4.2186e-02,
        6.1890e-03, 1.3062e-01, 5.1876e-02, 2.0412e-02, 5.2546e-03, 6.6484e-03,
        1.2153e-03, 3.3831e-01, 6.0553e-02, 6.7235e-02, 2.2184e-01, 3.3228e-02,
        3.7000e-02, 5.9688e-02, 1.0253e-03, 8.4826e-03, 6.3948e-02, 9.4544e-03,
        8.8763e-02, 1.0897e-02, 8.8731e-01, 4.5022e-02, 6.4363e-03, 2.7355e-02,
        3.3831e-02, 4.1635e-02, 5.2760e-01, 4.1210e-04, 2.6030e-02, 8.1586e-02,
        1.5161e-01, 1.9319e-01, 7.5725e-03, 1.5099e+00, 5.9658e+00, 3.2537e+01,
        4.2677e+01, 4.1815e+01, 4.0977e+01, 4.6595e+01, 1.0034e-01, 1.1201e-01,
        7.8433e-01, 2.0543e-01, 1.0514e-01, 2.7755e-01, 6.0645e-01, 2.1589e-01,
        4.0583e-01, 1.8810e+00, 4.3385e+01, 4.6791e-01, 1.1473e+00, 2.2314e-02,
        6.6785e+01, 3.5139e-02, 2.0984e+00, 5.4547e-01, 5.9810e-01, 2.3394e-02,
        5.9038e-01, 3.4796e-01, 1.9517e+00, 2.6569e-01, 2.7340e-03, 1.3808e-01,
        1.8495e+00, 5.5501e+00, 1.0437e+00, 2.6809e+01, 9.9800e+00, 1.1704e+00,
        1.0799e-01, 6.4263e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.6775e+00, 1.2394e-02, 4.9857e-01, 1.6863e-03, 5.5197e-04, 2.4907e-03,
        5.4320e-04, 1.2898e-01, 6.2819e-03, 2.1975e-03, 3.2746e-04, 3.4619e-04,
        1.4666e-04, 2.5618e-01, 7.6543e-03, 2.3576e-02, 3.5348e-02, 3.7576e-03,
        2.8559e-03, 1.7869e-03, 5.4365e-05, 1.3953e-04, 9.8801e-05, 3.6478e-04,
        4.8565e-02, 7.9435e-03, 9.7170e-01, 4.0173e-02, 2.7196e-03, 2.4708e-03,
        5.0099e-03, 3.7788e-02, 4.8280e-01, 9.6308e-05, 9.1421e-03, 2.1269e-02,
        8.9170e-02, 1.3957e-01, 8.5783e-03, 2.2434e+00, 4.2202e+00, 2.9575e+01,
        3.7715e+01, 4.0790e+01, 4.4223e+01, 4.8723e+01, 5.9342e-02, 7.4805e-02,
        5.0932e-01, 1.3543e-01, 5.4734e-02, 3.0602e-01, 5.3815e-01, 3.4455e-01,
        2.4974e-01, 1.4675e+00, 2.5150e+01, 5.1207e-01, 1.1912e+00, 8.1741e-03,
        5.9306e+01, 9.3063e-03, 1.5326e+00, 4.6005e-01, 5.9324e-01, 3.1560e-02,
        6.4516e-01, 3.5219e-01, 2.2342e+00, 3.5107e-01, 8.9092e-04, 1.2628e-01,
        2.0633e+00, 5.2933e+00, 1.0391e+00, 2.0031e+01, 1.1455e+01, 1.6203e-01,
        1.4592e-02, 2.6532e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.3244e+01, 7.8157e-01, 4.9944e+00, 9.5416e-02, 9.0797e-02, 1.5703e-01,
        1.5692e-01, 1.3861e+01, 2.8256e-01, 2.2139e-01, 1.0190e-01, 6.1376e-02,
        6.0764e-02, 1.5643e+01, 3.7922e-01, 1.1645e+00, 1.1337e+00, 2.3166e-01,
        1.3488e-01, 7.8585e-02, 6.5938e-02, 3.7807e-02, 1.3303e-02, 7.7967e-02,
        4.1412e+00, 1.4858e+00, 2.6267e+01, 7.8089e+00, 6.9100e-01, 3.2299e-01,
        7.5712e-01, 7.9441e+00, 8.7282e+00, 9.4141e-02, 1.1691e+00, 8.6112e-01,
        1.3005e+01, 1.0908e+01, 4.4115e-01, 4.6239e+01, 2.3939e+01, 5.8393e+01,
        6.0863e+01, 7.0097e+01, 4.9841e+01, 6.6182e+01, 9.3298e+00, 8.5311e+00,
        3.9145e+01, 1.1010e+01, 2.1622e+01, 4.5843e+01, 6.1090e+01, 2.8540e+01,
        5.8364e+01, 8.3331e+01, 9.4358e+01, 1.4893e+01, 8.9326e+00, 1.0346e+00,
        7.6166e+01, 4.7636e-01, 7.9455e+00, 1.2174e+01, 1.5318e+01, 1.9241e+00,
        1.1238e+01, 1.3633e+01, 5.9690e+00, 3.7062e+00, 1.0392e-01, 8.1385e+00,
        6.0347e+00, 3.5872e+01, 1.2246e+00, 2.0053e+01, 1.1456e+01, 1.6219e-01,
        2.8645e-02, 3.0237e-01], device='cuda:0')
Outer loop valEpocw Maximum [18/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 391.9
model_train val_loss valEpocw [18/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 348.3
model_train val_loss valEpocw [18/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1241.2
Sum_Val Meta Model:  tensor([1.3529e+01, 5.9989e-02, 3.2379e+00, 2.5509e-02, 5.5341e-03, 3.9566e-01,
        6.2520e-02, 4.0186e-01, 6.2418e-02, 4.6559e-01, 4.6112e-02, 8.4630e-02,
        8.8032e-04, 5.7980e-01, 6.8118e-01, 5.4870e-02, 7.3388e-02, 2.2282e-02,
        1.1466e-02, 2.5398e-02, 9.5022e-04, 3.1460e-03, 4.2725e-03, 5.9218e-03,
        6.0991e-01, 1.8335e-02, 2.1346e+00, 1.7434e-02, 2.2448e-01, 5.8156e-03,
        8.6032e-03, 1.4234e-03, 2.0944e-01, 6.0664e-03, 2.2991e-02, 2.1387e-01,
        4.4998e-03, 9.1364e-03, 1.2905e-01, 2.3069e+00, 3.8353e+00, 1.5549e+01,
        5.1951e+00, 5.0977e+00, 8.4741e+00, 1.1969e+01, 5.0422e-03, 8.8205e-03,
        2.5361e-02, 7.2397e-03, 3.6349e-03, 5.3227e-03, 4.2761e-03, 1.3411e-01,
        6.3481e-03, 4.4308e-02, 5.8834e+00, 2.1153e-01, 2.3858e+00, 1.9411e-02,
        3.4226e+01, 1.4851e-02, 9.0880e-01, 5.4028e-01, 3.1829e-01, 5.9742e-02,
        5.2735e-01, 2.7416e+00, 4.8910e-01, 3.7398e-01, 1.4478e-03, 4.4283e-02,
        2.4559e+00, 1.9789e+00, 3.8932e+02, 1.0263e+01, 1.3681e+00, 7.6863e+00,
        3.1682e-02, 1.0602e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([4.4646e+00, 1.4788e-01, 2.6692e+00, 2.9858e-02, 2.0021e-02, 3.3103e-01,
        8.1050e-02, 3.4619e-01, 2.2657e-01, 4.4628e-01, 2.8977e-02, 1.4040e-01,
        5.4054e-03, 5.2067e-01, 7.1948e-01, 2.0275e-02, 2.6076e-02, 2.0707e-02,
        3.2762e-03, 7.9190e-03, 8.4140e-04, 6.5423e-04, 2.4615e-03, 1.3095e-02,
        5.7140e-01, 3.8751e-02, 1.6015e+00, 2.3084e-02, 2.2509e-01, 2.4998e-03,
        4.2632e-03, 2.7745e-03, 1.6743e-02, 7.3283e-04, 3.9347e-03, 4.4518e-03,
        4.7183e-03, 6.3786e-03, 8.4288e-03, 1.8396e-01, 3.3505e-01, 1.0480e+00,
        6.6233e-01, 4.9675e-01, 1.5647e+00, 1.6960e+00, 2.3790e-03, 1.4230e-03,
        3.0456e-03, 4.4500e-03, 2.7972e-04, 1.4834e-03, 1.7993e-03, 8.6133e-03,
        2.6422e-03, 5.1122e-02, 1.0287e+00, 1.4488e-02, 1.9028e+00, 1.1512e-02,
        1.1568e+01, 1.2588e-02, 1.2448e-01, 2.6985e-02, 1.0763e-02, 1.0199e-02,
        1.9175e-02, 4.1599e-01, 5.9038e-02, 4.1867e-02, 4.9859e-04, 1.3398e-02,
        7.4535e-02, 1.2644e+00, 7.2519e+01, 3.7701e+00, 4.5959e-02, 8.4073e+00,
        2.1605e-03, 3.8180e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.7340e+01, 4.6262e+00, 1.9458e+01, 8.3756e-01, 1.3224e+00, 9.8853e+00,
        7.9149e+00, 1.5252e+01, 5.4004e+00, 1.9313e+01, 3.1498e+00, 9.4455e+00,
        7.1235e-01, 1.5702e+01, 1.8378e+01, 5.4299e-01, 5.3392e-01, 6.1431e-01,
        8.3786e-02, 1.9836e-01, 2.4795e-01, 6.5010e-02, 1.6238e-01, 1.1217e+00,
        2.3493e+01, 2.6289e+00, 2.2389e+01, 1.5899e+00, 1.9247e+01, 1.4312e-01,
        2.7183e-01, 2.4612e-01, 1.8441e-01, 2.0347e-01, 2.3018e-01, 9.0441e-02,
        3.1609e-01, 2.8214e-01, 1.8746e-01, 2.1104e+00, 1.3551e+00, 2.0674e+00,
        1.1809e+00, 9.6220e-01, 1.8739e+00, 2.4999e+00, 1.8234e-01, 8.2133e-02,
        1.3487e-01, 1.9728e-01, 4.8219e-02, 1.1758e-01, 1.0691e-01, 3.1040e-01,
        2.5609e-01, 1.6887e+00, 3.1902e+00, 2.3831e-01, 1.0868e+01, 6.6330e-01,
        1.5507e+01, 2.9725e-01, 5.2207e-01, 3.5500e-01, 1.4111e-01, 2.9503e-01,
        1.8004e-01, 6.8720e+00, 1.4565e-01, 2.8906e-01, 2.3135e-02, 3.9927e-01,
        1.8224e-01, 6.3666e+00, 9.1576e+01, 3.7844e+00, 4.5985e-02, 8.4391e+00,
        4.5789e-03, 2.7704e-02], device='cuda:0')
Outer loop valEpocw Maximum [18/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 538.1
model_train val_loss valEpocw [18/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 120.2
model_train val_loss valEpocw [18/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 409.4
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.84127874 97.45495303 93.25909772 98.01050182 98.65498715 97.6169881
 98.29924099 95.23275789 98.05679755 96.89818594 98.61721958 98.92788831
 99.44566952 95.35824369 97.48419244 97.33677709 96.40598921 97.66815706
 98.85966302 98.43203665 98.6488956  99.31165556 99.54557084 99.02047977
 95.24128605 96.83848881 94.47740646 97.04925622 98.07872711 98.30533254
 98.37721275 98.6062548  97.07240409 98.55021259 98.74879692 98.82798699
 97.57800222 98.41619863 98.93519816 93.39432999 98.12014961 94.57852609
 97.74856544 97.17230541 98.02268491 96.33167237 98.21761431 98.65498715
 98.15548056 98.75367016 98.76341663 98.6342759  99.02900793 98.36259305
 98.72077582 97.68765    93.07878803 96.79706631 96.64477772 97.57312898
 95.12067348 98.7037195  97.50003046 97.40743899 98.85113485 97.54998112
 98.63549421 95.98323607 98.93032492 98.3833043  99.81603538 97.53170649
 98.90108551 96.05024305 98.19202982 98.67935332 99.50049342 99.19713454
 99.84405648 99.15814866]
Accuracy th:0.7 is [86.75942057 97.37332635 92.65847151 97.79242456 98.51122672 97.54145296
 98.25538188 95.17062414 98.05679755 96.98346755 98.55386752 98.86819118
 99.42373996 95.32900428 97.41474885 97.15159416 96.35238362 97.5475445
 98.76707155 98.37843106 98.51853657 99.30921894 99.4846554  98.91448691
 95.22544803 96.71056639 94.29344184 96.94935491 98.03121307 98.21274107
 98.23345232 98.58067031 96.71300301 98.4113254  98.59407171 98.66960685
 97.27586165 98.3833043  99.0107333  93.08244295 98.04583277 94.52979374
 97.74003728 96.8238691  97.86186815 95.75784895 98.16644534 98.62452943
 98.07994542 98.74148707 98.63305759 98.59041678 99.02169808 98.40888878
 98.7183392  97.57678391 92.50983784 96.48517927 96.49858067 97.44520656
 95.9734896  98.54899429 97.1016435  97.31606584 98.72930398 97.41840377
 98.50757179 95.95156004 98.87550103 98.22736078 99.81603538 97.20032651
 98.77559971 95.91988402 98.02390322 98.40157893 99.45054276 99.07652197
 99.84405648 99.14718388]
Avg Prec: is [96.56721038 36.11877241 72.15747828 67.76639875 79.07905279 64.07160507
 73.67662852 48.10394746 57.73326961 52.34462692 31.35584572 54.05682851
 25.21344257 27.12802139 32.09276408 56.58215697 28.74820306 41.57097934
 47.97745562 38.0351878  60.21658853 47.89806127 90.11256579 82.05945515
 26.32911343 33.20423033 38.92529709 39.12475243 25.45232146 39.32724877
 72.52439029 36.73734604 58.99988164 62.05585689 73.65183405 79.77990349
 58.61346351 75.18077979 86.80482741 45.80561685 45.10932426 70.71178898
 67.6167817  63.31924197 70.73883516 76.25575459 38.40511012 33.67904106
 40.97501435 44.77434813 61.546779   37.07265699 23.67707082 72.51317123
 25.13193877 43.29548819 72.00715743 58.18184795 43.00762685 58.44391044
 88.63463242 82.80344705 70.82470065 49.26883444 59.8928273  43.90110521
 59.81372265 24.78894841 55.48811131 68.75998346 11.73409471 72.98013202
 78.40265133 50.54067533 81.22369038 85.70322774 68.69381546 84.05693014
 11.47021917 26.42012645]
Accuracy th:0.5 is [45.63540893 97.2137279  72.14215226 97.02489005 97.26733349 77.01904217
 77.38331648 76.49882433 78.30923113 96.45228494 78.54680133 98.52097319
 99.41399349 80.37791937 78.21054812 96.56680596 96.29512311 77.91449909
 98.65376884 98.30776915 80.72148244 79.13037122 98.38695922 78.42009722
 80.94686956 96.65086926 94.0778012  77.58433742 98.01293844 78.5151253
 97.30875598 98.57457877 96.36213009 98.02024829 87.9472716  78.16790731
 77.83530902 91.23914182 97.11504489 75.1294453  79.65546229 92.05906361
 77.36747847 76.97030982 96.9627563  93.87434364 98.02877645 98.57336046
 97.40134745 89.00841851 87.86929984 98.55508583 98.99976852 77.52220368
 98.70615611 77.96323144 72.61729267 93.51006932 96.24273583 96.9067141
 89.79300934 97.17717864 93.13970346 77.95226666 98.42838172 78.45786479
 98.20786784 77.15427444 79.01706851 97.55972759 79.55799759 95.99054592
 78.68447022 95.45083515 77.19691524 83.01068457 87.60370853 78.46273803
 79.58601869 99.14718388]
Accuracy th:0.7 is [45.67073988 97.2137279  72.14215226 97.02489005 97.26733349 77.01904217
 77.38331648 76.68400726 78.30923113 96.4754328  78.54680133 98.52097319
 99.41399349 80.84574993 78.21054812 96.56680596 96.29512311 77.91449909
 98.65376884 98.30776915 81.13936234 79.13037122 98.38695922 78.80386448
 81.54993238 96.65086926 94.0778012  77.58433742 98.01293844 78.5151253
 97.30875598 98.57457877 96.36213009 98.02024829 88.1007785  78.16790731
 77.83530902 91.53275423 97.11504489 75.1294453  80.28289129 92.05906361
 77.36747847 76.97030982 96.9627563  93.87434364 98.02877645 98.57336046
 97.80826257 89.56640392 88.08372218 98.55508583 98.99976852 77.52220368
 98.70615611 77.96323144 72.61729267 93.91211121 96.24273583 96.9067141
 89.79300934 97.17717864 93.29321037 77.95226666 98.42838172 78.45786479
 98.20786784 77.15427444 79.01706851 97.55972759 79.55799759 95.99054592
 78.76975183 95.45083515 77.19691524 83.08743802 87.7937647  78.46273803
 79.58601869 99.14718388]
Avg Prec: is [55.90306711  3.05907676 11.32742336  3.18199575  2.29929753  3.87672619
  3.23372455  5.60467166  2.4909606   3.74239555  1.61033699  1.75889276
  0.62767972  5.16672833  2.60697515  3.17388507  3.62758812  2.64460585
  1.39659408  1.69489726  1.95415614  0.84711302  1.79307034  2.38599262
  5.16703406  3.70890196  6.47262117  3.37214626  2.10009968  1.93603481
  2.71666018  1.38419705  3.65137554  1.63590479  2.30487955  2.32806817
  3.14803443  2.61320021  2.77759099  7.39136772  2.2236359   8.28228195
  3.39490178  4.02693152  3.27225925  6.40420784  2.03510813  1.46573125
  2.11339372  1.52698389  1.77081716  1.52311416  1.06533283  3.08614933
  1.34258779  2.62879761 11.06656692  3.59998761  3.98521371  2.82036094
 10.92311742  2.21444844  3.837417    2.91804137  1.62679995  2.46108799
  1.87002486  4.09194041  1.29923527  2.39466816  0.18071924  3.54842819
  1.999576    4.7406223   3.85874823  3.07496728  0.82405001  1.87071558
  0.15764065  0.73227525]
mAP score regular 54.81, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [87.95874131 97.32416474 93.32536064 98.22358422 98.97351571 97.67546154
 98.51508583 95.32849989 98.04669009 96.89314099 98.65709943 99.04825971
 99.38211625 95.28116202 97.3715026  97.37399407 96.35747565 97.73525675
 98.98348158 98.37556369 98.89378877 99.27000025 99.64122879 99.24259412
 95.46553056 96.76358472 94.44901213 97.2967586  97.91713382 98.20365249
 98.72436904 98.6820141  97.01771433 98.70692877 98.86139971 99.00590478
 97.92709968 98.35563196 98.99095598 93.30293744 97.95450582 93.1036201
 97.22699753 96.58669058 96.82587139 94.77290281 98.37058076 98.82901064
 98.13638289 98.6894885  98.81406184 98.6521165  98.85143384 98.36310636
 98.76921544 97.81249221 91.14781872 97.12235593 96.44716845 97.59573461
 91.68597553 98.79911304 97.2519122  97.50604181 98.7567581  97.58576874
 98.5773725  95.7694895  98.75426664 98.35064903 99.81563146 97.72778235
 98.44781623 95.89904577 97.40389167 97.44873807 99.15788425 98.7119117
 99.82559733 99.17034158]
Accuracy th:0.7 is [87.71208611 97.41385754 92.92174303 98.03921569 98.89877171 97.64058101
 98.5175773  95.38082069 98.15382316 97.042629   98.58982983 98.99593891
 99.37713332 95.13416548 97.43378927 97.16471087 96.30266338 97.62563221
 98.94112664 98.39300396 98.79911304 99.33228692 99.59139946 99.17283305
 95.44809029 96.66641752 94.58853427 97.15723647 97.83989835 98.17873782
 98.61225303 98.68450557 96.82587139 98.62471037 98.76921544 98.81406184
 97.68791888 98.25846476 99.07068291 93.19082144 97.99686075 93.60440491
 97.45621247 96.67140045 97.06256073 94.76293694 98.32075143 98.82153624
 98.03921569 98.7492837  98.6820141  98.5997957  98.88631437 98.43037596
 98.74430077 97.73774821 91.26491766 96.82088846 96.34501831 97.46119541
 92.83703316 98.65709943 97.00774846 97.41136607 98.70692877 97.52099061
 98.49764556 95.78692977 98.83150211 98.22109276 99.81563146 97.51351621
 98.4876797  95.92146897 97.4088746  97.50853327 99.21020505 98.67952263
 99.82559733 99.15539278]
Avg Prec: is [96.47612909 34.18440278 71.54057017 73.41321211 77.89446227 65.32916039
 80.73985939 48.37207927 61.64813398 55.7405267  36.27774774 56.77719376
 23.54103808 31.49436122 32.84656044 61.47702746 31.5336787  44.11666042
 50.54335227 36.23619911 69.64710314 55.81643531 92.57471874 87.97826532
 25.31795087 37.2384128  33.960146   45.55950992 27.62934154 37.21153286
 77.74057869 39.10633608 55.72478594 65.16443096 72.18414989 81.96150285
 60.83472312 77.10306737 89.51806134 44.72080819 37.14547417 54.49405096
 50.86056348 42.55355675 32.66318451 52.21541228 37.71866543 28.74448479
 41.41832366 40.45396834 67.0384177  36.4005413  22.99042515 75.74003575
 29.61041022 42.44697306 59.02074225 57.15924699 37.54822622 61.16447452
 68.13207948 86.54141849 67.2858237  52.24759456 61.41457118 38.1128381
 60.94823657 24.89553346 42.64093524 67.7341914   9.01113159 75.33321522
 57.21793686 45.09274492 65.24743434 50.32906427 12.75189608 52.86896726
  1.97925824 22.46598747]
Accuracy th:0.5 is [45.29237362 97.22450607 70.38642649 96.96290206 97.90716795 75.86765329
 75.95485462 74.88103246 77.43727733 96.41976231 77.59174826 98.5325261
 99.34972718 78.18970028 77.52447866 96.31262924 96.21047911 76.94147545
 98.78167277 98.34068316 79.1314747  78.23205521 98.31327703 78.12741361
 78.14983681 96.52938685 94.3393876  76.98881331 97.81747515 77.55437626
 97.52597354 98.67204823 96.39983058 98.18870369 88.9927     77.16570745
 77.13830132 92.40351795 97.0276802  74.26813165 77.7686424  92.37362035
 76.29867703 75.96731196 97.03764606 94.02795426 98.18621222 98.77668984
 97.75020555 88.51682986 86.1997658  98.55993223 98.87385704 76.28871116
 98.6969629  76.97137305 71.00431024 94.39419987 96.16314124 96.78102499
 90.13379176 97.04761193 93.25061664 76.96888158 98.32075143 77.80352293
 98.13139996 76.14420609 78.26444428 97.53593941 78.68799362 96.07843137
 78.02277201 95.44559882 76.05700476 83.85778708 89.69031069 77.6266288
 78.75775469 99.15040985]
Accuracy th:0.7 is [45.48421656 97.22450607 70.38642649 96.96290206 97.90716795 75.86765329
 75.95485462 75.028029   77.43727733 96.41976231 77.59174826 98.5325261
 99.34972718 78.60577522 77.52447866 96.31262924 96.21047911 76.94147545
 98.78167277 98.34068316 79.40055311 78.23205521 98.31327703 78.35912001
 78.59082642 96.52938685 94.3393876  76.98881331 97.81747515 77.55437626
 97.52597354 98.67204823 96.39983058 98.18870369 89.15962827 77.16570745
 77.13830132 92.63771582 97.0276802  74.26813165 78.21959788 92.37362035
 76.29867703 75.96731196 97.03764606 94.02795426 98.18621222 98.77668984
 97.88723622 88.83324613 86.38662581 98.55993223 98.87385704 76.28871116
 98.6969629  76.97137305 71.00431024 94.70065027 96.16314124 96.78102499
 90.13379176 97.04761193 93.37518997 76.96888158 98.32075143 77.80352293
 98.13139996 76.14420609 78.26444428 97.53593941 78.68799362 96.07843137
 78.04270374 95.44559882 76.05700476 83.92754815 89.84976456 77.6266288
 78.75775469 99.15040985]
Avg Prec: is [54.2284923   3.71688322 14.89968607  4.54214978  1.46186322  4.26101018
 12.99589529  8.64703556  7.86318773  5.24639548  2.32783538  4.91388874
  1.65295314  5.88031442  2.98399022  3.66830043 24.39199196  6.4488295
  1.55707969  2.69716325  3.54665601  1.47046527  1.20545213  5.43686056
  5.70174422  9.84465771  7.96186333  4.60144495  3.95674467  6.11912108
  2.24027932  0.8574045   3.15121321  1.09613631  1.70121679  2.34045792
  2.00227437  2.18168442  2.25455724  6.19436892  1.73474279  6.01638063
  2.20033921  2.7211434   2.3734532   4.8448337   1.68678191  1.01535078
  1.39355108  1.15326817  1.17346303  0.95945613  0.72833849  2.32036879
  0.85157026  1.82959715 10.08108706  2.90007939  3.84470045  2.81821092
  7.84202954  2.06032197  3.18509861  2.50494623  1.34758599  1.83281181
  1.52129485  3.46589341  1.07675348  2.22870809  0.19220346  3.220294
  1.5685663   3.95054357  3.54763777  2.31111519  0.60170109  1.48005684
  0.12739751  0.58018201]
mAP score regular 51.46, mAP score EMA 4.32
Train_data_mAP: current_mAP = 54.81, highest_mAP = 54.81
Val_data_mAP: current_mAP = 51.46, highest_mAP = 51.49
tensor([0.0837, 0.0209, 0.1124, 0.0230, 0.0084, 0.0208, 0.0052, 0.0138, 0.0281,
        0.0139, 0.0048, 0.0081, 0.0039, 0.0217, 0.0262, 0.0252, 0.0345, 0.0208,
        0.0258, 0.0272, 0.0015, 0.0053, 0.0086, 0.0065, 0.0159, 0.0081, 0.0520,
        0.0079, 0.0061, 0.0099, 0.0090, 0.0059, 0.0623, 0.0015, 0.0092, 0.0294,
        0.0082, 0.0135, 0.0263, 0.0633, 0.2015, 0.4952, 0.5990, 0.5481, 0.8787,
        0.7184, 0.0075, 0.0102, 0.0132, 0.0135, 0.0029, 0.0075, 0.0097, 0.0149,
        0.0055, 0.0197, 0.2868, 0.0406, 0.1537, 0.0098, 0.7624, 0.0268, 0.2065,
        0.0482, 0.0505, 0.0205, 0.0751, 0.0358, 0.4602, 0.1287, 0.0125, 0.0212,
        0.4123, 0.1664, 0.8637, 0.9984, 0.9999, 0.9986, 0.6161, 0.1107],
       device='cuda:0')
Sum Train Loss:  tensor([2.9069e+00, 1.8604e-01, 2.9554e+00, 1.7234e-01, 1.8836e-02, 3.3726e-01,
        3.6097e-02, 2.2131e-01, 6.6002e-02, 1.4810e-01, 5.7846e-02, 4.7983e-02,
        1.8991e-02, 5.0340e-01, 4.7691e-01, 3.5247e-01, 4.5109e-01, 1.1619e-01,
        4.9034e-02, 7.0743e-02, 8.3544e-03, 1.8805e-02, 1.2499e-02, 1.2640e-02,
        3.8284e-01, 1.0223e-01, 9.7531e-01, 3.0067e-02, 5.7483e-02, 9.5950e-02,
        9.7767e-03, 1.9996e-02, 7.2041e-01, 3.5551e-03, 4.9943e-02, 1.9278e-02,
        9.6737e-02, 1.3798e-01, 4.4335e-02, 1.8812e+00, 6.8541e-01, 1.0054e+01,
        4.7382e+00, 6.0502e+00, 3.8855e+00, 1.0883e+01, 4.0502e-02, 4.3138e-02,
        1.7276e-01, 5.1995e-02, 1.0103e-02, 6.9602e-02, 1.1186e-02, 3.1768e-02,
        5.8697e-03, 2.1357e-01, 4.8110e+00, 4.2049e-01, 2.3187e+00, 7.6899e-02,
        9.6603e+00, 7.0953e-02, 3.0839e+00, 8.4391e-01, 4.4074e-01, 1.8193e-01,
        4.1480e-01, 4.4349e-01, 2.4039e+00, 4.8847e-01, 3.1553e-03, 2.0964e-01,
        5.3702e+00, 2.2240e+00, 6.1397e+00, 4.0556e+00, 5.0706e+00, 1.5712e+00,
        4.8830e-02, 6.2697e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [19/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 101.8
Sum Train Loss:  tensor([3.0057e+00, 2.2417e-01, 3.1909e+00, 4.6131e-01, 6.0455e-02, 6.3338e-02,
        2.8533e-02, 3.3951e-01, 1.4039e-01, 1.6143e-01, 6.4372e-02, 3.9579e-02,
        1.7678e-02, 1.8073e-01, 4.8190e-01, 2.0249e-01, 7.3456e-01, 2.9260e-01,
        1.8076e-01, 1.6138e-01, 2.5802e-02, 2.1306e-02, 4.1826e-03, 9.4097e-03,
        1.1836e-01, 1.5912e-01, 1.2350e+00, 6.2330e-02, 4.0617e-02, 1.2912e-01,
        3.6878e-02, 4.6158e-02, 4.5040e-01, 1.0690e-02, 3.6682e-02, 4.1944e-02,
        5.6621e-02, 5.1513e-02, 9.9321e-02, 1.3164e+00, 9.3036e-01, 5.2504e+00,
        4.0058e+00, 3.1045e+00, 4.7492e+00, 8.0503e+00, 1.1344e-01, 1.7281e-02,
        1.0351e-01, 4.7007e-02, 1.3531e-02, 7.0522e-03, 1.2461e-02, 5.2549e-02,
        9.3199e-03, 1.2292e-01, 6.9239e+00, 3.0268e-01, 2.3591e+00, 2.6787e-02,
        7.3037e+00, 5.6374e-02, 8.0950e-01, 2.0410e-01, 1.1175e-01, 1.6558e-01,
        1.2422e-01, 7.1314e-01, 1.3710e+00, 2.6847e-01, 9.4848e-04, 9.8732e-02,
        1.1904e+00, 1.9942e+00, 4.5708e+00, 6.5629e+00, 2.2594e-01, 1.2606e+01,
        9.5489e-02, 2.5043e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [19/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 88.9
Sum Train Loss:  tensor([3.0558e+00, 2.0571e-01, 3.1001e+00, 1.8043e-01, 1.7498e-02, 2.2176e-01,
        3.4895e-02, 3.1223e-01, 1.9203e-01, 1.9110e-01, 2.2501e-02, 1.2799e-01,
        5.7503e-03, 4.6220e-01, 3.7586e-01, 3.1147e-01, 7.6351e-01, 3.7817e-01,
        1.5721e-01, 1.9794e-01, 2.0149e-03, 4.3761e-03, 3.5683e-03, 9.4115e-03,
        3.4239e-01, 1.1341e-01, 9.9489e-01, 1.1965e-01, 1.0168e-01, 3.6016e-02,
        6.5152e-02, 9.4357e-03, 4.8173e-01, 2.6103e-03, 4.7752e-02, 1.7004e-01,
        1.5955e-01, 1.4770e-01, 8.8257e-02, 1.6053e+00, 2.1823e+00, 7.6546e+00,
        4.9292e+00, 7.7653e+00, 5.3399e+00, 6.5073e+00, 5.3270e-02, 3.9595e-02,
        9.0505e-02, 6.8017e-02, 4.4649e-03, 5.1161e-03, 1.0638e-02, 6.5469e-02,
        1.5761e-02, 2.0000e-01, 5.1950e+00, 3.1362e-01, 3.2145e+00, 1.5133e-01,
        8.5633e+00, 2.3570e-01, 1.0487e+00, 3.2728e-01, 4.8189e-02, 2.7402e-01,
        1.4042e-01, 7.0708e-01, 1.1421e+01, 2.3219e+00, 3.6390e-03, 2.1175e-01,
        2.2897e+00, 3.8278e+00, 8.1680e+00, 3.1829e+00, 4.5386e-01, 1.8947e+00,
        7.3667e-02, 1.1967e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [19/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 103.9
Sum Train Loss:  tensor([2.7737e+00, 2.9279e-01, 2.7957e+00, 1.8628e-01, 1.4481e-02, 1.9108e-01,
        2.3322e-02, 1.9550e-01, 9.3672e-02, 1.1371e-01, 4.4385e-02, 1.7584e-02,
        5.9650e-02, 4.5259e-01, 1.2638e-01, 1.9188e-01, 4.3161e-01, 2.1698e-01,
        5.3247e-02, 2.3252e-01, 6.6519e-03, 1.6035e-02, 8.3806e-03, 1.4074e-02,
        2.9502e-01, 1.1542e-01, 1.4295e+00, 6.4620e-02, 4.5226e-02, 1.0416e-01,
        6.1492e-02, 8.2520e-03, 3.6878e-01, 6.1973e-03, 8.0585e-02, 1.5902e-01,
        6.5923e-02, 3.4837e-02, 1.4447e-01, 1.1284e+00, 7.3791e-01, 1.2647e+01,
        7.2220e+00, 4.2272e+00, 1.2286e+01, 1.4219e+01, 7.8560e-02, 6.5164e-02,
        1.2704e-01, 1.2496e-01, 2.4516e-02, 5.2037e-02, 7.6429e-02, 8.4987e-02,
        9.1004e-02, 1.1662e-01, 7.5635e+00, 2.7467e-01, 2.2395e+00, 4.5875e-02,
        1.1782e+01, 8.6935e-02, 8.0473e-01, 3.8309e-01, 4.2211e-02, 1.4807e-01,
        6.8199e-02, 4.4250e-01, 2.1819e+00, 2.8161e-01, 3.2843e-03, 3.4348e-01,
        1.4409e+00, 4.0030e+00, 1.1777e+01, 4.6098e+00, 3.9309e-01, 7.1166e-01,
        3.7212e+00, 6.1526e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [19/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 118.8
Sum Train Loss:  tensor([2.6298e+00, 1.7860e-01, 2.0474e+00, 1.7141e-01, 4.4522e-02, 1.5801e-01,
        4.5622e-02, 2.6740e-01, 1.4894e-01, 5.9138e-02, 2.3341e-02, 5.3013e-02,
        1.3555e-02, 2.9820e-01, 3.7719e-01, 2.7293e-01, 6.4105e-01, 1.2233e-01,
        2.3429e-01, 1.9927e-01, 6.8597e-03, 6.1858e-03, 9.6352e-03, 1.6759e-02,
        3.1136e-01, 1.2499e-01, 1.3114e+00, 6.7375e-02, 1.3630e-01, 6.8356e-02,
        3.1648e-02, 8.7869e-03, 6.7736e-01, 1.3758e-02, 3.2107e-02, 1.4037e-01,
        2.3202e-02, 9.9220e-02, 3.7764e-02, 1.2249e+00, 2.1912e+00, 8.3441e+00,
        2.5716e+00, 3.5237e+00, 6.4687e+00, 6.3291e+00, 6.5229e-02, 1.2974e-02,
        3.2211e-02, 2.1957e-02, 2.0792e-02, 3.2530e-02, 6.0248e-02, 1.0898e-01,
        3.3155e-02, 2.1758e-01, 5.0029e+00, 5.8621e-01, 1.8776e+00, 1.0599e-01,
        1.3562e+01, 3.1355e-01, 2.2995e+00, 3.0105e-01, 5.7957e-02, 2.0642e-01,
        4.3629e-01, 3.4860e-01, 2.1105e+00, 6.3634e-01, 3.7592e-03, 8.9164e-02,
        1.7897e+00, 2.8299e+00, 1.2035e+01, 1.7571e+00, 1.2599e+00, 7.3856e+00,
        3.4030e-01, 2.7399e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [19/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 98.0
Sum Train Loss:  tensor([2.2669e+00, 2.6703e-01, 2.8540e+00, 3.2897e-01, 2.1510e-02, 2.2241e-01,
        6.5546e-02, 2.2070e-01, 8.1441e-02, 2.1879e-01, 2.1898e-02, 7.6358e-02,
        9.1882e-03, 5.9513e-01, 1.3155e-01, 1.9413e-01, 4.0225e-01, 1.0982e-01,
        1.3148e-01, 4.3748e-02, 2.5881e-03, 4.8993e-03, 3.9605e-03, 1.1243e-02,
        3.1234e-01, 5.8551e-02, 1.1933e+00, 7.2295e-02, 5.9985e-02, 5.7884e-02,
        2.5822e-02, 2.9233e-02, 5.2652e-01, 6.7747e-03, 9.3289e-03, 1.5929e-01,
        1.0053e-01, 3.6405e-02, 6.8351e-02, 1.6012e+00, 1.4833e+00, 6.1482e+00,
        6.1400e+00, 5.2150e+00, 6.1979e+00, 1.1389e+01, 9.9934e-02, 9.5646e-02,
        8.6943e-02, 1.0956e-01, 1.0276e-02, 4.0142e-02, 6.4491e-02, 8.7534e-02,
        2.7805e-02, 2.0807e-01, 7.5932e+00, 2.2345e-01, 1.7173e+00, 8.4742e-02,
        1.1696e+01, 1.7901e-01, 2.3514e+00, 4.0897e-01, 7.8478e-01, 1.5423e-01,
        7.4563e-01, 8.2350e-01, 2.4648e+00, 2.3375e-01, 2.3333e-02, 1.0521e-01,
        7.1912e-01, 1.9717e+00, 7.9412e+00, 6.5428e+00, 4.6792e+00, 8.8587e-01,
        1.3837e-01, 3.2892e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [19/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 102.8
Sum Train Loss:  tensor([3.2905e+00, 1.7238e-01, 2.3221e+00, 7.4396e-02, 9.4420e-02, 1.0886e-01,
        9.8851e-03, 1.8828e-01, 1.7990e-01, 1.3625e-01, 4.9270e-02, 4.7305e-02,
        1.5035e-03, 4.8662e-01, 3.9741e-01, 2.9863e-01, 3.1874e-01, 5.7998e-01,
        1.3441e-01, 1.7222e-01, 1.1064e-02, 2.5756e-02, 3.7763e-02, 4.2928e-02,
        2.7593e-01, 4.7347e-02, 7.9125e-01, 8.3809e-02, 4.8885e-02, 6.0158e-02,
        1.4672e-01, 4.1414e-02, 1.2217e+00, 1.1796e-02, 7.5553e-02, 2.7493e-01,
        4.3387e-02, 1.5148e-01, 2.5354e-01, 8.5270e-01, 1.3565e+00, 8.2270e+00,
        5.0494e+00, 5.5681e+00, 8.4028e+00, 6.2710e+00, 1.2078e-01, 1.6556e-02,
        1.0736e-01, 7.2779e-02, 7.5978e-03, 5.9731e-02, 1.1278e-02, 6.1097e-02,
        9.9616e-03, 1.4938e-01, 4.7832e+00, 2.0350e-01, 2.2666e+00, 3.6597e-02,
        7.6954e+00, 2.3879e-01, 1.6787e+00, 1.8260e-01, 1.1556e-01, 8.1433e-02,
        1.0612e-01, 3.3786e-01, 4.5997e+00, 6.2978e-01, 1.6383e-03, 9.8715e-02,
        1.3837e+00, 1.3091e+00, 1.1558e+00, 2.5644e+00, 3.8666e-01, 2.0694e+00,
        2.4257e+00, 5.6471e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [19/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 84.0
Sum_Val Meta Model:  tensor([4.0341e+00, 1.4186e+00, 8.5971e+00, 9.1461e-01, 7.5292e-03, 2.8408e-01,
        2.1384e-02, 2.4046e-01, 1.7330e-01, 1.4705e-01, 3.0115e-02, 8.2640e-02,
        3.0985e-02, 3.1123e-01, 2.0617e-01, 1.8108e-01, 1.8589e+01, 5.9457e-02,
        2.9576e-02, 2.8860e-02, 1.5041e-03, 1.8682e-03, 4.7334e-03, 7.3964e-03,
        4.4150e-01, 1.4881e-01, 1.4734e+00, 4.0753e-02, 6.5225e-02, 1.2627e-01,
        1.9993e-02, 6.8493e-03, 4.5351e-01, 9.0034e-04, 9.3730e-03, 3.0546e-02,
        2.7677e-02, 8.3762e-02, 4.9698e-02, 3.1059e+00, 2.1606e+00, 1.2281e+01,
        4.2478e+00, 1.0276e+01, 1.7593e+01, 1.3770e+01, 1.1748e-02, 5.5123e-02,
        1.0420e-02, 6.3256e-02, 1.2476e-03, 7.3997e-03, 6.5159e-02, 1.0971e-02,
        7.4120e-03, 4.2457e-02, 8.3840e+00, 2.7048e-01, 2.2467e+00, 2.3661e-02,
        6.8848e+00, 3.6569e-01, 1.1506e+00, 2.4045e-01, 3.0977e-02, 2.8466e-02,
        4.0507e-02, 2.4469e-01, 3.8576e+00, 2.1248e+00, 1.9350e-03, 4.0647e-01,
        5.9692e+00, 1.5435e+00, 1.6887e+01, 7.9171e+00, 2.9509e-01, 8.5810e+00,
        4.6165e-02, 9.9493e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([4.0471e+00, 8.7911e-01, 5.5963e+00, 5.4868e-01, 2.3872e-03, 2.8193e-01,
        1.8099e-02, 2.3961e-01, 1.7635e-01, 1.3851e-01, 3.6009e-02, 8.9483e-02,
        3.9178e-02, 2.9689e-01, 2.1155e-01, 5.3067e-01, 1.2392e+01, 1.4685e-01,
        1.7600e-02, 4.8381e-02, 1.0817e-03, 1.6638e-03, 2.4297e-03, 2.5335e-02,
        4.2913e-01, 1.2713e-01, 1.3302e+00, 5.9847e-02, 9.1564e-02, 1.3355e-01,
        1.3655e-02, 6.5094e-03, 4.3474e-01, 1.0262e-03, 9.8213e-03, 2.8529e-02,
        6.0160e-02, 7.5174e-02, 2.6330e-02, 2.9018e+00, 2.1693e+00, 1.6707e+01,
        4.3339e+00, 8.4520e+00, 1.6859e+01, 1.2291e+01, 1.0683e-02, 6.0213e-02,
        2.7199e-03, 4.9797e-02, 1.7991e-04, 2.4577e-03, 7.9457e-02, 1.7047e-03,
        3.3493e-03, 1.3779e-02, 7.8473e+00, 2.5876e-01, 1.8663e+00, 2.3443e-02,
        1.3379e+01, 9.1496e-02, 9.3657e-01, 2.6767e-01, 1.2758e-02, 3.4917e-02,
        1.9022e-02, 2.8226e-01, 4.4485e+00, 2.0519e+00, 1.8810e-03, 4.0643e-01,
        5.3501e+00, 1.5396e+00, 1.9271e+01, 3.5587e+00, 8.1332e-02, 1.0767e+01,
        2.8800e-02, 9.1486e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.8330e+01, 4.2143e+01, 4.9796e+01, 2.3852e+01, 2.8335e-01, 1.3530e+01,
        3.4624e+00, 1.7411e+01, 6.2815e+00, 9.9800e+00, 7.5509e+00, 1.1096e+01,
        1.0071e+01, 1.3710e+01, 8.0618e+00, 2.1099e+01, 3.5952e+02, 7.0685e+00,
        6.8287e-01, 1.7782e+00, 7.2031e-01, 3.1278e-01, 2.8341e-01, 3.8807e+00,
        2.7044e+01, 1.5791e+01, 2.5585e+01, 7.5773e+00, 1.5027e+01, 1.3493e+01,
        1.5133e+00, 1.0963e+00, 6.9811e+00, 6.8708e-01, 1.0619e+00, 9.6986e-01,
        7.3175e+00, 5.5745e+00, 1.0007e+00, 4.5827e+01, 1.0767e+01, 3.3737e+01,
        7.2351e+00, 1.5422e+01, 1.9187e+01, 1.7109e+01, 1.4300e+00, 5.8953e+00,
        2.0533e-01, 3.6862e+00, 6.2930e-02, 3.2721e-01, 8.1601e+00, 1.1414e-01,
        6.0835e-01, 6.9815e-01, 2.7360e+01, 6.3748e+00, 1.2139e+01, 2.3894e+00,
        1.7549e+01, 3.4109e+00, 4.5365e+00, 5.5534e+00, 2.5272e-01, 1.7034e+00,
        2.5323e-01, 7.8749e+00, 9.6656e+00, 1.5949e+01, 1.5034e-01, 1.9208e+01,
        1.2977e+01, 9.2528e+00, 2.2312e+01, 3.5644e+00, 8.1343e-02, 1.0782e+01,
        4.6749e-02, 8.2617e-01], device='cuda:0')
Outer loop valEpocw Maximum [19/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 169.7
model_train val_loss valEpocw [19/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 165.1
model_train val_loss valEpocw [19/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1156.3
Sum_Val Meta Model:  tensor([7.6911e+00, 5.3110e-01, 8.3473e+00, 3.3068e-01, 7.8130e-03, 1.3799e+00,
        1.5156e+00, 1.4562e+00, 1.3913e+00, 9.8650e-01, 2.1147e-01, 3.9138e+00,
        9.3242e-02, 1.9614e-01, 6.1793e-01, 9.3597e-01, 7.2721e-01, 6.5055e-01,
        4.3590e-01, 2.0501e+00, 7.8798e-04, 1.5587e-03, 2.0460e-02, 6.5067e-02,
        6.2576e-01, 4.0811e-01, 2.4127e+00, 2.9156e-01, 1.9728e-01, 2.0282e-03,
        3.3812e-03, 1.9007e-03, 1.9623e-02, 9.9775e-04, 1.3330e-03, 3.5362e-03,
        6.5392e-02, 4.6208e-03, 4.9134e-03, 1.3764e-01, 4.9750e-02, 1.9872e+00,
        9.4738e-02, 1.9401e-01, 3.7821e-01, 2.9730e+00, 1.4114e-02, 1.1563e-02,
        2.8844e-03, 1.6949e-01, 5.0760e-04, 1.9127e-03, 1.1186e-03, 2.1898e-03,
        2.2699e-03, 1.1165e-01, 3.6312e+00, 3.8741e-01, 1.4345e+00, 8.6379e-02,
        1.5885e+00, 1.7546e-02, 4.7721e-01, 1.6231e-01, 3.0038e-02, 4.0785e-02,
        5.5197e-02, 9.6733e-01, 7.0108e-02, 6.7117e-01, 7.9309e-04, 2.9126e-02,
        1.9400e+00, 1.0434e+00, 3.8052e+00, 4.2154e-01, 1.5363e-01, 8.9356e-01,
        2.8311e-02, 1.6133e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([7.2061e+00, 6.4911e-01, 6.3740e+00, 4.2525e-01, 2.8624e-02, 1.4282e+00,
        8.3062e-01, 1.1270e+00, 1.2756e+00, 7.2720e-01, 1.4556e-01, 1.1177e+00,
        9.5708e-02, 2.4623e-01, 6.7815e-01, 1.2353e-01, 4.9030e-01, 5.7554e-01,
        9.9113e-02, 9.4563e-01, 2.3002e-03, 2.6116e-03, 6.4518e-03, 6.7106e-02,
        6.0505e-01, 3.5503e-01, 2.3550e+00, 2.5437e-01, 1.8675e-01, 1.4624e-02,
        1.3316e-02, 4.0268e-03, 5.7581e-02, 7.4113e-03, 8.3087e-03, 7.6740e-03,
        4.3681e-02, 1.9420e-02, 4.4670e-03, 1.1313e-01, 2.4451e-02, 1.2364e+00,
        1.3331e-02, 7.9510e-02, 8.3569e-03, 1.8222e+00, 7.4675e-03, 5.5684e-03,
        1.9919e-03, 1.4187e-01, 2.2570e-04, 2.5097e-03, 3.7867e-03, 5.7763e-03,
        3.6921e-03, 4.6921e-02, 2.7071e+00, 2.5321e-01, 1.1084e+00, 4.9257e-03,
        4.2405e-01, 9.6605e-03, 2.0912e-01, 4.4533e-02, 1.0159e-02, 5.3020e-03,
        9.7925e-03, 1.0700e+00, 2.5981e-02, 4.4431e-01, 1.8947e-04, 9.7478e-03,
        1.4999e+00, 3.3664e-01, 5.4755e+00, 5.0328e-02, 1.2398e-01, 4.7823e-02,
        2.0831e-03, 2.2879e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.2538e+01, 1.9705e+01, 4.4387e+01, 1.0851e+01, 1.7284e+00, 3.6525e+01,
        5.3567e+01, 4.0099e+01, 2.6417e+01, 2.8945e+01, 1.3888e+01, 5.9998e+01,
        9.8276e+00, 7.9218e+00, 1.5129e+01, 2.6110e+00, 9.1643e+00, 1.4553e+01,
        1.9131e+00, 1.6971e+01, 4.7867e-01, 2.0938e-01, 3.3089e-01, 5.0114e+00,
        2.3846e+01, 1.9315e+01, 3.1565e+01, 1.3693e+01, 1.2311e+01, 7.3219e-01,
        7.0562e-01, 2.9272e-01, 6.3457e-01, 1.6552e+00, 4.5187e-01, 1.8495e-01,
        2.4336e+00, 7.6099e-01, 9.3150e-02, 1.4553e+00, 1.2252e-01, 2.6186e+00,
        2.6311e-02, 1.6870e-01, 1.0301e-02, 2.7942e+00, 4.5418e-01, 2.6207e-01,
        7.3017e-02, 5.0607e+00, 3.0591e-02, 1.8395e-01, 1.9786e-01, 1.9552e-01,
        2.9332e-01, 1.4399e+00, 8.6310e+00, 3.7070e+00, 6.8516e+00, 2.2249e-01,
        6.0084e-01, 2.1322e-01, 9.2436e-01, 6.1505e-01, 1.4822e-01, 1.3600e-01,
        1.0557e-01, 2.0269e+01, 6.3640e-02, 3.0500e+00, 8.3802e-03, 2.7106e-01,
        4.1193e+00, 1.6471e+00, 6.9668e+00, 5.0561e-02, 1.2406e-01, 4.8028e-02,
        3.5774e-03, 1.7671e-01], device='cuda:0')
Outer loop valEpocw Maximum [19/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 61.7
model_train val_loss valEpocw [19/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 46.0
model_train val_loss valEpocw [19/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 665.8
Sum_Val Meta Model:  tensor([6.7290e+00, 7.9351e-02, 1.1920e+00, 6.8698e-02, 1.0465e-02, 6.6920e-02,
        1.0280e-02, 1.7823e-01, 7.1786e-02, 3.5440e-02, 9.4277e-03, 1.4434e-02,
        2.9574e-03, 4.2489e-01, 9.7475e-02, 1.0870e-01, 3.0410e-01, 6.8636e-02,
        4.9597e-02, 7.6998e-02, 1.4388e-03, 1.2889e-02, 1.3325e-01, 1.6339e-02,
        1.3139e-01, 2.3109e-02, 1.0829e+00, 7.3001e-02, 9.4768e-03, 5.2558e-02,
        5.4918e-02, 6.0184e-02, 7.1445e-01, 9.6008e-04, 3.9771e-02, 1.1041e-01,
        2.0169e-01, 2.2405e-01, 1.8317e-02, 1.7253e+00, 6.9589e+00, 3.3402e+01,
        4.3041e+01, 4.2110e+01, 4.1131e+01, 5.2914e+01, 1.4347e-01, 1.4961e-01,
        1.0162e+00, 3.0137e-01, 1.6577e-01, 3.5857e-01, 8.0842e-01, 3.0915e-01,
        5.0468e-01, 2.5619e+00, 4.9096e+01, 6.4733e-01, 1.3423e+00, 4.2009e-02,
        6.6748e+01, 6.1999e-02, 2.2197e+00, 6.9369e-01, 7.5532e-01, 3.9403e-02,
        7.2879e-01, 3.8585e-01, 2.0674e+00, 4.0804e-01, 6.5185e-03, 1.7408e-01,
        1.8784e+00, 6.1244e+00, 1.2135e+00, 2.9220e+01, 9.3344e+00, 1.5004e+00,
        2.1402e-01, 9.1125e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.9404e+00, 1.8573e-02, 4.8137e-01, 1.4590e-02, 1.0666e-03, 6.0880e-03,
        7.2818e-04, 1.7458e-01, 5.8930e-03, 2.4017e-03, 6.5293e-04, 6.7209e-04,
        1.4610e-04, 3.3386e-01, 1.1176e-02, 2.4603e-02, 4.0345e-02, 2.0113e-02,
        4.6116e-03, 2.0823e-03, 7.6989e-05, 1.4845e-04, 6.9387e-04, 7.2141e-04,
        7.1456e-02, 7.8744e-03, 1.1883e+00, 6.3952e-02, 5.5913e-03, 2.2066e-03,
        7.3528e-03, 5.4777e-02, 5.8445e-01, 8.9745e-05, 1.1149e-02, 2.8126e-02,
        1.0461e-01, 1.8234e-01, 3.4302e-03, 2.1875e+00, 4.6736e+00, 3.0505e+01,
        3.9172e+01, 4.2932e+01, 5.8288e+01, 4.8584e+01, 8.6662e-02, 8.3186e-02,
        6.4566e-01, 1.6117e-01, 7.7330e-02, 3.5912e-01, 7.1722e-01, 2.4116e-01,
        3.3491e-01, 1.5898e+00, 2.3978e+01, 6.7107e-01, 1.3295e+00, 1.1092e-02,
        1.0331e+02, 7.5822e-03, 1.5293e+00, 5.5032e-01, 6.0132e-01, 6.6518e-02,
        6.4506e-01, 4.4721e-01, 2.1588e+00, 3.0507e-01, 9.7600e-04, 1.5718e-01,
        1.6831e+00, 6.3132e+00, 1.0200e+00, 2.5528e+01, 1.2020e+01, 1.2699e+00,
        3.2454e-02, 1.0205e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.3592e+01, 9.8530e-01, 4.5585e+00, 6.6803e-01, 1.3356e-01, 3.0367e-01,
        1.4332e-01, 1.3882e+01, 2.2272e-01, 1.8344e-01, 1.4097e-01, 8.4669e-02,
        4.1198e-02, 1.6388e+01, 4.5290e-01, 9.7498e-01, 1.1472e+00, 9.3648e-01,
        1.7609e-01, 7.5896e-02, 6.1423e-02, 2.7433e-02, 6.2448e-02, 1.0940e-01,
        4.8737e+00, 1.0961e+00, 2.7529e+01, 9.1390e+00, 1.1009e+00, 2.0805e-01,
        7.7594e-01, 7.8725e+00, 9.0129e+00, 5.4723e-02, 1.0994e+00, 9.5811e-01,
        1.1109e+01, 1.2296e+01, 1.2973e-01, 3.9080e+01, 2.5052e+01, 6.0340e+01,
        6.5484e+01, 7.6751e+01, 6.6831e+01, 6.7772e+01, 1.0256e+01, 7.0946e+00,
        3.7402e+01, 1.0058e+01, 2.1752e+01, 4.2860e+01, 6.2626e+01, 1.4843e+01,
        5.6170e+01, 7.2600e+01, 8.8475e+01, 1.6452e+01, 9.8984e+00, 1.0127e+00,
        1.3530e+02, 3.0529e-01, 7.8835e+00, 1.2238e+01, 1.3220e+01, 3.1636e+00,
        9.7628e+00, 1.4103e+01, 5.4337e+00, 2.8177e+00, 8.8285e-02, 8.0482e+00,
        4.9909e+00, 3.9327e+01, 1.2249e+00, 2.5570e+01, 1.2022e+01, 1.2717e+00,
        5.4283e-02, 1.0070e+00], device='cuda:0')
Outer loop valEpocw Maximum [19/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 415.2
model_train val_loss valEpocw [19/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 419.8
model_train val_loss valEpocw [19/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1303.2
Sum_Val Meta Model:  tensor([1.2625e+01, 5.5889e-02, 2.9240e+00, 2.8225e-02, 4.7139e-03, 3.1340e-01,
        5.2775e-02, 3.4842e-01, 5.5460e-02, 4.1165e-01, 5.2075e-02, 7.9974e-02,
        1.0392e-03, 5.3502e-01, 5.7121e-01, 5.7820e-02, 7.6873e-02, 2.1142e-02,
        1.2813e-02, 2.2084e-02, 7.6248e-04, 3.2008e-03, 5.9629e-03, 6.1077e-03,
        5.2774e-01, 1.7289e-02, 2.0214e+00, 1.7770e-02, 1.6602e-01, 6.6570e-03,
        1.0425e-02, 2.1053e-03, 2.3658e-01, 8.6987e-03, 2.3240e-02, 2.3454e-01,
        4.1710e-03, 1.2144e-02, 1.7614e-01, 2.3302e+00, 3.9215e+00, 1.7766e+01,
        6.5664e+00, 6.2784e+00, 1.0701e+01, 1.2980e+01, 5.3166e-03, 1.0241e-02,
        2.6244e-02, 8.7244e-03, 3.8947e-03, 6.3447e-03, 4.7847e-03, 1.5294e-01,
        5.6408e-03, 5.6616e-02, 7.2414e+00, 2.5149e-01, 2.3883e+00, 2.7604e-02,
        3.7600e+01, 1.8374e-02, 1.1032e+00, 6.7037e-01, 4.5142e-01, 7.6748e-02,
        6.9170e-01, 3.0128e+00, 6.4634e-01, 3.9399e-01, 1.7479e-03, 5.1029e-02,
        2.3378e+00, 2.0728e+00, 3.8234e+02, 1.2345e+01, 1.9422e+00, 7.5794e+00,
        3.8920e-02, 1.4230e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([3.9990e+00, 9.2834e-02, 2.4800e+00, 6.5877e-02, 1.0542e-02, 2.8989e-01,
        5.6787e-02, 2.9690e-01, 1.1145e-01, 3.7342e-01, 3.2183e-02, 1.3072e-01,
        3.7199e-03, 4.8230e-01, 6.2352e-01, 2.4879e-02, 2.3712e-02, 2.1131e-02,
        1.9622e-03, 2.9184e-03, 6.4159e-04, 2.8133e-04, 1.7117e-03, 9.0718e-03,
        4.6491e-01, 1.8577e-02, 1.7762e+00, 1.9397e-02, 1.7321e-01, 1.7005e-03,
        6.4372e-03, 3.5150e-03, 1.6710e-02, 4.0401e-04, 2.3369e-03, 4.6258e-03,
        5.8994e-03, 2.4417e-03, 7.0809e-03, 1.3765e-01, 3.2980e-01, 5.6155e-01,
        4.7745e-02, 1.2104e-01, 1.7099e-02, 3.1288e-01, 2.2207e-03, 2.4297e-03,
        7.7390e-04, 2.5779e-03, 8.6342e-05, 3.8748e-04, 7.1749e-04, 1.5651e-03,
        2.4745e-03, 1.4837e-02, 1.3658e+00, 8.5397e-02, 1.7201e+00, 6.6542e-03,
        8.2465e+00, 5.2791e-03, 1.6861e-01, 4.1463e-02, 1.6518e-02, 1.7932e-02,
        3.0140e-02, 3.8720e-01, 5.7325e-02, 4.1750e-02, 2.9715e-04, 8.3660e-03,
        2.1348e-01, 1.0875e+00, 5.6845e+01, 4.8935e+00, 1.2668e-02, 6.7536e+00,
        4.1539e-03, 6.1285e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.5833e+01, 3.3363e+00, 2.0144e+01, 2.0909e+00, 8.0868e-01, 9.9195e+00,
        6.3393e+00, 1.5101e+01, 2.8976e+00, 1.8211e+01, 3.9784e+00, 9.7323e+00,
        5.8836e-01, 1.6045e+01, 1.7807e+01, 6.9840e-01, 5.4995e-01, 6.9251e-01,
        5.4891e-02, 8.0877e-02, 2.3559e-01, 3.1776e-02, 1.2092e-01, 8.6338e-01,
        2.1401e+01, 1.5001e+00, 2.6867e+01, 1.6193e+00, 1.8798e+01, 1.0982e-01,
        4.6726e-01, 3.6431e-01, 1.9893e-01, 1.3241e-01, 1.5147e-01, 1.0044e-01,
        4.6234e-01, 1.2743e-01, 1.7215e-01, 1.7253e+00, 1.3911e+00, 1.0847e+00,
        8.2988e-02, 2.2921e-01, 2.0072e-02, 4.5323e-01, 1.9762e-01, 1.5696e-01,
        3.7814e-02, 1.3068e-01, 1.8086e-02, 3.6136e-02, 4.6732e-02, 5.9954e-02,
        2.6805e-01, 5.4046e-01, 4.4702e+00, 1.5268e+00, 1.0320e+01, 4.1550e-01,
        1.1022e+01, 1.3971e-01, 7.3359e-01, 5.9247e-01, 2.2794e-01, 5.5524e-01,
        2.9605e-01, 6.9120e+00, 1.3546e-01, 3.0053e-01, 1.6749e-02, 2.7470e-01,
        5.5193e-01, 5.5473e+00, 7.0706e+01, 4.9084e+00, 1.2673e-02, 6.7733e+00,
        7.6850e-03, 4.4791e-02], device='cuda:0')
Outer loop valEpocw Maximum [19/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 546.0
model_train val_loss valEpocw [19/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 95.2
model_train val_loss valEpocw [19/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 371.6
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.81691256 97.4366784  93.43575249 98.10918483 98.58798017 97.41109392
 98.35771981 95.26687053 98.07994542 96.99565064 98.63671252 98.94250801
 99.43835967 95.39113802 97.50490369 97.2831715  96.38649627 97.74125559
 98.84991655 98.41254371 98.69519134 99.30556402 99.53582437 98.99245867
 95.15965936 96.78488323 94.43354735 97.03341821 98.05314263 98.27731144
 98.39305077 98.58188862 97.06874916 98.52828304 98.70615611 98.88768412
 97.65231905 98.31751562 98.96809249 93.36996382 98.09578343 94.52248389
 97.73760066 97.06753085 97.69008662 96.51198207 98.18715659 98.67204347
 98.14207917 98.76098001 98.79874758 98.65498715 99.01682484 98.34675503
 98.74270538 97.65353736 93.02396413 96.95179152 96.65330588 97.64866412
 95.8065813  98.67448009 97.55241773 97.43302348 98.84138838 97.58531207
 98.65255053 95.9588699  98.91083198 98.31142408 99.81603538 97.53657972
 98.90108551 96.05755291 98.42838172 98.86209963 99.50780327 99.00342345
 99.84405648 99.18373314]
Accuracy th:0.7 is [86.35372376 97.34774186 93.00934443 98.05923417 98.44787466 97.63404442
 98.23588894 95.01346231 97.95811455 96.95910138 98.59041678 98.91936014
 99.42130335 95.32534935 97.43058686 97.08458718 96.30974282 97.67668523
 98.7743814  98.34431842 98.55630414 99.25683167 99.53947929 99.01926146
 95.25712406 96.697165   94.27760383 97.00174218 98.03852292 98.19446644
 98.37233952 98.58919847 96.88356623 98.3552832  98.62331112 98.74514199
 97.43789671 98.10674821 98.86209963 93.10680913 98.01902998 93.68672409
 97.47566428 96.78731984 97.38794605 96.06729938 98.21152276 98.65255053
 98.05070601 98.74757861 98.69397303 98.60747311 99.01316992 98.14086086
 98.71590258 97.59140361 92.74253481 96.84579866 96.56193272 97.46226289
 94.89772298 98.50269855 97.33677709 97.35627003 98.77803633 97.4634812
 98.60016325 95.95034174 98.77681802 98.16766365 99.81603538 97.24418562
 98.90474044 95.75784895 98.28340298 98.82676868 99.46272584 98.85966302
 99.84405648 99.16789513]
Avg Prec: is [96.50372744 35.24259733 72.94342739 67.32001992 78.59894857 64.33888004
 74.49840747 47.4438928  58.79987575 52.48507854 31.12152531 53.07509455
 26.03330072 28.17566942 32.33752653 55.97214113 28.49137197 41.62956812
 47.58056315 39.11036416 61.75507081 47.80841277 89.7850638  81.95538568
 26.25449684 31.82990034 38.43942186 38.63819417 23.84605686 38.31184223
 73.62989104 35.89236877 58.01514362 63.09406402 72.59494841 79.41360099
 58.47485001 75.45021878 87.04943736 46.69217836 43.47737332 72.20895795
 68.00636859 61.5568345  71.18947261 78.90919727 37.83571138 33.69061061
 40.22118885 45.03590082 61.56706647 37.46942299 21.17312897 73.50689208
 26.66771133 41.34191375 72.67463813 58.12520145 43.4628403  59.25232841
 89.08875031 83.32268258 71.72890203 49.36895682 59.95610261 44.41234767
 60.0230296  24.60373508 56.46843281 66.77283232 11.47643487 73.60348223
 78.3328597  51.02118235 83.17699667 88.62198995 70.8627853  82.67609145
 11.27770866 28.09804488]
Accuracy th:0.5 is [45.50504989 97.2137279  71.80711736 97.02489005 97.26733349 76.7035002
 77.08483084 76.20521192 77.98394269 96.45837648 78.32628745 98.52097319
 99.41399349 80.14644071 77.902316   96.56680596 96.29512311 77.61601345
 98.65376884 98.30776915 80.49244039 78.82213911 98.38695922 78.27024525
 80.86158794 96.65086926 94.0778012  77.36869678 98.01293844 78.15328761
 97.30875598 98.57457877 96.36213009 98.02024829 88.0179335  77.88160476
 77.64159793 91.36219101 97.11504489 74.95766377 79.42763855 92.05906361
 77.15914767 76.70837344 96.9627563  93.87434364 98.02877645 98.57336046
 97.44033333 88.91217212 87.72919433 98.55508583 98.99976852 77.27732362
 98.70615611 77.6793655  72.37728585 93.50885101 96.24273583 96.9067141
 89.79300934 97.17717864 93.26884419 77.6513444  98.42838172 78.12770312
 98.20786784 76.75345086 78.70883639 97.55972759 79.24732886 95.99054592
 78.47979435 95.45083515 77.02026047 83.00215641 87.90950403 78.27146355
 79.3362654  99.14718388]
Accuracy th:0.7 is [45.61347937 97.2137279  71.80711736 97.02489005 97.26733349 76.7035002
 77.08483084 76.41841595 77.98394269 96.4742145  78.32628745 98.52097319
 99.41399349 80.62645436 77.902316   96.56680596 96.29512311 77.61601345
 98.65376884 98.30776915 80.88108088 78.82213911 98.38695922 78.71249132
 81.50850989 96.65086926 94.0778012  77.36869678 98.01293844 78.15328761
 97.30875598 98.57457877 96.36213009 98.02024829 88.20920798 77.88160476
 77.64159793 91.60707106 97.11504489 74.95766377 80.10136329 92.05906361
 77.15914767 76.70837344 96.9627563  93.87434364 98.02877645 98.57336046
 97.83506536 89.43238996 87.90706741 98.55508583 98.99976852 77.27732362
 98.70615611 77.6793655  72.37728585 93.96328017 96.24273583 96.9067141
 89.79300934 97.17717864 93.410168   77.6513444  98.42838172 78.12770312
 98.20786784 76.75345086 78.70883639 97.55972759 79.24854717 95.99054592
 78.55411118 95.45083515 77.02026047 83.12155066 88.13245453 78.27146355
 79.3362654  99.14718388]
Avg Prec: is [55.93121874  3.11955999 11.247998    3.36897686  2.22449964  3.69354645
  3.29818497  5.54175034  2.56278995  3.85763336  1.5618682   1.60679067
  0.75802794  5.22408036  2.7547813   3.16203596  3.76760904  2.66575098
  1.38799095  1.73926227  2.06672396  0.87736491  1.78666986  2.39582932
  5.04062653  3.55948487  6.44710756  3.39458452  2.09919703  1.98953616
  2.60192552  1.36359251  3.66812019  1.65703902  2.27979101  2.32688946
  3.10929491  2.56911763  2.80926239  7.57627242  2.36000589  8.22452223
  3.36409328  4.22842209  3.22435264  6.44708188  2.07546756  1.55640102
  2.09672612  1.55465888  1.82018558  1.53377918  0.97778136  2.94229512
  1.34253668  2.67304684 11.25731292  3.66854026  3.97423608  2.76713909
 10.81950849  2.25116825  3.73996838  2.98902554  1.52981072  2.45621274
  1.76567047  4.19367205  1.26302396  2.41735362  0.1936427   3.45334771
  1.93338627  4.51537223  3.94449149  2.98038553  0.81102528  1.83284573
  0.1295473   0.72307988]
mAP score regular 54.91, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [87.88150584 97.40638314 93.26307397 98.23853302 98.90375464 97.3341306
 98.5101029  95.40822682 98.19617809 97.00027406 98.6521165  99.04327678
 99.37962479 95.20143508 97.45621247 97.28679273 96.38488178 97.76017141
 98.98348158 98.41542716 98.88631437 99.34225278 99.64372026 99.24259412
 95.12669108 96.70877245 94.56860254 97.17467673 97.92709968 98.23604156
 98.73931784 98.62720183 96.98034233 98.64215063 98.94361811 99.19774771
 98.04918155 98.19617809 99.12051225 93.31788624 97.98689489 93.44744251
 97.37648554 96.64648579 97.07501806 94.96225428 98.29085383 98.79163864
 98.13389142 98.7418093  98.85641677 98.6521165  98.89877171 98.4353589
 98.76672397 97.72030795 90.68440591 97.09993273 96.46710018 97.60071754
 93.19082144 98.83399357 97.40389167 97.51849914 98.77419837 97.60570048
 98.60477863 95.83925057 98.8016045  98.30580263 99.81563146 97.68044448
 98.35064903 95.9339263  97.31669034 97.26436953 99.24259412 98.52006876
 99.82559733 99.16785011]
Accuracy th:0.7 is [87.80925331 97.3864514  93.27553131 98.26843063 98.83648504 97.70037621
 98.4054613  95.31105962 98.07907915 97.0575778  98.64215063 99.02832798
 99.36965892 95.12419962 97.43877221 97.10740713 96.26529138 97.71781648
 98.92368637 98.38054663 98.76921544 99.31235518 99.63126292 99.28245758
 95.47549642 96.63651992 94.49635    97.2145402  97.86232155 98.17873782
 98.6894885  98.68450557 96.92054713 98.54498343 98.87634851 99.07566584
 97.79256048 97.98689489 99.01088771 93.11358597 98.00931809 93.22071904
 97.32914767 96.65645165 97.080001   94.98218601 98.35064903 98.82402771
 98.02924982 98.77419837 98.75426664 98.61972743 98.89129731 98.23355009
 98.73931784 97.77761168 91.26491766 97.15723647 96.42723671 97.49856741
 92.87689663 98.67204823 97.17467673 97.49607594 98.77419837 97.57829434
 98.58235543 95.81931883 98.76921544 98.12143409 99.81563146 97.49607594
 98.49764556 95.7470663  97.41884047 97.50604181 99.26252585 98.50013703
 99.82559733 99.17034158]
Avg Prec: is [96.51821886 34.32039219 71.6740994  73.00908967 76.5748624  65.03479372
 80.24519444 48.40366077 63.05622211 56.15223886 37.99270591 57.25810526
 23.14115065 31.4369149  34.55455589 61.21493685 31.88738889 44.03279067
 49.27112285 37.29914166 69.29750407 56.66074013 92.1988532  87.9644104
 24.80289173 36.46541266 33.95032456 44.79550216 27.6378364  36.80487585
 77.30863276 37.82121165 55.62080048 65.78191348 74.5813346  83.33764723
 61.41810316 77.83462186 90.12190572 44.52595304 38.81506707 52.53350473
 49.15154778 42.23405482 33.23054726 54.33469519 35.99651982 29.49294631
 42.03895258 42.69493553 66.61629731 34.09968153 24.23428242 76.47921021
 30.0935068  41.4492109  58.12790618 57.89590774 38.83581024 61.82395188
 66.71255289 86.84103129 67.24918849 52.70224855 61.10279899 38.66214995
 61.52707804 26.32328698 39.83200384 66.0162762   9.16788497 75.51566009
 58.30530138 44.46386821 64.21751545 52.37393047 13.97608917 43.87191602
  2.02235529 22.55746181]
Accuracy th:0.5 is [45.26247602 97.22450607 70.24939582 96.96290206 97.90716795 75.72065675
 75.80785809 74.75645913 77.30024666 96.41976231 77.45471759 98.5325261
 99.34972718 78.14236241 77.37748212 96.31262924 96.21047911 76.79447891
 98.78167277 98.34068316 79.0343075  78.08505867 98.31327703 78.37905175
 78.12990508 96.52938685 94.3393876  76.85676558 97.81747515 77.40737972
 97.52597354 98.67204823 96.39983058 98.18870369 89.05000374 77.02369385
 76.99628771 92.44836435 97.0276802  74.17096445 77.68891546 92.37362035
 76.15666343 75.83028129 97.03764606 94.02795426 98.18621222 98.77668984
 97.79505195 88.62396293 86.296933   98.55993223 98.87385704 76.14171463
 98.6969629  76.82935944 70.89717717 94.38672547 96.16314124 96.78102499
 90.13379176 97.04761193 93.40508758 76.83683384 98.32075143 77.69638986
 98.13139996 76.02710716 78.11744774 97.53593941 78.54099708 96.07843137
 77.88324987 95.44559882 75.93492289 83.87024441 89.86222189 77.47963226
 78.61075815 99.15040985]
Accuracy th:0.7 is [45.47425069 97.22450607 70.24939582 96.96290206 97.90716795 75.72065675
 75.80785809 74.91093006 77.30024666 96.41976231 77.45471759 98.5325261
 99.34972718 78.52853975 77.37748212 96.31262924 96.21047911 76.79447891
 98.78167277 98.34068316 79.33577497 78.08505867 98.31327703 78.64314722
 78.53103122 96.52938685 94.3393876  76.85676558 97.81747515 77.40737972
 97.52597354 98.67204823 96.39983058 98.18870369 89.22938934 77.02369385
 76.99628771 92.69003662 97.0276802  74.17096445 78.14983681 92.37362035
 76.15666343 75.83028129 97.03764606 94.02795426 98.18621222 98.77668984
 97.92211675 88.9478536  86.45887834 98.55993223 98.87385704 76.14171463
 98.6969629  76.82935944 70.89717717 94.70812467 96.16314124 96.78102499
 90.13379176 97.04761193 93.54211825 76.83683384 98.32075143 77.69638986
 98.13139996 76.02710716 78.11744774 97.53593941 78.54099708 96.07843137
 77.90567307 95.44559882 75.93492289 83.97239455 90.05904776 77.47963226
 78.61075815 99.15040985]
Avg Prec: is [54.28507146  3.71995966 14.84107258  4.5475461   1.48492491  4.26516603
 12.85857344  8.63329154  7.81548457  5.21289753  2.29569318  4.94964306
  1.56044161  5.8702346   3.04535488  3.68921859 24.43287536  6.44608347
  1.57130751  2.7159088   3.56158133  1.46963137  1.15567353  5.52272164
  5.69678924 10.04762248  7.95745677  4.54723184  3.94603192  6.09815185
  2.2529885   0.85852154  3.08378164  1.10911555  1.68048666  2.34828764
  2.00283356  2.20890216  2.24522897  6.20239446  1.72771913  5.99470527
  2.20087088  2.726394    2.37254098  4.857875    1.7265432   1.03884709
  1.37752628  1.16187486  1.19271807  0.98691941  0.73546978  2.34766002
  0.8460811   1.82849269 10.04413757  2.89261945  3.86213391  2.75718876
  7.84276669  2.03447843  3.18848779  2.5011155   1.35221979  1.82166892
  1.52876361  3.43162497  1.08094904  2.23498884  0.19219882  3.1920071
  1.5760118   3.91475117  3.53821002  2.31430187  0.6010047   1.49991533
  0.12967244  0.59206643]
mAP score regular 51.45, mAP score EMA 4.32
Train_data_mAP: current_mAP = 54.91, highest_mAP = 54.91
Val_data_mAP: current_mAP = 51.45, highest_mAP = 51.49
tensor([0.0765, 0.0171, 0.0985, 0.0194, 0.0069, 0.0175, 0.0043, 0.0114, 0.0248,
        0.0117, 0.0040, 0.0069, 0.0031, 0.0190, 0.0227, 0.0236, 0.0295, 0.0181,
        0.0229, 0.0240, 0.0011, 0.0045, 0.0077, 0.0056, 0.0135, 0.0064, 0.0465,
        0.0062, 0.0045, 0.0084, 0.0076, 0.0048, 0.0557, 0.0012, 0.0080, 0.0268,
        0.0067, 0.0110, 0.0233, 0.0559, 0.1929, 0.5067, 0.6183, 0.5608, 0.8927,
        0.7350, 0.0061, 0.0087, 0.0117, 0.0115, 0.0023, 0.0061, 0.0087, 0.0136,
        0.0047, 0.0173, 0.2666, 0.0360, 0.1443, 0.0088, 0.7651, 0.0230, 0.1972,
        0.0427, 0.0466, 0.0183, 0.0704, 0.0323, 0.4885, 0.1226, 0.0098, 0.0186,
        0.3960, 0.1601, 0.8709, 0.9988, 0.9999, 0.9990, 0.6887, 0.1089],
       device='cuda:0')
Sum Train Loss:  tensor([3.6209e+00, 5.4259e-02, 1.9271e+00, 8.2809e-02, 1.6409e-02, 2.2584e-01,
        2.4875e-02, 1.7179e-01, 1.0920e-01, 1.2128e-01, 1.6937e-02, 4.6610e-02,
        8.5569e-03, 2.4359e-01, 3.2722e-01, 2.2554e-01, 6.7393e-01, 7.9112e-02,
        1.9689e-02, 1.1285e-01, 1.9801e-03, 1.7438e-02, 5.6380e-03, 8.3866e-03,
        1.5971e-01, 6.6949e-02, 6.9181e-01, 8.2353e-02, 2.6668e-02, 2.7753e-02,
        5.0703e-02, 1.6531e-02, 3.7732e-01, 7.3874e-03, 9.2151e-02, 6.8766e-02,
        1.6716e-02, 2.7145e-02, 2.5751e-02, 1.8783e+00, 1.5930e+00, 8.5103e+00,
        4.8167e+00, 3.7268e+00, 3.7179e+00, 1.6296e+01, 5.3533e-02, 7.6635e-02,
        8.0653e-02, 8.2909e-02, 1.3388e-02, 3.4341e-02, 1.6776e-02, 1.9060e-01,
        7.9594e-02, 1.6666e-01, 8.1054e+00, 4.1557e-01, 2.5725e+00, 9.4729e-02,
        9.4318e+00, 1.9551e-02, 1.7332e+00, 3.7921e-01, 5.9685e-01, 2.0845e-01,
        5.3881e-01, 4.5962e-01, 1.9076e+00, 1.0429e+00, 1.7695e-03, 1.3317e-01,
        8.8750e-01, 2.0905e+00, 5.5135e+00, 5.1190e+00, 1.3769e+00, 1.1110e+00,
        8.9501e-02, 5.7357e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [20/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 95.6
Sum Train Loss:  tensor([2.3308e+00, 2.4833e-01, 2.4025e+00, 4.8235e-02, 6.3926e-02, 1.1329e-01,
        1.1149e-02, 2.6418e-01, 1.9287e-01, 1.1950e-01, 5.4598e-02, 1.2013e-01,
        1.0609e-02, 4.1948e-01, 2.2984e-01, 4.9881e-01, 5.8572e-01, 1.1198e-01,
        6.9660e-02, 1.5274e-01, 2.1777e-03, 3.6786e-02, 7.7139e-03, 1.2268e-02,
        1.5343e-01, 6.3640e-02, 7.1693e-01, 1.9545e-02, 2.6117e-02, 4.8181e-02,
        9.2108e-03, 5.2095e-03, 8.3221e-01, 4.5265e-03, 2.3527e-02, 7.8809e-02,
        4.5799e-02, 7.4601e-02, 9.7442e-02, 1.5989e+00, 1.7513e+00, 9.7335e+00,
        6.4089e+00, 6.5698e+00, 6.8367e+00, 1.0340e+01, 6.9908e-02, 7.9811e-02,
        9.1341e-02, 3.7104e-02, 2.0444e-02, 5.5635e-02, 1.2651e-02, 3.4673e-02,
        1.9879e-02, 1.4695e-01, 7.4554e+00, 3.2118e-01, 8.4171e-01, 4.6084e-02,
        6.8472e+00, 7.0678e-02, 1.3820e+00, 5.1486e-01, 1.5390e-01, 1.9911e-01,
        2.6347e-01, 4.7120e-01, 1.0100e+00, 2.7436e-01, 3.4733e-02, 6.0716e-02,
        1.2973e+00, 3.1606e+00, 4.2841e+00, 3.2012e+00, 2.1660e+00, 9.2262e+00,
        3.3395e-01, 4.7236e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [20/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 98.2
Sum Train Loss:  tensor([3.5107e+00, 2.3808e-01, 3.1217e+00, 1.4158e-01, 1.5587e-02, 1.0557e-01,
        4.1657e-02, 2.2345e-01, 8.8410e-02, 1.5228e-01, 3.1184e-02, 6.7054e-02,
        2.6703e-02, 3.6775e-01, 2.7784e-01, 2.4591e-01, 3.3673e-01, 8.1446e-02,
        1.0088e-01, 1.5548e-01, 5.2640e-03, 2.9741e-02, 4.7322e-03, 2.3142e-02,
        2.1740e-01, 5.4831e-02, 1.1968e+00, 1.0142e-01, 8.3021e-02, 2.1463e-02,
        2.5134e-02, 5.5371e-02, 3.9821e-01, 1.0116e-02, 6.5803e-02, 8.9569e-02,
        4.9042e-02, 9.4676e-02, 1.0191e-01, 1.0612e+00, 2.8609e+00, 6.7217e+00,
        3.9472e+00, 6.4677e+00, 3.3585e+00, 5.1079e+00, 8.5185e-02, 9.1206e-02,
        4.4883e-02, 5.8745e-02, 7.5791e-03, 3.5208e-02, 9.7717e-02, 4.2561e-02,
        1.4875e-02, 1.6416e-01, 5.3781e+00, 3.3797e-01, 1.2299e+00, 2.2663e-02,
        6.2822e+00, 7.3939e-02, 1.5785e+00, 1.4930e-01, 2.5375e-01, 1.3855e-01,
        2.1990e-01, 5.8301e-01, 4.7833e+00, 5.3655e-01, 7.9702e-04, 6.4208e-02,
        1.1797e+00, 2.0999e+00, 1.0204e+01, 1.3061e+00, 8.0366e-01, 3.5036e-01,
        8.5341e-02, 6.2450e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [20/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 79.8
Sum Train Loss:  tensor([3.3426e+00, 2.4356e-01, 3.4193e+00, 1.2485e-01, 1.5519e-02, 8.7778e-02,
        4.5171e-02, 2.0046e-01, 4.2322e-01, 2.4578e-01, 2.5243e-02, 3.2391e-02,
        4.4089e-03, 2.6661e-01, 1.7425e-01, 2.7800e-01, 5.1702e-01, 1.8093e-01,
        3.7474e-02, 1.4734e-01, 5.1533e-03, 5.8506e-03, 5.4103e-03, 5.2464e-03,
        2.5964e-01, 6.2320e-02, 1.2961e+00, 7.5842e-02, 4.8927e-02, 1.4706e-01,
        7.4289e-02, 1.5832e-02, 5.9621e-01, 5.9459e-03, 1.1573e-02, 6.4725e-02,
        7.1686e-02, 3.7555e-02, 6.1261e-02, 5.7837e-01, 1.4880e+00, 6.1650e+00,
        5.0652e+00, 4.3853e+00, 1.8757e+00, 3.1569e+00, 4.1364e-02, 5.0412e-02,
        8.4691e-02, 7.1026e-02, 3.0589e-03, 3.5098e-02, 8.5008e-03, 5.6628e-02,
        2.8296e-02, 1.4824e-01, 7.9192e+00, 4.8174e-01, 1.2621e+00, 6.8804e-02,
        9.4977e+00, 2.0984e-02, 1.8967e+00, 1.2568e+00, 4.1104e-01, 1.2128e-01,
        3.5394e-01, 8.6059e-01, 2.8111e+00, 2.7950e-01, 2.2031e-03, 7.8428e-02,
        1.4177e+00, 1.7157e+00, 3.4966e+00, 2.4117e+00, 2.5381e-01, 7.1018e+00,
        6.8779e-02, 1.2723e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [20/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 79.8
Sum Train Loss:  tensor([2.4841e+00, 7.5028e-02, 2.4456e+00, 9.6828e-02, 5.1340e-02, 1.8932e-01,
        2.9688e-02, 3.3033e-01, 3.0606e-01, 1.2946e-01, 1.5414e-02, 4.5887e-02,
        1.5037e-03, 3.3488e-01, 2.8523e-01, 1.8062e-01, 6.6217e-01, 2.2106e-01,
        1.7418e-01, 3.6697e-02, 1.1439e-02, 4.0341e-02, 2.8789e-03, 1.5428e-02,
        2.1758e-01, 6.1793e-02, 1.3490e+00, 4.4291e-02, 2.9779e-02, 1.4077e-01,
        1.3756e-01, 4.0686e-02, 2.2675e-01, 4.2059e-03, 2.8408e-02, 1.5680e-02,
        2.4735e-02, 1.2109e-01, 1.4179e-01, 1.1067e+00, 1.9924e+00, 1.0438e+01,
        4.1599e+00, 5.0968e+00, 7.1274e+00, 9.4112e+00, 6.1323e-02, 3.6204e-02,
        1.4080e-01, 2.3986e-02, 1.2162e-02, 7.5054e-02, 1.6758e-02, 7.1087e-02,
        5.1392e-02, 1.0292e-01, 5.6994e+00, 3.6913e-01, 1.9866e+00, 5.6435e-02,
        8.5782e+00, 8.7511e-02, 1.5623e+00, 2.0193e-01, 1.3029e-01, 1.5108e-01,
        7.7292e-02, 6.2465e-01, 2.1997e+00, 6.7318e-01, 1.9454e-03, 6.3693e-02,
        9.0780e-01, 1.0928e+00, 9.7783e+00, 2.0762e+00, 4.5326e-01, 2.9246e+00,
        8.3625e-02, 4.9558e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [20/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 90.9
Sum Train Loss:  tensor([2.6373e+00, 2.2552e-01, 1.8415e+00, 3.7789e-01, 2.3583e-02, 1.6088e-01,
        5.1677e-02, 1.8487e-01, 1.1706e-01, 1.2584e-01, 3.4351e-02, 4.2476e-02,
        1.3626e-02, 4.1289e-01, 1.6087e-01, 2.7310e-01, 1.8277e-01, 2.2646e-01,
        2.5097e-02, 1.7423e-02, 3.1309e-03, 1.9796e-02, 2.9991e-03, 4.2174e-03,
        3.7398e-01, 1.2706e-01, 7.8129e-01, 7.0825e-02, 4.9780e-02, 9.3967e-02,
        3.8856e-02, 8.2938e-03, 2.9688e-01, 4.6065e-03, 1.2675e-02, 1.0616e-01,
        2.0484e-02, 1.0374e-01, 5.6927e-02, 1.7121e+00, 5.0625e-01, 1.2091e+01,
        6.5391e+00, 3.7540e+00, 4.8649e+00, 1.6683e+01, 2.9160e-02, 3.1448e-02,
        1.2334e-01, 6.3878e-02, 1.4593e-02, 5.6390e-02, 5.2869e-02, 9.9527e-02,
        1.0936e-02, 1.9595e-01, 8.1495e+00, 3.5043e-01, 3.3141e+00, 6.2502e-02,
        8.6225e+00, 1.3593e-01, 1.0471e+00, 5.7819e-01, 1.4239e-01, 1.6204e-01,
        2.2908e-01, 4.0621e-01, 2.1249e+00, 3.3992e-01, 9.3497e-04, 1.9323e-01,
        7.2611e-01, 1.5413e+00, 4.1879e+00, 6.7115e+00, 7.8199e-01, 1.9973e+00,
        2.7487e-01, 5.0427e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [20/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 98.7
Sum Train Loss:  tensor([3.0922e+00, 1.9357e-01, 2.5178e+00, 1.0181e-01, 4.7691e-02, 1.3223e-01,
        1.3568e-02, 2.0817e-01, 7.8281e-02, 1.7507e-01, 6.0419e-03, 5.5197e-02,
        2.2472e-02, 5.3090e-01, 2.9103e-01, 3.4242e-01, 2.8582e-01, 3.4595e-01,
        1.2724e-01, 1.3512e-01, 1.0246e-02, 6.8449e-02, 2.4078e-02, 1.7317e-02,
        3.3237e-01, 5.2008e-02, 9.9125e-01, 1.6213e-01, 6.5663e-02, 4.1184e-02,
        3.0961e-02, 3.5587e-02, 3.5880e-01, 6.7817e-03, 2.2019e-02, 9.1083e-02,
        4.1727e-02, 2.2055e-02, 1.1314e-01, 1.3899e+00, 1.2255e+00, 9.3907e+00,
        4.8430e+00, 2.7853e+00, 6.1449e+00, 9.2431e+00, 5.4470e-02, 1.1188e-02,
        6.9482e-02, 2.0326e-02, 5.2752e-03, 4.8216e-02, 6.9862e-02, 1.4367e-01,
        2.2365e-02, 1.6269e-01, 8.4528e+00, 4.6619e-01, 3.0062e+00, 2.5918e-02,
        1.1827e+01, 1.2718e-01, 1.9321e+00, 6.2143e-01, 5.0005e-02, 3.2048e-01,
        4.6774e-01, 7.3467e-01, 2.1189e+00, 3.0274e-01, 1.6600e-02, 2.0140e-01,
        1.1990e+00, 1.3072e+00, 1.4887e+01, 1.0383e+00, 7.2705e+00, 1.1385e+00,
        2.5811e-01, 6.3252e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [20/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 105.2
Sum_Val Meta Model:  tensor([3.5658e+00, 1.0876e+00, 8.2018e+00, 8.7890e-01, 9.7191e-03, 2.0755e-01,
        1.5361e-02, 1.9984e-01, 1.4898e-01, 1.1613e-01, 2.5874e-02, 7.6190e-02,
        2.4026e-02, 2.7002e-01, 1.7024e-01, 1.5186e-01, 1.5306e+01, 2.4855e-02,
        1.7455e-02, 4.2505e-02, 1.2260e-03, 1.6308e-03, 1.9428e-03, 4.2039e-03,
        3.7026e-01, 1.1349e-01, 1.3787e+00, 1.8094e-02, 4.8523e-02, 1.1323e-01,
        1.8464e-02, 5.1436e-03, 4.3621e-01, 1.3774e-03, 5.3622e-03, 3.5265e-02,
        2.1938e-02, 5.5986e-02, 3.3532e-02, 3.1360e+00, 1.9452e+00, 1.4119e+01,
        4.9464e+00, 1.0816e+01, 1.7858e+01, 1.4474e+01, 1.2655e-02, 4.9725e-02,
        1.2961e-02, 5.7026e-02, 1.6799e-03, 7.3715e-03, 5.6813e-02, 9.3617e-03,
        7.5227e-03, 5.1952e-02, 8.0459e+00, 2.2459e-01, 2.1114e+00, 2.4050e-02,
        9.5822e+00, 3.0106e-01, 1.1714e+00, 2.3039e-01, 2.2322e-02, 2.6932e-02,
        5.3646e-02, 2.0912e-01, 4.0360e+00, 2.3149e+00, 1.1208e-03, 3.5952e-01,
        5.7589e+00, 1.5732e+00, 1.6766e+01, 6.7731e+00, 1.7521e-01, 8.9908e+00,
        1.2139e-01, 5.8886e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([3.4094e+00, 6.8091e-01, 5.5196e+00, 4.5683e-01, 4.4716e-03, 1.9907e-01,
        1.0859e-02, 1.8739e-01, 1.2094e-01, 1.0274e-01, 3.3323e-02, 7.6559e-02,
        2.7948e-02, 2.6688e-01, 1.7490e-01, 4.8824e-01, 1.0196e+01, 4.4368e-02,
        1.4482e-02, 9.0050e-02, 1.0841e-03, 1.2514e-03, 1.1038e-03, 1.1842e-02,
        3.5211e-01, 1.0485e-01, 1.2649e+00, 2.5259e-02, 5.8410e-02, 1.3468e-01,
        9.5007e-03, 4.2953e-03, 4.4267e-01, 2.1968e-03, 5.7100e-03, 3.7776e-02,
        5.5359e-02, 4.1173e-02, 3.4546e-02, 2.8827e+00, 2.0306e+00, 1.6473e+01,
        4.3349e+00, 8.6492e+00, 1.6695e+01, 1.3877e+01, 1.0052e-02, 5.2541e-02,
        6.0465e-03, 4.1858e-02, 4.2718e-04, 3.5203e-03, 6.4870e-02, 2.6789e-03,
        5.8784e-03, 3.8367e-02, 6.9115e+00, 2.0426e-01, 1.8130e+00, 3.5144e-02,
        1.3956e+01, 9.2846e-02, 8.9353e-01, 2.6505e-01, 2.2794e-02, 4.4161e-02,
        5.5724e-02, 2.3340e-01, 4.2426e+00, 1.9892e+00, 1.8618e-03, 3.5134e-01,
        5.2599e+00, 1.5979e+00, 1.9859e+01, 8.2142e+00, 1.4346e-01, 1.0522e+01,
        2.1252e-01, 1.1063e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.4543e+01, 3.9721e+01, 5.6060e+01, 2.3545e+01, 6.4473e-01, 1.1357e+01,
        2.5143e+00, 1.6495e+01, 4.8705e+00, 8.8020e+00, 8.3995e+00, 1.1112e+01,
        9.0822e+00, 1.4079e+01, 7.7220e+00, 2.0694e+01, 3.4544e+02, 2.4494e+00,
        6.3249e-01, 3.7543e+00, 9.7153e-01, 2.7781e-01, 1.4276e-01, 2.0959e+00,
        2.6164e+01, 1.6395e+01, 2.7183e+01, 4.0801e+00, 1.2855e+01, 1.6033e+01,
        1.2555e+00, 8.9728e-01, 7.9485e+00, 1.8931e+00, 7.1774e-01, 1.4084e+00,
        8.2911e+00, 3.7565e+00, 1.4805e+00, 5.1602e+01, 1.0529e+01, 3.2514e+01,
        7.0107e+00, 1.5422e+01, 1.8702e+01, 1.8880e+01, 1.6372e+00, 6.0324e+00,
        5.1858e-01, 3.6550e+00, 1.8954e-01, 5.7362e-01, 7.4974e+00, 1.9682e-01,
        1.2441e+00, 2.2192e+00, 2.5923e+01, 5.6784e+00, 1.2561e+01, 3.9893e+00,
        1.8240e+01, 4.0372e+00, 4.5309e+00, 6.2079e+00, 4.8892e-01, 2.4150e+00,
        7.9137e-01, 7.2309e+00, 8.6845e+00, 1.6230e+01, 1.8952e-01, 1.8893e+01,
        1.3284e+01, 9.9785e+00, 2.2802e+01, 8.2241e+00, 1.4347e-01, 1.0532e+01,
        3.0860e-01, 1.0161e+00], device='cuda:0')
Outer loop valEpocw Maximum [20/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 169.7
model_train val_loss valEpocw [20/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 166.9
model_train val_loss valEpocw [20/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1146.6
Sum_Val Meta Model:  tensor([8.0302e+00, 4.4477e-01, 8.0552e+00, 3.2723e-01, 6.9299e-03, 1.3691e+00,
        8.5249e-01, 1.3003e+00, 1.5009e+00, 1.0418e+00, 1.9086e-01, 3.5984e+00,
        8.2885e-02, 1.7249e-01, 6.2334e-01, 9.6526e-01, 6.2536e-01, 9.0225e-01,
        3.3945e-01, 2.3084e+00, 3.9002e-04, 9.6106e-04, 1.1199e-02, 6.6582e-02,
        7.7747e-01, 3.2838e-01, 2.2100e+00, 2.6732e-01, 1.5544e-01, 2.0473e-03,
        3.0228e-03, 1.5814e-03, 1.3831e-02, 8.0489e-04, 1.0877e-03, 2.8927e-03,
        5.7407e-02, 3.6220e-03, 3.0430e-03, 8.9494e-02, 2.3041e-02, 2.0546e+00,
        4.9834e-02, 1.1711e-01, 2.3012e-01, 3.1945e+00, 1.0388e-02, 9.0865e-03,
        2.3796e-03, 1.3381e-01, 3.1111e-04, 1.1497e-03, 8.1979e-04, 1.3317e-03,
        1.6222e-03, 9.0929e-02, 3.6989e+00, 3.2609e-01, 1.4428e+00, 5.8933e-02,
        9.8384e-01, 2.0249e-02, 5.4075e-01, 9.7678e-02, 2.5385e-02, 2.0960e-02,
        4.5887e-02, 8.8328e-01, 4.6772e-02, 6.5617e-01, 3.7027e-04, 3.0363e-02,
        2.2362e+00, 9.6396e-01, 4.6245e+00, 3.2180e-01, 1.1217e-01, 4.4065e-01,
        1.6720e-02, 1.3093e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([6.4503e+00, 5.7915e-01, 5.1278e+00, 3.4091e-01, 4.6875e-02, 1.2490e+00,
        6.9400e-01, 9.1956e-01, 1.2285e+00, 6.2747e-01, 1.7011e-01, 8.6935e-01,
        7.6936e-02, 2.6002e-01, 6.3521e-01, 1.8357e-01, 4.2571e-01, 5.1741e-01,
        1.1045e-01, 6.4664e-01, 2.3120e-03, 1.6214e-03, 2.8511e-03, 6.7737e-02,
        5.3795e-01, 2.6022e-01, 2.1450e+00, 2.5046e-01, 1.5845e-01, 2.1616e-02,
        8.2749e-03, 2.9886e-03, 5.6456e-02, 1.1721e-02, 3.6330e-03, 8.4432e-03,
        4.3670e-02, 2.8590e-02, 4.3735e-03, 5.8616e-02, 3.2438e-02, 9.9781e-01,
        4.3369e-02, 1.1453e-01, 1.2700e-01, 7.8084e-01, 8.7588e-03, 5.5802e-03,
        6.0034e-03, 1.1645e-01, 7.7717e-04, 4.6594e-03, 2.7182e-03, 5.1559e-03,
        5.2351e-03, 3.7941e-02, 3.7311e+00, 2.8801e-01, 6.9604e-01, 6.0700e-03,
        2.6473e+00, 3.1838e-02, 1.7050e-01, 2.0537e-02, 8.5630e-03, 1.0351e-02,
        1.9451e-02, 1.0048e+00, 4.0289e-02, 4.7277e-01, 1.9549e-04, 9.0710e-03,
        1.9536e+00, 3.7278e-01, 4.3588e+00, 5.5725e-02, 4.4566e-01, 3.8336e-01,
        1.2125e-02, 2.9159e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.2775e+01, 2.0666e+01, 4.1001e+01, 1.0078e+01, 3.1499e+00, 3.6809e+01,
        5.6312e+01, 4.0010e+01, 2.8195e+01, 2.7607e+01, 1.7821e+01, 5.3131e+01,
        9.7887e+00, 8.9133e+00, 1.5429e+01, 4.0424e+00, 9.1391e+00, 1.4575e+01,
        2.3581e+00, 1.3258e+01, 6.0928e-01, 1.4520e-01, 1.6343e-01, 5.5339e+00,
        2.4245e+01, 1.7314e+01, 3.3367e+01, 1.6173e+01, 1.3025e+01, 1.2533e+00,
        5.2850e-01, 2.5901e-01, 6.9535e-01, 3.2488e+00, 2.1636e-01, 2.1346e-01,
        2.7975e+00, 1.2995e+00, 1.0168e-01, 8.5203e-01, 1.7304e-01, 2.1101e+00,
        8.3230e-02, 2.4196e-01, 1.5463e-01, 1.1697e+00, 6.3420e-01, 2.9034e-01,
        2.4374e-01, 4.8068e+00, 1.2705e-01, 4.1818e-01, 1.5519e-01, 1.8584e-01,
        4.6785e-01, 1.3681e+00, 1.2433e+01, 4.6382e+00, 4.4773e+00, 3.0542e-01,
        3.7638e+00, 7.6958e-01, 7.6265e-01, 3.1725e-01, 1.3201e-01, 2.7984e-01,
        2.1970e-01, 2.1029e+01, 9.9205e-02, 3.4991e+00, 1.0466e-02, 2.7989e-01,
        5.5370e+00, 1.8661e+00, 5.5174e+00, 5.5932e-02, 4.4586e-01, 3.8465e-01,
        2.0996e-02, 2.3113e-01], device='cuda:0')
Outer loop valEpocw Maximum [20/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 60.3
model_train val_loss valEpocw [20/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 43.9
model_train val_loss valEpocw [20/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 676.8
Sum_Val Meta Model:  tensor([6.5023e+00, 5.9184e-02, 1.0839e+00, 5.8969e-02, 1.0220e-02, 5.9623e-02,
        9.4555e-03, 1.4773e-01, 6.9024e-02, 2.9777e-02, 6.5944e-03, 1.2767e-02,
        2.1177e-03, 3.9341e-01, 9.9774e-02, 9.7627e-02, 2.6503e-01, 1.0305e-01,
        5.6702e-02, 7.9442e-02, 2.9601e-03, 1.6225e-02, 1.4098e-01, 1.5886e-02,
        1.2748e-01, 2.4814e-02, 1.0322e+00, 5.6914e-02, 1.0611e-02, 5.6608e-02,
        5.4322e-02, 4.7679e-02, 6.6499e-01, 6.3346e-04, 3.8264e-02, 1.1505e-01,
        1.5217e-01, 1.8210e-01, 1.5740e-02, 1.5313e+00, 6.2343e+00, 3.2845e+01,
        5.0560e+01, 4.9605e+01, 5.4540e+01, 4.8494e+01, 1.5484e-01, 1.9983e-01,
        7.0374e-01, 3.6797e-01, 1.4638e-01, 2.9006e-01, 7.0869e-01, 2.7139e-01,
        4.3567e-01, 2.1338e+00, 4.6558e+01, 5.9500e-01, 1.2720e+00, 2.9886e-02,
        7.2579e+01, 6.0047e-02, 2.1144e+00, 6.0970e-01, 7.0331e-01, 3.5738e-02,
        6.5894e-01, 8.0571e-01, 2.0523e+00, 3.4298e-01, 5.2295e-03, 1.6603e-01,
        1.6038e+00, 6.2502e+00, 1.2153e+00, 1.9419e+01, 1.0261e+01, 1.2347e+00,
        1.5013e-01, 8.8163e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.7789e+00, 1.1680e-02, 3.9802e-01, 7.3068e-03, 6.8478e-04, 1.8211e-03,
        4.8497e-04, 1.4899e-01, 2.9568e-03, 1.6988e-03, 6.2152e-04, 4.5540e-04,
        1.4028e-04, 2.9702e-01, 8.0805e-03, 1.2477e-02, 3.1085e-02, 5.2645e-03,
        3.3417e-03, 2.0218e-03, 8.8411e-05, 1.0088e-04, 2.8994e-04, 3.0820e-04,
        5.5996e-02, 7.3221e-03, 1.1943e+00, 4.4260e-02, 2.0759e-03, 3.1315e-03,
        5.0911e-03, 4.4974e-02, 5.7687e-01, 2.6770e-04, 1.1187e-02, 4.8355e-02,
        9.5261e-02, 1.4906e-01, 9.9336e-03, 1.6183e+00, 3.9391e+00, 2.9770e+01,
        3.9115e+01, 4.1952e+01, 4.0327e+01, 5.3167e+01, 7.9691e-02, 8.1270e-02,
        5.5608e-01, 1.4438e-01, 6.5500e-02, 2.7143e-01, 7.2044e-01, 3.0271e-01,
        3.0558e-01, 1.2289e+00, 3.3092e+01, 5.3730e-01, 1.2518e+00, 1.2745e-02,
        8.0032e+01, 1.5448e-02, 1.6904e+00, 4.7082e-01, 5.7580e-01, 9.2925e-02,
        6.2525e-01, 3.9053e-01, 2.0229e+00, 2.9497e-01, 1.1071e-03, 1.5084e-01,
        1.3817e+00, 5.9823e+00, 1.1856e+00, 1.8601e+01, 1.0217e+01, 5.2847e+00,
        1.3588e-01, 8.9480e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.3698e+01, 7.3386e-01, 4.2050e+00, 3.8927e-01, 9.3723e-02, 1.0244e-01,
        1.0486e-01, 1.4232e+01, 1.2504e-01, 1.4115e-01, 1.4545e-01, 6.1088e-02,
        4.5526e-02, 1.5855e+01, 3.5606e-01, 5.1736e-01, 1.0095e+00, 2.4375e-01,
        1.4226e-01, 8.1172e-02, 7.3962e-02, 1.8781e-02, 2.8260e-02, 5.0867e-02,
        4.1932e+00, 1.1344e+00, 2.9634e+01, 6.8610e+00, 4.5696e-01, 3.3295e-01,
        5.9127e-01, 7.7385e+00, 9.5196e+00, 1.9867e-01, 1.1998e+00, 1.7214e+00,
        1.1368e+01, 1.1747e+01, 4.0824e-01, 3.2616e+01, 2.2257e+01, 5.9383e+01,
        6.4301e+01, 7.5583e+01, 4.6079e+01, 7.2642e+01, 1.0916e+01, 7.0571e+00,
        3.6977e+01, 9.7676e+00, 2.1733e+01, 3.9988e+01, 7.0439e+01, 1.9523e+01,
        5.6116e+01, 6.7877e+01, 1.2924e+02, 1.4184e+01, 9.5902e+00, 1.2785e+00,
        1.0573e+02, 6.5917e-01, 8.9056e+00, 1.1714e+01, 1.3129e+01, 4.5573e+00,
        1.0054e+01, 1.3299e+01, 5.1417e+00, 2.9485e+00, 1.2046e-01, 8.4767e+00,
        4.2286e+00, 3.7709e+01, 1.4231e+00, 1.8626e+01, 1.0218e+01, 5.2908e+00,
        2.3095e-01, 9.0875e-01], device='cuda:0')
Outer loop valEpocw Maximum [20/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 429.9
model_train val_loss valEpocw [20/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 382.7
model_train val_loss valEpocw [20/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1280.5
Sum_Val Meta Model:  tensor([1.2495e+01, 7.6835e-02, 3.3033e+00, 5.4369e-02, 1.4390e-02, 3.6643e-01,
        5.7674e-02, 3.5994e-01, 8.5127e-02, 4.5346e-01, 4.9262e-02, 8.1670e-02,
        3.0708e-03, 5.1967e-01, 5.4829e-01, 8.8705e-02, 1.3141e-01, 6.7308e-02,
        3.4192e-02, 4.7063e-02, 6.0639e-03, 1.1497e-02, 1.4942e-02, 1.8804e-02,
        5.4423e-01, 3.5621e-02, 2.0797e+00, 4.1368e-02, 1.6449e-01, 1.6914e-02,
        2.4075e-02, 6.3276e-03, 5.3161e-01, 8.8876e-02, 1.5624e-01, 3.7452e-01,
        1.2203e-02, 6.2589e-02, 2.8421e-01, 2.0449e+00, 2.8436e+00, 1.5615e+01,
        7.5492e+00, 6.9887e+00, 1.1016e+01, 1.2839e+01, 1.2589e-02, 1.7855e-02,
        4.8343e-02, 1.9977e-02, 9.6391e-03, 1.0949e-02, 9.8426e-03, 3.1232e-01,
        1.8865e-02, 6.9413e-02, 9.2031e+00, 3.4326e-01, 2.6318e+00, 5.0636e-02,
        3.6528e+01, 5.4133e-02, 1.3150e+00, 6.3039e-01, 4.9588e-01, 1.0420e-01,
        7.1968e-01, 2.5415e+00, 6.4332e-01, 4.2949e-01, 4.0288e-03, 8.6484e-02,
        1.5723e+00, 2.6324e+00, 3.1890e+02, 1.5500e+01, 1.3836e+00, 9.3462e+00,
        7.8097e-02, 2.1183e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([4.2976e+00, 1.3680e-01, 2.2253e+00, 7.2735e-02, 3.7477e-02, 3.0774e-01,
        7.7390e-02, 3.1270e-01, 1.1440e-01, 3.8044e-01, 4.0371e-02, 1.7349e-01,
        5.3396e-03, 4.7449e-01, 6.7850e-01, 1.4079e-02, 3.0792e-02, 1.6981e-02,
        2.8064e-03, 7.5102e-03, 1.1620e-03, 3.2389e-04, 1.5245e-03, 1.4863e-02,
        4.8845e-01, 3.5093e-02, 1.8964e+00, 1.5995e-02, 2.1518e-01, 3.5890e-03,
        6.1338e-03, 4.3195e-03, 2.0734e-02, 1.5918e-03, 2.0237e-03, 5.3978e-03,
        9.9043e-03, 6.3711e-03, 8.3805e-03, 1.5582e-01, 3.2032e-01, 4.8799e-01,
        2.2355e-01, 4.2577e-01, 4.7793e-01, 1.1545e+00, 4.0151e-03, 2.7380e-03,
        3.0228e-03, 3.3607e-03, 7.1740e-04, 1.8173e-03, 6.0378e-04, 5.4472e-03,
        6.5922e-03, 7.2734e-02, 7.9532e-01, 3.7404e-02, 1.9559e+00, 1.2551e-02,
        9.5973e+00, 1.4874e-02, 1.7700e-01, 1.5433e-02, 1.3885e-02, 2.4264e-02,
        4.6570e-02, 3.9599e-01, 1.0741e-01, 4.3658e-02, 6.0464e-04, 7.7262e-03,
        3.0341e-02, 1.2326e+00, 6.9901e+01, 2.8899e+00, 5.5832e-02, 9.2026e+00,
        3.5767e-02, 7.7090e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.7166e+01, 5.0061e+00, 1.8265e+01, 2.2263e+00, 2.4533e+00, 9.8622e+00,
        7.1426e+00, 1.5348e+01, 2.8978e+00, 1.6463e+01, 4.1581e+00, 1.0926e+01,
        7.2447e-01, 1.5246e+01, 1.7908e+01, 3.6813e-01, 7.0868e-01, 4.7060e-01,
        7.4056e-02, 1.9319e-01, 3.2631e-01, 3.0043e-02, 9.1313e-02, 1.2419e+00,
        2.2588e+01, 2.4646e+00, 2.9541e+01, 1.1282e+00, 2.0329e+01, 2.0806e-01,
        3.8956e-01, 4.1625e-01, 2.3111e-01, 3.6315e-01, 1.0694e-01, 1.0936e-01,
        6.8256e-01, 2.7666e-01, 1.7849e-01, 1.8720e+00, 1.4138e+00, 9.7123e-01,
        3.9780e-01, 8.2724e-01, 5.7391e-01, 1.6925e+00, 3.2615e-01, 1.5350e-01,
        1.2570e-01, 1.4873e-01, 1.1972e-01, 1.6657e-01, 3.7066e-02, 1.8248e-01,
        6.1450e-01, 2.6340e+00, 2.5949e+00, 6.1525e-01, 1.1663e+01, 6.6843e-01,
        1.3133e+01, 3.6077e-01, 7.6059e-01, 2.2467e-01, 1.8220e-01, 6.4687e-01,
        4.5334e-01, 6.9197e+00, 2.6988e-01, 3.3363e-01, 3.3100e-02, 2.3620e-01,
        8.2952e-02, 5.9960e+00, 8.8250e+01, 2.9004e+00, 5.5861e-02, 9.2351e+00,
        7.2187e-02, 5.7487e-02], device='cuda:0')
Outer loop valEpocw Maximum [20/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 488.1
model_train val_loss valEpocw [20/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 112.1
model_train val_loss valEpocw [20/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 406.3
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [88.00940534 97.39891083 93.31513992 98.09822005 98.57579708 97.56703744
 98.29924099 95.28149024 98.03730461 97.01270696 98.65376884 98.9546911
 99.43104982 95.3887014  97.51099524 97.23565746 96.41329906 97.68155846
 98.843825   98.34310011 98.72321244 99.33967666 99.56628209 99.05093749
 95.27174377 96.87016484 94.40187132 97.02123512 98.05314263 98.27731144
 98.46127606 98.6208745  97.1857068  98.63183928 98.72443075 98.92545169
 97.65475567 98.44178312 98.94494463 93.20305552 98.06045248 94.8453357
 97.96907932 97.09798857 98.26634666 96.75077058 98.25538188 98.68544487
 98.16644534 98.75732508 98.76341663 98.61478296 99.02047977 98.41985356
 98.73905045 97.55850928 92.89482341 96.96397461 96.70325654 97.62917118
 96.61675662 98.69884626 97.58287545 97.3806362  98.84017008 97.58287545
 98.65864207 95.98323607 98.97783896 98.37477614 99.81603538 97.40134745
 98.91692353 96.17694716 98.46005775 98.91083198 99.57481025 99.30800063
 99.84405648 99.18129652]
Accuracy th:0.7 is [86.29768156 97.35627003 92.48425336 98.04217785 98.63549421 97.4220587
 98.02268491 95.07194113 97.88745264 96.83117896 98.61234634 98.94007139
 99.42373996 95.32778597 97.39891083 97.05169284 96.32436252 97.57678391
 98.76219832 98.41741694 98.60138156 99.27023306 99.51633143 98.94859955
 95.23397619 96.74833396 94.22034332 96.91524226 98.02024829 98.25538188
 98.32238886 98.59650833 96.84579866 98.54899429 98.579452   98.84626162
 97.38794605 98.44909297 98.80849405 92.85827414 97.97760749 94.08632936
 97.54388957 96.61797493 98.1603538  96.18669363 98.22614247 98.64767729
 98.07263557 98.72321244 98.79996589 98.59650833 99.00707837 98.29071283
 98.72199413 97.68155846 91.92504965 96.67767206 96.57167919 97.52561494
 96.09775709 98.57579708 97.3392137  97.22225606 98.76219832 97.47931921
 98.65742376 95.96008821 98.87428272 98.14817071 99.81603538 96.93595351
 98.78412787 95.95521497 98.26391004 98.87306441 99.554099   99.31531049
 99.84405648 99.18251483]
Avg Prec: is [96.59402735 35.39551991 72.87705529 67.36788333 78.62236904 64.22350899
 74.4061632  47.97689343 57.27618693 52.85855849 32.51871095 53.88197767
 25.04968289 28.45429848 34.2134747  56.30736209 29.61429952 42.16101695
 49.53077064 38.2685231  61.52255314 51.07613382 89.98239952 83.28238715
 26.33312075 33.56249251 38.7353638  39.32762666 25.28380272 38.93247813
 74.22538026 38.27012044 58.74466167 64.05678774 73.79552067 80.30095426
 58.12479997 75.25219167 86.81558877 47.04577587 44.93616007 74.40486642
 72.52188777 66.06886374 77.46326027 81.33180479 39.62190771 34.63984662
 41.14298388 46.11004677 63.04254456 36.06892844 24.22970349 72.85422879
 26.61226395 44.03369957 73.22493251 59.30329529 44.95522879 58.65454105
 90.96972283 82.70220566 71.83428733 49.80718512 58.347978   43.28832961
 59.67079684 25.86591088 59.90707107 69.10445     9.71221343 73.9501465
 79.26499374 51.38257832 84.95130694 89.32697946 75.15415232 85.74398014
 13.48297268 28.1606719 ]
Accuracy th:0.5 is [45.60495121 97.2137279  71.84975817 97.02489005 97.26733349 76.61700028
 76.98371121 76.1174937  77.82921748 96.46203141 78.13501297 98.52097319
 99.41399349 80.27314482 77.720788   96.56680596 96.29512311 77.54900647
 98.65376884 98.30776915 80.48756716 78.66254066 98.38695922 78.3616184
 81.08210183 96.65086926 94.0778012  77.22859127 98.01293844 78.04973136
 97.30875598 98.57457877 96.36213009 98.02024829 87.94118005 77.71956969
 77.45276008 91.22330381 97.11504489 74.71522033 79.2668218  92.05906361
 76.87284512 76.51709896 96.9627563  93.87434364 98.02877645 98.57336046
 97.53170649 88.98892557 88.00940534 98.55508583 98.99976852 77.09092238
 98.70615611 77.43448545 72.16651844 93.41260462 96.24273583 96.9067141
 89.79300934 97.17717864 93.34194272 77.44545023 98.42838172 77.94617512
 98.20786784 76.67913403 78.5979703  97.55972759 79.10722335 95.99054592
 78.29461142 95.45083515 76.80218321 82.95586067 88.02889828 78.10211864
 79.18154019 99.14718388]
Accuracy th:0.7 is [45.66464834 97.2137279  71.84975817 97.02489005 97.26733349 76.61700028
 76.98371121 76.35384559 77.82921748 96.4754328  78.13501297 98.52097319
 99.41399349 80.72391906 77.720788   96.56680596 96.29512311 77.54900647
 98.65376884 98.30776915 80.93224985 78.66254066 98.38695922 78.86477991
 81.73877024 96.65086926 94.0778012  77.22859127 98.01293844 78.04973136
 97.30875598 98.57457877 96.36213009 98.02024829 88.11052497 77.71956969
 77.45276008 91.47793034 97.11504489 74.71522033 79.93080006 92.05906361
 76.87284512 76.51709896 96.9627563  93.87434364 98.02877645 98.57336046
 97.87161462 89.55787576 88.20920798 98.55508583 98.99976852 77.09092238
 98.70615611 77.43448545 72.16651844 93.84510423 96.24273583 96.9067141
 89.79300934 97.17717864 93.48570315 77.44545023 98.42838172 77.94617512
 98.20786784 76.67913403 78.5979703  97.55972759 79.10722335 95.99054592
 78.38720288 95.45083515 76.80218321 83.03383243 88.21651783 78.10211864
 79.18154019 99.14718388]
Avg Prec: is [56.03944347  3.08199728 11.17996178  3.39927267  2.18905019  3.80341814
  3.24686297  5.51825528  2.50312121  3.80904188  1.60920324  1.59873556
  0.60531223  5.09081389  2.65781979  3.16661326  3.72547565  2.63306116
  1.32313781  1.76030527  1.97728134  0.86209657  1.79341838  2.38474334
  4.97921189  3.64223762  6.43450557  3.32733053  2.04321231  1.96109879
  2.59321946  1.38569917  3.7547349   1.6569278   2.52695769  2.50690806
  3.03092409  2.57909487  2.74455078  7.29689416  2.29989629  8.18363
  3.39300225  4.05080905  3.22700195  6.33936602  2.12022488  1.59678191
  2.034386    1.63912901  1.83801553  1.57899941  1.02361219  2.99270111
  1.31278801  2.66629096 11.3125399   3.79069066  4.00152872  2.85220476
 10.74107511  2.15513136  3.85486638  3.11592108  1.54290062  2.55196502
  1.76080848  4.25030745  1.28267117  2.45006678  0.19423957  3.3979298
  1.93057865  4.61651831  3.88181865  3.03778214  0.87606982  1.96350883
  0.17924028  0.75969017]
mAP score regular 55.83, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [88.18795625 97.37648554 93.35774971 98.24102449 98.86139971 97.72778235
 98.44034183 95.32102549 98.14136582 97.0575778  98.65709943 99.00590478
 99.38460772 95.24877295 97.50604181 97.266861   96.45215138 97.74273115
 98.98597304 98.29583676 98.91122904 99.34225278 99.60634826 99.26750878
 95.38829509 96.75860179 94.55116227 97.23945487 97.85733862 98.14385729
 98.73931784 98.59730423 96.98781673 98.7343349  98.95109251 99.17532451
 98.05665595 98.35314049 99.11552931 93.11607743 97.97194608 93.40508758
 97.43877221 96.65894312 96.85576899 94.88003588 98.40296983 98.8016045
 98.14884022 98.72436904 98.85392531 98.5997957  98.89877171 98.45529063
 98.75924957 97.30672447 91.26242619 97.2369634  96.50945512 97.58327728
 92.83952463 98.86638264 97.41385754 97.56583701 98.75924957 97.57580288
 98.57488103 95.81682737 98.76921544 98.24351596 99.81563146 97.61566634
 98.5175773  95.73710043 97.3939258  97.3341306  99.11054638 98.5101029
 99.82559733 99.16037571]
Accuracy th:0.7 is [87.73700077 97.41385754 92.91177716 98.32075143 98.93365224 97.55587114
 98.27092209 95.35590602 98.07907915 96.84829459 98.64962503 99.04576824
 99.37215038 95.13914842 97.40638314 97.06006926 96.27774871 97.63559808
 98.90873757 98.4353589  98.84146797 99.29491492 99.59389092 99.20522211
 95.46553056 96.68136632 94.45399507 97.0575778  97.83242395 98.23105862
 98.62720183 98.69197997 96.85327752 98.72436904 98.82402771 99.13047811
 97.79754341 98.29334529 99.01088771 92.8494905  97.95450582 93.29297157
 97.3266562  96.61907965 96.99279966 94.82522361 98.38303809 98.83897651
 98.04918155 98.76921544 98.85890824 98.6371677  98.88631437 98.38054663
 98.7418093  97.75269701 91.01826245 97.08498393 96.46959165 97.58327728
 93.0438249  98.7642325  97.22699753 97.3490794  98.76921544 97.58327728
 98.61972743 95.78942123 98.8165533  98.02675835 99.81563146 97.266861
 98.46276503 95.8965543  97.38894287 97.49109301 99.18030745 98.63218477
 99.82559733 99.16785011]
Avg Prec: is [96.54773106 35.01589063 72.01226493 73.19868565 77.7028272  64.69924405
 80.26386477 48.4519183  62.40030036 56.06541007 38.82067588 56.91004168
 24.26247923 32.0357612  35.51284321 62.37403479 32.55902999 45.37357474
 52.21483008 37.74285103 70.75474856 58.86628922 91.99255808 88.05668085
 24.42495658 36.59944555 33.77567597 45.25047793 27.50236926 37.7030135
 77.30563596 37.41715262 55.0848413  65.6314598  75.32201427 83.07966644
 61.21155065 78.0055009  89.96247617 44.71469993 37.65106038 52.68672878
 50.44479894 42.92161646 33.09283552 52.92962569 39.41835999 28.93174746
 42.12608204 41.91483958 68.21229747 34.80769274 23.27530586 76.1921242
 27.8822498  40.87477344 58.27185346 57.80412936 40.08961813 61.43198856
 66.22565595 86.97141284 67.1094619  53.46514878 61.61835042 38.38142028
 61.73492543 26.3796073  42.08273176 66.26288541  9.08463016 75.80357766
 58.64736951 44.74299101 64.9396323  52.66675368 13.69446865 53.19808775
  1.99027709 22.54358778]
Accuracy th:0.5 is [45.28489922 97.22450607 70.15721155 96.96290206 97.90716795 75.61352368
 75.70072502 74.67673219 77.20806239 96.41976231 77.34758452 98.5325261
 99.34972718 78.11993921 77.27533199 96.31262924 96.21047911 76.69731171
 98.78167277 98.34068316 79.00191843 77.98290854 98.31327703 78.72785709
 78.12741361 96.52938685 94.3393876  76.75959838 97.81747515 77.30024666
 97.52597354 98.67204823 96.39983058 98.18870369 89.15464534 76.92652665
 76.90410345 92.47826195 97.0276802  74.07379724 77.6266288  92.37362035
 76.05451329 75.72314822 97.03764606 94.02795426 98.18621222 98.77668984
 97.82744101 88.7385704  86.40655754 98.55993223 98.87385704 76.03956449
 98.6969629  76.72720931 70.80000997 94.38672547 96.16314124 96.78102499
 90.13379176 97.04761193 93.63181105 76.74464958 98.32075143 77.59922266
 98.13139996 75.92993996 78.01529761 97.53593941 78.43386402 96.07843137
 77.78608267 95.44559882 75.83775569 83.89017615 90.06652216 77.37748212
 78.50362508 99.15040985]
Accuracy th:0.7 is [45.48919949 97.22450607 70.15721155 96.96290206 97.90716795 75.61352368
 75.70072502 74.8336946  77.20806239 96.41976231 77.34758452 98.5325261
 99.34972718 78.49365922 77.27533199 96.31262924 96.21047911 76.69731171
 98.78167277 98.34068316 79.29092857 77.98290854 98.31327703 79.05922216
 78.52355682 96.52938685 94.3393876  76.75959838 97.81747515 77.30024666
 97.52597354 98.67204823 96.39983058 98.18870369 89.35396268 76.92652665
 76.90410345 92.70747689 97.0276802  74.07379724 78.12492214 92.37362035
 76.05451329 75.72314822 97.03764606 94.02795426 98.18621222 98.77668984
 97.93706555 89.05498667 86.55853701 98.55993223 98.87385704 76.03956449
 98.6969629  76.72720931 70.80000997 94.71310761 96.16314124 96.78102499
 90.13379176 97.04761193 93.76884172 76.74714104 98.32075143 77.59922266
 98.13139996 75.92993996 78.01529761 97.53593941 78.43386402 96.07843137
 77.81847173 95.44559882 75.83775569 83.99730922 90.2807883  77.37748212
 78.50362508 99.15040985]
Avg Prec: is [54.27653114  3.72232256 14.85395957  4.545478    1.48782978  4.26884705
 12.80100065  8.64558428  7.78984251  5.20603113  2.29286676  4.97454403
  1.55621445  5.88008453  3.06453857  3.68283816 24.25676524  6.41695754
  1.5703625   2.69474552  3.57122727  1.46218624  1.15246852  5.54486314
  5.69957081 10.12855918  7.98653332  4.53320896  3.94964982  6.15018185
  2.27431371  0.85548835  3.06813003  1.10693387  1.69804542  2.37384491
  2.01528723  2.19687133  2.24505371  6.21505494  1.73801428  6.01420941
  2.18682588  2.71972227  2.37545257  4.85946979  1.74099813  1.0430979
  1.37351661  1.16840265  1.19333349  0.9788091   0.73342824  2.37674078
  0.85533222  1.83840359 10.04632828  2.8807338   3.88144832  2.73736328
  7.83760623  2.04181904  3.20582641  2.50233113  1.35536     1.82934499
  1.53456151  3.4334971   1.08457464  2.23531787  0.19435283  3.20494174
  1.56826818  3.92293392  3.53257012  2.31116177  0.60022013  1.50277595
  0.12977512  0.59097509]
mAP score regular 51.82, mAP score EMA 4.32
Train_data_mAP: current_mAP = 55.83, highest_mAP = 55.83
Val_data_mAP: current_mAP = 51.82, highest_mAP = 51.82
tensor([5.6595e-02, 9.2775e-03, 7.6230e-02, 1.0945e-02, 3.6180e-03, 9.8868e-03,
        2.0778e-03, 5.6757e-03, 1.4912e-02, 6.2778e-03, 1.9477e-03, 3.6367e-03,
        1.3946e-03, 1.0962e-02, 1.4042e-02, 1.5241e-02, 1.8824e-02, 1.1617e-02,
        1.3715e-02, 1.5284e-02, 4.4700e-04, 2.2688e-03, 4.2766e-03, 2.9251e-03,
        8.0665e-03, 3.1884e-03, 3.0736e-02, 3.0345e-03, 2.0277e-03, 4.2903e-03,
        4.0612e-03, 2.0548e-03, 3.7739e-02, 5.6386e-04, 4.8166e-03, 1.9397e-02,
        3.1741e-03, 6.6476e-03, 1.4042e-02, 3.9858e-02, 1.7967e-01, 5.0261e-01,
        6.7538e-01, 6.0162e-01, 9.1891e-01, 7.7149e-01, 2.9260e-03, 4.7794e-03,
        6.6062e-03, 6.4204e-03, 1.0969e-03, 2.9981e-03, 4.3166e-03, 7.4496e-03,
        2.3438e-03, 9.8566e-03, 2.2451e-01, 2.3338e-02, 1.3243e-01, 4.7521e-03,
        7.9503e-01, 1.3118e-02, 1.7524e-01, 2.7304e-02, 3.4247e-02, 1.1089e-02,
        5.2536e-02, 2.0891e-02, 4.6459e-01, 8.2604e-02, 4.7024e-03, 1.0081e-02,
        3.6962e-01, 1.3738e-01, 8.9596e-01, 9.9958e-01, 9.9998e-01, 9.9966e-01,
        6.5982e-01, 8.2930e-02], device='cuda:0')
Sum Train Loss:  tensor([1.7266e+00, 1.7954e-01, 1.1945e+00, 3.3266e-02, 6.3584e-03, 3.2985e-02,
        1.2459e-02, 6.3633e-02, 1.4479e-01, 2.5888e-02, 4.1553e-03, 1.7414e-02,
        7.1946e-04, 2.5503e-01, 1.6519e-01, 1.4422e-01, 1.8016e-01, 6.6109e-02,
        2.3031e-01, 7.1287e-02, 1.6109e-03, 3.1081e-03, 1.7326e-03, 6.5023e-03,
        1.2348e-01, 3.2015e-02, 7.0817e-01, 3.9356e-02, 3.1147e-02, 2.0802e-02,
        1.7170e-02, 1.4620e-02, 5.8675e-01, 2.9105e-03, 4.3890e-02, 1.5470e-01,
        1.5457e-02, 4.0142e-02, 1.6138e-01, 8.6984e-01, 1.9469e+00, 5.5176e+00,
        4.2128e+00, 5.8069e+00, 4.8194e+00, 4.8279e+00, 1.3734e-02, 1.8363e-02,
        4.6543e-02, 5.7671e-02, 5.0780e-03, 1.2991e-02, 5.7343e-03, 4.8722e-02,
        2.7122e-02, 9.5581e-02, 4.0852e+00, 3.0024e-01, 1.9804e+00, 3.4814e-02,
        7.3526e+00, 1.0620e-01, 1.1501e+00, 1.6601e-01, 7.0638e-02, 1.4034e-01,
        1.5393e-01, 2.6042e-01, 8.4730e-01, 2.0247e-01, 1.5395e-02, 8.2830e-02,
        5.1791e-01, 2.6661e+00, 4.2580e+00, 6.7428e+00, 1.2214e-01, 5.6595e+00,
        3.2931e+00, 2.7919e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [21/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 75.4
Sum Train Loss:  tensor([2.1064e+00, 5.5097e-02, 1.3573e+00, 9.8423e-02, 2.6150e-02, 5.3998e-02,
        1.8984e-02, 1.3423e-01, 1.7093e-01, 6.4415e-02, 2.6026e-02, 4.7173e-02,
        9.9190e-04, 1.2409e-01, 3.0214e-01, 1.4338e-01, 1.8942e-01, 1.3697e-01,
        1.3770e-01, 5.4127e-02, 2.9179e-03, 4.9880e-03, 7.0771e-03, 6.6174e-03,
        2.0152e-01, 4.1403e-02, 2.6808e-01, 3.0085e-02, 1.0179e-02, 4.9143e-03,
        1.1134e-02, 2.0647e-02, 3.2829e-01, 2.8592e-03, 1.9174e-02, 1.0550e-01,
        1.6409e-02, 6.9400e-02, 1.0704e-01, 7.8704e-01, 5.9557e-01, 5.0109e+00,
        1.9911e+00, 6.4148e+00, 4.5777e+00, 7.5422e+00, 1.6020e-02, 1.1226e-02,
        2.1067e-02, 7.1590e-02, 1.1428e-03, 1.9807e-02, 7.9737e-03, 5.8132e-02,
        2.7380e-02, 6.7856e-02, 2.8204e+00, 2.4691e-01, 1.9616e+00, 1.9528e-02,
        6.4645e+00, 6.7846e-02, 1.4147e+00, 1.4194e-01, 3.7050e-02, 2.0702e-01,
        1.0260e-01, 2.8061e-01, 4.6302e+00, 6.6929e-01, 1.3249e-02, 9.8434e-02,
        2.2887e+00, 2.4683e+00, 6.6620e+00, 2.8383e+00, 6.9836e+00, 8.8883e+00,
        1.2260e-01, 3.2077e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [21/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 83.5
Sum Train Loss:  tensor([1.9393e+00, 7.8724e-02, 1.4348e+00, 2.9873e-02, 1.6375e-02, 4.6870e-02,
        8.0403e-03, 1.0183e-01, 1.6761e-01, 5.4650e-02, 1.2084e-02, 8.5599e-03,
        1.4163e-02, 1.1565e-01, 2.6167e-01, 2.0648e-01, 2.5064e-01, 9.8914e-02,
        4.6831e-02, 3.3445e-02, 2.4879e-03, 1.9072e-02, 6.0457e-03, 3.3112e-03,
        1.8288e-01, 2.0468e-02, 9.1712e-01, 2.2968e-02, 3.3324e-02, 3.5744e-02,
        3.9098e-02, 1.0332e-02, 1.6792e-01, 5.1489e-03, 5.7953e-03, 6.0677e-02,
        1.9094e-02, 8.7994e-02, 3.9416e-02, 7.2903e-01, 1.3442e+00, 8.6010e+00,
        2.1664e+00, 6.6539e+00, 3.7285e+00, 6.2693e+00, 1.5888e-02, 3.3199e-02,
        3.8981e-02, 7.4458e-03, 8.8489e-03, 1.3965e-02, 1.9571e-02, 3.1783e-02,
        4.1262e-03, 5.2168e-02, 4.4102e+00, 6.8813e-02, 1.2458e+00, 4.3366e-02,
        4.4898e+00, 3.8914e-02, 6.6545e-01, 2.3956e-01, 1.3249e-01, 4.8547e-02,
        7.5800e-02, 2.4502e-01, 2.9669e+00, 5.6992e-01, 1.5353e-02, 6.7807e-02,
        1.0055e+00, 1.6223e+00, 7.0303e+00, 1.3175e+00, 3.1219e+00, 4.5789e+00,
        9.7237e-02, 9.3513e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [21/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 70.5
Sum Train Loss:  tensor([1.9252e+00, 1.1748e-01, 1.5533e+00, 9.4288e-02, 3.6338e-02, 3.6349e-02,
        8.0246e-03, 7.8886e-02, 6.1127e-02, 2.8979e-02, 4.9079e-03, 5.6327e-03,
        5.2303e-04, 1.9124e-01, 2.0425e-01, 1.9300e-01, 1.5572e-01, 7.2102e-02,
        1.2989e-02, 8.8003e-02, 3.0206e-03, 4.9942e-03, 1.5209e-02, 9.8608e-03,
        1.8241e-01, 2.9717e-02, 7.4024e-01, 6.6322e-02, 1.0517e-02, 6.3432e-02,
        2.4255e-02, 8.9217e-03, 3.0171e-01, 3.4958e-03, 1.7054e-02, 4.7745e-02,
        8.4931e-03, 3.0503e-02, 6.2192e-02, 1.0405e+00, 7.6802e-01, 5.5893e+00,
        3.1179e+00, 3.0715e+00, 1.5299e+00, 5.4794e+00, 2.9775e-02, 2.5551e-02,
        2.5024e-02, 3.7745e-02, 3.9115e-03, 1.1202e-02, 1.8197e-02, 2.8429e-02,
        2.8192e-02, 4.7755e-02, 5.7400e+00, 4.8780e-01, 2.0028e+00, 3.7252e-02,
        6.4916e+00, 2.4246e-02, 1.3409e+00, 4.9944e-01, 5.5516e-01, 7.3960e-02,
        5.5040e-01, 3.1541e-01, 9.8792e-01, 4.5073e-01, 9.4128e-04, 1.0903e-01,
        1.7208e+00, 2.7037e+00, 2.2399e+00, 8.9624e+00, 3.5732e+00, 7.1087e+00,
        2.1004e-01, 5.7218e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [21/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 74.1
Sum Train Loss:  tensor([1.5044e+00, 5.9131e-02, 1.4656e+00, 1.1824e-01, 5.5655e-03, 1.3476e-01,
        3.5310e-03, 8.3467e-02, 1.5676e-01, 5.3912e-02, 7.5153e-03, 1.4178e-02,
        3.7011e-04, 1.9680e-01, 1.5276e-01, 6.0169e-02, 1.4472e-01, 5.5578e-02,
        2.2611e-02, 1.1462e-01, 6.4570e-04, 2.4276e-02, 1.1818e-02, 5.1434e-03,
        1.3110e-01, 2.4364e-02, 9.5004e-01, 2.5359e-02, 1.0867e-02, 2.7647e-02,
        2.4561e-02, 1.4698e-02, 2.7064e-01, 4.0214e-03, 5.5428e-03, 5.7821e-02,
        2.6756e-02, 9.3772e-02, 2.2957e-01, 7.2768e-01, 1.0853e+00, 3.6527e+00,
        6.3316e+00, 5.5183e+00, 4.8019e+00, 6.2246e+00, 3.9565e-02, 4.3497e-02,
        2.7357e-02, 7.9411e-02, 5.6162e-03, 2.5957e-02, 5.5673e-03, 1.4330e-02,
        8.9710e-03, 8.9584e-02, 3.9853e+00, 1.6702e-01, 2.0081e+00, 2.9800e-02,
        9.0991e+00, 9.9006e-02, 1.1615e+00, 3.2013e-01, 3.5922e-01, 9.5448e-02,
        1.0593e-01, 3.5861e-01, 5.2862e+00, 3.9327e-01, 1.3476e-02, 8.0663e-02,
        2.4410e+00, 1.9301e+00, 7.7546e+00, 5.2046e+00, 5.5806e-01, 3.8379e+00,
        1.5636e-01, 8.6766e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [21/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 80.5
Sum Train Loss:  tensor([2.1701e+00, 1.1161e-01, 2.0212e+00, 3.7630e-02, 5.7478e-03, 7.0488e-02,
        1.5586e-02, 5.7207e-02, 4.9369e-02, 2.5408e-02, 8.9443e-03, 1.9581e-02,
        8.5345e-03, 2.3775e-01, 1.8782e-01, 9.0518e-02, 2.4613e-01, 1.0942e-01,
        6.0297e-02, 8.0568e-02, 1.3810e-03, 2.3657e-03, 2.9834e-03, 3.4361e-03,
        1.0566e-01, 2.5661e-02, 6.6646e-01, 4.7434e-02, 8.3190e-03, 4.5709e-02,
        1.1769e-02, 6.5920e-03, 8.6241e-01, 9.3268e-04, 4.7548e-02, 1.8172e-01,
        1.8233e-02, 3.9338e-02, 1.9501e-01, 7.8672e-01, 5.7586e-01, 5.4448e+00,
        4.9033e+00, 6.6938e+00, 1.2208e+01, 1.5809e+01, 3.5724e-02, 2.7586e-02,
        5.2331e-02, 3.7248e-02, 8.8792e-03, 3.3216e-02, 3.2067e-02, 2.2232e-02,
        2.4216e-02, 9.5203e-02, 5.0798e+00, 1.9385e-01, 3.7197e+00, 5.2077e-02,
        1.0392e+01, 1.2029e-01, 2.5899e+00, 1.4484e-01, 1.1641e-01, 1.0121e-01,
        2.1515e-01, 2.3884e-01, 7.6854e-01, 5.1482e-01, 6.3014e-04, 6.8338e-02,
        1.5465e+00, 1.5607e+00, 9.1599e+00, 1.0497e+01, 3.8115e-01, 1.2698e+00,
        1.2752e-01, 1.0045e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [21/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 103.6
Sum Train Loss:  tensor([2.4487e+00, 9.3372e-02, 1.6160e+00, 1.1802e-01, 2.6806e-02, 9.1843e-02,
        1.1096e-02, 1.1168e-01, 2.7758e-01, 4.5142e-02, 3.0705e-02, 6.1530e-02,
        6.0236e-03, 1.3000e-01, 2.2932e-01, 1.4648e-01, 1.0369e-01, 1.0802e-01,
        4.5448e-02, 1.6897e-01, 4.0001e-03, 1.0907e-03, 2.6510e-03, 2.6578e-02,
        1.4063e-01, 3.8514e-02, 4.4442e-01, 3.0659e-02, 9.2715e-03, 2.8216e-02,
        3.6099e-02, 7.0153e-03, 2.6153e-01, 3.1073e-03, 3.2654e-02, 6.9855e-02,
        3.1244e-02, 4.2785e-02, 2.6692e-02, 7.3281e-01, 1.7517e+00, 1.0515e+01,
        5.5405e+00, 6.8356e+00, 7.9262e+00, 4.4002e+00, 1.7906e-02, 1.9242e-02,
        6.1355e-02, 2.3421e-02, 1.7772e-03, 1.1135e-02, 3.6185e-03, 6.5148e-02,
        3.6771e-03, 6.7997e-02, 5.7876e+00, 1.3707e-01, 1.7721e+00, 2.2528e-02,
        6.4861e+00, 7.1619e-02, 1.0661e+00, 1.9162e-01, 6.1602e-02, 3.9351e-02,
        1.0055e-01, 3.5971e-01, 1.4803e+00, 9.3468e-01, 1.4386e-03, 5.1752e-02,
        5.7451e-01, 1.0943e+00, 1.0631e+01, 4.9014e+00, 3.4794e-01, 8.9804e-01,
        2.2891e-01, 7.2948e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [21/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 82.4
Sum_Val Meta Model:  tensor([2.8094e+00, 5.9702e-01, 6.2725e+00, 4.6405e-01, 5.0912e-03, 1.3307e-01,
        8.5479e-03, 1.0179e-01, 8.8721e-02, 6.0614e-02, 1.1897e-02, 3.8806e-02,
        9.9777e-03, 1.5183e-01, 1.3266e-01, 9.5861e-02, 9.4932e+00, 3.3876e-02,
        1.6343e-02, 2.0610e-02, 3.5450e-04, 7.6315e-04, 1.1824e-03, 6.3737e-04,
        2.4823e-01, 5.9324e-02, 8.4557e-01, 1.2081e-02, 2.1158e-02, 6.1652e-02,
        8.4094e-03, 1.9811e-03, 3.1258e-01, 5.1290e-04, 6.5486e-03, 1.9174e-02,
        6.8580e-03, 3.2884e-02, 3.5788e-02, 2.0691e+00, 2.2818e+00, 1.2316e+01,
        5.6205e+00, 1.1748e+01, 1.9830e+01, 1.3444e+01, 3.9912e-03, 2.6398e-02,
        6.8907e-03, 3.3767e-02, 4.3411e-04, 2.2705e-03, 2.9248e-02, 1.0119e-02,
        1.3672e-03, 2.3440e-02, 6.3049e+00, 1.6645e-01, 1.6155e+00, 9.3726e-03,
        9.4283e+00, 1.9559e-01, 1.0388e+00, 1.3661e-01, 1.2875e-02, 1.5659e-02,
        4.4932e-02, 1.5923e-01, 4.3460e+00, 1.5255e+00, 7.2155e-04, 1.8029e-01,
        5.5630e+00, 1.4512e+00, 1.5136e+01, 1.2088e+01, 3.4960e-01, 1.0186e+01,
        4.0895e-02, 3.2593e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([2.8063e+00, 3.8414e-01, 4.2038e+00, 2.6740e-01, 1.9419e-03, 1.2147e-01,
        5.4057e-03, 9.1486e-02, 8.1605e-02, 5.6747e-02, 1.3786e-02, 4.2535e-02,
        1.3022e-02, 1.5571e-01, 1.3204e-01, 3.0587e-01, 5.8430e+00, 5.8003e-02,
        1.2823e-02, 3.0246e-02, 3.4600e-04, 6.9027e-04, 4.3420e-04, 8.6502e-04,
        2.2306e-01, 5.7470e-02, 8.1093e-01, 1.3691e-02, 3.0091e-02, 7.4270e-02,
        2.8315e-03, 1.9941e-03, 3.0400e-01, 6.7900e-04, 5.8654e-03, 1.5448e-02,
        1.2233e-02, 2.7223e-02, 2.6832e-02, 1.7457e+00, 2.2931e+00, 1.5261e+01,
        5.2333e+00, 1.1165e+01, 1.9809e+01, 1.0904e+01, 2.2501e-03, 2.6124e-02,
        2.7257e-03, 2.7188e-02, 1.0811e-04, 1.0433e-03, 3.3417e-02, 2.0058e-03,
        6.1554e-04, 8.6536e-03, 5.7100e+00, 1.3430e-01, 1.5895e+00, 1.1695e-02,
        1.6248e+01, 8.2096e-02, 7.4801e-01, 1.9808e-01, 1.5421e-02, 2.6650e-02,
        5.9866e-02, 1.8439e-01, 3.7166e+00, 1.2575e+00, 7.7957e-04, 2.1532e-01,
        4.3641e+00, 1.4530e+00, 1.8774e+01, 1.1091e+01, 1.4138e-01, 1.1912e+01,
        5.4821e-02, 4.6478e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.9585e+01, 4.1406e+01, 5.5145e+01, 2.4431e+01, 5.3674e-01, 1.2286e+01,
        2.6016e+00, 1.6119e+01, 5.4725e+00, 9.0394e+00, 7.0779e+00, 1.1696e+01,
        9.3371e+00, 1.4205e+01, 9.4033e+00, 2.0069e+01, 3.1040e+02, 4.9928e+00,
        9.3496e-01, 1.9789e+00, 7.7404e-01, 3.0424e-01, 1.0153e-01, 2.9572e-01,
        2.7653e+01, 1.8025e+01, 2.6384e+01, 4.5116e+00, 1.4840e+01, 1.7311e+01,
        6.9722e-01, 9.7046e-01, 8.0553e+00, 1.2042e+00, 1.2178e+00, 7.9642e-01,
        3.8540e+00, 4.0953e+00, 1.9109e+00, 4.3797e+01, 1.2763e+01, 3.0364e+01,
        7.7487e+00, 1.8558e+01, 2.1557e+01, 1.4134e+01, 7.6900e-01, 5.4659e+00,
        4.1260e-01, 4.2347e+00, 9.8558e-02, 3.4800e-01, 7.7415e+00, 2.6926e-01,
        2.6262e-01, 8.7795e-01, 2.5433e+01, 5.7548e+00, 1.2003e+01, 2.4610e+00,
        2.0437e+01, 6.2584e+00, 4.2685e+00, 7.2547e+00, 4.5027e-01, 2.4034e+00,
        1.1395e+00, 8.8264e+00, 7.9998e+00, 1.5223e+01, 1.6578e-01, 2.1358e+01,
        1.1807e+01, 1.0577e+01, 2.0954e+01, 1.1095e+01, 1.4139e-01, 1.1916e+01,
        8.3085e-02, 5.6045e-01], device='cuda:0')
Outer loop valEpocw Maximum [21/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 159.7
model_train val_loss valEpocw [21/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 160.8
model_train val_loss valEpocw [21/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1117.7
Sum_Val Meta Model:  tensor([7.4476e+00, 3.7119e-01, 7.9622e+00, 3.0071e-01, 6.6010e-03, 1.1581e+00,
        9.0008e-01, 1.1005e+00, 1.3374e+00, 8.1829e-01, 1.6205e-01, 3.3444e+00,
        6.9969e-02, 1.4932e-01, 5.6454e-01, 7.6801e-01, 5.1833e-01, 6.0854e-01,
        3.3860e-01, 1.8037e+00, 3.7202e-04, 1.1523e-03, 6.0176e-03, 4.6884e-02,
        5.0128e-01, 3.2800e-01, 1.8606e+00, 2.2168e-01, 1.1069e-01, 1.5488e-03,
        2.1225e-03, 8.3635e-04, 1.4647e-02, 5.5359e-04, 1.0082e-03, 3.3039e-03,
        6.3864e-02, 2.9430e-03, 2.3460e-03, 1.1441e-01, 4.7491e-02, 2.0321e+00,
        1.0630e-01, 1.8750e-01, 3.2861e-01, 3.1895e+00, 6.2814e-03, 9.4309e-03,
        2.0625e-03, 1.2553e-01, 2.7842e-04, 1.7312e-03, 6.6257e-04, 1.0261e-03,
        1.9393e-03, 8.0375e-02, 3.7183e+00, 2.8917e-01, 1.3331e+00, 6.7450e-02,
        1.3591e+00, 1.3279e-02, 4.6017e-01, 9.1230e-02, 2.2077e-02, 3.0456e-02,
        3.8211e-02, 8.5076e-01, 7.6560e-02, 5.6348e-01, 7.4126e-04, 2.1516e-02,
        2.8762e+00, 9.9732e-01, 4.5587e+00, 3.8256e-01, 1.4890e-01, 7.5856e-01,
        2.8553e-02, 1.6707e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.4923e+00, 4.5221e-01, 5.6733e+00, 2.6901e-01, 3.7288e-02, 9.7593e-01,
        6.5707e-01, 7.7401e-01, 9.8217e-01, 5.6918e-01, 1.3205e-01, 8.2252e-01,
        6.0059e-02, 1.8598e-01, 5.6622e-01, 1.6974e-01, 4.0826e-01, 5.4330e-01,
        1.4011e-01, 7.1093e-01, 1.8902e-03, 2.1689e-03, 1.4428e-03, 4.7812e-02,
        4.9954e-01, 2.4419e-01, 1.8639e+00, 1.8704e-01, 1.2375e-01, 1.6714e-02,
        3.1927e-03, 2.4973e-03, 4.3861e-02, 1.1600e-02, 6.5000e-03, 5.2578e-03,
        2.7802e-02, 3.2262e-02, 7.5611e-03, 1.4079e-01, 2.5099e-02, 1.3752e+00,
        1.1056e-02, 1.0222e-01, 3.9627e-02, 5.2914e-01, 4.5381e-03, 6.7991e-03,
        4.4873e-03, 1.1877e-01, 5.6082e-04, 6.0640e-03, 3.5966e-03, 1.0955e-02,
        1.1507e-03, 2.9114e-02, 2.6484e+00, 2.2235e-01, 8.2055e-01, 4.0785e-03,
        3.1303e+00, 1.4775e-02, 1.5031e-01, 2.9489e-02, 1.0635e-02, 9.7839e-03,
        3.9820e-02, 8.6857e-01, 2.6749e-02, 4.5860e-01, 1.3685e-04, 1.0644e-02,
        1.3511e+00, 4.8100e-01, 6.3002e+00, 7.8840e-02, 1.8612e+00, 1.2598e+00,
        2.9195e-03, 2.2479e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.4820e+01, 1.9164e+01, 4.7140e+01, 9.4557e+00, 3.0394e+00, 3.4141e+01,
        5.8954e+01, 3.9738e+01, 2.6190e+01, 2.8658e+01, 1.6748e+01, 5.7538e+01,
        8.9033e+00, 7.6503e+00, 1.5717e+01, 4.2196e+00, 9.8926e+00, 1.5701e+01,
        3.4698e+00, 1.6059e+01, 5.9241e-01, 2.2382e-01, 9.4301e-02, 4.9140e+00,
        2.4843e+01, 1.8521e+01, 3.0011e+01, 1.4121e+01, 1.2380e+01, 1.1002e+00,
        2.2599e-01, 2.6544e-01, 5.6411e-01, 3.3632e+00, 4.1205e-01, 1.3362e-01,
        2.1250e+00, 1.5184e+00, 1.9416e-01, 2.1849e+00, 1.2498e-01, 2.8958e+00,
        1.9399e-02, 2.0808e-01, 4.6920e-02, 7.7605e-01, 3.9152e-01, 4.1233e-01,
        1.9979e-01, 5.5421e+00, 1.0286e-01, 6.1947e-01, 2.4634e-01, 4.4763e-01,
        1.1779e-01, 1.1750e+00, 9.2841e+00, 3.9936e+00, 5.3481e+00, 2.2941e-01,
        4.3180e+00, 4.3700e-01, 6.9017e-01, 4.8938e-01, 1.7291e-01, 2.9462e-01,
        4.8575e-01, 1.9570e+01, 6.4273e-02, 3.8511e+00, 8.6515e-03, 3.9028e-01,
        3.8119e+00, 2.5053e+00, 7.8278e+00, 7.9083e-02, 1.8618e+00, 1.2633e+00,
        5.1083e-03, 1.8648e-01], device='cuda:0')
Outer loop valEpocw Maximum [21/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 57.8
model_train val_loss valEpocw [21/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 45.0
model_train val_loss valEpocw [21/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 675.5
Sum_Val Meta Model:  tensor([5.0372e+00, 4.8264e-02, 1.0823e+00, 4.2240e-02, 5.6175e-03, 5.2024e-02,
        6.2442e-03, 9.6791e-02, 4.6456e-02, 2.0398e-02, 4.6267e-03, 8.7656e-03,
        1.2027e-03, 2.6271e-01, 5.8728e-02, 7.2893e-02, 2.2401e-01, 5.2755e-02,
        3.4619e-02, 6.2942e-02, 1.3353e-03, 6.8021e-03, 7.1346e-02, 1.0452e-02,
        7.0580e-02, 1.2081e-02, 8.1985e-01, 3.6322e-02, 5.7840e-03, 2.9132e-02,
        3.3334e-02, 2.6293e-02, 4.6314e-01, 4.2652e-04, 1.9121e-02, 7.3120e-02,
        1.0106e-01, 1.4675e-01, 1.1532e-02, 1.2295e+00, 5.4505e+00, 3.3224e+01,
        4.2089e+01, 4.2493e+01, 4.2112e+01, 5.9000e+01, 7.0019e-02, 8.8129e-02,
        7.3688e-01, 1.7966e-01, 7.8918e-02, 1.8983e-01, 4.8772e-01, 1.9140e-01,
        2.6270e-01, 1.3468e+00, 4.0349e+01, 3.8629e-01, 1.2081e+00, 2.2531e-02,
        6.8115e+01, 2.8248e-02, 2.0744e+00, 4.8958e-01, 5.5335e-01, 2.6829e-02,
        5.4480e-01, 2.7286e-01, 2.2833e+00, 2.7553e-01, 3.7443e-03, 1.0761e-01,
        1.6047e+00, 5.7184e+00, 1.5079e+00, 2.9826e+01, 1.0762e+01, 1.3935e+00,
        1.9826e-01, 8.0121e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.5842e+00, 6.6527e-03, 3.4237e-01, 4.6869e-03, 3.2829e-04, 6.6617e-04,
        2.0857e-04, 1.0371e-01, 2.8448e-03, 2.7187e-04, 1.5515e-04, 2.7981e-04,
        5.4353e-05, 2.2805e-01, 6.5648e-03, 9.6109e-03, 3.1440e-02, 5.1756e-03,
        2.3855e-03, 8.3761e-04, 2.7024e-05, 4.8277e-05, 1.1790e-04, 6.1165e-05,
        3.7040e-02, 4.2475e-03, 8.9140e-01, 2.3561e-02, 1.6629e-03, 1.3404e-03,
        3.2873e-03, 2.4992e-02, 4.2347e-01, 7.9796e-05, 1.2408e-02, 4.0690e-02,
        6.9297e-02, 1.0942e-01, 2.0586e-03, 1.6012e+00, 4.5367e+00, 3.2306e+01,
        4.3907e+01, 4.4569e+01, 5.3786e+01, 5.5202e+01, 4.1387e-02, 4.2269e-02,
        4.1073e-01, 7.9617e-02, 3.7282e-02, 1.8520e-01, 4.6344e-01, 2.6247e-01,
        2.7334e-01, 8.9133e-01, 2.1681e+01, 4.0317e-01, 1.1145e+00, 5.0628e-03,
        7.8920e+01, 4.3001e-03, 1.5512e+00, 3.8739e-01, 4.3891e-01, 7.7348e-02,
        5.6661e-01, 3.3127e-01, 2.0186e+00, 3.6706e-01, 5.5842e-04, 9.4172e-02,
        1.1150e+00, 5.2564e+00, 1.8295e+00, 2.5008e+01, 1.1704e+01, 5.4492e+00,
        3.5292e-02, 3.8592e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.4791e+01, 6.2773e-01, 4.1909e+00, 3.7561e-01, 7.4525e-02, 5.5602e-02,
        7.4640e-02, 1.5580e+01, 1.7188e-01, 3.3457e-02, 6.0021e-02, 5.9197e-02,
        3.0551e-02, 1.8057e+01, 4.1897e-01, 5.4245e-01, 1.3618e+00, 3.4412e-01,
        1.4339e-01, 4.5846e-02, 4.2533e-02, 1.6087e-02, 1.7791e-02, 1.6834e-02,
        3.9281e+00, 1.0792e+00, 2.8017e+01, 6.0466e+00, 6.1803e-01, 2.1701e-01,
        6.2111e-01, 7.6193e+00, 8.9218e+00, 8.9013e-02, 1.8567e+00, 1.7513e+00,
        1.4155e+01, 1.1286e+01, 1.1629e-01, 4.1115e+01, 2.4962e+01, 6.3274e+01,
        6.5125e+01, 7.5145e+01, 5.9180e+01, 7.3120e+01, 9.4999e+00, 6.2934e+00,
        3.7268e+01, 8.6986e+00, 2.0808e+01, 4.3505e+01, 7.0720e+01, 2.5175e+01,
        7.7150e+01, 6.9286e+01, 9.6033e+01, 1.4239e+01, 9.1233e+00, 7.4023e-01,
        1.0008e+02, 2.9061e-01, 8.7278e+00, 1.1890e+01, 1.2022e+01, 5.3337e+00,
        1.0910e+01, 1.4741e+01, 4.9995e+00, 4.7171e+00, 9.6416e-02, 8.1297e+00,
        3.4681e+00, 3.6971e+01, 2.1210e+00, 2.5026e+01, 1.1705e+01, 5.4525e+00,
        5.8377e-02, 4.4914e-01], device='cuda:0')
Outer loop valEpocw Maximum [21/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 406.2
model_train val_loss valEpocw [21/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 401.0
model_train val_loss valEpocw [21/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1311.1
Sum_Val Meta Model:  tensor([1.3075e+01, 3.7371e-02, 2.5422e+00, 2.0521e-02, 3.9024e-03, 2.8432e-01,
        5.7947e-02, 2.9001e-01, 4.1028e-02, 3.9890e-01, 4.0201e-02, 6.7337e-02,
        5.7477e-04, 5.3029e-01, 7.2021e-01, 4.2165e-02, 5.5700e-02, 2.3450e-02,
        8.3787e-03, 1.6668e-02, 9.0330e-04, 2.3136e-03, 3.4185e-03, 5.0137e-03,
        5.0119e-01, 1.2912e-02, 1.8023e+00, 1.3131e-02, 1.5396e-01, 4.6413e-03,
        9.2884e-03, 8.7592e-04, 2.4100e-01, 1.2396e-02, 2.2722e-02, 3.3060e-01,
        2.2908e-03, 1.1948e-02, 2.0580e-01, 1.8091e+00, 2.9864e+00, 1.4503e+01,
        6.1797e+00, 5.5244e+00, 1.1555e+01, 1.3808e+01, 2.9221e-03, 7.0905e-03,
        1.8769e-02, 5.0604e-03, 2.2287e-03, 2.9926e-03, 2.6232e-03, 1.1748e-01,
        5.1701e-03, 3.6511e-02, 5.7594e+00, 1.8472e-01, 2.4164e+00, 1.8582e-02,
        3.4902e+01, 1.2691e-02, 9.2951e-01, 3.8215e-01, 2.9089e-01, 6.5612e-02,
        4.5785e-01, 3.5487e+00, 4.1326e-01, 2.2841e-01, 8.8505e-04, 3.7668e-02,
        1.2611e+00, 1.9172e+00, 4.2152e+02, 9.8890e+00, 1.1677e+00, 1.0864e+01,
        2.1916e-02, 1.0059e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([4.0960e+00, 8.3084e-02, 2.0143e+00, 5.3927e-02, 1.9960e-02, 2.4030e-01,
        6.8462e-02, 2.7465e-01, 8.5066e-02, 3.3131e-01, 2.8670e-02, 1.2509e-01,
        6.5741e-03, 4.2146e-01, 6.0164e-01, 1.4425e-02, 3.2633e-02, 2.8503e-02,
        4.4849e-03, 4.6204e-03, 6.6814e-04, 1.9157e-04, 9.7502e-04, 5.0724e-03,
        4.9330e-01, 2.7212e-02, 1.5223e+00, 1.7395e-02, 1.7011e-01, 2.2398e-03,
        2.0979e-03, 2.4762e-03, 1.5354e-02, 6.1556e-04, 2.3598e-03, 3.9096e-03,
        3.8382e-03, 4.5655e-03, 1.4327e-02, 2.1449e-01, 1.8230e-01, 1.0571e+00,
        1.1338e-01, 6.3399e-01, 3.5314e-01, 1.7472e+00, 1.5436e-03, 2.6489e-03,
        2.0909e-03, 2.2249e-03, 2.8801e-04, 9.4621e-04, 7.8524e-04, 7.7536e-03,
        7.0949e-04, 3.2858e-02, 1.5724e+00, 3.2705e-02, 1.9586e+00, 6.8418e-03,
        1.0042e+01, 6.8093e-03, 1.8536e-01, 2.8157e-02, 1.5932e-02, 2.8037e-02,
        9.2979e-02, 3.7953e-01, 3.2389e-02, 4.8105e-02, 4.3456e-04, 1.8013e-02,
        5.8588e-02, 1.0567e+00, 6.4873e+01, 4.0596e+00, 1.4303e-01, 1.0627e+01,
        8.3288e-03, 3.7138e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.7152e+01, 3.6265e+00, 1.7625e+01, 1.9997e+00, 1.6290e+00, 9.4108e+00,
        7.7985e+00, 1.6067e+01, 2.5136e+00, 1.7011e+01, 3.6411e+00, 9.4304e+00,
        1.1262e+00, 1.5892e+01, 1.8981e+01, 4.3810e-01, 8.5637e-01, 8.9582e-01,
        1.3699e-01, 1.3554e-01, 2.4374e-01, 2.2653e-02, 6.9356e-02, 5.3906e-01,
        2.4875e+01, 2.3169e+00, 2.4882e+01, 1.4872e+00, 1.9754e+01, 1.5283e-01,
        1.5948e-01, 2.9322e-01, 1.8330e-01, 1.7268e-01, 1.5001e-01, 8.1156e-02,
        3.1538e-01, 2.3923e-01, 3.2578e-01, 2.9889e+00, 7.6279e-01, 2.0915e+00,
        1.8631e-01, 1.1985e+00, 4.1055e-01, 2.4914e+00, 1.5348e-01, 1.8968e-01,
        1.0903e-01, 1.2500e-01, 6.0215e-02, 1.0369e-01, 5.6954e-02, 3.0054e-01,
        7.7996e-02, 1.4021e+00, 5.4577e+00, 6.3521e-01, 1.1989e+01, 4.3783e-01,
        1.3378e+01, 2.0208e-01, 8.4115e-01, 4.4009e-01, 2.2693e-01, 8.7060e-01,
        1.0102e+00, 6.9534e+00, 7.7819e-02, 3.9639e-01, 2.8194e-02, 6.4920e-01,
        1.6008e-01, 5.5609e+00, 8.1318e+01, 4.0711e+00, 1.4308e-01, 1.0657e+01,
        1.6107e-02, 2.8177e-02], device='cuda:0')
Outer loop valEpocw Maximum [21/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 574.5
model_train val_loss valEpocw [21/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 110.5
model_train val_loss valEpocw [21/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 400.9
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.37222987 97.40012914 93.46621021 98.06167079 98.58676186 97.56581913
 98.30655085 95.28880009 98.12014961 96.92742535 98.6208745  98.95834602
 99.43835967 95.37530001 97.48662906 97.27707996 96.45593986 97.74734713
 98.88646581 98.43812819 98.69275472 99.31531049 99.52851452 98.87428272
 95.2351945  96.83848881 94.4408572  97.060221   98.07385388 98.29924099
 98.44787466 98.56726892 97.2137279  98.64645899 98.69275472 98.86819118
 97.48297414 98.47345914 99.00098683 93.46377359 98.05070601 95.1791523
 97.87526955 97.3672348  98.35162827 96.88356623 98.21517769 98.69153641
 98.1603538  98.75610677 98.78169126 98.62940266 99.03266286 98.46858591
 98.71102935 97.6596289  93.22376677 97.05291115 96.6703622  97.62429795
 96.53756655 98.71468428 97.65110074 97.44520656 98.79874758 97.69617817
 98.41010709 95.95765159 99.03144455 98.44909297 99.81481707 97.53901634
 99.00342345 96.16232746 98.58188862 98.97783896 99.60770458 99.30800063
 99.84527479 99.18251483]
Accuracy th:0.7 is [88.02037012 97.3392137  92.92040789 97.98613565 98.48564223 97.41840377
 98.12624115 95.01589893 98.00319197 96.72031286 98.58067031 98.9412897
 99.43104982 95.31925781 97.46104458 97.04438299 96.38162303 97.65110074
 98.82311375 98.36746628 98.55508583 99.28485277 99.45785261 98.72686736
 95.22301142 96.74589735 94.2873503  96.95422814 98.02877645 98.2297974
 98.20055799 98.61965619 96.80315786 98.53193796 98.70250119 98.81336728
 97.24296731 98.37721275 98.98758543 93.17868934 97.94593146 94.65527954
 97.54998112 97.06387593 97.99953704 96.40842582 98.14938902 98.64767729
 98.05557924 98.68788148 98.70737442 98.61112803 99.01438823 98.33335364
 98.70981104 97.65597398 92.71938695 96.71300301 96.55218626 97.39647421
 95.92475725 98.55874076 97.3526151  97.32581231 98.81702221 97.53292479
 98.61356465 95.97227129 98.87671934 98.35406489 99.81603538 97.32215738
 98.92301507 95.87724321 98.43569157 98.95225448 99.5955215  99.35795129
 99.84405648 99.15936697]
Avg Prec: is [96.6908263  34.88402191 73.32413453 66.14872365 77.72467805 63.76751632
 74.2512311  47.50372062 58.81742227 52.30970766 29.85664898 53.32904494
 24.08040924 27.92976673 32.55807628 56.9493917  30.05730338 42.21260188
 47.94226601 38.70185576 62.46906963 49.78834535 89.5853658  83.34190597
 25.29746631 33.09253449 38.43329755 40.2032046  24.99509796 38.70626726
 73.30759061 37.69332375 59.70687292 64.0996938  73.27535546 79.34056505
 58.50345775 75.23004757 87.60137855 47.42126409 44.16351272 74.88985853
 72.65353416 67.98318801 81.23067384 82.02711477 38.20361162 34.20532699
 41.81373678 45.72453736 61.88122852 37.85884558 23.55064465 73.26129745
 27.33072854 43.41301203 73.59800574 59.49869075 45.5778054  60.36531004
 91.29971753 83.72908305 73.11051903 50.87576648 59.31461243 45.38832823
 59.42491588 25.58135569 62.85872343 69.59090137 10.79644812 73.3363375
 80.84341844 51.72322726 86.39760136 89.74644407 77.21346537 87.19149538
 13.64756988 29.44297251]
Accuracy th:0.5 is [45.35397961 97.2137279  71.75473008 97.02489005 97.26733349 76.51466235
 76.87162681 76.05901488 77.74393587 96.45228494 78.06678769 98.52097319
 99.41399349 80.26827159 77.68667536 96.56680596 96.29512311 77.31996443
 98.65376884 98.30776915 80.5204615  78.56751258 98.38695922 78.58335059
 81.23926365 96.65086926 94.0778012  77.11894348 98.01293844 77.90840755
 97.30875598 98.57457877 96.36213009 98.02024829 88.05570108 77.5538797
 77.30168979 91.38777549 97.11504489 74.75176959 79.36184988 92.05906361
 76.84847894 76.47323985 96.9627563  93.87434364 98.02877645 98.57336046
 97.57800222 88.95481293 87.86320829 98.55508583 98.99976852 77.01782386
 98.70615611 77.36626016 71.99595521 93.45524543 96.24273583 96.9067141
 89.79300934 97.17717864 93.50032285 77.45032346 98.42838172 77.82434425
 98.20786784 76.57192286 78.4590831  97.55850928 79.01706851 95.99054592
 78.26902694 95.45083515 76.77781703 83.07769155 88.22382768 77.96323144
 79.05970931 99.14718388]
Accuracy th:0.7 is [45.43073306 97.2137279  71.75473008 97.02489005 97.26733349 76.51466235
 76.87162681 76.31120479 77.74393587 96.4754328  78.06678769 98.52097319
 99.41399349 80.7166092  77.68667536 96.56680596 96.29512311 77.31996443
 98.65376884 98.30776915 80.89935551 78.56751258 98.38695922 79.22052607
 81.91177008 96.65086926 94.0778012  77.11894348 98.01293844 77.90840755
 97.30875598 98.57457877 96.36213009 98.02024829 88.26281356 77.5538797
 77.30168979 91.64727525 97.11504489 74.75176959 80.0051169  92.05906361
 76.84847894 76.47323985 96.9627563  93.87434364 98.02877645 98.57336046
 97.91547374 89.5225448  88.06057431 98.55508583 98.99976852 77.01782386
 98.70615611 77.36626016 71.99595521 93.88409011 96.24273583 96.9067141
 89.79300934 97.17717864 93.64164667 77.45032346 98.42838172 77.82434425
 98.20786784 76.57192286 78.4590831  97.55972759 79.02072343 95.99054592
 78.36283671 95.45083515 76.77781703 83.18855764 88.4345951  77.96323144
 79.05970931 99.14718388]
Avg Prec: is [55.92632564  3.24552326 11.30362558  3.39713391  2.30899382  3.88798948
  3.15753004  5.65384989  2.53280448  3.88134416  1.52041294  1.60610043
  0.6272068   4.99468893  2.62304668  3.17354395  3.51193032  2.75534857
  1.39444442  1.75175576  1.93744919  0.84584735  1.88396278  2.46368177
  5.19154603  3.7423712   6.53521672  3.39223099  2.11163183  1.93808018
  2.49388791  1.31724012  3.70582859  1.60465424  2.34613522  2.46947964
  3.00867767  2.54557807  2.77147667  7.3473996   2.27796343  8.28345652
  3.3683648   4.01586347  3.19151413  6.25993987  2.07397735  1.46166676
  2.07259737  1.54291743  1.7524183   1.51168197  1.02924313  2.97713567
  1.33066988  2.61359881 11.28949882  3.6615433   3.80767554  2.87609727
 10.8671634   2.1800528   3.85107042  3.04336914  1.64435367  2.54229506
  1.88769965  4.26982624  1.27360811  2.34811525  0.19004528  3.33822792
  1.89332515  4.53128469  3.89010769  3.17375869  0.88940676  1.86957401
  0.14860647  0.76269568]
mAP score regular 56.02, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [87.33338316 97.38395994 93.38266437 98.25846476 98.93116077 97.74273115
 98.5026285  95.36088896 98.19617809 97.03266313 98.64713357 99.05075118
 99.38460772 95.21140095 97.378977   97.21703167 96.41976231 97.74023968
 98.97351571 98.39798689 98.93365224 99.36218452 99.60883972 99.12549518
 95.43812442 96.77105912 94.46396093 97.229489   97.89720208 98.18870369
 98.67703117 98.58733837 97.03266313 98.73682637 98.92617784 99.19276478
 97.87727035 98.34068316 99.09808905 93.23317637 97.99187782 92.88187956
 97.43378927 96.57921618 96.98283379 94.74549667 98.38054663 98.8165533
 98.14136582 98.76921544 98.85890824 98.63218477 98.89378877 98.4204101
 98.73931784 97.63061514 90.92856965 97.17965967 96.48454045 97.59573461
 93.06375663 98.82901064 97.43378927 97.56085407 98.79911304 97.53843087
 98.32822583 95.82430177 98.79911304 98.33320876 99.81563146 97.55337967
 98.43037596 95.82430177 97.32167327 97.10491566 99.04825971 98.61723597
 99.82310586 99.15290131]
Accuracy th:0.7 is [88.33495279 97.41136607 93.18334704 98.32075143 98.91122904 97.57081994
 98.38054663 95.36836336 98.14884022 96.77853352 98.61972743 99.03331091
 99.37464185 95.12170815 97.47614421 96.91307273 96.36495005 97.71532501
 98.97102424 98.41542716 98.81157037 99.34225278 99.57146772 98.97600718
 95.45058176 96.66641752 94.51628174 97.10740713 97.85235568 98.22856716
 98.50761143 98.6969629  96.85078606 98.63965917 98.92368637 99.09559758
 97.60071754 98.22856716 99.05822558 93.16590677 97.92460822 93.1708897
 97.341605   96.68634925 97.04512046 94.74549667 98.33071729 98.87634851
 98.06662182 98.7418093  98.78914717 98.64713357 98.88133144 98.40296983
 98.69447144 97.85484715 91.07805765 97.0575778  96.42723671 97.43129781
 93.13850064 98.68450557 97.24942073 97.45621247 98.81406184 97.60819194
 98.5773725  95.83177617 98.7866557  98.23604156 99.81563146 97.56832847
 98.4951541  95.84921643 97.39641727 97.35157087 99.15788425 98.74430077
 99.82559733 99.16535865]
Avg Prec: is [96.57143211 34.28405054 71.7634276  73.40297056 77.33235337 65.12470203
 80.07714528 48.27589671 62.95131973 55.88842422 36.67181814 56.9658278
 25.009049   31.06988258 34.35658293 61.80224421 32.97496551 45.41573996
 50.72519889 37.91825174 71.28634426 58.06507982 92.15278476 87.45206315
 24.75055617 36.60556282 33.40855808 45.00781934 28.23905095 37.70743711
 78.2986877  37.66635044 55.69715842 65.43597277 74.36152384 83.06940017
 60.85616273 77.23129683 89.57782949 44.56744023 37.10341093 50.20511895
 50.82769619 41.61420103 32.98797851 50.878854   38.76771922 29.62900136
 41.84919267 41.28219051 66.91283482 36.22255136 24.84094563 76.0362169
 28.86739978 42.21615685 57.09340681 58.07927243 39.59911672 62.91107722
 66.50250507 87.31541271 67.42410954 54.07820139 62.71115887 39.33796699
 62.49070008 26.18609562 43.33766715 66.9798959   9.48246059 74.5104089
 57.85180033 44.82257817 63.88106367 49.06900667 11.83788157 56.64128462
  2.30939128 22.87455474]
Accuracy th:0.5 is [45.26247602 97.22450607 70.04509555 96.96290206 97.90716795 75.48645888
 75.56867728 74.58454792 77.06604878 96.41976231 77.20557092 98.5325261
 99.34972718 78.06761841 77.13830132 96.31262924 96.21047911 76.56028104
 98.78167277 98.34068316 78.93714029 77.8508608  98.31327703 79.0941027
 78.10000747 96.52938685 94.3393876  76.63751651 97.81747515 77.16321599
 97.52597354 98.67204823 96.39983058 98.18870369 89.18703441 76.78949598
 76.76208984 92.52310835 97.0276802  73.94174951 77.53195306 92.37362035
 75.92246556 75.58611755 97.03764606 94.02795426 98.18621222 98.77668984
 97.87727035 88.8207888  86.49625034 98.55993223 98.87385704 75.89755089
 98.6969629  76.59516157 70.69785983 94.4066572  96.16314124 96.78102499
 90.13379176 97.04761193 93.83361985 76.63502504 98.32075143 77.47215786
 98.13139996 75.81284102 77.88324987 97.53593941 78.29185041 96.07843137
 77.649052   95.44559882 75.71069088 83.93751401 90.22597603 77.23546852
 78.36659441 99.15040985]
Accuracy th:0.7 is [45.45930189 97.22450607 70.04509555 96.96290206 97.90716795 75.48645888
 75.56867728 74.72905299 77.06604878 96.41976231 77.20557092 98.5325261
 99.34972718 78.45379575 77.13830132 96.31262924 96.21047911 76.56028104
 98.78167277 98.34068316 79.25355657 77.8508608  98.31327703 79.48526297
 78.54099708 96.52938685 94.3393876  76.63751651 97.81747515 77.16321599
 97.52597354 98.67204823 96.39983058 98.18870369 89.37389441 76.78949598
 76.76208984 92.74484889 97.0276802  73.94174951 78.02775494 92.37362035
 75.92246556 75.58611755 97.03764606 94.02795426 98.18621222 98.77668984
 97.94453995 89.14467947 86.67812741 98.55993223 98.87385704 75.89755089
 98.6969629  76.59516157 70.69785983 94.73054787 96.16314124 96.78102499
 90.13379176 97.04761193 93.95321025 76.63502504 98.32075143 77.47215786
 98.13139996 75.81284102 77.88324987 97.53593941 78.29185041 96.07843137
 77.68393253 95.44559882 75.71069088 84.02969828 90.4377507  77.23546852
 78.36659441 99.15040985]
Avg Prec: is [54.4472644   3.72235819 14.77136972  4.53509549  1.51040288  4.27213839
 12.49814218  8.65934099  7.7646577   5.15944905  2.30031494  5.02824025
  1.53740006  5.88309747  3.06765071  3.64664827 24.42921921  6.40607388
  1.54793435  2.66169912  3.57514332  1.45276212  1.27592852  5.61398862
  5.71423276 10.39174551  8.05107754  4.5596744   3.92929801  6.37230324
  2.29260169  0.87950387  2.97427811  1.15597127  1.72198842  2.40317479
  2.02470107  2.15374938  2.16698567  6.20953853  1.7307456   6.00372206
  2.19068186  2.72268495  2.38677493  4.91753611  1.74826074  1.05954634
  1.44923734  1.19932957  1.20755197  1.06768062  0.76855397  2.28232626
  0.90528415  1.8731806  10.17452212  2.98892617  3.99162216  2.83937621
  7.90600168  2.03996942  3.35942762  2.50614391  1.35006154  1.8991213
  1.54621096  3.51578514  1.06478992  2.21383316  0.19553275  3.19913021
  1.55068935  4.01032203  3.19459987  2.31003026  0.57026826  1.45096381
  0.121242    0.61010629]
mAP score regular 51.72, mAP score EMA 4.34
Train_data_mAP: current_mAP = 56.02, highest_mAP = 56.02
Val_data_mAP: current_mAP = 51.72, highest_mAP = 51.82
tensor([5.4657e-02, 8.6879e-03, 7.4052e-02, 9.7861e-03, 3.3516e-03, 8.8663e-03,
        2.0081e-03, 5.3886e-03, 1.3772e-02, 6.1313e-03, 1.8936e-03, 3.5894e-03,
        1.3213e-03, 1.0292e-02, 1.2926e-02, 1.4564e-02, 1.7860e-02, 1.1497e-02,
        1.3326e-02, 1.4693e-02, 4.3421e-04, 2.1284e-03, 4.1394e-03, 2.6965e-03,
        8.2512e-03, 3.0763e-03, 3.1705e-02, 2.9066e-03, 1.9865e-03, 4.2853e-03,
        3.9476e-03, 1.9830e-03, 3.7562e-02, 5.8335e-04, 4.4796e-03, 1.9966e-02,
        3.1612e-03, 6.1581e-03, 1.4686e-02, 3.6382e-02, 1.8286e-01, 4.9221e-01,
        7.0865e-01, 6.1223e-01, 9.2722e-01, 7.8151e-01, 2.7981e-03, 4.1452e-03,
        5.8435e-03, 5.6923e-03, 1.0774e-03, 2.9508e-03, 4.3190e-03, 7.4387e-03,
        2.3710e-03, 9.2621e-03, 2.1547e-01, 2.1190e-02, 1.3032e-01, 4.5292e-03,
        8.0362e-01, 1.1865e-02, 1.6768e-01, 2.7795e-02, 3.4695e-02, 1.0681e-02,
        5.1251e-02, 1.9432e-02, 4.7967e-01, 8.1414e-02, 4.5062e-03, 9.5003e-03,
        3.7406e-01, 1.3037e-01, 9.0140e-01, 9.9959e-01, 9.9998e-01, 9.9964e-01,
        6.7636e-01, 8.5661e-02], device='cuda:0')
Sum Train Loss:  tensor([1.9634e+00, 5.7047e-02, 1.7990e+00, 9.6160e-02, 2.3025e-02, 8.5774e-02,
        1.9025e-02, 1.3064e-01, 6.1137e-02, 3.2408e-02, 1.0589e-02, 1.7988e-02,
        1.6685e-03, 2.8998e-01, 1.1252e-01, 1.1037e-01, 3.0949e-01, 9.1737e-02,
        1.7848e-02, 1.8468e-01, 2.5053e-03, 3.9078e-03, 6.6005e-04, 6.4705e-03,
        2.9626e-01, 2.4076e-02, 1.2394e+00, 1.7647e-02, 9.2431e-03, 7.1797e-02,
        4.2795e-02, 2.9272e-02, 6.2851e-01, 1.4041e-03, 3.7517e-02, 5.8458e-02,
        5.3155e-02, 1.6764e-02, 3.6511e-02, 8.8259e-01, 1.7648e+00, 8.8833e+00,
        5.1983e+00, 7.0791e+00, 7.7768e+00, 2.5914e+01, 5.2099e-02, 4.3864e-02,
        5.9855e-02, 3.4612e-02, 4.4376e-03, 1.3587e-02, 3.1303e-02, 3.1947e-02,
        2.2033e-02, 8.0265e-02, 6.2069e+00, 4.4909e-01, 1.3009e+00, 3.3055e-02,
        1.7111e+01, 3.4876e-02, 2.6186e+00, 2.8589e-01, 1.9581e-01, 1.8115e-01,
        1.4907e-01, 1.9017e-01, 1.0412e+00, 5.1989e-01, 5.4522e-04, 1.5329e-01,
        3.8568e+00, 3.0186e+00, 8.7209e+00, 3.2149e+00, 5.3765e-01, 1.1529e+00,
        1.8252e+00, 2.2666e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [22/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 118.9
Sum Train Loss:  tensor([2.2962e+00, 1.8282e-01, 1.8197e+00, 1.2578e-01, 2.1666e-02, 1.0773e-01,
        1.5481e-02, 5.1813e-02, 1.2162e-01, 3.5477e-02, 7.2182e-03, 2.3207e-02,
        7.3453e-03, 2.6800e-01, 1.0766e-01, 1.4234e-01, 4.5143e-01, 1.7853e-01,
        3.5696e-02, 8.9111e-02, 2.2454e-03, 2.7055e-03, 1.2463e-02, 1.5835e-02,
        1.1259e-01, 2.3968e-02, 5.0082e-01, 6.0710e-02, 1.5705e-02, 1.9484e-02,
        1.3611e-02, 2.7691e-03, 3.2721e-01, 5.3520e-03, 1.3311e-02, 3.0778e-02,
        4.7068e-02, 2.0111e-02, 4.1826e-02, 3.1842e-01, 6.9252e-01, 6.3483e+00,
        2.7081e+00, 4.6211e+00, 4.9739e+00, 6.3987e+00, 9.8604e-03, 1.8891e-02,
        1.9463e-02, 1.7031e-02, 4.9649e-03, 5.9324e-03, 3.2967e-03, 2.9660e-02,
        7.4024e-03, 7.0034e-02, 3.7928e+00, 1.8645e-01, 1.6006e+00, 2.1980e-02,
        9.8208e+00, 3.7582e-02, 1.0779e+00, 4.6479e-01, 5.1397e-02, 1.5798e-01,
        8.9573e-01, 2.6036e-01, 1.5019e+00, 9.0877e-01, 2.4512e-02, 5.9222e-02,
        1.8103e+00, 9.0567e-01, 3.2704e+00, 4.3114e+00, 2.1768e+00, 3.7913e+00,
        8.4881e-02, 1.1555e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [22/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 70.9
Sum Train Loss:  tensor([2.2002e+00, 1.2541e-01, 1.4388e+00, 5.0434e-02, 1.2629e-02, 7.4549e-02,
        1.0042e-02, 7.1798e-02, 7.1484e-02, 5.0993e-02, 4.0702e-03, 1.7980e-02,
        1.7953e-03, 1.6683e-01, 1.7730e-01, 1.7385e-01, 6.0600e-01, 4.5702e-02,
        3.2415e-02, 5.9927e-02, 5.3149e-03, 7.6208e-03, 1.2304e-02, 6.4613e-03,
        1.9700e-01, 1.7726e-02, 6.5801e-01, 4.0151e-02, 2.1197e-02, 5.5850e-02,
        1.7279e-02, 8.2489e-03, 6.3143e-01, 8.3464e-03, 4.4578e-02, 1.2790e-01,
        5.6062e-02, 2.8692e-02, 4.6163e-02, 8.3415e-01, 3.3103e+00, 1.4466e+01,
        2.4924e+00, 4.0563e+00, 7.2624e+00, 5.4429e+00, 3.7093e-02, 3.7472e-02,
        2.3434e-02, 9.1727e-02, 2.8438e-03, 1.7159e-02, 1.7056e-02, 3.0911e-02,
        1.0360e-02, 6.9798e-02, 4.8338e+00, 3.0464e-01, 1.0353e+00, 4.9232e-02,
        1.4654e+01, 9.4816e-03, 1.8615e+00, 1.2633e-01, 1.0125e-01, 4.8659e-02,
        4.4436e-01, 4.0124e-01, 4.5278e+00, 5.4923e-01, 1.6414e-03, 2.9629e-02,
        4.3679e+00, 2.0720e+00, 6.0501e+00, 9.2945e+00, 3.9411e-01, 2.9906e+00,
        5.0968e-02, 8.1955e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [22/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 100.6
Sum Train Loss:  tensor([2.1726e+00, 1.3079e-01, 2.5435e+00, 6.6781e-02, 2.8217e-02, 1.0737e-01,
        1.0443e-02, 6.2257e-02, 9.8594e-02, 3.5381e-02, 1.1042e-02, 6.1880e-02,
        5.5308e-03, 2.0757e-01, 1.0702e-01, 1.2114e-01, 3.2840e-01, 2.7876e-01,
        2.3275e-02, 6.0304e-02, 1.0643e-03, 4.9207e-04, 1.9661e-02, 9.5296e-03,
        1.9032e-01, 4.5202e-02, 5.6369e-01, 4.9099e-02, 1.2214e-02, 4.8636e-02,
        4.5124e-02, 1.0857e-02, 4.6470e-01, 2.2335e-03, 6.6959e-02, 1.8879e-01,
        2.1320e-02, 1.2253e-02, 2.7777e-02, 8.9802e-01, 1.8980e+00, 8.9122e+00,
        6.2721e+00, 1.2156e+01, 1.9527e+00, 8.0341e+00, 1.6689e-02, 4.6550e-02,
        5.2598e-02, 1.0295e-01, 5.4140e-03, 3.3247e-03, 1.3868e-02, 1.6187e-02,
        1.0674e-02, 7.2055e-02, 3.2024e+00, 1.1237e-01, 2.3652e+00, 3.2149e-02,
        9.8354e+00, 3.1733e-02, 1.1465e+00, 1.1094e-01, 5.3076e-02, 5.2337e-02,
        7.7638e-02, 2.0409e-01, 9.2652e-01, 4.1866e-01, 9.9901e-04, 9.5088e-02,
        7.0621e-01, 7.9061e-01, 2.3081e+00, 7.0490e-01, 2.5170e-01, 4.1014e+00,
        2.5267e+00, 1.7313e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [22/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 78.9
Sum Train Loss:  tensor([1.5480e+00, 6.7146e-02, 1.5003e+00, 5.7641e-02, 4.9823e-03, 4.2368e-02,
        6.7410e-03, 4.5287e-02, 9.3572e-02, 5.1453e-02, 3.3301e-03, 1.4147e-02,
        7.7379e-04, 2.1209e-01, 7.9627e-02, 1.1687e-01, 1.4556e-01, 1.1789e-01,
        1.5894e-01, 1.3569e-01, 2.4772e-03, 6.3979e-03, 4.8403e-03, 8.6719e-03,
        1.4549e-01, 4.5001e-02, 4.1364e-01, 2.8516e-02, 2.5225e-02, 1.8270e-02,
        1.0934e-02, 4.1656e-03, 2.8110e-01, 2.5217e-03, 2.2875e-02, 2.1364e-02,
        8.3693e-03, 6.9060e-02, 3.5574e-02, 9.2059e-01, 1.3215e+00, 9.3045e+00,
        9.6696e+00, 6.7267e+00, 7.0866e+00, 7.0734e+00, 2.5661e-02, 8.8473e-03,
        3.6755e-02, 9.5814e-03, 1.1320e-02, 2.3919e-02, 1.1646e-02, 3.8162e-02,
        1.7623e-02, 1.8099e-01, 5.5303e+00, 3.1031e-01, 1.3441e+00, 8.6118e-02,
        8.3274e+00, 1.0797e-01, 1.9970e+00, 2.9990e-01, 3.9011e-01, 1.0061e-01,
        3.6665e-01, 5.2231e-01, 3.3100e+00, 5.4729e-01, 1.4987e-03, 6.2794e-02,
        4.5444e-01, 2.6157e+00, 4.4339e+00, 8.7588e+00, 2.2478e+00, 9.2863e+00,
        3.9491e+00, 2.5993e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [22/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 103.3
Sum Train Loss:  tensor([1.8112e+00, 1.0694e-01, 1.9002e+00, 3.6701e-02, 2.1386e-03, 6.3464e-02,
        9.9804e-03, 1.1649e-01, 8.5302e-02, 3.1494e-02, 1.5075e-02, 1.1641e-02,
        3.5960e-03, 2.4276e-01, 1.5958e-01, 6.1391e-02, 3.9897e-01, 8.2417e-02,
        1.3882e-02, 1.2564e-01, 1.5330e-03, 1.1986e-02, 2.1927e-02, 1.7773e-02,
        1.7897e-01, 4.5275e-02, 6.1045e-01, 1.9121e-02, 1.9047e-02, 5.4121e-02,
        1.2126e-02, 1.8950e-03, 2.5487e-01, 1.1191e-03, 2.8924e-02, 1.0050e-01,
        3.7265e-02, 1.4612e-02, 3.9328e-02, 1.0828e+00, 7.1377e-01, 8.2348e+00,
        5.1454e+00, 4.5805e+00, 7.7980e+00, 6.0807e+00, 8.9462e-03, 1.0362e-02,
        2.1899e-02, 1.7053e-02, 3.7683e-03, 3.6987e-02, 2.3136e-02, 3.2645e-02,
        6.3439e-03, 5.6033e-02, 3.8407e+00, 1.4556e-01, 1.0452e+00, 2.0906e-02,
        7.0880e+00, 7.0900e-02, 1.1136e+00, 1.8394e-01, 1.3146e-01, 1.0035e-01,
        3.2834e-01, 3.1831e-01, 6.1716e-01, 2.4749e-01, 9.5538e-04, 4.9711e-02,
        3.4374e-01, 1.7281e+00, 7.3512e+00, 3.8964e+00, 8.9757e+00, 1.9645e+00,
        2.8161e+00, 2.2680e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [22/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 83.2
Sum Train Loss:  tensor([2.1550e+00, 1.7823e-01, 2.0642e+00, 1.0233e-01, 5.4497e-03, 1.0084e-01,
        1.1399e-02, 1.3503e-01, 8.6078e-02, 8.6503e-02, 5.7870e-03, 3.8770e-02,
        3.9972e-03, 8.5633e-02, 1.2184e-01, 2.0546e-01, 3.0275e-01, 1.0542e-01,
        2.1184e-02, 1.1536e-01, 2.6769e-03, 1.8191e-03, 1.5512e-02, 6.9313e-03,
        2.3017e-01, 5.1386e-02, 7.4695e-01, 2.1837e-02, 1.9226e-02, 2.4125e-02,
        5.9717e-03, 1.0678e-02, 7.2033e-01, 4.2753e-03, 4.5425e-02, 1.6981e-01,
        5.5564e-02, 4.2810e-02, 2.3531e-02, 5.4133e-01, 1.8212e+00, 6.0762e+00,
        3.5097e+00, 3.4857e+00, 7.2631e+00, 6.4468e+00, 1.5668e-02, 1.8180e-02,
        6.3528e-02, 1.4691e-02, 6.1838e-03, 3.1960e-03, 1.2214e-02, 3.8519e-02,
        3.6902e-03, 1.3645e-01, 3.3011e+00, 1.1624e-01, 1.0476e+00, 2.5081e-02,
        4.4882e+00, 3.0900e-02, 1.1731e+00, 4.5341e-01, 2.5341e-01, 9.9153e-02,
        4.5047e-01, 3.9479e-01, 9.5177e-01, 3.5773e-01, 2.4556e-04, 4.3361e-02,
        1.1232e+00, 2.0572e+00, 5.2332e+00, 6.5053e+00, 7.3866e-01, 8.3387e+00,
        1.0243e-01, 9.8129e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [22/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 75.0
Sum_Val Meta Model:  tensor([2.7590e+00, 5.4978e-01, 5.9040e+00, 4.5001e-01, 4.7930e-03, 1.1285e-01,
        6.6081e-03, 9.4400e-02, 8.4147e-02, 6.1015e-02, 1.2252e-02, 3.9176e-02,
        1.1423e-02, 1.6458e-01, 1.0696e-01, 1.3355e-01, 8.7998e+00, 3.0312e-02,
        1.1871e-02, 1.6888e-02, 5.9654e-04, 6.2448e-04, 1.4277e-03, 1.4585e-03,
        2.5058e-01, 5.6918e-02, 8.5850e-01, 1.2601e-02, 2.0631e-02, 5.6025e-02,
        3.5932e-03, 1.0003e-03, 2.7015e-01, 4.1735e-04, 6.0955e-03, 2.8889e-02,
        9.1182e-03, 2.7695e-02, 3.7407e-02, 1.7765e+00, 1.9365e+00, 1.0927e+01,
        4.4322e+00, 1.1526e+01, 1.5420e+01, 1.3664e+01, 3.4050e-03, 2.3768e-02,
        5.5693e-03, 3.0252e-02, 3.9092e-04, 2.6547e-03, 2.5729e-02, 1.4046e-02,
        3.2567e-03, 1.7870e-02, 6.7674e+00, 1.5659e-01, 1.9612e+00, 1.1879e-02,
        1.3235e+01, 1.1627e-01, 9.4179e-01, 1.5255e-01, 1.4753e-02, 1.5682e-02,
        4.2069e-02, 1.2926e-01, 4.5146e+00, 1.5385e+00, 9.1420e-04, 1.4760e-01,
        4.9006e+00, 1.4001e+00, 2.1376e+01, 7.5632e+00, 3.7897e-01, 1.0242e+01,
        5.7483e-02, 1.0941e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([2.8117e+00, 3.5349e-01, 4.1090e+00, 2.4292e-01, 8.7170e-04, 1.1034e-01,
        6.8516e-03, 9.3527e-02, 7.2379e-02, 6.6053e-02, 1.5965e-02, 4.6745e-02,
        1.3404e-02, 1.7891e-01, 1.0672e-01, 5.0261e-01, 5.5220e+00, 4.2226e-02,
        8.1000e-03, 1.7582e-02, 6.1873e-04, 4.4673e-04, 5.3731e-04, 2.0658e-03,
        2.2574e-01, 4.9659e-02, 8.6363e-01, 1.7485e-02, 2.9417e-02, 6.4226e-02,
        1.7137e-03, 8.5092e-04, 2.4770e-01, 7.0028e-04, 4.7817e-03, 1.6995e-02,
        1.6681e-02, 2.3345e-02, 1.4636e-02, 1.5981e+00, 1.9813e+00, 1.3118e+01,
        2.8908e+00, 1.0686e+01, 2.1363e+01, 1.3881e+01, 1.8203e-03, 2.6548e-02,
        1.8458e-03, 2.7153e-02, 5.9986e-05, 7.4947e-04, 2.3871e-02, 2.5148e-03,
        1.5988e-03, 5.8691e-03, 6.7151e+00, 1.3545e-01, 1.7187e+00, 2.0603e-02,
        1.4793e+01, 4.5786e-02, 5.3049e-01, 1.7711e-01, 5.7002e-03, 2.0723e-02,
        1.7167e-02, 1.4820e-01, 5.4909e+00, 1.4952e+00, 2.4074e-03, 1.6885e-01,
        3.8780e+00, 1.5182e+00, 2.7259e+01, 1.5561e+01, 8.3234e-02, 1.0962e+01,
        7.8639e-02, 1.8735e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.1443e+01, 4.0687e+01, 5.5488e+01, 2.4823e+01, 2.6009e-01, 1.2445e+01,
        3.4120e+00, 1.7356e+01, 5.2557e+00, 1.0773e+01, 8.4307e+00, 1.3023e+01,
        1.0145e+01, 1.7383e+01, 8.2561e+00, 3.4510e+01, 3.0918e+02, 3.6729e+00,
        6.0783e-01, 1.1966e+00, 1.4249e+00, 2.0990e-01, 1.2980e-01, 7.6611e-01,
        2.7359e+01, 1.6142e+01, 2.7239e+01, 6.0155e+00, 1.4808e+01, 1.4987e+01,
        4.3411e-01, 4.2910e-01, 6.5945e+00, 1.2004e+00, 1.0674e+00, 8.5121e-01,
        5.2769e+00, 3.7910e+00, 9.9661e-01, 4.3926e+01, 1.0835e+01, 2.6650e+01,
        4.0793e+00, 1.7454e+01, 2.3039e+01, 1.7762e+01, 6.5054e-01, 6.4047e+00,
        3.1587e-01, 4.7701e+00, 5.5676e-02, 2.5398e-01, 5.5269e+00, 3.3806e-01,
        6.7430e-01, 6.3366e-01, 3.1166e+01, 6.3923e+00, 1.3188e+01, 4.5488e+00,
        1.8408e+01, 3.8590e+00, 3.1637e+00, 6.3719e+00, 1.6429e-01, 1.9402e+00,
        3.3495e-01, 7.6265e+00, 1.1447e+01, 1.8365e+01, 5.3425e-01, 1.7773e+01,
        1.0367e+01, 1.1646e+01, 3.0241e+01, 1.5567e+01, 8.3236e-02, 1.0966e+01,
        1.1627e-01, 2.1871e+00], device='cuda:0')
Outer loop valEpocw Maximum [22/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 156.6
model_train val_loss valEpocw [22/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 172.5
model_train val_loss valEpocw [22/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1147.9
Sum_Val Meta Model:  tensor([5.6582e+00, 2.8096e-01, 6.3896e+00, 2.0539e-01, 5.0533e-03, 8.5943e-01,
        4.6997e-01, 8.6900e-01, 9.1885e-01, 5.5987e-01, 1.1881e-01, 2.4326e+00,
        4.2295e-02, 1.2115e-01, 5.3695e-01, 6.2649e-01, 4.1368e-01, 4.8447e-01,
        1.4854e-01, 9.1835e-01, 7.1313e-05, 1.7658e-04, 1.3210e-03, 3.1745e-02,
        3.8099e-01, 2.3842e-01, 1.5675e+00, 1.6541e-01, 7.8678e-02, 4.5898e-04,
        6.6135e-04, 2.0206e-04, 4.4170e-03, 2.4556e-04, 2.0934e-04, 9.5687e-04,
        4.3108e-02, 1.0354e-03, 8.0616e-04, 4.6338e-02, 1.1882e-02, 1.8980e+00,
        3.3076e-02, 8.5786e-02, 1.4051e-01, 3.5710e+00, 3.3709e-03, 3.6515e-03,
        4.6698e-04, 8.7835e-02, 4.7312e-05, 2.7271e-04, 1.5388e-04, 2.1807e-04,
        4.1062e-04, 6.1824e-02, 2.8865e+00, 1.8142e-01, 1.2476e+00, 2.8567e-02,
        9.7439e-01, 6.6396e-03, 2.8262e-01, 4.4113e-02, 9.9372e-03, 7.8228e-03,
        1.7742e-02, 6.1787e-01, 2.2583e-02, 5.5771e-01, 9.3040e-05, 1.5434e-02,
        2.2690e+00, 7.7097e-01, 6.8654e+00, 2.2173e-01, 7.9232e-02, 4.2691e-01,
        8.5900e-03, 6.3162e-03], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([4.9529e+00, 3.2710e-01, 4.5759e+00, 2.0770e-01, 2.4161e-02, 6.9595e-01,
        4.6479e-01, 5.0763e-01, 6.0822e-01, 3.6192e-01, 8.6493e-02, 5.8613e-01,
        3.4941e-02, 2.1337e-01, 4.7176e-01, 1.5362e-01, 3.0745e-01, 3.8803e-01,
        6.7597e-02, 5.6253e-01, 1.8981e-03, 6.0212e-04, 1.9320e-03, 3.4617e-02,
        3.6149e-01, 1.5784e-01, 1.3981e+00, 1.2659e-01, 8.0065e-02, 4.6340e-03,
        9.4343e-04, 9.1757e-04, 3.7760e-02, 8.9149e-03, 5.2889e-03, 6.1619e-03,
        1.5891e-02, 2.8454e-02, 4.4092e-03, 1.0223e-01, 3.9240e-02, 1.5824e+00,
        4.9166e-02, 1.6639e-01, 6.2580e-02, 1.8584e-01, 2.3771e-03, 2.3326e-03,
        2.8173e-03, 8.2915e-02, 1.4627e-04, 1.1497e-03, 2.2058e-03, 7.2864e-03,
        2.4518e-03, 1.6049e-02, 2.9116e+00, 1.6551e-01, 9.5267e-01, 6.4413e-03,
        5.8701e+00, 2.1260e-02, 1.3753e-01, 2.3804e-02, 4.1214e-03, 5.7850e-03,
        1.3801e-02, 6.4977e-01, 2.7357e-02, 3.7328e-01, 1.2670e-04, 9.7090e-03,
        1.4148e+00, 4.1662e-01, 5.3171e+00, 8.6282e-03, 8.9410e-02, 1.0394e+00,
        6.1339e-03, 6.8945e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.0319e+01, 1.9487e+01, 4.6862e+01, 1.0625e+01, 2.8760e+00, 3.5424e+01,
        6.8648e+01, 3.8962e+01, 2.2729e+01, 2.6029e+01, 1.5905e+01, 6.0705e+01,
        8.6675e+00, 1.1804e+01, 1.6882e+01, 5.0080e+00, 9.8267e+00, 1.5101e+01,
        2.2779e+00, 1.7542e+01, 1.0101e+00, 9.5070e-02, 1.8402e-01, 5.1465e+00,
        2.5601e+01, 1.8316e+01, 2.9769e+01, 1.4280e+01, 1.2509e+01, 4.6117e-01,
        9.5506e-02, 1.5165e-01, 6.1450e-01, 4.1148e+00, 4.9604e-01, 2.0192e-01,
        1.8165e+00, 2.0090e+00, 1.5158e-01, 2.0634e+00, 2.3176e-01, 3.5595e+00,
        8.3505e-02, 3.2147e-01, 7.2588e-02, 2.6344e-01, 3.0430e-01, 2.2373e-01,
        1.9712e-01, 5.9822e+00, 4.2959e-02, 1.7669e-01, 2.1894e-01, 4.3737e-01,
        3.7821e-01, 9.0956e-01, 1.1941e+01, 4.1620e+00, 6.9366e+00, 5.4525e-01,
        7.9644e+00, 9.0127e-01, 7.2070e-01, 5.1030e-01, 8.0437e-02, 2.4622e-01,
        2.0010e-01, 2.0168e+01, 6.6691e-02, 3.8526e+00, 1.2677e-02, 5.0397e-01,
        4.1338e+00, 2.5288e+00, 6.3333e+00, 8.6423e-03, 8.9422e-02, 1.0411e+00,
        1.0496e-02, 6.5467e-01], device='cuda:0')
Outer loop valEpocw Maximum [22/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 48.1
model_train val_loss valEpocw [22/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 39.7
model_train val_loss valEpocw [22/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 701.8
Sum_Val Meta Model:  tensor([3.5626e+00, 2.4852e-02, 7.8036e-01, 2.2361e-02, 4.3944e-03, 2.5372e-02,
        2.9549e-03, 6.2833e-02, 3.3543e-02, 1.2217e-02, 2.3995e-03, 5.4307e-03,
        5.5896e-04, 1.7652e-01, 4.3360e-02, 5.1183e-02, 1.3962e-01, 5.0533e-02,
        2.1981e-02, 3.6161e-02, 5.4506e-04, 4.7661e-03, 4.8769e-02, 4.7266e-03,
        6.5106e-02, 8.4605e-03, 6.3197e-01, 2.2426e-02, 2.8341e-03, 2.1303e-02,
        2.5687e-02, 1.7313e-02, 3.5658e-01, 1.7414e-04, 1.5861e-02, 6.1007e-02,
        6.4169e-02, 9.5459e-02, 6.7658e-03, 9.0323e-01, 5.1282e+00, 3.2153e+01,
        4.5750e+01, 4.4806e+01, 4.5330e+01, 5.3967e+01, 5.5272e-02, 7.2810e-02,
        4.0091e-01, 1.2886e-01, 4.0998e-02, 1.1942e-01, 3.8973e-01, 1.1740e-01,
        1.8513e-01, 8.6871e-01, 3.2257e+01, 2.6060e-01, 1.0454e+00, 1.0892e-02,
        6.5297e+01, 1.8297e-02, 1.8163e+00, 3.2363e-01, 4.2398e-01, 1.2369e-02,
        3.9273e-01, 1.7793e-01, 2.3109e+00, 1.8767e-01, 1.4555e-03, 7.4454e-02,
        1.4880e+00, 4.8558e+00, 1.2494e+00, 1.6190e+01, 1.1367e+01, 9.7465e-01,
        1.2800e-01, 4.9529e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.0713e+00, 4.6708e-03, 2.7835e-01, 2.3729e-03, 2.0834e-04, 9.2087e-04,
        8.6304e-05, 6.3945e-02, 2.4112e-03, 3.6476e-04, 1.7033e-04, 1.8143e-04,
        3.8548e-05, 1.3506e-01, 3.7006e-03, 1.7317e-02, 2.6128e-02, 3.1934e-03,
        1.6053e-03, 3.8145e-04, 4.0083e-05, 2.9643e-05, 3.8437e-05, 9.0132e-05,
        2.6557e-02, 2.5850e-03, 6.5559e-01, 1.7366e-02, 1.0735e-03, 5.1354e-04,
        2.6717e-03, 1.6291e-02, 3.6192e-01, 4.5449e-05, 1.2731e-02, 3.9111e-02,
        4.3367e-02, 7.2329e-02, 1.6261e-03, 1.3321e+00, 3.4328e+00, 3.2202e+01,
        4.2731e+01, 4.5651e+01, 5.0122e+01, 5.0191e+01, 2.4430e-02, 2.7013e-02,
        2.1668e-01, 5.0647e-02, 2.4253e-02, 1.2154e-01, 2.7067e-01, 2.0775e-01,
        1.1761e-01, 5.9675e-01, 1.4110e+01, 2.7774e-01, 9.4217e-01, 6.7327e-03,
        6.6828e+01, 4.0785e-03, 1.3432e+00, 3.1415e-01, 4.2761e-01, 4.3961e-02,
        4.4101e-01, 2.0942e-01, 2.0418e+00, 1.9649e-01, 3.3196e-04, 6.7369e-02,
        1.8519e+00, 4.1660e+00, 3.6894e+00, 2.4230e+01, 1.3681e+01, 4.7371e+00,
        4.3161e-02, 1.0375e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.1356e+01, 6.4801e-01, 4.3512e+00, 2.9439e-01, 7.2081e-02, 1.1842e-01,
        4.9674e-02, 1.4874e+01, 2.0409e-01, 6.8927e-02, 1.0128e-01, 5.7374e-02,
        3.5927e-02, 1.5510e+01, 3.1684e-01, 1.3455e+00, 1.5776e+00, 2.7161e-01,
        1.3598e-01, 2.8646e-02, 1.0853e-01, 1.4518e-02, 8.6034e-03, 3.7997e-02,
        3.9115e+00, 9.8550e-01, 2.7206e+01, 6.6895e+00, 6.3703e-01, 1.2824e-01,
        6.9171e-01, 7.9620e+00, 9.7399e+00, 8.4183e-02, 2.9689e+00, 2.2566e+00,
        1.3537e+01, 1.1915e+01, 1.2835e-01, 4.6258e+01, 2.3145e+01, 6.7349e+01,
        6.0910e+01, 7.3835e+01, 5.4451e+01, 6.4204e+01, 8.4963e+00, 6.3217e+00,
        3.4458e+01, 8.6247e+00, 2.2169e+01, 4.3905e+01, 6.4041e+01, 3.0192e+01,
        5.2794e+01, 6.8408e+01, 7.6307e+01, 1.4533e+01, 8.7855e+00, 1.5565e+00,
        8.3237e+01, 4.1228e-01, 8.8471e+00, 1.3319e+01, 1.4380e+01, 4.4256e+00,
        1.0415e+01, 1.3601e+01, 5.0678e+00, 3.2676e+00, 9.5518e-02, 8.6698e+00,
        6.0120e+00, 3.6411e+01, 4.1443e+00, 2.4239e+01, 1.3681e+01, 4.7387e+00,
        7.1249e-02, 1.4394e+00], device='cuda:0')
Outer loop valEpocw Maximum [22/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 377.8
model_train val_loss valEpocw [22/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 369.9
model_train val_loss valEpocw [22/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1237.6
Sum_Val Meta Model:  tensor([1.0287e+01, 4.6468e-02, 2.3963e+00, 2.6709e-02, 4.6036e-03, 4.8445e-01,
        3.5941e-02, 2.2121e-01, 3.7662e-02, 2.9862e-01, 2.7915e-02, 4.9804e-02,
        6.7316e-04, 3.1583e-01, 3.9449e-01, 4.4261e-02, 5.3825e-02, 3.2665e-02,
        9.9894e-03, 1.6735e-02, 7.0755e-04, 3.1317e-03, 4.6345e-03, 6.1370e-03,
        3.6854e-01, 1.4405e-02, 1.4702e+00, 1.3303e-02, 9.5972e-02, 5.8164e-03,
        1.0882e-02, 1.3080e-03, 2.8336e-01, 2.4998e-02, 5.5283e-02, 2.6755e-01,
        2.8017e-03, 2.1643e-02, 1.7427e-01, 1.4867e+00, 3.0528e+00, 1.4387e+01,
        6.6453e+00, 6.0944e+00, 1.1132e+01, 1.5824e+01, 3.9336e-03, 6.6272e-03,
        2.3056e-02, 6.2748e-03, 2.5152e-03, 3.0419e-03, 3.4921e-03, 1.0649e-01,
        5.1314e-03, 4.4027e-02, 6.9294e+00, 2.1923e-01, 2.3273e+00, 2.2456e-02,
        3.6162e+01, 1.5072e-02, 1.0167e+00, 4.4585e-01, 4.1591e-01, 5.2416e-02,
        6.1792e-01, 2.3957e+00, 5.8327e-01, 2.5073e-01, 1.2248e-03, 3.8301e-02,
        1.7322e+00, 1.9805e+00, 3.7812e+02, 1.2711e+01, 1.5153e+00, 8.8454e+00,
        4.6776e-02, 1.1202e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([3.3702e+00, 5.1408e-02, 1.7952e+00, 2.6086e-02, 9.2355e-03, 1.5574e-01,
        4.4228e-02, 1.8691e-01, 1.0086e-01, 2.2575e-01, 2.0088e-02, 9.4153e-02,
        2.8005e-03, 3.2895e-01, 4.8917e-01, 2.4363e-02, 2.2384e-02, 1.5766e-02,
        1.4288e-03, 1.2127e-03, 6.0177e-04, 1.8398e-04, 6.9233e-04, 3.8603e-03,
        3.3715e-01, 1.6589e-02, 1.2358e+00, 1.2854e-02, 1.0663e-01, 5.7042e-04,
        7.4057e-04, 8.3633e-04, 8.7995e-03, 3.4390e-04, 1.9331e-03, 3.4769e-03,
        3.0796e-03, 4.1870e-03, 3.6573e-03, 1.6244e-01, 3.4388e-01, 1.0696e+00,
        1.8531e-01, 4.2817e-01, 4.2465e-01, 1.3086e+00, 9.7592e-04, 1.2685e-03,
        1.3805e-03, 1.0815e-03, 1.0329e-04, 4.0145e-04, 5.8683e-04, 5.4937e-03,
        2.9031e-03, 1.9286e-02, 1.7881e+00, 3.0936e-02, 1.7435e+00, 8.1877e-03,
        8.2533e+00, 1.2378e-02, 1.6610e-01, 2.9285e-02, 1.5022e-02, 1.7501e-02,
        8.7196e-02, 2.8123e-01, 2.7379e-02, 3.2621e-02, 2.6767e-04, 9.2093e-03,
        2.4405e-01, 9.0817e-01, 6.2474e+01, 1.4657e+00, 4.5791e-02, 8.6269e+00,
        1.1372e-02, 1.3897e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.7871e+01, 3.0641e+00, 1.9328e+01, 1.3820e+00, 1.0915e+00, 8.6459e+00,
        7.6923e+00, 1.5893e+01, 3.9685e+00, 1.6231e+01, 3.7138e+00, 1.0123e+01,
        7.2911e-01, 1.6892e+01, 1.9649e+01, 9.4863e-01, 7.7373e-01, 6.1568e-01,
        5.5347e-02, 4.5738e-02, 3.4059e-01, 2.9479e-02, 6.8675e-02, 5.7790e-01,
        2.3725e+01, 2.0282e+00, 2.5933e+01, 1.5339e+00, 1.8054e+01, 5.6495e-02,
        7.4458e-02, 1.4378e-01, 1.3360e-01, 1.4304e-01, 1.6646e-01, 9.2023e-02,
        3.7049e-01, 2.8430e-01, 1.1380e-01, 2.7510e+00, 1.5932e+00, 2.2278e+00,
        2.9138e-01, 7.6374e-01, 4.8459e-01, 1.7817e+00, 1.3091e-01, 1.2346e-01,
        9.8378e-02, 8.1671e-02, 2.9583e-02, 6.0681e-02, 5.9580e-02, 3.0968e-01,
        4.7089e-01, 1.0672e+00, 6.9858e+00, 7.7355e-01, 1.1720e+01, 7.1366e-01,
        1.0836e+01, 5.1108e-01, 8.3320e-01, 5.6742e-01, 2.4120e-01, 7.1247e-01,
        1.0461e+00, 7.0881e+00, 6.5286e-02, 3.2946e-01, 2.6599e-02, 4.7753e-01,
        6.6572e-01, 5.4687e+00, 7.4449e+01, 1.4679e+00, 4.5798e-02, 8.6410e+00,
        2.1686e-02, 1.2290e-01], device='cuda:0')
Outer loop valEpocw Maximum [22/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 533.0
model_train val_loss valEpocw [22/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 99.0
model_train val_loss valEpocw [22/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 388.7
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.88148293 97.38185451 93.35047088 98.08969189 98.67326178 97.5901853
 98.20786784 95.24372266 98.12014961 97.03341821 98.67448009 98.95590941
 99.4420146  95.37042677 97.506122   97.40256576 96.4608131  97.72298096
 98.86575456 98.40036062 98.662297   99.30800063 99.57602856 99.0387544
 95.26687053 96.82996065 94.51273742 97.06143931 98.06167079 98.2297974
 98.25416357 98.59163509 97.09798857 98.61234634 98.69397303 98.93519816
 97.62917118 98.40157893 99.01804315 93.40042153 98.1055299  95.44108868
 98.02390322 97.37210804 98.38695922 97.16865048 98.26025511 98.68300825
 98.13476931 98.73417722 98.75001523 98.63183928 99.02778962 98.37355783
 98.76098001 97.71079787 93.03736553 97.03341821 96.70447485 97.61820641
 96.6703622  98.72564905 97.67424861 97.40622068 98.81214897 97.6730303
 98.66473362 95.99420085 99.00586007 98.38086768 99.81603538 97.588967
 99.00464176 96.03318673 98.65255053 98.6903181  99.61866936 99.32749357
 99.84405648 99.14718388]
Accuracy th:0.7 is [85.84081578 97.35505172 92.87533047 98.0080652  98.58432524 97.50124877
 97.96420609 95.05244819 98.01172013 96.78000999 98.60259987 98.91936014
 99.42252166 95.35824369 97.4086573  97.29169966 96.3767498  97.60845994
 98.79874758 98.3418818  98.68666317 99.24830351 99.54435253 98.85844471
 95.24006774 96.74467904 94.31293478 96.97859432 98.034868   98.17619181
 98.00319197 98.57579708 96.75442551 98.45518451 98.63183928 98.86819118
 97.35139679 98.45640282 98.87550103 93.21767522 98.04095954 95.24737759
 97.83262874 97.07484071 98.22370585 96.94204505 98.20421291 98.65133222
 98.08725527 98.67448009 98.59407171 98.6062548  99.01682484 98.38939584
 98.74514199 97.64744582 93.27128081 96.68620022 96.54974964 97.56338251
 96.92986197 98.63062097 97.38550944 97.32946723 98.75732508 97.55850928
 98.67935332 95.96252482 98.83286022 98.27609313 99.81603538 97.27707996
 99.00098683 96.08557401 98.61112803 98.43812819 99.57846517 99.37866254
 99.84405648 99.16545851]
Avg Prec: is [96.5818895  34.58976695 72.88380479 66.78614403 79.07601156 63.92030855
 74.31626154 47.50280448 59.15326017 52.70228216 33.04414581 53.92207607
 25.62172815 27.98462456 33.92812733 57.87202961 30.29535643 43.14037203
 47.69816864 38.27929591 60.76230494 52.19794866 90.54151775 84.12537706
 26.41354807 32.58319827 38.67946288 40.22746709 25.04277531 38.73710791
 73.18584293 37.70911151 59.01072387 63.87637115 72.71502037 80.50978543
 59.13352911 75.52863867 87.26734468 46.9023308  45.64061739 77.46897249
 72.05419195 68.36651774 79.77468422 84.11772795 40.41982684 34.01740713
 40.91415565 46.67911493 61.15333368 37.65049962 22.72477505 72.17303239
 28.23783181 44.42107476 74.43420005 59.41684755 46.27327573 59.55386386
 92.74662708 82.92031782 74.25587207 49.69629005 60.31794335 45.31552186
 61.40328202 26.25520603 64.99078839 68.69401731 11.67228014 73.95412128
 82.16229557 51.57163231 87.46954322 91.19312066 79.82835738 87.91585223
 22.74451955 27.40672337]
Accuracy th:0.5 is [45.47215555 97.2137279  71.85097647 97.02489005 97.26733349 76.6620777
 77.07021113 76.25028935 77.84505549 96.45593986 78.25075231 98.52097319
 99.41399349 80.56188399 77.82190763 96.56680596 96.29512311 77.62819654
 98.65376884 98.30776915 80.88473581 78.78071661 98.38695922 79.21687114
 81.54871408 96.65086926 94.0778012  77.32483766 98.01293844 78.1386679
 97.30875598 98.57457877 96.36213009 98.02024829 88.23479246 77.8182527
 77.55875294 91.3475713  97.11504489 74.83826951 79.64937074 92.05906361
 77.04219003 76.62309182 96.9627563  93.87434364 98.02877645 98.57336046
 97.63282611 89.2849746  88.1702221  98.55508583 98.99976852 77.20666171
 98.70615611 77.58433742 72.32611688 93.4515905  96.24273583 96.9067141
 89.79300934 97.17717864 93.67575931 77.63428808 98.42838172 78.08871724
 98.20786784 76.80461983 78.69178007 97.55972759 79.21808945 95.99054592
 78.43471693 95.45083515 76.99102106 83.37130396 88.59663016 78.29095649
 79.27047672 99.14718388]
Accuracy th:0.7 is [45.52454283 97.2137279  71.85097647 97.02489005 97.26733349 76.6620777
 77.07021113 76.54877499 77.84505549 96.4754328  78.25075231 98.52097319
 99.41399349 81.06870043 77.82190763 96.56680596 96.29512311 77.62819654
 98.65376884 98.30776915 81.27703123 78.78071661 98.38695922 79.85161
 82.20294587 96.65086926 94.0778012  77.32483766 98.01293844 78.1386679
 97.30875598 98.57457877 96.36213009 98.02024829 88.40901061 77.8182527
 77.55875294 91.63509217 97.11504489 74.83826951 80.27801806 92.05906361
 77.04219003 76.62309182 96.9627563  93.87434364 98.02877645 98.57336046
 97.92643852 89.81250228 88.35784164 98.55508583 98.99976852 77.20666171
 98.70615611 77.58433742 72.32611688 93.93404076 96.24273583 96.9067141
 89.79300934 97.17717864 93.80124511 77.63428808 98.42838172 78.08871724
 98.20786784 76.80461983 78.69178007 97.55972759 79.22174437 95.99054592
 78.54436471 95.45083515 76.99102106 83.47729682 88.76475676 78.29095649
 79.27047672 99.14718388]
Avg Prec: is [56.11740898  3.09731621 11.12075021  3.26643444  2.27441785  3.68934444
  3.25766852  5.56968884  2.58222402  3.74172971  1.59546982  1.6334576
  0.64455244  5.02854338  2.66830684  3.11200643  3.57362308  2.69132386
  1.37660296  1.72405621  1.96371585  0.88562041  1.87505997  2.42836119
  5.13998056  3.70344385  6.66187341  3.26612742  2.02932534  1.81744347
  2.74729587  1.35035795  3.6868277   1.64934746  2.40128749  2.43405881
  3.10134872  2.61051499  2.81047095  7.34628945  2.32904324  8.29015562
  3.32733175  4.04962712  3.28216279  6.48315808  2.01571979  1.49057802
  2.14974063  1.53093177  1.82994713  1.61140562  1.15281868  3.07122773
  1.34723368  2.79506274 11.30096501  3.63298669  3.83977887  2.91706243
 10.86664993  2.1416078   3.78209412  3.11742168  1.60839126  2.45082387
  1.80300453  4.11940106  1.23325894  2.37315919  0.19348255  3.3033576
  1.87998559  4.4832688   3.87116567  3.05927629  0.80151803  1.82154614
  0.13194379  0.72548068]
mAP score regular 56.53, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [88.04594265 97.33662207 93.25559957 98.30331116 98.95856691 97.69539328
 98.37307223 95.34594015 98.11395969 97.09245833 98.69197997 99.02583651
 99.38709919 95.20143508 97.44126367 97.43877221 96.40730498 97.79256048
 98.99843038 98.40047836 98.78914717 99.31484665 99.62378852 99.24259412
 95.47549642 96.78351646 94.4515036  97.2743354  97.91215088 98.20116102
 98.59730423 98.68450557 97.00525699 98.7717069  98.96105838 99.18529038
 97.97194608 98.36061489 99.08563171 93.19829584 97.97692902 92.66512196
 97.31669034 96.55679298 96.79846526 94.74300521 98.39798689 98.81157037
 98.09651942 98.7791813  98.82153624 98.66208237 98.89628024 98.39549543
 98.78416424 97.78757755 90.00423549 97.192117   96.48454045 97.57829434
 92.41348382 98.83897651 97.3565538  97.49856741 98.7791813  97.53344794
 98.6296933  95.85419937 98.82651917 98.28587089 99.81563146 97.68791888
 98.39051249 95.43563296 97.3490794  97.58078581 99.16286718 98.40795276
 99.82559733 99.05573411]
Accuracy th:0.7 is [87.53768343 97.38146847 93.2531081  98.26344769 98.94610957 97.57829434
 98.13887436 95.36338042 98.11894262 96.89064953 98.66208237 99.03829384
 99.36965892 95.19396068 97.4014002  97.33163914 96.35000125 97.68542741
 98.94361811 98.38303809 98.90624611 99.27498318 99.64870319 99.10805491
 95.45058176 96.69133219 94.51628174 97.21703167 97.85484715 98.15133169
 98.36061489 98.6745397  96.76856766 98.6521165  98.91870344 99.13546105
 97.72279941 98.29085383 98.99344744 93.2456337  97.97194608 93.33283504
 97.34658794 96.68136632 96.94047886 94.99215188 98.33819169 98.83399357
 98.07409622 98.7268605  98.68699704 98.63467623 98.90624611 98.4652565
 98.7791813  97.83242395 91.09300645 97.04512046 96.38488178 97.65552981
 92.97406383 98.74679224 97.18215113 97.50853327 98.72935197 97.63061514
 98.61723597 95.7993871  98.78416424 98.21610982 99.81563146 97.55836261
 98.47522236 95.9115031  97.45870394 97.57829434 99.21020505 98.58733837
 99.82559733 99.15788425]
Avg Prec: is [96.53967372 33.29723627 71.41977201 73.37900811 77.83421501 64.36046874
 79.43862608 48.21624068 62.435075   56.25378201 39.54160767 56.90843837
 25.07360329 31.07269451 33.99421282 62.75941294 32.82157021 45.5262225
 51.48838159 37.43337999 69.37644728 55.90503494 92.20443522 87.99466342
 25.0926647  37.43630953 33.36900205 46.43094229 26.88555939 37.92598081
 78.06783491 38.22800319 55.92291739 65.19945505 74.77104139 83.39659085
 61.18006625 77.27100964 89.79600656 45.34913933 38.23956949 52.64669571
 47.66903884 42.42279526 31.46441039 54.20010763 38.55870499 29.59949424
 41.89524191 42.14815622 67.84770715 36.10963792 25.78838047 76.59136433
 31.06336785 43.47074075 57.75906859 57.98992164 39.11973186 63.06693907
 66.83104542 86.27572995 66.81887997 53.45217951 61.00025641 39.82243673
 61.50658737 27.41074699 43.34325906 67.0623826  10.70137392 75.54396678
 56.77789357 44.88249781 64.60402883 50.45087039 16.49342215 54.70502335
  2.62586436 23.18706146]
Accuracy th:0.5 is [45.31479682 97.22450607 69.93297955 96.96290206 97.90716795 75.36935994
 75.46154421 74.49983805 76.95393278 96.41976231 77.08847198 98.5325261
 99.34972718 78.00034881 77.02618532 96.31262924 96.21047911 76.44816503
 98.78167277 98.34068316 78.90724269 77.7387448  98.31327703 79.55004111
 78.09004161 96.52938685 94.3393876  76.55031517 97.81747515 77.05608292
 97.52597354 98.67204823 96.39983058 98.18870369 89.29416748 76.68236291
 76.64997384 92.57293769 97.0276802  73.84956524 77.49707253 92.37362035
 75.80536662 75.46901861 97.03764606 94.02795426 98.18621222 98.77668984
 97.91215088 89.01014027 86.63826395 98.55993223 98.87385704 75.79041782
 98.6969629  76.48304557 70.62062436 94.4066572  96.16314124 96.78102499
 90.13379176 97.04761193 93.96566759 76.52290904 98.32075143 77.36502479
 98.13139996 75.70570795 77.76615093 97.53593941 78.17475148 96.07843137
 77.53195306 95.44559882 75.60854075 83.98734335 90.47263124 77.12833545
 78.24949548 99.15040985]
Accuracy th:0.7 is [45.48919949 97.22450607 69.93297955 96.96290206 97.90716795 75.36935994
 75.46154421 74.66427486 76.95393278 96.41976231 77.08847198 98.5325261
 99.34972718 78.40645788 77.02618532 96.31262924 96.21047911 76.44816503
 98.78167277 98.34068316 79.2211675  77.7387448  98.31327703 80.02092832
 78.56092882 96.52938685 94.3393876  76.55031517 97.81747515 77.05608292
 97.52597354 98.67204823 96.39983058 98.18870369 89.45860428 76.68236291
 76.64997384 92.79966116 97.0276802  73.84956524 77.9779256  92.37362035
 75.80536662 75.46901861 97.03764606 94.02795426 98.18621222 98.77668984
 97.95201435 89.36392854 86.81017515 98.55993223 98.87385704 75.79041782
 98.6969629  76.48304557 70.62062436 94.73303934 96.16314124 96.78102499
 90.13379176 97.04761193 94.06532626 76.52290904 98.32075143 77.36502479
 98.13139996 75.70570795 77.76615093 97.53593941 78.17475148 96.07843137
 77.57181653 95.44559882 75.60854075 84.09945935 90.69188031 77.12833545
 78.24949548 99.15040985]
Avg Prec: is [54.44548188  3.74179086 14.78121128  4.5582212   1.51576481  4.26666356
 12.44178231  8.66180724  7.74362444  5.17886651  2.30215385  5.04464843
  1.54386745  5.91339414  3.06782376  3.63181883 24.60388     6.36979377
  1.54629469  2.65691695  3.58075157  1.44539121  1.26600963  5.6439404
  5.71043661 10.44319227  8.05144788  4.55066599  3.92467164  6.42100569
  2.28749127  0.85764836  3.00272974  1.15564546  1.70976396  2.40242972
  2.00614045  2.19786785  2.12616574  6.23033711  1.69112223  6.03067391
  2.18284428  2.71995022  2.34720608  4.87160599  1.71293608  1.04189931
  1.39815093  1.19915187  1.23373414  0.99830102  0.77030242  2.27673152
  0.9091042   1.8510808  10.19956849  3.00817692  4.06051646  2.82151377
  7.94730328  2.04898218  3.33250089  2.51326573  1.37667704  1.89733447
  1.54200703  3.51423948  1.07001026  2.21175543  0.19567244  3.20785177
  1.54543979  4.00538011  3.20904437  2.24446554  0.5589682   1.46345961
  0.12059289  0.60448925]
mAP score regular 51.91, mAP score EMA 4.34
Train_data_mAP: current_mAP = 56.53, highest_mAP = 56.53
Val_data_mAP: current_mAP = 51.91, highest_mAP = 51.91
tensor([4.4007e-02, 6.3897e-03, 6.0091e-02, 6.8160e-03, 2.4085e-03, 6.3068e-03,
        1.3816e-03, 3.7621e-03, 1.0789e-02, 4.4197e-03, 1.3408e-03, 2.5918e-03,
        8.9686e-04, 7.6739e-03, 1.0276e-02, 1.1310e-02, 1.3669e-02, 9.6307e-03,
        1.0600e-02, 1.1477e-02, 2.9280e-04, 1.6138e-03, 3.0447e-03, 1.9875e-03,
        6.1100e-03, 2.2170e-03, 2.5078e-02, 2.1000e-03, 1.3979e-03, 3.0289e-03,
        3.0889e-03, 1.4169e-03, 3.0302e-02, 3.9666e-04, 3.3840e-03, 1.6133e-02,
        2.2341e-03, 4.8988e-03, 1.1405e-02, 3.0103e-02, 1.6364e-01, 4.6868e-01,
        7.3314e-01, 6.4095e-01, 9.3729e-01, 8.0783e-01, 2.1145e-03, 3.1983e-03,
        4.3917e-03, 4.3146e-03, 8.0449e-04, 2.1602e-03, 3.1278e-03, 5.3542e-03,
        1.6638e-03, 7.1792e-03, 1.9019e-01, 1.6627e-02, 1.1728e-01, 3.4826e-03,
        8.1156e-01, 8.6241e-03, 1.5283e-01, 2.2214e-02, 3.0919e-02, 8.4471e-03,
        4.5763e-02, 1.4819e-02, 4.9033e-01, 6.4628e-02, 2.9846e-03, 6.6953e-03,
        3.7132e-01, 1.1058e-01, 9.2098e-01, 9.9976e-01, 9.9999e-01, 9.9978e-01,
        6.8798e-01, 7.1825e-02], device='cuda:0')
Sum Train Loss:  tensor([1.7976e+00, 8.5838e-02, 1.3142e+00, 3.4467e-02, 7.9983e-03, 4.9320e-02,
        6.4718e-03, 1.0044e-01, 7.7235e-02, 6.5453e-02, 5.8616e-03, 2.5602e-02,
        7.9302e-04, 1.7886e-01, 1.0086e-01, 7.3150e-02, 1.7708e-01, 3.1479e-02,
        6.9364e-02, 7.9228e-02, 3.6389e-03, 1.5618e-03, 1.2247e-02, 6.8564e-03,
        1.6852e-01, 3.5077e-02, 4.8854e-01, 2.0512e-02, 9.5819e-03, 2.5684e-02,
        2.3482e-02, 2.3690e-02, 2.5485e-01, 1.9314e-03, 6.3409e-03, 1.9637e-02,
        5.1377e-02, 3.0726e-02, 2.1045e-02, 4.5447e-01, 3.7091e-01, 2.9678e+00,
        1.5810e+00, 2.9134e+00, 4.2150e+00, 8.3343e+00, 2.5503e-02, 4.5036e-03,
        1.5082e-02, 3.9204e-03, 2.9403e-03, 4.0682e-03, 6.0439e-03, 1.9010e-02,
        6.2525e-03, 4.6575e-02, 3.6901e+00, 6.8627e-02, 1.2528e+00, 2.7444e-02,
        7.3892e+00, 4.2654e-02, 6.2755e-01, 2.0958e-01, 1.7210e-01, 6.8073e-02,
        2.7565e-01, 8.3903e-02, 6.5236e-01, 3.2394e-01, 6.3148e-04, 3.5714e-02,
        4.1042e-01, 9.8201e-01, 6.7049e+00, 6.0102e-01, 6.9621e-01, 3.1454e-01,
        4.3509e-02, 1.1705e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [23/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 51.2
Sum Train Loss:  tensor([1.5502e+00, 8.1990e-02, 2.0660e+00, 6.8608e-02, 2.3470e-02, 1.0584e-01,
        2.2508e-02, 8.5569e-02, 9.5834e-02, 8.9199e-02, 2.0135e-02, 7.3660e-03,
        3.3508e-03, 1.8178e-01, 1.3540e-01, 7.2994e-02, 1.4397e-01, 9.3737e-02,
        1.0620e-01, 1.0444e-01, 1.3244e-03, 3.0479e-03, 1.0991e-03, 1.8084e-03,
        1.3678e-01, 3.7089e-02, 4.6268e-01, 3.2079e-02, 1.7735e-02, 1.7806e-02,
        2.9355e-02, 4.5927e-03, 2.4661e-01, 3.3472e-03, 2.7222e-02, 1.2312e-01,
        3.5932e-02, 3.9559e-02, 9.9163e-03, 8.5062e-01, 3.2721e-01, 2.6284e+00,
        4.7743e+00, 1.8309e+00, 1.6789e+00, 3.7386e+00, 1.0074e-02, 1.3922e-02,
        1.4638e-02, 1.3173e-02, 6.6648e-03, 1.4021e-02, 2.3025e-02, 2.1876e-02,
        1.1802e-02, 7.8917e-02, 3.6547e+00, 7.7637e-02, 1.0901e+00, 1.3056e-02,
        8.9767e+00, 4.0485e-02, 1.8574e+00, 4.2254e-02, 3.1947e-02, 6.5605e-02,
        1.9936e-01, 2.5061e-01, 1.3083e+00, 1.0302e-01, 3.0831e-04, 6.5519e-02,
        3.0648e-01, 9.4604e-01, 5.1704e+00, 4.2665e+00, 6.4771e-01, 2.3009e+00,
        6.6288e-02, 4.0383e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [23/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 54.2
Sum Train Loss:  tensor([1.6307e+00, 3.5264e-02, 1.5858e+00, 3.0744e-02, 1.4202e-02, 3.9500e-02,
        1.0425e-02, 3.1282e-02, 8.3284e-02, 3.2609e-02, 1.0343e-02, 3.2480e-02,
        3.1812e-03, 2.1947e-01, 1.4365e-01, 9.4281e-02, 1.4358e-01, 6.4280e-02,
        8.4003e-02, 4.8987e-02, 1.9335e-03, 8.9997e-04, 6.7955e-03, 1.0905e-02,
        1.6509e-01, 4.3394e-02, 4.5446e-01, 3.9668e-02, 1.4911e-02, 1.0011e-02,
        6.8200e-03, 8.7085e-03, 4.2460e-01, 1.6683e-03, 2.6635e-02, 2.0056e-01,
        2.3735e-02, 1.1510e-02, 3.8622e-02, 4.9005e-01, 1.4460e+00, 5.5644e+00,
        7.7389e+00, 3.4298e+00, 3.4744e+00, 5.8510e+00, 2.1447e-02, 2.2075e-02,
        5.7997e-03, 6.9873e-03, 2.7930e-03, 7.8601e-03, 1.9702e-03, 3.2203e-02,
        3.2267e-03, 5.2766e-02, 4.0446e+00, 7.2404e-02, 1.3497e+00, 2.7162e-02,
        6.3055e+00, 7.2064e-02, 1.4088e+00, 2.6904e-01, 2.1906e-01, 4.5893e-02,
        2.9164e-01, 2.3918e-01, 7.3538e-01, 1.0303e-01, 3.6102e-04, 4.3200e-02,
        3.5635e+00, 1.7853e+00, 8.9105e-01, 2.7496e+00, 8.7750e-01, 4.4811e+00,
        2.5598e+00, 6.5823e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [23/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 66.2
Sum Train Loss:  tensor([1.1022e+00, 1.0112e-01, 1.6345e+00, 2.3286e-02, 9.7733e-03, 4.3662e-02,
        1.5582e-02, 3.6149e-02, 9.0341e-02, 5.0071e-02, 1.8482e-02, 1.5564e-02,
        5.0842e-04, 2.7237e-01, 1.2153e-01, 1.1963e-01, 1.3755e-01, 4.7521e-02,
        3.6929e-02, 7.2268e-02, 1.2174e-03, 9.1614e-03, 1.8925e-02, 9.3295e-03,
        1.3031e-01, 2.1219e-02, 4.8668e-01, 1.9984e-02, 2.4827e-02, 4.2692e-02,
        5.6359e-03, 5.6453e-03, 2.7521e-01, 4.1795e-03, 1.3965e-02, 6.8308e-02,
        2.1303e-02, 8.5859e-03, 6.3541e-02, 8.1673e-01, 8.6040e-01, 3.9131e+00,
        5.1154e+00, 4.8773e+00, 4.1412e+00, 5.4015e+00, 2.4711e-02, 1.0979e-02,
        2.7553e-02, 3.1230e-02, 3.4089e-03, 2.1883e-02, 3.4924e-03, 3.9815e-02,
        9.9088e-03, 1.2307e-01, 5.3555e+00, 2.4525e-01, 2.0654e+00, 1.0034e-02,
        1.3635e+01, 1.6240e-02, 2.2000e-01, 1.2627e-01, 8.3814e-02, 5.1674e-02,
        2.2282e-01, 2.1640e-01, 1.2100e+00, 3.5841e-01, 4.1861e-04, 8.1410e-02,
        1.4166e+00, 1.3133e+00, 5.1390e+00, 2.2455e+00, 3.2741e+00, 2.2502e+00,
        1.3850e-01, 7.9359e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [23/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 71.1
Sum Train Loss:  tensor([1.6211e+00, 4.9986e-02, 9.2005e-01, 2.7743e-02, 1.0785e-02, 7.2296e-02,
        9.0242e-03, 4.7094e-02, 6.5591e-02, 3.8047e-02, 1.1403e-02, 1.8712e-02,
        9.3302e-04, 1.7653e-01, 1.7983e-01, 1.0726e-01, 1.6262e-01, 5.3120e-02,
        6.3620e-02, 1.3992e-01, 1.4873e-03, 8.4537e-03, 3.6873e-03, 3.6429e-03,
        6.3411e-02, 3.1865e-02, 4.9826e-01, 8.9402e-03, 5.6866e-03, 1.2160e-02,
        2.8360e-02, 9.2942e-03, 2.0740e-01, 1.9700e-03, 1.1192e-02, 1.2193e-02,
        1.0775e-02, 1.8332e-02, 2.1029e-02, 6.0248e-01, 3.1146e-01, 4.8567e+00,
        1.2494e+00, 6.6703e+00, 2.6493e+00, 5.4191e+00, 2.0917e-02, 1.8372e-02,
        2.1738e-02, 9.5403e-03, 8.9832e-04, 4.9834e-03, 2.9882e-03, 1.1983e-02,
        1.8331e-02, 4.5964e-02, 2.8097e+00, 2.3295e-01, 3.3854e+00, 9.2005e-03,
        1.1665e+01, 1.9648e-02, 6.0977e-01, 1.3213e-01, 1.4325e-01, 3.2487e-02,
        2.2231e-01, 3.3500e-01, 6.3840e+00, 4.5493e-01, 7.4523e-04, 5.3304e-02,
        1.7037e+00, 1.2197e+00, 4.3855e+00, 4.0997e+00, 1.3604e+00, 4.9217e-01,
        1.8599e-01, 2.9159e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [23/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 66.9
Sum Train Loss:  tensor([1.6486e+00, 3.8252e-02, 6.2392e-01, 2.9306e-02, 4.7993e-03, 2.5488e-02,
        1.5647e-02, 3.0497e-02, 1.6445e-01, 5.1322e-02, 1.9245e-03, 7.9965e-03,
        4.8950e-04, 9.3735e-02, 1.7026e-01, 2.0190e-01, 8.2979e-02, 1.3695e-01,
        3.5307e-02, 1.0054e-01, 4.9206e-04, 2.0980e-03, 1.5350e-03, 6.2350e-03,
        1.5236e-01, 2.7139e-02, 8.3745e-01, 1.9022e-02, 1.4576e-02, 1.6447e-02,
        1.6020e-02, 7.2333e-03, 2.6855e-01, 1.9013e-03, 1.2020e-02, 1.2746e-01,
        1.9234e-02, 2.7905e-02, 4.9922e-02, 6.3718e-01, 1.2123e+00, 1.0654e+01,
        9.0397e+00, 8.3387e+00, 8.2577e+00, 3.8445e+00, 2.2685e-02, 7.5337e-02,
        2.6032e-02, 3.3756e-02, 1.1492e-03, 2.3206e-02, 1.0478e-02, 4.1550e-02,
        7.3900e-03, 9.8389e-02, 3.8448e+00, 2.9852e-01, 1.1563e+00, 4.0498e-02,
        1.3183e+01, 4.8053e-02, 1.2212e+00, 2.3852e-01, 1.9419e-01, 6.0499e-02,
        1.3542e-01, 2.5803e-01, 2.8600e+00, 3.8477e-01, 1.0836e-03, 5.3381e-02,
        6.9711e-01, 1.9248e+00, 8.0791e+00, 1.1521e+01, 2.4619e+00, 1.4051e+00,
        1.2352e-01, 3.9357e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [23/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 98.0
Sum Train Loss:  tensor([1.1992e+00, 4.6484e-02, 1.0849e+00, 4.4198e-02, 7.2803e-03, 5.8592e-02,
        2.9689e-03, 4.0738e-02, 6.1673e-02, 3.0498e-02, 1.7878e-03, 2.5193e-02,
        7.6837e-04, 9.9221e-02, 7.7374e-02, 7.1214e-02, 2.1698e-01, 5.4116e-02,
        4.0606e-02, 1.4729e-01, 6.5644e-04, 1.5927e-03, 1.0582e-03, 9.4585e-03,
        9.2337e-02, 3.2300e-02, 7.6940e-01, 4.5096e-02, 8.7536e-03, 1.6591e-02,
        9.5434e-03, 4.4796e-03, 4.8269e-01, 3.6235e-03, 2.7905e-02, 9.1834e-02,
        3.4070e-02, 7.7990e-03, 1.2595e-01, 5.5116e-01, 1.4119e+00, 8.5704e+00,
        4.1514e+00, 6.8110e+00, 7.5012e+00, 8.2940e+00, 4.6794e-03, 1.5288e-02,
        1.0566e-02, 5.1145e-02, 2.5722e-03, 2.1735e-02, 4.3899e-03, 4.9868e-02,
        6.9261e-03, 6.2604e-02, 3.6752e+00, 2.1104e-01, 1.4663e+00, 1.0724e-02,
        4.5085e+00, 1.3707e-02, 8.5479e-01, 1.3705e-01, 4.4312e-02, 9.8540e-02,
        8.2488e-02, 2.2483e-01, 5.4209e+00, 5.4039e-01, 2.2091e-03, 4.5583e-02,
        6.9516e-01, 1.5861e+00, 2.2968e+00, 6.5548e+00, 3.0268e+00, 4.3611e+00,
        6.5921e-02, 3.9336e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [23/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 78.6
Sum_Val Meta Model:  tensor([2.2126e+00, 4.6057e-01, 4.7261e+00, 3.4655e-01, 3.0247e-03, 7.9854e-02,
        4.3610e-03, 6.5679e-02, 7.1467e-02, 4.5737e-02, 8.7694e-03, 2.9716e-02,
        7.0150e-03, 1.1173e-01, 8.0011e-02, 9.6705e-02, 6.6742e+00, 1.8613e-02,
        1.7674e-02, 1.2306e-02, 3.6939e-04, 6.0309e-04, 2.2398e-03, 1.1715e-03,
        1.8712e-01, 3.8685e-02, 6.8774e-01, 1.1616e-02, 1.5337e-02, 3.7546e-02,
        3.7971e-03, 9.9662e-04, 2.5991e-01, 6.1320e-04, 2.8579e-03, 2.7901e-02,
        5.9515e-03, 2.9339e-02, 2.5365e-02, 1.4814e+00, 1.8542e+00, 1.2637e+01,
        6.7430e+00, 1.3780e+01, 1.9110e+01, 1.5430e+01, 5.0323e-03, 1.7239e-02,
        4.6666e-03, 2.2721e-02, 3.2151e-04, 1.4078e-03, 2.0385e-02, 4.9707e-03,
        2.6989e-03, 8.5302e-03, 5.7685e+00, 1.0784e-01, 1.5608e+00, 1.1681e-02,
        1.2033e+01, 9.1715e-02, 7.4910e-01, 1.1663e-01, 1.4113e-02, 9.8138e-03,
        2.7019e-02, 1.1653e-01, 4.5780e+00, 1.2714e+00, 8.5080e-04, 1.1675e-01,
        5.2874e+00, 1.1684e+00, 2.0403e+01, 7.3933e+00, 3.3165e-01, 1.2474e+01,
        7.4043e-02, 4.4952e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([2.2486e+00, 3.1577e-01, 3.2387e+00, 1.9803e-01, 9.4870e-04, 7.6281e-02,
        5.8623e-03, 5.4572e-02, 5.7628e-02, 5.2308e-02, 9.8348e-03, 3.5247e-02,
        8.3909e-03, 1.0245e-01, 8.1076e-02, 4.2458e-01, 3.9124e+00, 3.7722e-02,
        1.7806e-02, 2.3446e-02, 2.2059e-04, 4.1834e-04, 1.1047e-03, 3.2027e-03,
        1.7288e-01, 3.8705e-02, 6.6893e-01, 1.7875e-02, 2.1405e-02, 4.7755e-02,
        1.7246e-03, 9.2348e-04, 2.5526e-01, 1.0345e-03, 2.6239e-03, 2.9001e-02,
        1.3774e-02, 2.2154e-02, 2.3270e-02, 1.3684e+00, 1.9681e+00, 1.5208e+01,
        6.7996e+00, 1.5664e+01, 2.5012e+01, 1.3212e+01, 3.1348e-03, 1.8026e-02,
        9.9539e-04, 1.8963e-02, 4.4870e-05, 3.7431e-04, 2.2508e-02, 8.7781e-04,
        1.2007e-03, 2.0878e-03, 5.0507e+00, 1.0436e-01, 1.5956e+00, 1.5650e-02,
        1.5868e+01, 3.2264e-02, 4.9217e-01, 1.4369e-01, 1.2240e-02, 1.4107e-02,
        1.9131e-02, 1.4027e-01, 6.2045e+00, 1.4039e+00, 1.7045e-03, 1.1838e-01,
        4.4747e+00, 1.1659e+00, 2.6496e+01, 9.4811e+00, 2.8218e-01, 1.2244e+01,
        1.6576e-01, 7.6086e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.1098e+01, 4.9418e+01, 5.3897e+01, 2.9054e+01, 3.9390e-01, 1.2095e+01,
        4.2432e+00, 1.4506e+01, 5.3415e+00, 1.1835e+01, 7.3350e+00, 1.3599e+01,
        9.3559e+00, 1.3350e+01, 7.8903e+00, 3.7539e+01, 2.8622e+02, 3.9169e+00,
        1.6798e+00, 2.0429e+00, 7.5337e-01, 2.5923e-01, 3.6282e-01, 1.6114e+00,
        2.8294e+01, 1.7458e+01, 2.6674e+01, 8.5119e+00, 1.5312e+01, 1.5767e+01,
        5.5831e-01, 6.5175e-01, 8.4238e+00, 2.6079e+00, 7.7540e-01, 1.7977e+00,
        6.1652e+00, 4.5223e+00, 2.0404e+00, 4.5457e+01, 1.2027e+01, 3.2450e+01,
        9.2747e+00, 2.4439e+01, 2.6686e+01, 1.6355e+01, 1.4826e+00, 5.6362e+00,
        2.2665e-01, 4.3951e+00, 5.5774e-02, 1.7328e-01, 7.1961e+00, 1.6395e-01,
        7.2163e-01, 2.9082e-01, 2.6556e+01, 6.2765e+00, 1.3605e+01, 4.4937e+00,
        1.9552e+01, 3.7412e+00, 3.2203e+00, 6.4686e+00, 3.9587e-01, 1.6700e+00,
        4.1806e-01, 9.4660e+00, 1.2654e+01, 2.1722e+01, 5.7109e-01, 1.7682e+01,
        1.2051e+01, 1.0543e+01, 2.8770e+01, 9.4833e+00, 2.8218e-01, 1.2246e+01,
        2.4095e-01, 1.0593e+00], device='cuda:0')
Outer loop valEpocw Maximum [23/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 161.3
model_train val_loss valEpocw [23/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 177.1
model_train val_loss valEpocw [23/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1167.6
Sum_Val Meta Model:  tensor([5.8654e+00, 2.4114e-01, 5.8131e+00, 1.8482e-01, 7.1803e-03, 6.8819e-01,
        4.7836e-01, 6.9355e-01, 8.3387e-01, 5.4157e-01, 1.0780e-01, 2.0846e+00,
        3.8840e-02, 1.0700e-01, 4.5387e-01, 5.9912e-01, 3.4870e-01, 5.7712e-01,
        2.0700e-01, 1.4548e+00, 8.0098e-05, 2.5620e-04, 2.6491e-03, 2.9252e-02,
        4.4308e-01, 1.7979e-01, 1.4825e+00, 1.3869e-01, 7.7579e-02, 4.4062e-04,
        1.2068e-03, 3.6593e-04, 5.7188e-03, 2.6077e-04, 3.7740e-04, 1.4377e-03,
        3.7556e-02, 1.5018e-03, 1.0311e-03, 7.3276e-02, 2.0818e-02, 2.1127e+00,
        6.2636e-02, 1.5258e-01, 2.5597e-01, 3.7960e+00, 4.7284e-03, 4.7262e-03,
        7.2984e-04, 7.9306e-02, 9.4378e-05, 4.6910e-04, 2.6745e-04, 3.0250e-04,
        5.6992e-04, 6.0817e-02, 2.9140e+00, 1.7453e-01, 1.1214e+00, 3.7024e-02,
        1.1143e+00, 9.5828e-03, 3.8125e-01, 6.5014e-02, 2.0403e-02, 1.2051e-02,
        3.5414e-02, 5.6583e-01, 4.0851e-02, 4.5287e-01, 1.1961e-04, 1.5661e-02,
        2.0265e+00, 8.2636e-01, 3.9995e+00, 3.0483e-01, 1.3093e-01, 5.4058e-01,
        1.6859e-02, 1.0013e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([4.4975e+00, 2.7759e-01, 3.8570e+00, 1.7701e-01, 1.8177e-02, 6.5895e-01,
        4.3475e-01, 5.3921e-01, 6.0699e-01, 3.6651e-01, 7.8831e-02, 6.1001e-01,
        3.1173e-02, 1.2940e-01, 4.5729e-01, 1.0677e-01, 2.9454e-01, 3.7526e-01,
        1.2731e-01, 5.0571e-01, 1.1134e-03, 1.0333e-03, 4.0131e-03, 2.4814e-02,
        3.7143e-01, 1.4631e-01, 1.4209e+00, 1.1930e-01, 8.0859e-02, 8.8208e-03,
        2.3198e-03, 1.2411e-03, 6.3025e-02, 8.3505e-03, 2.5184e-03, 1.1060e-02,
        1.7059e-02, 1.5861e-02, 5.8187e-03, 1.0565e-01, 2.9288e-02, 3.0272e+00,
        1.3074e-02, 6.6956e-02, 3.2039e-02, 6.4653e-01, 5.0126e-03, 2.9797e-03,
        2.5215e-03, 7.3608e-02, 1.3304e-04, 7.5948e-04, 1.4772e-03, 6.1143e-03,
        2.3296e-03, 2.2510e-02, 3.1653e+00, 1.6963e-01, 5.3866e-01, 5.6495e-03,
        1.3245e+00, 1.3672e-02, 2.7033e-01, 2.2200e-02, 6.6486e-03, 4.3408e-03,
        9.6826e-03, 5.7635e-01, 4.3299e-02, 4.2017e-01, 1.5009e-04, 3.6024e-03,
        1.3522e+00, 3.6729e-01, 6.8888e+00, 4.4213e-02, 5.7893e-01, 1.1345e+00,
        6.6978e-03, 2.6845e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.1140e+01, 1.8963e+01, 4.3272e+01, 1.0203e+01, 2.3789e+00, 3.7150e+01,
        7.0192e+01, 4.6985e+01, 2.4077e+01, 2.8765e+01, 1.5877e+01, 7.0081e+01,
        8.4729e+00, 7.6042e+00, 1.7479e+01, 3.8903e+00, 1.0568e+01, 1.4587e+01,
        4.3929e+00, 1.6419e+01, 6.4078e-01, 1.7025e-01, 4.0756e-01, 3.8729e+00,
        2.8253e+01, 1.8119e+01, 3.2079e+01, 1.4358e+01, 1.4028e+01, 9.4941e-01,
        2.4239e-01, 2.1848e-01, 1.1301e+00, 4.2101e+00, 2.5585e-01, 3.7765e-01,
        2.1450e+00, 1.1452e+00, 2.0905e-01, 2.2588e+00, 1.7508e-01, 6.7729e+00,
        2.1301e-02, 1.2380e-01, 3.6607e-02, 8.8078e-01, 6.9310e-01, 2.9771e-01,
        1.8553e-01, 5.6709e+00, 3.9890e-02, 1.2516e-01, 1.5914e-01, 4.0014e-01,
        3.9150e-01, 1.3177e+00, 1.3030e+01, 4.5191e+00, 4.0615e+00, 4.8846e-01,
        1.7853e+00, 6.2139e-01, 1.4487e+00, 4.9475e-01, 1.2532e-01, 1.8743e-01,
        1.3628e-01, 1.9104e+01, 1.0338e-01, 4.6198e+00, 1.7115e-02, 2.1121e-01,
        4.0318e+00, 2.3365e+00, 8.1075e+00, 4.4276e-02, 5.7899e-01, 1.1360e+00,
        1.1631e-02, 2.6617e-01], device='cuda:0')
Outer loop valEpocw Maximum [23/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 45.2
model_train val_loss valEpocw [23/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 37.5
model_train val_loss valEpocw [23/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 722.3
Sum_Val Meta Model:  tensor([2.8422e+00, 1.6102e-02, 5.7852e-01, 1.4117e-02, 2.8057e-03, 1.3526e-02,
        2.0971e-03, 4.5219e-02, 2.2960e-02, 6.5981e-03, 1.3092e-03, 2.9773e-03,
        2.5536e-04, 1.3846e-01, 3.1372e-02, 3.6857e-02, 1.0593e-01, 3.3208e-02,
        1.4999e-02, 2.6902e-02, 2.8473e-04, 2.7659e-03, 3.2691e-02, 3.2800e-03,
        4.7749e-02, 5.6689e-03, 5.5706e-01, 1.6222e-02, 1.3035e-03, 1.0035e-02,
        1.5002e-02, 1.0768e-02, 2.5812e-01, 9.9155e-05, 1.1223e-02, 5.6141e-02,
        4.4620e-02, 6.7895e-02, 3.6905e-03, 7.4290e-01, 4.4952e+00, 3.2767e+01,
        5.4722e+01, 6.1505e+01, 4.9857e+01, 6.9602e+01, 3.2339e-02, 4.1995e-02,
        2.3738e-01, 9.7782e-02, 3.0085e-02, 9.3189e-02, 2.3058e-01, 7.5236e-02,
        1.3205e-01, 9.8614e-01, 3.1260e+01, 2.0332e-01, 8.8190e-01, 8.2353e-03,
        9.6522e+01, 1.2474e-02, 1.7241e+00, 2.8341e-01, 4.2242e-01, 8.9752e-03,
        4.2089e-01, 1.4761e-01, 2.0960e+00, 1.1861e-01, 6.4679e-04, 4.9890e-02,
        1.3547e+00, 4.1082e+00, 1.1110e+00, 3.1003e+01, 1.1250e+01, 8.4690e-01,
        1.1243e-01, 3.8519e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.0598e+00, 1.6770e-03, 2.5556e-01, 8.2092e-04, 9.2482e-05, 3.8238e-04,
        5.2187e-05, 4.7721e-02, 2.1993e-03, 1.8600e-04, 1.3621e-04, 5.8928e-05,
        3.4824e-05, 1.0849e-01, 2.8281e-03, 2.0056e-02, 2.8114e-02, 2.4685e-03,
        5.0960e-03, 9.3589e-04, 1.1484e-05, 2.8865e-05, 1.0096e-04, 7.9826e-05,
        2.0865e-02, 1.5845e-03, 5.4400e-01, 1.8670e-02, 8.9946e-04, 9.0090e-04,
        1.4617e-03, 1.0834e-02, 2.6517e-01, 9.4143e-05, 2.1965e-03, 2.3597e-02,
        3.0637e-02, 5.3369e-02, 3.1834e-03, 1.0125e+00, 3.3226e+00, 3.9523e+01,
        5.1116e+01, 5.0364e+01, 7.8111e+01, 7.5377e+01, 1.9789e-02, 1.7379e-02,
        1.7361e-01, 4.0897e-02, 1.9121e-02, 1.0653e-01, 2.1781e-01, 9.8066e-02,
        9.2919e-02, 7.0588e-01, 1.7193e+01, 2.4402e-01, 8.8541e-01, 4.6622e-03,
        9.2493e+01, 5.4255e-03, 1.5080e+00, 2.5204e-01, 3.7239e-01, 2.6365e-02,
        4.5058e-01, 1.8839e-01, 2.1061e+00, 8.5529e-02, 3.0718e-04, 4.4666e-02,
        1.8227e+00, 4.2433e+00, 5.0688e-01, 2.5233e+01, 1.1628e+01, 4.9388e+00,
        7.3450e-02, 5.4703e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.6543e+01, 3.3195e-01, 4.7548e+00, 1.4210e-01, 4.5260e-02, 6.8340e-02,
        4.4891e-02, 1.5409e+01, 2.3448e-01, 4.8637e-02, 1.1823e-01, 2.6299e-02,
        4.8201e-02, 1.6172e+01, 3.0337e-01, 1.9887e+00, 2.2162e+00, 2.6197e-01,
        5.3307e-01, 8.8834e-02, 4.6620e-02, 1.9673e-02, 3.1573e-02, 4.5110e-02,
        3.8495e+00, 8.4184e-01, 2.6498e+01, 1.0112e+01, 7.8716e-01, 3.1884e-01,
        5.0017e-01, 7.6437e+00, 9.3040e+00, 2.7018e-01, 6.9065e-01, 1.5621e+00,
        1.4031e+01, 1.1016e+01, 3.2251e-01, 4.2540e+01, 2.3679e+01, 8.2214e+01,
        6.8126e+01, 7.5584e+01, 8.3129e+01, 9.1499e+01, 9.8742e+00, 5.5966e+00,
        3.7060e+01, 9.1647e+00, 2.3569e+01, 5.0922e+01, 6.9161e+01, 1.9840e+01,
        5.8387e+01, 9.5980e+01, 9.6683e+01, 1.5986e+01, 8.7110e+00, 1.4146e+00,
        1.1261e+02, 7.5117e-01, 1.0459e+01, 1.2843e+01, 1.3830e+01, 3.2900e+00,
        1.1410e+01, 1.5569e+01, 5.1062e+00, 1.6883e+00, 1.2980e-01, 8.0704e+00,
        6.0976e+00, 4.2168e+01, 5.5657e-01, 2.5238e+01, 1.1628e+01, 4.9397e+00,
        1.1996e-01, 8.6822e-01], device='cuda:0')
Outer loop valEpocw Maximum [23/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 464.7
model_train val_loss valEpocw [23/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 467.2
model_train val_loss valEpocw [23/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1413.8
Sum_Val Meta Model:  tensor([9.3475e+00, 2.9893e-02, 2.3101e+00, 1.7253e-02, 4.4814e-03, 2.2707e-01,
        4.2441e-02, 2.2107e-01, 2.8507e-02, 3.1178e-01, 2.7630e-02, 4.9114e-02,
        5.2200e-04, 3.4300e-01, 4.0527e-01, 4.4596e-02, 4.1708e-02, 2.7947e-02,
        8.5779e-03, 1.6509e-02, 5.6230e-04, 1.9375e-03, 3.5327e-03, 4.3307e-03,
        3.5239e-01, 1.2719e-02, 1.5451e+00, 1.2475e-02, 1.0427e-01, 4.0186e-03,
        1.1006e-02, 1.0793e-03, 2.3390e-01, 3.4333e-02, 7.8327e-02, 3.2842e-01,
        2.7437e-03, 2.0556e-02, 1.6927e-01, 1.1970e+00, 2.7657e+00, 1.6319e+01,
        7.2414e+00, 7.0772e+00, 1.0701e+01, 1.5227e+01, 3.7236e-03, 6.7057e-03,
        1.5381e-02, 6.5376e-03, 1.9665e-03, 2.2508e-03, 2.2564e-03, 1.1289e-01,
        5.0632e-03, 3.1236e-02, 6.0824e+00, 1.9625e-01, 2.1877e+00, 2.2974e-02,
        3.9808e+01, 1.0343e-02, 9.1482e-01, 3.9251e-01, 3.4655e-01, 5.2471e-02,
        5.2768e-01, 2.0707e+00, 4.3520e-01, 1.9385e-01, 6.2389e-04, 3.1037e-02,
        1.1366e+00, 1.9063e+00, 3.9757e+02, 1.3955e+01, 1.3940e+00, 9.2711e+00,
        3.1236e-02, 1.0398e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([3.1075e+00, 2.5677e-02, 1.7393e+00, 2.6476e-02, 9.7654e-03, 1.8326e-01,
        4.3999e-02, 2.0018e-01, 6.1215e-02, 2.3061e-01, 2.1571e-02, 8.2262e-02,
        5.7994e-03, 3.1488e-01, 4.8297e-01, 3.1227e-02, 3.5670e-02, 1.6265e-02,
        4.5601e-03, 3.1188e-03, 5.3082e-04, 3.3515e-04, 2.1955e-03, 6.7653e-03,
        3.7789e-01, 1.6415e-02, 1.2555e+00, 2.3973e-02, 1.1021e-01, 2.0035e-03,
        2.3416e-03, 1.7818e-03, 2.0295e-02, 1.0837e-03, 1.2837e-03, 8.5789e-03,
        4.3163e-03, 4.0699e-03, 7.8596e-03, 1.5309e-01, 2.1049e-01, 7.7977e-01,
        8.9793e-02, 2.0592e-01, 3.1655e-01, 5.9357e-01, 2.4178e-03, 1.7451e-03,
        1.3880e-03, 1.1354e-03, 1.4914e-04, 3.0141e-04, 4.7279e-04, 2.9047e-03,
        2.8073e-03, 1.2055e-02, 1.3786e+00, 7.3361e-02, 1.6534e+00, 1.1841e-02,
        8.9112e+00, 1.1821e-02, 4.9562e-01, 2.5155e-02, 2.0251e-02, 2.2130e-02,
        4.4723e-02, 2.6328e-01, 7.9977e-02, 1.8173e-02, 4.0257e-04, 8.2056e-03,
        9.0490e-02, 7.5521e-01, 1.0827e+02, 1.7844e+00, 9.2578e-02, 8.2961e+00,
        1.9365e-02, 7.6713e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.5764e+01, 1.5838e+00, 1.9114e+01, 1.3694e+00, 1.0930e+00, 9.8099e+00,
        7.0613e+00, 1.6566e+01, 2.3577e+00, 1.6162e+01, 3.7530e+00, 8.5484e+00,
        1.4031e+00, 1.5818e+01, 1.8973e+01, 1.2360e+00, 1.2487e+00, 5.9650e-01,
        1.7006e-01, 1.1305e-01, 2.6454e-01, 4.9610e-02, 2.0039e-01, 9.2673e-01,
        2.6271e+01, 1.8660e+00, 2.6460e+01, 2.6440e+00, 1.7609e+01, 1.9135e-01,
        2.2462e-01, 2.7814e-01, 3.1266e-01, 4.1173e-01, 1.0742e-01, 2.2818e-01,
        4.9328e-01, 2.5804e-01, 2.3096e-01, 2.6520e+00, 9.7855e-01, 1.6329e+00,
        1.4274e-01, 3.6835e-01, 3.6104e-01, 8.0326e-01, 3.1962e-01, 1.6918e-01,
        9.1688e-02, 8.4289e-02, 3.9023e-02, 4.4792e-02, 4.5588e-02, 1.5846e-01,
        4.3415e-01, 6.4157e-01, 5.3969e+00, 1.7972e+00, 1.1362e+01, 9.2593e-01,
        1.1754e+01, 4.8018e-01, 2.4874e+00, 4.7577e-01, 3.1058e-01, 8.3464e-01,
        5.2026e-01, 6.6162e+00, 1.9167e-01, 1.8578e-01, 4.0019e-02, 4.1824e-01,
        2.5056e-01, 4.5993e+00, 1.2867e+02, 1.7874e+00, 9.2592e-02, 8.3103e+00,
        3.9200e-02, 6.6250e-02], device='cuda:0')
Outer loop valEpocw Maximum [23/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 555.8
model_train val_loss valEpocw [23/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 143.2
model_train val_loss valEpocw [23/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 438.3
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.78401823 97.37332635 93.38336521 98.08238204 98.59163509 97.62064302
 98.20664953 95.19377201 98.09456512 96.92742535 98.6488956  98.95225448
 99.44079629 95.36677185 97.51343185 97.38672774 96.40720751 97.70592464
 98.79387434 98.47833238 98.7317406  99.32749357 99.50536665 99.08626844
 95.24372266 96.80681278 94.42136426 96.9067141  98.08725527 98.29436776
 98.32482548 98.56605061 97.0041788  98.53071965 98.66107869 98.86453625
 97.59871347 98.44178312 98.94859955 93.45037219 98.11040314 95.43134221
 98.16157211 97.4086573  98.37721275 97.26733349 98.27000158 98.67204347
 98.1603538  98.7183392  98.79752927 98.662297   99.02900793 98.44421973
 98.7463603  97.6596289  93.34681595 97.00661542 96.7117847  97.6596289
 96.8226508  98.71712089 97.70226971 97.44033333 98.87671934 97.57312898
 98.72443075 96.01613041 99.01804315 98.31507901 99.81359876 97.54267126
 99.04850087 96.08801062 98.58310693 99.09357829 99.64669046 99.42739489
 99.85380295 99.20200777]
Accuracy th:0.7 is [86.12590003 97.3111926  92.6852743  97.91669205 98.50635348 97.42327701
 97.98857226 94.86604695 97.98369903 96.72762271 98.61356465 98.91448691
 99.42495827 95.32534935 97.42449532 97.32581231 96.39015119 97.60967824
 98.8852475  98.36746628 98.64036744 99.28119784 99.52485959 99.08505013
 95.22422972 96.7251861  94.1764842  97.02489005 98.04095954 98.19690306
 98.10674821 98.59163509 96.95788307 98.55264921 98.43325496 98.82555037
 97.32337569 98.29802268 98.8292053  93.24813294 97.98979057 95.24494097
 97.83019213 96.99321402 98.33579026 96.82143249 98.22370585 98.63914913
 98.11405806 98.67448009 98.68666317 98.60259987 99.01316992 98.37599444
 98.73783214 97.55485435 92.76933761 96.74467904 96.58386228 97.58165714
 96.2098415  98.73905045 97.60724163 97.29291797 98.82189544 97.43302348
 98.61843788 95.97592622 98.92545169 98.13720593 99.81603538 97.28073488
 98.96565588 95.83703902 98.39670569 99.01316992 99.64912708 99.41886673
 99.8464931  99.16789513]
Avg Prec: is [96.47582824 35.60226321 73.03114405 67.70801265 77.78373226 64.1098827
 73.37264923 47.60090309 57.79062588 52.48953883 32.33468153 54.19949073
 24.73458971 28.74069772 34.12398335 57.85021048 29.48258918 42.04715139
 48.32982286 41.99012201 62.84009588 48.51635241 89.55473365 83.48947394
 25.99271214 34.20404182 39.1104428  39.12538348 25.61478306 38.42719687
 73.52950985 36.32556112 57.72783732 62.15204612 73.40563163 79.84768002
 57.73636452 75.02891116 86.97892799 47.40873304 47.8232928  77.35093296
 77.23245165 72.12983363 80.18142926 85.74690576 40.74315817 33.89604929
 41.60565307 46.95235099 63.41111287 39.5063143  23.28097196 73.49250694
 26.95398428 44.02173148 74.00635965 58.65724703 45.91939401 60.49497315
 92.79114448 83.27792882 74.22114995 51.25196568 62.4663531  44.26883767
 62.07796068 27.3646119  65.24869262 68.23974949 11.35715033 73.71826676
 82.32868314 51.88450624 88.92401167 91.43358667 81.79649031 88.39591028
 27.29329078 30.79423058]
Accuracy th:0.5 is [45.44657107 97.2137279  71.45258952 97.02489005 97.26733349 76.12724017
 76.47202154 75.79586019 77.42717559 96.45106663 77.68667536 98.52097319
 99.41399349 80.09527174 77.32118273 96.56680596 96.29512311 77.03244356
 98.65376884 98.30776915 80.39132077 78.19714672 98.38695922 79.16448386
 81.40495364 96.65086926 94.0778012  76.71446498 98.01293844 77.60383036
 97.30875598 98.57457877 96.36213009 98.02024829 87.99478564 77.23468281
 77.04584496 91.35975439 97.11504489 74.36922065 79.20590636 92.05906361
 76.51222573 76.1174937  96.9627563  93.87434364 98.02877645 98.57336046
 97.63891765 89.32396048 88.18240519 98.55508583 98.99976852 76.6888805
 98.70615611 77.00320415 71.68650479 93.47839329 96.24273583 96.9067141
 89.79300934 97.17717864 93.8548507  77.1079787  98.42838172 77.57580926
 98.20786784 76.21861332 78.09115386 97.55972759 78.66132235 95.99054592
 77.91693571 95.45083515 76.37821177 82.96317053 88.41997539 77.63428808
 78.70396316 99.14718388]
Accuracy th:0.7 is [45.5476907  97.2137279  71.45258952 97.02489005 97.26733349 76.12724017
 76.47202154 76.06388811 77.42717559 96.4754328  77.68667536 98.52097319
 99.41399349 80.58381355 77.32118273 96.56680596 96.29512311 77.03244356
 98.65376884 98.30776915 80.84331331 78.19714672 98.38695922 79.90399727
 82.20050925 96.65086926 94.0778012  76.71446498 98.01293844 77.60383036
 97.30875598 98.57457877 96.36213009 98.02024829 88.15803901 77.23468281
 77.04584496 91.6253457  97.11504489 74.36922065 79.85891985 92.05906361
 76.51222573 76.1174937  96.9627563  93.87434364 98.02877645 98.57336046
 97.92278359 89.86123463 88.36271488 98.55508583 98.99976852 76.6888805
 98.70615611 77.00320415 71.68650479 93.99130128 96.24273583 96.9067141
 89.79300934 97.17717864 93.99251958 77.1079787  98.42838172 77.57580926
 98.20786784 76.21861332 78.09115386 97.55972759 78.66985051 95.99054592
 78.02292857 95.45083515 76.37821177 83.09109294 88.64536251 77.63428808
 78.70396316 99.14718388]
Avg Prec: is [56.08675583  3.06567475 11.32042692  3.25384758  2.22029862  3.77424637
  3.17571575  5.55695039  2.51208051  3.73045046  1.52562535  1.57733251
  0.6306923   5.32732813  2.64664135  3.16029427  3.76898046  2.85336955
  1.3777347   1.75863438  1.99518872  0.88212176  1.82404855  2.38896458
  5.08059806  3.56547888  6.32800435  3.47177195  2.0983519   1.87348774
  2.74135887  1.36456466  3.71699966  1.67250408  2.38023257  2.35653253
  2.98051887  2.51130143  2.75924697  7.45042876  2.19671202  8.26993822
  3.27448047  3.98539249  3.2844428   6.36585013  2.15645673  1.53164218
  2.06740945  1.59767726  1.78299143  1.53588656  1.10707566  2.96340119
  1.39256026  2.7853598  11.36297489  3.81384026  4.09218505  2.79212683
 10.87183141  2.1194599   3.86580704  2.99484687  1.59814832  2.58029734
  1.7999013   4.19889367  1.24782078  2.35944565  0.17840884  3.34177469
  1.89439864  4.6601435   3.98022673  3.19857241  0.85032901  1.85789197
  0.15289587  0.71042569]
mAP score regular 56.87, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [87.66973117 97.41136607 93.35525824 98.30580263 98.92617784 97.68791888
 98.39549543 95.38829509 98.14385729 97.04761193 98.6969629  99.03331091
 99.37464185 95.16655455 97.47365274 97.33662207 96.33256098 97.80252635
 98.89877171 98.39549543 98.92368637 99.34972718 99.57395919 99.27000025
 95.44809029 96.74116152 94.533722   97.12235593 97.94952288 98.19368662
 98.6595909  98.62471037 96.87320926 98.71440317 98.86139971 99.11552931
 98.00682662 98.29085383 99.07317438 93.29048011 98.00682662 92.97406383
 97.47863567 96.65894312 96.63402845 94.84764681 98.40047836 98.7941301
 98.13389142 98.78167277 98.84395944 98.6521165  98.90126317 98.49017116
 98.75924957 97.79006901 90.64205098 97.17965967 96.45962578 97.64307248
 93.02638463 98.80658744 97.2743354  97.51351621 98.80409597 97.54341381
 98.5848469  95.84174203 98.7941301  98.15631462 99.81563146 97.64307248
 98.49266263 95.93641777 97.48611007 97.43378927 99.00839624 98.57488103
 99.81812293 99.13047811]
Accuracy th:0.7 is [87.5177517  97.34658794 92.9616065  98.18122929 98.93614371 97.53344794
 98.20863542 95.17153748 98.12890849 96.79099086 98.6745397  99.00092184
 99.37215038 95.11921668 97.39890874 97.29925007 96.36495005 97.69290181
 98.97849864 98.37805516 98.88880584 99.33477838 99.64621172 99.24010265
 95.44310736 96.64399432 94.42658893 97.23198047 97.88723622 98.20365249
 98.4802053  98.67952263 97.0276802  98.7268605  98.6894885  99.15290131
 97.76515435 98.15382316 98.98348158 93.29297157 97.94703142 93.41505344
 97.44126367 96.61409672 96.82088846 94.87505294 98.39798689 98.82901064
 98.09153649 98.73682637 98.75924957 98.64962503 98.89378877 98.44034183
 98.75177517 97.69041034 91.08553205 97.05259486 96.40232205 97.55587114
 93.04880783 98.83897651 97.36153674 97.4014002  98.78416424 97.54839674
 98.63218477 95.81931883 98.81157037 97.99436929 99.81563146 97.48860154
 98.4951541  95.84174203 97.4088746  97.50604181 99.11054638 98.67204823
 99.82310586 99.15788425]
Avg Prec: is [96.35788193 34.71482255 71.33236325 73.25576916 77.48340512 65.27230282
 79.73940975 47.95506277 62.65333688 55.39908949 39.75979798 57.06174462
 24.91747374 31.82475206 35.16716231 62.4319424  32.18433605 46.27408534
 50.87259003 37.21174409 71.62272138 56.57204426 92.21698505 88.33378915
 24.76309078 37.36337361 33.10910506 45.0227587  28.77390692 37.097181
 77.78772953 36.85489265 54.78175816 65.4855622  73.96225292 82.22728368
 60.56873731 77.6727953  89.20946239 45.35731645 37.76612261 53.27220011
 51.28335918 40.44059092 29.30301959 52.3835348  39.76120825 28.35028772
 41.94610759 42.32420355 67.84689989 36.08862526 24.45620705 76.8863087
 28.96453022 42.38881589 56.24394666 57.19576583 39.68030398 62.35941459
 66.16096716 86.91172544 66.93546532 53.63315099 61.72986742 38.57022797
 61.95338482 26.92041581 42.86511243 65.47813238  9.51322533 74.64097285
 58.30361871 45.17942779 65.31724247 51.87337435 13.93144713 53.52068292
  2.48050652 23.47320248]
Accuracy th:0.5 is [45.27991629 97.22450607 69.82833794 96.96290206 97.90716795 75.22485487
 75.32700501 74.38772205 76.81939358 96.41976231 76.94894985 98.5325261
 99.34972718 77.948028   76.89662905 96.31262924 96.21047911 76.31362583
 98.78167277 98.34068316 78.82751576 77.59922266 98.31327703 80.17539926
 78.09502454 96.52938685 94.3393876  76.4058101  97.81747515 76.91656078
 97.52597354 98.67204823 96.39983058 98.18870369 89.35894561 76.53785784
 76.5254005  92.61778409 97.0276802  73.72997484 77.42481999 92.37362035
 75.66086155 75.32451354 97.03764606 94.02795426 98.18621222 98.77668984
 97.92211675 89.14966241 86.75785435 98.55993223 98.87385704 75.65089568
 98.6969629  76.34352343 70.51598276 94.41911453 96.16314124 96.78102499
 90.13379176 97.04761193 94.07778359 76.39335277 98.32075143 77.24045145
 98.13139996 75.56618581 77.62164586 97.53593941 78.03024641 96.07843137
 77.38993946 95.44559882 75.47898448 84.03218975 90.64703391 76.98383038
 78.10499041 99.15040985]
Accuracy th:0.7 is [45.47923362 97.22450607 69.82833794 96.96290206 97.90716795 75.22485487
 75.32700501 74.54468446 76.81939358 96.41976231 76.94894985 98.5325261
 99.34972718 78.36410295 76.89662905 96.31262924 96.21047911 76.31362583
 98.78167277 98.34068316 79.16386377 77.59922266 98.31327703 80.62386327
 78.59082642 96.52938685 94.3393876  76.4058101  97.81747515 76.91656078
 97.52597354 98.67204823 96.39983058 98.18870369 89.53833122 76.53785784
 76.5254005  92.82208436 97.0276802  73.72997484 77.91314747 92.37362035
 75.66086155 75.32451354 97.03764606 94.02795426 98.18621222 98.77668984
 97.95699728 89.52338242 86.91979969 98.55993223 98.87385704 75.65089568
 98.6969629  76.34352343 70.51598276 94.74798814 96.16314124 96.78102499
 90.13379176 97.04761193 94.18989959 76.39335277 98.32075143 77.24045145
 98.13139996 75.56618581 77.62164586 97.53593941 78.03024641 96.07843137
 77.43229439 95.44559882 75.47898448 84.16922042 90.87126591 76.98383038
 78.10499041 99.15040985]
Avg Prec: is [54.46636315  3.72869123 14.76037234  4.55880216  1.52314371  4.27756737
 12.2733491   8.66007765  7.63412847  5.14063213  2.29997807  5.08119027
  1.54669743  5.88267393  3.0610941   3.67850481 24.71860927  6.32546929
  1.5437265   2.64374782  3.59126493  1.43494242  1.15609444  5.68120857
  5.72393597 10.69654447  8.13815563  4.55084206  3.90840044  6.68702623
  2.32949251  0.86954335  3.02374968  1.15409249  1.75657348  2.40884018
  1.99504628  2.12623696  2.19809888  6.22492235  1.72855647  6.08183171
  2.18913208  2.73370471  2.34457287  4.91602695  1.75393263  1.03627316
  1.45579515  1.17383114  1.21058269  1.04382224  0.7544685   2.27983956
  0.9114857   1.87541139 10.21179268  3.03265955  3.99265096  2.81383689
  7.93379297  2.01793568  3.3351637   2.52143952  1.34598949  1.89316776
  1.55908262  3.51706488  1.07200708  2.21058219  0.19635603  3.20789273
  1.55051319  4.00171142  3.18569992  2.32658112  0.57185814  1.47586603
  0.12166083  0.60864406]
mAP score regular 51.74, mAP score EMA 4.35
Train_data_mAP: current_mAP = 56.87, highest_mAP = 56.87
Val_data_mAP: current_mAP = 51.74, highest_mAP = 51.91
tensor([3.5463e-02, 4.3232e-03, 4.9864e-02, 4.8867e-03, 1.6174e-03, 4.4359e-03,
        8.9825e-04, 2.5902e-03, 8.0575e-03, 3.0319e-03, 8.6700e-04, 1.7805e-03,
        5.8160e-04, 5.6979e-03, 7.5814e-03, 8.3985e-03, 1.0240e-02, 7.3313e-03,
        7.8777e-03, 8.7657e-03, 1.7918e-04, 1.0757e-03, 2.2087e-03, 1.4205e-03,
        4.8961e-03, 1.4946e-03, 2.0830e-02, 1.3704e-03, 8.8697e-04, 2.0654e-03,
        2.1326e-03, 9.3616e-04, 2.2619e-02, 2.4283e-04, 2.3594e-03, 1.2783e-02,
        1.4702e-03, 3.6369e-03, 8.3648e-03, 2.3284e-02, 1.5718e-01, 4.7555e-01,
        7.6077e-01, 6.7560e-01, 9.5251e-01, 8.3328e-01, 1.3649e-03, 2.0763e-03,
        3.0897e-03, 2.9529e-03, 5.3105e-04, 1.4814e-03, 2.2242e-03, 3.7299e-03,
        1.1121e-03, 5.5212e-03, 1.6818e-01, 1.2660e-02, 1.0844e-01, 2.5572e-03,
        8.3049e-01, 5.8369e-03, 1.4175e-01, 1.7163e-02, 2.6533e-02, 6.4950e-03,
        3.9626e-02, 1.1343e-02, 4.8631e-01, 5.1023e-02, 1.8612e-03, 4.5914e-03,
        3.6527e-01, 9.4933e-02, 9.3578e-01, 9.9987e-01, 1.0000e+00, 9.9989e-01,
        6.6437e-01, 6.3617e-02], device='cuda:0')
Sum Train Loss:  tensor([1.5075e+00, 5.6066e-02, 1.0300e+00, 8.3230e-03, 3.9550e-03, 4.1180e-02,
        1.1751e-02, 3.5962e-02, 5.1155e-02, 3.0068e-02, 1.4888e-03, 1.8423e-02,
        5.1156e-04, 1.1644e-01, 5.5780e-02, 1.1212e-01, 8.6440e-02, 9.9282e-02,
        1.7701e-02, 7.7656e-02, 1.0957e-03, 7.1556e-03, 1.5328e-02, 3.7773e-03,
        7.2545e-02, 1.5613e-02, 2.8126e-01, 1.6420e-02, 6.4169e-03, 9.9303e-03,
        6.3473e-03, 5.9698e-03, 2.6353e-01, 1.8170e-03, 1.9190e-02, 3.0456e-02,
        3.0734e-02, 2.8123e-02, 6.8299e-02, 8.5346e-01, 1.1893e+00, 9.6570e+00,
        4.3327e+00, 4.4250e+00, 3.8050e+00, 1.1597e+01, 8.3260e-03, 4.0107e-02,
        1.2543e-02, 2.7970e-02, 1.7731e-03, 1.4572e-02, 2.9958e-03, 2.6104e-02,
        4.0147e-03, 4.1457e-02, 5.6719e+00, 1.4077e-01, 1.3345e+00, 2.8917e-02,
        1.6096e+01, 2.7725e-02, 7.9792e-01, 1.5599e-01, 1.3876e-01, 5.3622e-02,
        1.0763e-01, 1.7859e-01, 1.9084e+00, 1.4229e-01, 6.0644e-04, 7.1494e-02,
        5.3923e+00, 1.1617e+00, 3.3787e+00, 1.8756e+00, 4.6808e-01, 4.2281e+00,
        1.3522e-01, 5.1014e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [24/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 83.8
Sum Train Loss:  tensor([1.2278e+00, 4.0121e-02, 1.2490e+00, 3.5368e-02, 4.3317e-03, 4.3761e-02,
        5.9561e-03, 4.0479e-02, 1.4046e-01, 3.3485e-02, 4.5603e-03, 8.2074e-03,
        1.8134e-03, 1.6403e-01, 5.4003e-02, 7.4048e-02, 1.1993e-01, 3.6886e-02,
        2.1808e-02, 7.0969e-02, 8.4756e-04, 2.2256e-03, 1.9846e-03, 1.9678e-02,
        6.6518e-02, 2.0591e-02, 4.0344e-01, 1.2037e-02, 7.7957e-03, 1.1878e-02,
        2.1121e-02, 5.4669e-03, 2.7483e-01, 1.5063e-03, 4.1931e-03, 1.8967e-02,
        1.2870e-02, 1.5884e-02, 1.9846e-02, 5.6977e-01, 1.4675e+00, 7.0350e+00,
        2.2196e+00, 9.2236e+00, 3.5186e+00, 5.3325e+00, 4.8116e-03, 1.3307e-02,
        9.1138e-03, 4.5373e-03, 1.6153e-03, 2.5321e-02, 7.8717e-03, 2.7613e-02,
        5.5904e-03, 7.0799e-02, 3.3086e+00, 1.5831e-01, 1.6858e+00, 2.0630e-02,
        1.4067e+01, 1.8746e-02, 1.4052e+00, 1.0537e-01, 1.3109e-01, 4.2723e-02,
        5.0350e-02, 2.3065e-01, 3.4619e+00, 4.2036e-01, 1.7240e-04, 9.8064e-02,
        3.4501e+00, 1.3029e+00, 6.2386e+00, 2.6637e+00, 4.0791e+00, 4.7049e-01,
        6.4292e-02, 2.5676e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [24/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 77.6
Sum Train Loss:  tensor([8.5438e-01, 5.3030e-02, 8.5202e-01, 3.5822e-02, 3.1381e-03, 2.5543e-02,
        7.6303e-03, 2.7903e-02, 1.4333e-01, 1.9122e-02, 3.4599e-03, 9.4426e-03,
        2.5170e-04, 1.2462e-01, 8.6181e-02, 3.5475e-02, 1.4546e-01, 5.3490e-02,
        1.2289e-02, 4.4260e-02, 1.3930e-03, 1.3383e-02, 4.1648e-04, 1.2023e-03,
        1.0269e-01, 1.6990e-02, 4.7605e-01, 1.4242e-02, 1.0366e-02, 1.2223e-02,
        1.0403e-02, 5.0262e-03, 2.6168e-01, 8.1682e-04, 8.8219e-03, 3.2187e-02,
        7.7567e-03, 2.1418e-02, 4.4649e-02, 4.8705e-01, 2.1203e+00, 1.2055e+01,
        3.3720e+00, 9.0581e+00, 4.2632e+00, 1.2900e+01, 7.4623e-03, 1.7788e-02,
        7.6706e-03, 1.6736e-02, 6.3596e-03, 8.2312e-03, 8.3059e-03, 2.7450e-02,
        7.6171e-03, 7.9028e-02, 5.3050e+00, 1.5139e-01, 1.1458e+00, 1.3962e-02,
        1.0527e+01, 4.2685e-02, 1.1956e+00, 1.7807e-01, 9.8616e-02, 6.0417e-02,
        2.1775e-01, 1.5044e-01, 2.2425e+00, 6.0379e-01, 3.6919e-04, 4.5136e-02,
        8.5086e-01, 1.2570e+00, 5.7757e+00, 4.2164e+00, 3.8785e-01, 5.5050e-01,
        1.3259e-01, 6.1603e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [24/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 83.8
Sum Train Loss:  tensor([1.5214e+00, 6.4895e-02, 1.0155e+00, 6.9724e-02, 6.7444e-03, 4.4423e-02,
        8.9027e-03, 3.6330e-02, 7.9669e-02, 3.8960e-02, 1.2683e-02, 7.2098e-03,
        6.7421e-04, 1.2336e-01, 1.2370e-01, 6.6637e-02, 2.1975e-01, 9.1186e-02,
        5.9432e-03, 1.3914e-02, 1.9894e-03, 4.0906e-03, 4.8595e-03, 5.2626e-03,
        1.0671e-01, 2.6656e-02, 3.9991e-01, 1.3093e-02, 2.2420e-02, 8.1076e-03,
        2.0288e-02, 7.7161e-03, 1.1718e-01, 3.6721e-04, 6.2543e-03, 2.5238e-02,
        7.1781e-03, 1.9309e-02, 2.1597e-02, 3.2607e-01, 5.2590e-01, 7.3797e+00,
        3.3507e+00, 3.5411e+00, 1.5029e+00, 1.0831e+01, 3.6374e-03, 1.6255e-02,
        1.1695e-02, 2.8296e-03, 1.7768e-03, 1.5383e-03, 2.9815e-03, 2.1080e-02,
        8.2844e-03, 2.2743e-02, 3.9408e+00, 8.0648e-02, 5.6093e-01, 1.2226e-02,
        7.0862e+00, 1.0721e-02, 1.2672e+00, 1.4021e-01, 1.1338e-01, 3.9219e-02,
        3.4813e-01, 1.2655e-01, 1.8358e+00, 1.8936e-01, 3.8407e-04, 5.3750e-02,
        8.3629e-01, 7.4885e-01, 5.1860e+00, 6.6809e+00, 1.4519e+00, 4.2000e+00,
        1.3082e-01, 2.8024e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [24/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 67.2
Sum Train Loss:  tensor([1.8287e+00, 2.3709e-02, 1.2329e+00, 3.7992e-02, 6.8428e-03, 4.7414e-02,
        9.1257e-03, 4.5688e-02, 4.8532e-02, 4.7183e-02, 8.6405e-03, 1.2861e-02,
        3.4749e-04, 9.1969e-02, 5.0724e-02, 7.2956e-02, 1.7243e-01, 5.6702e-02,
        8.0636e-02, 9.4845e-02, 1.5845e-03, 8.2538e-03, 1.7201e-03, 3.0171e-03,
        7.8981e-02, 1.4600e-02, 4.2185e-01, 1.2199e-02, 7.9629e-03, 5.1220e-03,
        1.3178e-02, 3.4368e-03, 2.2714e-01, 4.9662e-04, 1.1539e-02, 3.0688e-02,
        1.3144e-02, 9.9564e-03, 6.4041e-02, 5.7667e-01, 3.3498e+00, 9.0394e+00,
        3.0310e+00, 5.3659e+00, 7.1810e+00, 1.3735e+01, 8.4468e-03, 1.6543e-02,
        2.1490e-02, 1.7482e-02, 1.9715e-03, 1.7529e-02, 1.8255e-02, 3.9699e-02,
        1.2741e-02, 3.8571e-02, 4.1218e+00, 1.3808e-01, 1.0861e+00, 1.1344e-02,
        1.2343e+01, 5.9621e-02, 2.1486e+00, 2.8353e-01, 1.3356e-01, 4.1608e-02,
        4.7992e-01, 2.5658e-01, 4.7691e-01, 9.7112e-02, 1.6739e-04, 5.1221e-02,
        3.9376e+00, 2.5415e+00, 2.1044e+00, 3.5502e+00, 1.4045e+00, 6.2983e+00,
        5.6867e+00, 3.2574e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [24/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 94.7
Sum Train Loss:  tensor([1.2948e+00, 5.3176e-02, 1.4019e+00, 2.9552e-02, 3.9664e-03, 2.3122e-02,
        2.0980e-03, 3.7519e-02, 2.6864e-02, 1.7749e-02, 7.3831e-04, 2.5786e-03,
        4.2689e-04, 1.4099e-01, 1.1668e-01, 2.7883e-02, 9.9865e-02, 8.8803e-02,
        1.2218e-02, 5.7151e-02, 1.5846e-03, 8.6281e-03, 5.4257e-04, 1.4008e-03,
        1.4619e-01, 1.5239e-02, 5.2717e-01, 2.0794e-02, 3.5913e-03, 2.6459e-02,
        3.8289e-02, 8.6761e-03, 2.6562e-01, 8.5733e-04, 2.9259e-03, 1.0356e-01,
        1.0913e-02, 1.5856e-02, 1.8307e-02, 6.9016e-01, 2.1567e+00, 7.3029e+00,
        2.9158e+00, 4.8049e+00, 6.1112e+00, 1.4831e+01, 8.7743e-03, 6.8819e-03,
        2.4686e-02, 1.5394e-02, 1.9172e-03, 6.4003e-03, 2.1303e-02, 1.9714e-02,
        1.3816e-03, 3.5851e-02, 3.3153e+00, 1.0986e-01, 1.5350e+00, 2.4964e-02,
        1.5586e+01, 2.5749e-02, 8.6698e-01, 2.3834e-01, 8.9859e-02, 5.2484e-02,
        4.5923e-01, 1.5260e-01, 2.7074e+00, 4.3710e-01, 9.9825e-04, 1.8288e-02,
        2.0419e+00, 1.9098e+00, 2.2144e+00, 3.6608e+00, 4.1856e+00, 1.1732e+00,
        7.1610e-02, 6.9020e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [24/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 84.6
Sum Train Loss:  tensor([1.5162e+00, 4.3918e-02, 8.3964e-01, 7.3532e-02, 3.6741e-03, 5.3482e-02,
        8.1387e-03, 3.6385e-02, 6.1981e-02, 2.1872e-02, 1.3048e-03, 1.1836e-02,
        6.6331e-04, 1.2156e-01, 1.1312e-01, 1.1443e-01, 2.0049e-01, 7.0661e-02,
        6.2154e-02, 2.3420e-02, 2.5220e-04, 1.1748e-02, 6.1554e-04, 3.1081e-03,
        1.1693e-01, 1.2362e-02, 4.7404e-01, 4.3836e-03, 1.1487e-02, 1.9775e-03,
        1.6220e-02, 7.4995e-03, 1.5627e-01, 1.8524e-03, 1.4922e-02, 8.4739e-02,
        1.0519e-02, 3.4162e-02, 1.3117e-02, 6.3252e-01, 3.0388e+00, 4.6801e+00,
        9.1597e+00, 1.0194e+01, 3.5115e+00, 1.5541e+01, 4.4793e-03, 1.4855e-02,
        3.1836e-02, 1.1994e-02, 3.2049e-03, 8.0237e-03, 2.4300e-02, 1.2146e-02,
        6.6998e-03, 1.6628e-02, 5.4856e+00, 2.6376e-01, 1.0979e+00, 1.9883e-02,
        1.0197e+01, 1.3635e-02, 1.0173e+00, 1.4310e-01, 8.6312e-02, 6.1286e-02,
        1.8388e-01, 1.1803e-01, 1.5357e+00, 6.7989e-01, 8.4952e-03, 6.7913e-02,
        1.2585e+00, 1.2813e+00, 4.9800e+00, 1.0241e+01, 5.2970e-01, 4.5208e+00,
        1.1377e-01, 3.6970e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [24/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 95.5
Sum_Val Meta Model:  tensor([1.7887e+00, 2.9374e-01, 4.0860e+00, 1.9292e-01, 2.7087e-03, 5.3374e-02,
        3.3941e-03, 4.6648e-02, 4.7819e-02, 2.4133e-02, 5.2056e-03, 2.0357e-02,
        5.2162e-03, 8.4134e-02, 6.7204e-02, 6.8025e-02, 4.6124e+00, 1.2365e-02,
        8.1482e-03, 7.7118e-03, 1.6916e-04, 7.5696e-04, 8.1005e-04, 1.0120e-03,
        1.3031e-01, 2.6937e-02, 5.7556e-01, 5.3638e-03, 8.9862e-03, 2.8356e-02,
        4.5689e-03, 6.8533e-04, 1.8411e-01, 2.8131e-04, 3.1434e-03, 1.3221e-02,
        4.1930e-03, 1.7856e-02, 1.5170e-02, 1.0873e+00, 1.9420e+00, 1.1319e+01,
        6.7087e+00, 1.3394e+01, 1.9570e+01, 1.9560e+01, 2.5012e-03, 1.0992e-02,
        2.0231e-03, 1.5075e-02, 3.0612e-04, 7.1680e-04, 1.3943e-02, 3.5024e-03,
        9.0730e-04, 4.4133e-03, 5.2087e+00, 8.8646e-02, 1.3816e+00, 9.5316e-03,
        8.8702e+00, 6.3806e-02, 7.8320e-01, 7.3710e-02, 1.3228e-02, 7.4994e-03,
        1.8697e-02, 8.3264e-02, 5.4267e+00, 9.4845e-01, 2.5235e-04, 8.2373e-02,
        7.2427e+00, 9.2808e-01, 2.5498e+01, 1.0587e+01, 1.9471e-01, 1.0297e+01,
        1.4323e-01, 3.9012e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.7134e+00, 1.9052e-01, 2.5253e+00, 1.1572e-01, 1.0996e-03, 5.3809e-02,
        2.6783e-03, 4.9319e-02, 5.1660e-02, 2.7198e-02, 5.8175e-03, 2.1078e-02,
        6.2436e-03, 7.9003e-02, 6.5360e-02, 2.4031e-01, 2.8149e+00, 1.8071e-02,
        6.3185e-03, 1.3020e-02, 1.3733e-04, 7.5845e-04, 2.3467e-04, 1.8841e-03,
        1.1878e-01, 2.4566e-02, 5.5559e-01, 8.0455e-03, 1.3798e-02, 3.0278e-02,
        3.9642e-03, 9.4514e-04, 1.8480e-01, 3.3882e-04, 4.1760e-03, 1.5544e-02,
        1.1259e-02, 1.2655e-02, 1.0709e-02, 9.7950e-01, 1.8924e+00, 1.4364e+01,
        4.6731e+00, 1.0161e+01, 1.8011e+01, 1.7661e+01, 1.5685e-03, 1.1096e-02,
        7.2093e-04, 1.1581e-02, 7.4848e-05, 3.2034e-04, 1.3353e-02, 1.0436e-03,
        4.5336e-04, 2.3796e-03, 4.3117e+00, 7.6668e-02, 1.3312e+00, 1.3314e-02,
        1.2802e+01, 3.1682e-02, 5.4688e-01, 1.0051e-01, 5.8998e-03, 1.2814e-02,
        8.0434e-03, 1.1107e-01, 5.5148e+00, 7.6253e-01, 7.3689e-04, 8.5696e-02,
        5.3970e+00, 9.2348e-01, 3.1744e+01, 1.7571e+01, 4.8496e-01, 1.1040e+01,
        2.7595e-01, 5.7602e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.8315e+01, 4.4070e+01, 5.0643e+01, 2.3682e+01, 6.7988e-01, 1.2130e+01,
        2.9818e+00, 1.9041e+01, 6.4115e+00, 8.9707e+00, 6.7100e+00, 1.1838e+01,
        1.0735e+01, 1.3865e+01, 8.6211e+00, 2.8613e+01, 2.7491e+02, 2.4649e+00,
        8.0207e-01, 1.4854e+00, 7.6645e-01, 7.0507e-01, 1.0625e-01, 1.3263e+00,
        2.4261e+01, 1.6436e+01, 2.6672e+01, 5.8711e+00, 1.5556e+01, 1.4660e+01,
        1.8588e+00, 1.0096e+00, 8.1701e+00, 1.3953e+00, 1.7699e+00, 1.2160e+00,
        7.6578e+00, 3.4795e+00, 1.2802e+00, 4.2068e+01, 1.2040e+01, 3.0206e+01,
        6.1425e+00, 1.5040e+01, 1.8909e+01, 2.1195e+01, 1.1492e+00, 5.3440e+00,
        2.3334e-01, 3.9218e+00, 1.4095e-01, 2.1624e-01, 6.0034e+00, 2.7978e-01,
        4.0766e-01, 4.3099e-01, 2.5638e+01, 6.0561e+00, 1.2277e+01, 5.2066e+00,
        1.5415e+01, 5.4278e+00, 3.8582e+00, 5.8564e+00, 2.2236e-01, 1.9729e+00,
        2.0298e-01, 9.7921e+00, 1.1340e+01, 1.4945e+01, 3.9591e-01, 1.8664e+01,
        1.4776e+01, 9.7277e+00, 3.3923e+01, 1.7573e+01, 4.8496e-01, 1.1041e+01,
        4.1536e-01, 9.0545e-01], device='cuda:0')
Outer loop valEpocw Maximum [24/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 164.1
model_train val_loss valEpocw [24/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 170.0
model_train val_loss valEpocw [24/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1101.0
Sum_Val Meta Model:  tensor([5.0885e+00, 2.1310e-01, 5.2680e+00, 1.3666e-01, 4.4066e-03, 5.7198e-01,
        3.9068e-01, 5.4972e-01, 7.5983e-01, 4.2691e-01, 8.9261e-02, 1.7453e+00,
        2.7954e-02, 9.3281e-02, 3.4899e-01, 5.3269e-01, 3.5966e-01, 3.9075e-01,
        2.3441e-01, 1.0330e+00, 8.7678e-05, 1.6963e-04, 1.5506e-03, 2.6718e-02,
        3.0420e-01, 1.5554e-01, 1.2484e+00, 1.2811e-01, 4.9517e-02, 3.6805e-04,
        7.2582e-04, 2.1132e-04, 5.6692e-03, 2.0501e-04, 2.5728e-04, 1.1381e-03,
        3.8400e-02, 1.0703e-03, 7.4926e-04, 4.9036e-02, 2.3854e-02, 2.0052e+00,
        7.3015e-02, 1.3875e-01, 2.3495e-01, 3.4325e+00, 3.0687e-03, 3.7268e-03,
        6.4296e-04, 6.0707e-02, 9.2011e-05, 5.2026e-04, 1.7841e-04, 2.1287e-04,
        5.4016e-04, 5.6650e-02, 2.6750e+00, 1.5283e-01, 9.0612e-01, 2.5016e-02,
        1.0942e+00, 5.6840e-03, 3.9894e-01, 5.1885e-02, 1.7614e-02, 1.0863e-02,
        2.6857e-02, 5.3008e-01, 3.5533e-02, 4.0196e-01, 1.4875e-04, 9.4855e-03,
        2.3036e+00, 6.1714e-01, 3.4173e+00, 3.8479e-01, 1.3461e-01, 6.0564e-01,
        1.7160e-02, 8.1818e-03], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([4.1497e+00, 2.5212e-01, 3.9140e+00, 2.0121e-01, 2.4425e-02, 5.2674e-01,
        2.7927e-01, 4.0078e-01, 5.5627e-01, 3.0616e-01, 6.3440e-02, 4.1209e-01,
        2.2900e-02, 1.1939e-01, 3.6033e-01, 8.1818e-02, 2.6383e-01, 3.6094e-01,
        5.6055e-02, 4.4014e-01, 5.6781e-04, 1.8393e-03, 1.0189e-03, 2.3049e-02,
        2.8017e-01, 1.2266e-01, 1.2791e+00, 9.6204e-02, 5.6726e-02, 3.6434e-03,
        5.5809e-03, 1.5808e-03, 2.4687e-02, 4.6249e-03, 6.4966e-03, 5.4421e-03,
        1.6425e-02, 2.0091e-02, 2.1892e-03, 1.3754e-01, 4.1650e-02, 2.6631e+00,
        1.4604e-01, 3.2197e-01, 8.5848e-01, 5.4531e-01, 2.9483e-03, 2.5548e-03,
        1.6535e-03, 6.5050e-02, 3.3526e-04, 9.6737e-04, 2.1866e-03, 3.0376e-03,
        1.0598e-03, 1.9555e-02, 2.9886e+00, 1.3090e-01, 4.3593e-01, 3.6474e-03,
        3.8603e+00, 2.1494e-02, 9.7400e-02, 1.0623e-02, 4.8648e-03, 3.5773e-03,
        5.4289e-03, 5.2368e-01, 7.2487e-02, 2.8305e-01, 2.1371e-04, 7.1166e-03,
        1.7189e+00, 3.2861e-01, 4.7534e+00, 2.8705e-02, 2.4309e+00, 4.9677e-01,
        2.0022e-02, 2.8641e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.0290e+01, 2.0610e+01, 4.7357e+01, 1.3912e+01, 3.9161e+00, 3.5574e+01,
        5.6568e+01, 4.1487e+01, 2.5986e+01, 2.9320e+01, 1.6151e+01, 5.6797e+01,
        7.7655e+00, 8.4148e+00, 1.7172e+01, 3.5310e+00, 1.1259e+01, 1.6010e+01,
        2.2537e+00, 1.7142e+01, 4.1073e-01, 3.7476e-01, 1.2369e-01, 4.3746e+00,
        2.4276e+01, 1.8948e+01, 3.1549e+01, 1.4306e+01, 1.2542e+01, 4.7660e-01,
        7.2251e-01, 3.5106e-01, 4.9551e-01, 2.9709e+00, 8.0771e-01, 2.1233e-01,
        2.6168e+00, 1.6994e+00, 9.0140e-02, 3.4278e+00, 2.4321e-01, 6.0681e+00,
        2.2948e-01, 5.8545e-01, 9.6426e-01, 7.3753e-01, 5.2166e-01, 3.2759e-01,
        1.4073e-01, 6.2425e+00, 1.2560e-01, 1.9333e-01, 2.8190e-01, 2.4339e-01,
        2.2201e-01, 1.3056e+00, 1.3169e+01, 3.9895e+00, 3.6067e+00, 3.6130e-01,
        5.0854e+00, 1.2832e+00, 5.4648e-01, 2.7024e-01, 1.0033e-01, 1.7473e-01,
        8.4125e-02, 1.9483e+01, 1.6903e-01, 3.5959e+00, 3.0294e-02, 5.1401e-01,
        5.0335e+00, 2.3714e+00, 5.5044e+00, 2.8733e-02, 2.4310e+00, 4.9724e-01,
        3.7590e-02, 2.9966e-01], device='cuda:0')
Outer loop valEpocw Maximum [24/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 40.1
model_train val_loss valEpocw [24/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 37.8
model_train val_loss valEpocw [24/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 699.4
Sum_Val Meta Model:  tensor([2.3563e+00, 1.2937e-02, 5.1336e-01, 9.2734e-03, 1.3564e-03, 1.1042e-02,
        1.1991e-03, 2.9308e-02, 1.3475e-02, 4.4643e-03, 7.9430e-04, 2.0732e-03,
        1.8209e-04, 9.6874e-02, 2.0542e-02, 2.6585e-02, 7.7141e-02, 2.4361e-02,
        1.0239e-02, 1.6727e-02, 1.0754e-04, 1.1590e-03, 1.7353e-02, 2.2115e-03,
        3.6331e-02, 2.3475e-03, 4.1951e-01, 9.9060e-03, 6.0589e-04, 9.3219e-03,
        8.7749e-03, 5.8460e-03, 2.0069e-01, 6.6883e-05, 4.9697e-03, 2.9790e-02,
        2.8983e-02, 5.3042e-02, 3.0287e-03, 5.5786e-01, 4.6301e+00, 3.0735e+01,
        5.5313e+01, 5.3535e+01, 5.0631e+01, 6.1261e+01, 1.5274e-02, 2.0578e-02,
        2.2270e-01, 4.3824e-02, 1.3688e-02, 6.0224e-02, 1.3598e-01, 5.0701e-02,
        7.2987e-02, 5.9625e-01, 2.6809e+01, 1.3840e-01, 7.6251e-01, 5.5949e-03,
        7.4656e+01, 9.4661e-03, 1.5431e+00, 2.1286e-01, 3.2999e-01, 7.1595e-03,
        3.4250e-01, 1.4796e-01, 2.2113e+00, 1.1579e-01, 5.4082e-04, 3.2130e-02,
        1.4245e+00, 3.4066e+00, 1.3257e+00, 3.0638e+01, 1.0586e+01, 8.6848e-01,
        9.5443e-02, 3.7248e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([7.2019e-01, 2.2521e-03, 2.1027e-01, 1.8927e-03, 1.3766e-04, 3.5279e-04,
        7.4304e-05, 2.9057e-02, 8.9938e-04, 2.3101e-04, 5.3908e-05, 1.0638e-04,
        2.3753e-05, 7.2247e-02, 2.4732e-03, 8.2566e-03, 2.0208e-02, 7.5315e-04,
        1.3176e-03, 2.9156e-04, 6.4198e-06, 4.4170e-05, 1.8524e-05, 7.0786e-05,
        1.8854e-02, 1.2274e-03, 3.9030e-01, 8.1257e-03, 5.9371e-04, 3.9999e-04,
        9.2205e-04, 6.5478e-03, 1.9761e-01, 2.9062e-05, 5.4800e-03, 2.4352e-02,
        1.8178e-02, 3.6678e-02, 6.6921e-04, 8.5968e-01, 3.7259e+00, 3.3977e+01,
        4.1353e+01, 4.9757e+01, 7.6266e+01, 6.1280e+01, 1.1612e-02, 9.6365e-03,
        1.2039e-01, 2.6902e-02, 9.3812e-03, 6.7215e-02, 1.3396e-01, 6.7574e-02,
        6.4311e-02, 4.8381e-01, 1.7467e+01, 1.6042e-01, 6.8221e-01, 2.9007e-03,
        8.8544e+01, 3.5952e-03, 1.3297e+00, 1.5319e-01, 2.5072e-01, 1.9260e-02,
        3.7010e-01, 1.3909e-01, 2.3743e+00, 1.3076e-01, 2.1518e-04, 2.6591e-02,
        2.5287e+00, 3.2092e+00, 2.0239e+00, 2.3266e+01, 1.0903e+01, 2.0014e+00,
        1.6171e-01, 3.9264e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.1966e+01, 6.6056e-01, 4.6037e+00, 4.9335e-01, 1.0721e-01, 9.3635e-02,
        1.0869e-01, 1.4228e+01, 1.4152e-01, 9.2503e-02, 7.8727e-02, 7.4306e-02,
        5.6201e-02, 1.5446e+01, 3.9641e-01, 1.1579e+00, 2.2174e+00, 1.1321e-01,
        1.9511e-01, 3.9363e-02, 5.2590e-02, 5.6042e-02, 9.1971e-03, 6.1203e-02,
        4.4923e+00, 1.1433e+00, 2.4867e+01, 7.3038e+00, 8.7595e-01, 2.1163e-01,
        5.2633e-01, 7.8266e+00, 9.1596e+00, 1.4903e-01, 2.7468e+00, 2.2430e+00,
        1.3639e+01, 1.0809e+01, 9.6079e-02, 4.8712e+01, 2.5729e+01, 7.0846e+01,
        5.2140e+01, 7.1079e+01, 7.9562e+01, 7.2411e+01, 1.0028e+01, 5.4775e+00,
        3.7091e+01, 1.0194e+01, 2.1007e+01, 4.9679e+01, 6.3246e+01, 2.1506e+01,
        6.6287e+01, 8.9677e+01, 1.1780e+02, 1.4424e+01, 7.7347e+00, 1.2748e+00,
        1.0395e+02, 8.2974e-01, 1.0281e+01, 1.0272e+01, 1.1340e+01, 3.3704e+00,
        1.1102e+01, 1.5522e+01, 5.4927e+00, 3.3609e+00, 1.4618e-01, 7.6766e+00,
        8.2741e+00, 3.8686e+01, 2.1749e+00, 2.3268e+01, 1.0903e+01, 2.0016e+00,
        2.8107e-01, 7.2786e-01], device='cuda:0')
Outer loop valEpocw Maximum [24/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 417.7
model_train val_loss valEpocw [24/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 425.8
model_train val_loss valEpocw [24/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1354.1
Sum_Val Meta Model:  tensor([8.8139e+00, 2.9115e-02, 1.9577e+00, 1.4850e-02, 3.6052e-03, 1.8167e-01,
        3.5699e-02, 1.6711e-01, 2.6565e-02, 2.6031e-01, 2.4477e-02, 4.1462e-02,
        4.9990e-04, 2.8182e-01, 3.6704e-01, 3.1242e-02, 4.0309e-02, 2.6171e-02,
        7.1848e-03, 1.3151e-02, 5.6320e-04, 1.8094e-03, 2.6417e-03, 3.2179e-03,
        3.1510e-01, 9.4727e-03, 1.2899e+00, 1.1691e-02, 8.4826e-02, 3.9745e-03,
        7.4888e-03, 8.3745e-04, 2.0390e-01, 8.3931e-03, 1.9170e-02, 1.9584e-01,
        1.5832e-03, 7.3713e-03, 1.9415e-01, 1.2825e+00, 3.6441e+00, 1.4322e+01,
        6.3982e+00, 5.7234e+00, 9.5260e+00, 1.3378e+01, 2.2803e-03, 4.4251e-03,
        1.0191e-02, 4.5912e-03, 1.4530e-03, 1.7529e-03, 1.9126e-03, 8.2592e-02,
        3.2003e-03, 2.8794e-02, 5.3937e+00, 1.3671e-01, 2.0090e+00, 1.6367e-02,
        3.6336e+01, 1.0909e-02, 9.8701e-01, 3.7991e-01, 3.1773e-01, 6.1712e-02,
        5.4760e-01, 1.0966e+00, 7.8041e-01, 2.0169e-01, 8.1529e-04, 2.3877e-02,
        2.2586e+00, 1.5654e+00, 4.2936e+02, 1.0594e+01, 1.4337e+00, 1.1284e+01,
        3.6012e-02, 1.0524e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([2.9232e+00, 4.4016e-02, 1.4558e+00, 4.4030e-02, 1.3783e-02, 1.5347e-01,
        3.6729e-02, 1.7473e-01, 4.1678e-02, 2.2180e-01, 1.6624e-02, 6.7928e-02,
        3.9640e-03, 2.6195e-01, 3.7495e-01, 1.6006e-02, 4.1585e-02, 1.2315e-02,
        2.6644e-03, 2.2950e-03, 3.2261e-04, 6.0587e-04, 4.8445e-04, 4.7092e-03,
        2.9975e-01, 1.3823e-02, 1.0302e+00, 9.0928e-03, 9.3035e-02, 7.5920e-04,
        2.5347e-03, 1.5262e-03, 7.7249e-03, 5.1463e-04, 1.7048e-03, 3.0200e-03,
        3.8471e-03, 3.6650e-03, 2.4105e-03, 1.6164e-01, 1.5582e-01, 1.4396e+00,
        1.0703e-01, 2.5119e-01, 5.9819e-01, 2.7647e+00, 1.8049e-03, 1.9329e-03,
        8.5394e-04, 1.5060e-03, 2.4704e-04, 2.4295e-04, 7.3620e-04, 3.5529e-03,
        1.3593e-03, 1.0551e-02, 8.5645e-01, 2.6125e-02, 1.4716e+00, 6.8623e-03,
        8.4575e+00, 1.2356e-02, 1.4448e-01, 1.3188e-02, 1.3259e-02, 1.2201e-02,
        3.4384e-02, 2.1586e-01, 1.2340e-01, 4.8957e-02, 4.6467e-04, 1.9226e-02,
        4.0679e-01, 8.0437e-01, 7.6536e+01, 1.2181e+00, 2.8783e-02, 4.2820e+00,
        7.0533e-02, 1.1870e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.6851e+01, 3.2974e+00, 1.7450e+01, 2.8784e+00, 1.8732e+00, 9.9164e+00,
        7.6105e+00, 1.7572e+01, 1.9368e+00, 1.8765e+01, 3.5824e+00, 8.7145e+00,
        1.2374e+00, 1.5292e+01, 1.7723e+01, 7.4837e-01, 1.7198e+00, 5.3181e-01,
        1.1781e-01, 9.9964e-02, 2.1704e-01, 1.2238e-01, 5.5627e-02, 7.8737e-01,
        2.2813e+01, 2.0867e+00, 2.4315e+01, 1.2296e+00, 1.8819e+01, 8.8269e-02,
        3.0516e-01, 3.1025e-01, 1.3303e-01, 2.6963e-01, 1.8157e-01, 9.4592e-02,
        5.5828e-01, 3.1090e-01, 8.1481e-02, 3.2621e+00, 6.8984e-01, 3.0306e+00,
        1.5983e-01, 4.3544e-01, 6.6819e-01, 3.6658e+00, 3.0612e-01, 2.4544e-01,
        7.2000e-02, 1.4383e-01, 8.6522e-02, 4.5430e-02, 8.6293e-02, 2.3858e-01,
        2.6473e-01, 6.6518e-01, 3.6062e+00, 7.9364e-01, 1.0999e+01, 7.0088e-01,
        1.0873e+01, 6.4593e-01, 7.5863e-01, 2.8560e-01, 2.2138e-01, 5.6623e-01,
        4.2813e-01, 6.3347e+00, 2.7682e-01, 5.7300e-01, 5.7321e-02, 1.2294e+00,
        1.0764e+00, 5.6170e+00, 9.0048e+01, 1.2194e+00, 2.8785e-02, 4.2869e+00,
        1.4664e-01, 1.0610e-01], device='cuda:0')
Outer loop valEpocw Maximum [24/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 574.0
model_train val_loss valEpocw [24/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 107.7
model_train val_loss valEpocw [24/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 395.6
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.75477882 97.38794605 93.42600602 98.04461447 98.57336046 97.58287545
 98.34919165 95.17184245 98.04095954 97.02245343 98.65255053 98.94250801
 99.44445121 95.40332111 97.4915023  97.37332635 96.41451737 97.70592464
 98.86575456 98.42107187 98.7037195  99.31287387 99.56993701 99.07774028
 95.20595509 96.84701697 94.40796287 97.03829144 98.07629049 98.2310157
 98.41010709 98.58798017 96.99686895 98.61721958 98.68544487 98.90230382
 97.68399508 98.45883944 98.90474044 93.31757654 98.1189313  95.79074329
 98.27121989 97.55363604 98.50635348 97.34896017 98.23345232 98.68422656
 98.16400872 98.74392369 98.81093067 98.63793082 99.02291639 98.43569157
 98.72930398 97.70592464 93.2700625  97.04194637 96.69960161 97.67912184
 97.08336887 98.75245185 97.65475567 97.43546009 98.8718461  97.58774869
 98.67813501 95.98201776 99.09357829 98.35893812 99.81603538 97.50003046
 99.04119102 96.12212327 98.75488846 98.92179676 99.68080311 99.45176107
 99.83674663 99.19835285]
Accuracy th:0.7 is [86.92876549 97.34530525 92.9362459  98.07385388 98.55021259 97.35505172
 98.14207917 95.11945517 97.92034697 96.85676344 98.58676186 98.9132686
 99.42617658 95.32291273 97.41718546 97.23078423 96.42060891 97.59262192
 98.82433206 98.35406489 98.57579708 99.3177471  99.49927511 98.93276154
 95.25712406 96.73736918 94.28004045 96.99565064 98.0360863  98.18228335
 98.36259305 98.59163509 96.66305235 98.46249437 98.65011391 98.77925464
 97.50855862 98.37964937 98.7037195  93.32123147 98.02390322 95.30829303
 98.20664953 97.62429795 98.6208745  97.3940376  98.19690306 98.65742376
 98.07994542 98.68422656 98.75854339 98.60016325 99.02047977 98.26878328
 98.71346597 97.57800222 92.31369014 96.74102411 96.60213691 97.61089655
 96.53634824 98.66838854 97.32093907 97.24540393 98.77559971 97.4634812
 98.53802951 95.97227129 98.97418404 98.3284804  99.81603538 97.27464334
 98.94859955 95.86627843 98.69640964 98.69884626 99.67836649 99.4005921
 99.84405648 99.18007822]
Avg Prec: is [96.50915997 34.01792629 73.33457429 66.74244959 77.85760632 63.81871811
 74.59208618 47.25981155 57.29701965 52.49521608 33.02630339 53.31169236
 24.50629716 28.39653244 33.43681138 57.68312529 29.5217817  42.9119371
 47.81699742 38.7879706  61.35987974 50.88298833 89.92546336 83.92725963
 26.42550377 33.97320173 37.83333542 39.42810946 23.90665758 38.50898696
 73.14801987 37.03939831 58.08342946 63.72385048 72.63662815 79.27647666
 58.45607574 75.33964827 86.76584204 46.67621141 46.90280109 79.47317288
 78.23950049 74.49268895 84.21887698 86.30274944 38.08410836 34.86554381
 42.31683194 45.04864428 62.27918096 38.09748644 22.45744868 73.80413153
 27.62808246 44.4479544  74.77583919 59.59849254 45.64347732 60.59931895
 93.2631457  83.73659499 73.06935414 51.85019059 62.27232422 44.14032361
 61.54284727 26.40358425 68.30586291 69.32680414 13.00912762 72.73371317
 82.29323778 52.08384655 89.45710723 91.89127969 84.77085729 90.73138587
 20.17320144 30.48638805]
Accuracy th:0.5 is [45.46119077 97.2137279  71.36243467 97.02489005 97.26733349 76.0760712
 76.38430331 75.76540247 77.33214751 96.45715817 77.6087036  98.52097319
 99.41399349 79.98806057 77.1213801  96.56680596 96.29512311 76.91548592
 98.65376884 98.30776915 80.29385607 78.06313276 98.38695922 79.41301885
 81.48658033 96.65086926 94.0778012  76.65598616 98.01293844 77.48930934
 97.30875598 98.57457877 96.36213009 98.02024829 88.12270806 77.15427444
 76.97274643 91.45356416 97.11504489 74.24007992 79.14499092 92.05906361
 76.40501456 75.98104312 96.9627563  93.87434364 98.02877645 98.57336046
 97.6730303  89.37269283 88.24941217 98.55508583 98.99976852 76.53050036
 98.70615611 76.8570071  71.5622373  93.48326653 96.24273583 96.9067141
 89.79300934 97.17717864 94.01323083 76.82411277 98.42838172 77.41011927
 98.20786784 76.13333171 77.9985624  97.55972759 78.51268869 95.99054592
 77.80119638 95.45083515 76.28318368 83.06063523 88.66729207 77.4417953
 78.56263934 99.14718388]
Accuracy th:0.7 is [45.59520474 97.2137279  71.36243467 97.02489005 97.26733349 76.0760712
 76.38430331 76.05048671 77.33214751 96.47177788 77.6087036  98.52097319
 99.41399349 80.51071503 77.1213801  96.56680596 96.29512311 76.91548592
 98.65376884 98.30776915 80.69102472 78.06313276 98.38695922 80.20491953
 82.22974866 96.65086926 94.0778012  76.65598616 98.01293844 77.48930934
 97.30875598 98.57457877 96.36213009 98.02024829 88.29814452 77.15427444
 76.97274643 91.70453576 97.11504489 74.24007992 79.77607485 92.05906361
 76.40501456 75.98104312 96.9627563  93.87434364 98.02877645 98.57336046
 97.93983991 89.95991764 88.44312326 98.55508583 98.99976852 76.53050036
 98.70615611 76.8570071  71.5622373  93.94135062 96.24273583 96.9067141
 89.79300934 97.17717864 94.1484631  76.82411277 98.42838172 77.41011927
 98.20786784 76.13333171 77.9985624  97.55972759 78.52121685 95.99054592
 77.94008358 95.45083515 76.28318368 83.19464919 88.87927779 77.4417953
 78.56263934 99.14718388]
Avg Prec: is [55.89197651  3.07942737 11.30464449  3.29654718  2.31793531  3.70117698
  3.1962901   5.49595848  2.3137208   3.78655173  1.51381347  1.61096553
  0.59699937  5.04340545  2.66613487  3.16896323  3.66442265  2.66886585
  1.28448756  1.81246593  1.91137238  0.88848427  1.82807814  2.41692206
  5.17602432  3.54162165  6.53554606  3.27483187  2.01227128  1.9162619
  2.65956963  1.31426388  3.62849652  1.63133021  2.37681099  2.38512232
  3.11303323  2.60483606  2.78724611  7.279255    2.2145096   8.15053976
  3.31596454  3.9663796   3.2033474   6.52258793  2.05884682  1.45781103
  2.09539332  1.5757184   1.84250283  1.65087484  1.03450989  3.06185774
  1.31568853  2.65778999 11.24753099  3.65855902  4.04121484  2.74750081
 10.75835216  2.18488109  3.85515845  3.06764484  1.60064233  2.45428304
  1.87604217  4.30948856  1.25837647  2.4225411   0.1683798   3.40262772
  1.96229367  4.56655116  3.91362278  3.14611035  0.89257867  1.89323111
  0.14746695  0.81783046]
mAP score regular 56.89, mAP score EMA 3.75
starting validation
Accuracy th:0.5 is [87.91887784 97.39890874 93.21075317 98.21361836 98.93365224 97.69539328
 98.4503077  95.27368762 98.17873782 97.08498393 98.67204823 99.01088771
 99.36218452 95.22884122 97.43628074 97.4088746  96.33505245 97.75020555
 99.00092184 98.4129357  98.92368637 99.29491492 99.60883972 99.29491492
 95.28116202 96.80344819 94.38672547 97.30174154 97.91464235 98.25597329
 98.67703117 98.5997957  97.0351546  98.77419837 98.90873757 99.15539278
 97.94703142 98.33071729 99.05573411 93.02638463 98.01679249 92.9317089
 97.27931833 95.85669083 96.37989885 94.24720333 98.37556369 98.79163864
 98.13638289 98.77419837 98.90873757 98.6371677  98.88382291 98.49266263
 98.78167277 97.81000075 91.09798939 97.2444378  96.42474525 97.63559808
 93.06873957 98.87634851 97.34658794 97.46866981 98.7642325  97.62064928
 98.6147445  95.85669083 98.76921544 98.29334529 99.81563146 97.68791888
 98.39300396 95.9040287  97.1248474  97.51849914 99.10307198 98.64215063
 99.76580213 99.13795251]
Accuracy th:0.7 is [88.12816105 97.4088746  93.11358597 98.26095622 98.94610957 97.46866981
 98.36310636 95.42068416 98.11146822 96.98034233 98.6446421  99.00839624
 99.38460772 95.12170815 97.41884047 97.20208287 96.41976231 97.65552981
 98.97600718 98.37307223 98.83399357 99.37215038 99.61631412 99.21518798
 95.45556469 96.67638339 94.533722   97.2145402  97.87228742 98.17873782
 98.64713357 98.69447144 96.72372125 98.68699704 98.95109251 99.12798665
 97.84238981 98.26344769 98.88880584 93.30792037 98.00931809 93.41754491
 97.43129781 96.38239031 96.64897725 94.70314174 98.37307223 98.82651917
 98.03423275 98.7567581  98.86389117 98.62720183 98.89877171 98.37556369
 98.7268605  97.74771408 91.03570272 97.07003513 96.42225378 97.59822608
 93.12355183 98.82402771 97.17467673 97.2967586  98.7567581  97.59075168
 98.53750903 95.81931883 98.8016045  98.20614396 99.81563146 97.54839674
 98.37058076 95.86914817 97.2967586  97.47116127 99.17781598 98.67703117
 99.81064853 99.14791838]
Avg Prec: is [96.5369619  33.82224247 71.06280239 73.74851862 77.91995718 64.59004591
 79.45372241 48.110447   62.78414131 56.2159663  38.86072886 55.63155526
 23.2928322  31.96690561 34.35046288 63.2760139  32.81440825 45.90591484
 51.53597673 36.55287532 70.76198199 57.37156036 92.10041922 88.36486077
 24.60669467 37.76765174 33.30868712 46.69659142 27.12465499 37.80718925
 76.91384265 38.01511188 56.1055132  66.16627517 74.65631178 83.15577465
 58.99356247 77.21662735 89.57608327 44.84870855 39.65451833 51.76680305
 51.4252999  41.04086836 31.43823311 50.96106481 38.23922653 29.09293098
 41.43043962 42.18815494 67.74906019 35.22315508 24.55962064 76.58877385
 30.06763784 42.43447749 57.31928917 58.42317434 39.10485956 63.14340837
 65.94577179 87.1902206  66.58889695 53.53571872 62.2848223  39.68352753
 62.52684416 27.10209029 41.99361043 66.51809072 10.16822925 74.97161594
 54.4574957  45.41038725 62.84156125 49.64002978 15.1377715  51.67635572
  2.57392197 22.80016914]
Accuracy th:0.5 is [45.29984802 97.22450607 69.67386701 96.96290206 97.90716795 75.06041807
 75.17253407 74.25318285 76.65993971 96.41976231 76.79447891 98.5325261
 99.34972718 77.90816454 76.73717518 96.31262924 96.21047911 76.14918903
 98.78167277 98.34068316 78.77270349 77.43478586 98.31327703 80.6587438
 78.10249894 96.52938685 94.3393876  76.24635623 97.81747515 76.75710691
 97.52597354 98.67204823 96.39983058 98.18870369 89.37389441 76.3833869
 76.36594663 92.64768169 97.0276802  73.5904527  77.33014426 92.37362035
 75.50639061 75.17502554 97.03764606 94.02795426 98.18621222 98.77668984
 97.92460822 89.32157361 86.84007275 98.55993223 98.87385704 75.48645888
 98.6969629  76.1890525  70.39140942 94.421606   96.16314124 96.78102499
 90.13379176 97.04761193 94.21232279 76.2338989  98.32075143 77.08598052
 98.13139996 75.42168074 77.46219199 97.53593941 77.8658096  96.07843137
 77.24294292 95.44559882 75.33447941 84.11689962 90.86379151 76.82437651
 77.9405536  99.15040985]
Accuracy th:0.7 is [45.49667389 97.22450607 69.67386701 96.96290206 97.90716795 75.06041807
 75.17253407 74.43256845 76.65993971 96.41976231 76.79447891 98.5325261
 99.34972718 78.31427361 76.73717518 96.31262924 96.21047911 76.14918903
 98.78167277 98.34068316 79.06669656 77.43478586 98.31327703 81.17946035
 78.62072402 96.52938685 94.3393876  76.24635623 97.81747515 76.75710691
 97.52597354 98.67204823 96.39983058 98.18870369 89.55577148 76.3833869
 76.36594663 92.84450756 97.0276802  73.5904527  77.8134888  92.37362035
 75.50639061 75.17502554 97.03764606 94.02795426 98.18621222 98.77668984
 97.95948875 89.67536189 86.99454369 98.55993223 98.87385704 75.48645888
 98.6969629  76.1890525  70.39140942 94.75297107 96.16314124 96.78102499
 90.13379176 97.04761193 94.3020156  76.2338989  98.32075143 77.08598052
 98.13139996 75.42168074 77.46219199 97.53593941 77.8658096  96.07843137
 77.29277226 95.44559882 75.33447941 84.24396442 91.08304059 76.82437651
 77.9405536  99.15040985]
Avg Prec: is [54.4520184   3.74488923 14.79268756  4.56000068  1.5355043   4.27987329
 12.06337482  8.67017434  7.56041441  5.14248826  2.2944669   5.13522251
  1.55584676  5.883993    3.04558888  3.74611192 25.12583948  6.34200293
  1.54649261  2.63805928  3.58671506  1.42884465  1.24346056  5.77358646
  5.72549519 10.90409784  8.14033741  4.56496105  3.90598563  7.01248966
  2.32598362  0.86460279  3.00933368  1.15092219  1.71184904  2.4146327
  2.00373861  2.1834002   2.19814727  6.23170277  1.72729646  6.04192845
  2.18918492  2.72694849  2.34809311  4.85290858  1.72999471  1.03542487
  1.45175676  1.17738524  1.22210111  0.99584346  0.75815557  2.28630024
  0.91396789  1.87397561 10.19429986  2.98163917  3.97994977  2.86950301
  7.93494355  2.0545068   3.36962729  2.54560325  1.34772996  1.89264581
  1.56821591  3.51549149  1.07398054  2.21026657  0.19636976  3.20777011
  1.54380058  4.00529274  3.18437146  2.34963643  0.586319    1.48781488
  0.12237191  0.60600097]
mAP score regular 51.69, mAP score EMA 4.36
Train_data_mAP: current_mAP = 56.89, highest_mAP = 56.89
Val_data_mAP: current_mAP = 51.69, highest_mAP = 51.91
tensor([3.3880e-02, 3.8469e-03, 4.8408e-02, 4.1904e-03, 1.4851e-03, 4.0545e-03,
        7.9147e-04, 2.3559e-03, 7.1218e-03, 2.8335e-03, 8.0666e-04, 1.6084e-03,
        5.1119e-04, 5.3714e-03, 6.8896e-03, 7.6117e-03, 9.3844e-03, 6.8259e-03,
        7.2601e-03, 7.9105e-03, 1.5447e-04, 8.8001e-04, 1.9804e-03, 1.3166e-03,
        4.8545e-03, 1.2613e-03, 1.9342e-02, 1.2741e-03, 8.1591e-04, 1.9601e-03,
        1.8971e-03, 8.3953e-04, 2.1721e-02, 2.1867e-04, 2.0705e-03, 1.1538e-02,
        1.3213e-03, 2.9287e-03, 7.7764e-03, 2.0604e-02, 1.6601e-01, 4.6307e-01,
        7.9170e-01, 6.9082e-01, 9.5945e-01, 8.4037e-01, 1.1979e-03, 1.7235e-03,
        2.6234e-03, 2.5218e-03, 4.5953e-04, 1.3299e-03, 2.0476e-03, 3.3378e-03,
        9.9332e-04, 5.0680e-03, 1.5717e-01, 1.0565e-02, 1.0066e-01, 2.1368e-03,
        8.4031e-01, 4.9329e-03, 1.3883e-01, 1.6326e-02, 2.6440e-02, 5.6171e-03,
        3.9653e-02, 1.0555e-02, 5.2899e-01, 4.6491e-02, 1.6877e-03, 3.9140e-03,
        3.8260e-01, 8.4731e-02, 9.4188e-01, 9.9991e-01, 1.0000e+00, 9.9992e-01,
        6.5330e-01, 6.3785e-02], device='cuda:0')
Sum Train Loss:  tensor([1.0724e+00, 2.5633e-02, 1.2439e+00, 4.9554e-02, 8.4322e-03, 4.4204e-02,
        2.8518e-03, 4.9802e-02, 6.2799e-02, 2.2117e-02, 9.7032e-03, 2.7642e-03,
        5.0427e-04, 9.5568e-02, 3.8478e-02, 3.8144e-02, 1.2838e-01, 1.2951e-01,
        4.7154e-02, 1.0199e-02, 2.0861e-04, 5.6055e-03, 8.9658e-03, 6.2689e-03,
        8.0089e-02, 1.9208e-02, 3.1422e-01, 1.2879e-02, 1.1592e-02, 1.8685e-02,
        7.4876e-03, 2.5804e-03, 1.3164e-01, 2.2321e-03, 8.2607e-03, 1.6607e-02,
        1.3247e-02, 1.4936e-02, 8.2343e-03, 4.7009e-01, 1.1671e+00, 6.2459e+00,
        1.4203e+00, 3.5097e+00, 1.4615e+00, 9.0054e+00, 9.8389e-03, 9.3845e-03,
        2.0072e-02, 2.3746e-02, 1.1233e-03, 1.1877e-02, 1.8805e-03, 1.2365e-02,
        5.4780e-03, 1.9988e-02, 3.4487e+00, 3.1745e-02, 1.5027e+00, 1.6004e-02,
        7.9750e+00, 3.7824e-02, 1.2767e+00, 4.6042e-02, 1.2091e-01, 1.2441e-02,
        5.4951e-02, 1.0274e-01, 5.8136e-01, 1.3530e-01, 6.4852e-03, 4.4780e-02,
        1.6975e+00, 1.2108e+00, 1.0271e+00, 3.2508e+00, 2.2470e-01, 2.1495e+00,
        3.0829e-01, 9.7723e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [25/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 52.5
Sum Train Loss:  tensor([1.0564e+00, 1.1490e-01, 1.3968e+00, 4.9916e-02, 4.0722e-03, 3.6859e-02,
        7.5203e-03, 4.5647e-02, 2.9724e-02, 2.6425e-02, 5.3108e-03, 1.1277e-02,
        2.6452e-03, 1.0499e-01, 7.6403e-02, 8.1580e-02, 1.7686e-01, 6.9413e-02,
        1.1230e-02, 1.9313e-02, 3.5437e-04, 3.1014e-04, 6.3818e-04, 1.1099e-02,
        6.8150e-02, 2.4684e-02, 4.0072e-01, 3.1950e-02, 5.5931e-03, 1.4822e-02,
        1.3577e-02, 5.6963e-03, 1.8128e-01, 1.1950e-03, 6.9581e-03, 3.2014e-02,
        7.6150e-03, 1.3560e-02, 3.0785e-02, 4.2557e-01, 4.5444e-01, 5.5725e+00,
        3.0103e+00, 5.2807e+00, 4.6299e+00, 6.5280e+00, 8.3388e-03, 2.6972e-02,
        1.2102e-02, 2.6407e-02, 7.1687e-04, 1.2777e-02, 1.1885e-02, 3.4748e-02,
        1.0821e-03, 1.3333e-02, 2.9694e+00, 8.9857e-02, 1.2290e+00, 1.9775e-02,
        4.2390e+00, 2.3299e-02, 1.5277e+00, 1.9221e-01, 1.1404e-01, 3.9013e-02,
        2.1399e-01, 1.7850e-01, 8.9073e-01, 2.7345e-01, 4.3866e-03, 5.1154e-02,
        1.8167e+00, 1.5311e+00, 3.3000e+00, 2.8555e+00, 3.7303e-01, 8.5637e-01,
        1.9059e-01, 1.5214e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [25/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 53.4
Sum Train Loss:  tensor([1.1710e+00, 4.5353e-02, 1.4147e+00, 3.3297e-02, 1.8646e-02, 4.5161e-02,
        3.2382e-03, 3.9988e-02, 3.1421e-02, 3.2941e-02, 6.0960e-03, 3.7351e-03,
        3.6906e-04, 1.5252e-01, 3.8183e-02, 8.8123e-02, 1.8895e-01, 6.7324e-02,
        3.1388e-02, 7.2170e-02, 1.3108e-03, 1.8422e-03, 7.8983e-04, 6.2495e-03,
        1.1569e-01, 1.4900e-02, 3.7484e-01, 1.9394e-02, 8.3566e-03, 2.9215e-02,
        6.3255e-03, 3.1171e-03, 1.4925e-01, 1.4976e-03, 2.6273e-03, 2.8348e-02,
        1.1929e-02, 1.7277e-02, 1.9413e-02, 4.1340e-01, 4.9613e-01, 6.2396e+00,
        6.8128e+00, 2.9480e+00, 3.3055e+00, 6.0265e+00, 4.0889e-03, 2.0815e-03,
        1.2587e-02, 2.7227e-03, 1.2547e-03, 8.3575e-03, 2.3344e-02, 2.2760e-02,
        5.4598e-03, 2.6611e-02, 3.5382e+00, 1.7584e-01, 9.5112e-01, 2.8206e-02,
        9.6975e+00, 3.6310e-02, 1.1272e+00, 1.6361e-01, 5.6200e-02, 3.9267e-02,
        2.2497e-01, 2.0139e-01, 5.1264e-01, 1.4419e-01, 8.4041e-03, 5.6149e-02,
        1.4843e+00, 8.2691e-01, 8.2063e+00, 1.1456e+00, 2.7335e-01, 6.6489e-01,
        6.4941e-02, 6.4453e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [25/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 60.9
Sum Train Loss:  tensor([1.1655e+00, 2.7656e-02, 1.0255e+00, 2.7724e-02, 4.5180e-03, 4.4180e-02,
        2.0026e-03, 2.9909e-02, 1.1418e-01, 1.6275e-02, 3.8031e-03, 1.5570e-02,
        3.0724e-04, 1.0322e-01, 8.6514e-02, 7.8052e-02, 2.4638e-01, 2.4617e-02,
        3.3591e-02, 7.9684e-03, 6.0560e-04, 2.6071e-03, 1.5334e-02, 5.0727e-03,
        1.2482e-01, 1.9786e-02, 2.2190e-01, 1.4869e-02, 7.3407e-03, 1.6420e-02,
        5.4736e-03, 3.0662e-03, 2.2762e-01, 1.5143e-03, 1.0523e-02, 5.2762e-02,
        6.2713e-03, 2.8017e-02, 4.5980e-02, 5.4398e-01, 1.2653e+00, 8.3794e+00,
        4.0006e+00, 3.9456e+00, 4.6930e+00, 6.5680e+00, 1.4728e-02, 5.1057e-03,
        2.6328e-02, 9.4316e-03, 2.1366e-03, 6.5984e-03, 1.4669e-02, 1.6123e-02,
        4.6678e-03, 3.6360e-02, 5.3325e+00, 8.2609e-02, 1.4026e+00, 2.1996e-02,
        1.0490e+01, 5.9472e-02, 9.5271e-01, 2.3802e-01, 6.8006e-02, 4.4801e-02,
        2.2676e-01, 2.3127e-01, 4.4297e+00, 2.1560e-01, 1.0897e-02, 4.0740e-02,
        3.0830e+00, 1.4823e+00, 3.8450e+00, 4.0191e+00, 3.2126e-01, 4.8893e-01,
        3.5986e-01, 2.3376e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [25/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 71.1
Sum Train Loss:  tensor([1.3238e+00, 5.6924e-02, 7.5693e-01, 2.0284e-02, 2.5309e-03, 2.6669e-02,
        3.3510e-03, 3.0686e-02, 2.2027e-02, 2.0514e-02, 9.5241e-04, 5.6380e-03,
        3.7276e-04, 8.1444e-02, 1.5536e-01, 8.2420e-02, 8.6509e-02, 1.6874e-01,
        6.8621e-02, 1.2732e-01, 1.5173e-03, 1.5283e-03, 1.4625e-02, 1.0567e-02,
        1.1160e-01, 1.8521e-02, 2.3258e-01, 1.9709e-02, 6.0145e-03, 1.1098e-02,
        1.4271e-02, 8.5396e-03, 2.6764e-01, 3.3046e-04, 3.5879e-03, 2.4548e-02,
        1.3098e-02, 9.4423e-03, 4.5527e-02, 6.5190e-01, 9.1030e-01, 9.3416e+00,
        2.5897e+00, 3.1430e+00, 1.3812e+00, 7.7066e+00, 1.2368e-02, 1.8310e-02,
        1.4823e-02, 1.8231e-02, 2.2681e-03, 1.5347e-03, 7.7315e-03, 2.1785e-02,
        2.0090e-03, 2.7277e-02, 3.5015e+00, 1.2106e-01, 1.6528e+00, 1.0257e-02,
        6.2982e+00, 5.9229e-03, 6.3129e-01, 1.2164e-01, 2.7516e-02, 5.1400e-02,
        1.0122e-01, 1.0824e-01, 8.5221e-01, 5.1839e-02, 4.2110e-04, 3.2756e-02,
        3.8737e+00, 9.7755e-01, 7.7038e+00, 1.6845e+01, 1.8048e-01, 5.5817e-01,
        5.1097e-02, 3.7166e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [25/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 73.8
Sum Train Loss:  tensor([9.9399e-01, 2.4485e-02, 1.5621e+00, 3.4752e-02, 1.3268e-02, 2.6750e-02,
        7.1477e-03, 3.7001e-02, 1.6098e-02, 2.8036e-02, 1.2143e-03, 1.6968e-03,
        5.7209e-04, 1.1367e-01, 1.3361e-01, 7.0678e-02, 1.8120e-01, 4.8510e-02,
        3.4703e-02, 3.4599e-02, 1.7007e-03, 8.4623e-04, 8.3214e-04, 1.1659e-02,
        7.5657e-02, 1.9095e-02, 2.3445e-01, 1.9212e-02, 5.5485e-03, 2.9606e-02,
        8.1748e-03, 5.7767e-03, 2.4400e-01, 6.4396e-04, 3.5871e-03, 1.7024e-02,
        1.5764e-02, 3.3493e-03, 2.5587e-02, 4.0098e-01, 1.0856e+00, 6.4964e+00,
        6.6287e+00, 5.3962e+00, 9.8244e+00, 9.4622e+00, 2.1723e-03, 8.2564e-03,
        1.3849e-02, 4.7763e-03, 3.1962e-03, 9.1832e-03, 2.2885e-03, 1.4662e-02,
        9.8208e-03, 2.4696e-02, 3.0279e+00, 1.1699e-01, 1.6726e+00, 1.1235e-02,
        1.4425e+01, 3.3465e-02, 5.8677e-01, 8.0866e-02, 3.6520e-02, 4.3334e-02,
        1.6903e-01, 1.1702e-01, 2.4193e+00, 2.0920e-01, 1.2507e-03, 1.5780e-02,
        2.9882e+00, 9.0654e-01, 1.7129e+00, 1.0284e+01, 4.3136e+00, 4.9373e-01,
        2.2438e-01, 4.3906e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [25/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 87.4
Sum Train Loss:  tensor([1.3270e+00, 4.5512e-02, 1.4138e+00, 5.0665e-02, 5.0540e-03, 3.1666e-02,
        6.6985e-03, 3.6485e-02, 1.5915e-01, 3.4189e-02, 2.8967e-03, 1.0408e-02,
        2.7697e-04, 1.4356e-01, 1.7352e-01, 2.1809e-01, 1.2412e-01, 4.5484e-02,
        8.5096e-02, 5.8175e-02, 1.8788e-03, 4.0514e-03, 1.5289e-03, 1.2794e-03,
        1.0415e-01, 1.6612e-02, 4.9458e-01, 8.4888e-03, 8.2063e-03, 1.6797e-02,
        2.2422e-02, 5.7433e-03, 1.3085e-01, 1.9994e-03, 8.5356e-03, 4.1898e-02,
        2.0217e-02, 3.5656e-02, 1.7532e-02, 5.3785e-01, 1.9976e+00, 7.5069e+00,
        8.7526e+00, 4.8343e+00, 7.6000e+00, 7.5142e+00, 6.0544e-03, 1.2921e-02,
        2.7619e-02, 2.4922e-02, 2.4083e-03, 4.4668e-03, 1.4264e-02, 1.9927e-02,
        4.4453e-03, 4.5111e-02, 4.3593e+00, 1.3386e-01, 5.5930e-01, 9.2026e-03,
        6.9503e+00, 5.6525e-02, 1.0960e+00, 9.2611e-02, 6.2928e-02, 5.0703e-02,
        4.1437e-02, 1.3905e-01, 1.1298e+00, 5.4705e-01, 1.0736e-03, 5.1882e-02,
        1.3685e+00, 1.4117e+00, 3.2728e+00, 2.5319e+00, 1.0623e-01, 1.1211e+00,
        6.6724e-02, 6.4801e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [25/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 69.0
Sum_Val Meta Model:  tensor([1.8006e+00, 2.6847e-01, 4.1070e+00, 1.7837e-01, 1.6227e-03, 5.0569e-02,
        3.0346e-03, 4.0192e-02, 4.9210e-02, 2.6445e-02, 5.2064e-03, 1.8153e-02,
        4.7965e-03, 8.2318e-02, 5.2627e-02, 6.2512e-02, 4.9319e+00, 8.0878e-03,
        1.1531e-02, 1.2686e-02, 2.0346e-04, 3.3177e-04, 1.5936e-03, 5.8329e-04,
        1.3337e-01, 2.1797e-02, 5.4673e-01, 4.4489e-03, 8.5942e-03, 2.2978e-02,
        2.5457e-03, 7.2963e-04, 1.8336e-01, 1.8869e-04, 3.4419e-03, 1.4420e-02,
        5.2080e-03, 1.7124e-02, 1.2456e-02, 9.4879e-01, 1.9586e+00, 1.2427e+01,
        6.8642e+00, 1.5976e+01, 1.8847e+01, 1.3166e+01, 2.9259e-03, 8.8115e-03,
        2.1660e-03, 1.3171e-02, 1.4849e-04, 1.0948e-03, 1.3514e-02, 5.0235e-03,
        1.4878e-03, 8.9173e-03, 4.6873e+00, 8.0625e-02, 1.3834e+00, 5.1665e-03,
        1.0735e+01, 6.7678e-02, 7.0073e-01, 8.9731e-02, 9.7063e-03, 7.3370e-03,
        1.6288e-02, 7.7576e-02, 5.5226e+00, 8.9894e-01, 5.3599e-04, 6.7743e-02,
        6.5967e+00, 7.4735e-01, 2.1548e+01, 1.0426e+01, 4.1864e-01, 1.1098e+01,
        3.2799e-02, 3.7719e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.7185e+00, 1.6853e-01, 2.4142e+00, 1.1264e-01, 5.8418e-04, 4.8789e-02,
        2.1690e-03, 3.8453e-02, 3.9897e-02, 2.6561e-02, 5.5848e-03, 1.8950e-02,
        5.0466e-03, 8.2661e-02, 5.4922e-02, 2.7069e-01, 3.1609e+00, 1.4180e-02,
        1.0640e-02, 2.4888e-02, 2.6736e-04, 5.2373e-04, 7.5847e-04, 1.0571e-03,
        1.2720e-01, 2.0484e-02, 5.1719e-01, 5.2633e-03, 1.0536e-02, 2.8121e-02,
        1.7550e-03, 7.7055e-04, 1.6250e-01, 2.6876e-04, 3.0961e-03, 1.0518e-02,
        1.4524e-02, 1.4180e-02, 1.6409e-02, 8.8235e-01, 2.0663e+00, 1.6842e+01,
        6.5549e+00, 1.4388e+01, 1.9476e+01, 1.2594e+01, 1.9944e-03, 1.0491e-02,
        4.0650e-04, 1.0356e-02, 2.1819e-05, 4.2432e-04, 1.5065e-02, 9.4194e-04,
        6.4149e-04, 3.0869e-03, 4.0598e+00, 6.9915e-02, 1.2803e+00, 6.1407e-03,
        1.7549e+01, 2.7725e-02, 4.0958e-01, 9.7557e-02, 3.2093e-03, 1.1971e-02,
        4.7915e-03, 8.7907e-02, 5.5804e+00, 1.0833e+00, 8.1757e-04, 7.3093e-02,
        5.2119e+00, 8.1524e-01, 2.6509e+01, 1.5218e+01, 8.8761e-01, 1.2099e+01,
        3.9193e-02, 5.5194e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.0724e+01, 4.3810e+01, 4.9871e+01, 2.6880e+01, 3.9336e-01, 1.2033e+01,
        2.7405e+00, 1.6322e+01, 5.6020e+00, 9.3739e+00, 6.9233e+00, 1.1782e+01,
        9.8722e+00, 1.5389e+01, 7.9717e+00, 3.5562e+01, 3.3682e+02, 2.0774e+00,
        1.4656e+00, 3.1462e+00, 1.7308e+00, 5.9514e-01, 3.8298e-01, 8.0290e-01,
        2.6204e+01, 1.6240e+01, 2.6739e+01, 4.1311e+00, 1.2913e+01, 1.4347e+01,
        9.2507e-01, 9.1783e-01, 7.4812e+00, 1.2290e+00, 1.4953e+00, 9.1159e-01,
        1.0992e+01, 4.8418e+00, 2.1100e+00, 4.2824e+01, 1.2447e+01, 3.6370e+01,
        8.2796e+00, 2.0827e+01, 2.0299e+01, 1.4986e+01, 1.6649e+00, 6.0870e+00,
        1.5495e-01, 4.1065e+00, 4.7480e-02, 3.1907e-01, 7.3571e+00, 2.8221e-01,
        6.4581e-01, 6.0909e-01, 2.5831e+01, 6.6176e+00, 1.2719e+01, 2.8738e+00,
        2.0884e+01, 5.6205e+00, 2.9503e+00, 5.9756e+00, 1.2138e-01, 2.1313e+00,
        1.2084e-01, 8.3283e+00, 1.0549e+01, 2.3302e+01, 4.8442e-01, 1.8675e+01,
        1.3622e+01, 9.6215e+00, 2.8145e+01, 1.5219e+01, 8.8761e-01, 1.2100e+01,
        5.9991e-02, 8.6531e-01], device='cuda:0')
Outer loop valEpocw Maximum [25/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 158.2
model_train val_loss valEpocw [25/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 173.2
model_train val_loss valEpocw [25/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1188.8
Sum_Val Meta Model:  tensor([5.4314e+00, 2.2267e-01, 5.6355e+00, 1.4251e-01, 4.1651e-03, 6.6615e-01,
        4.3513e-01, 6.7157e-01, 7.1308e-01, 4.5773e-01, 9.5890e-02, 2.0439e+00,
        3.5746e-02, 1.0022e-01, 4.4216e-01, 4.9302e-01, 2.7217e-01, 4.8237e-01,
        1.6709e-01, 1.2715e+00, 5.9635e-05, 9.7155e-05, 1.5717e-03, 2.9962e-02,
        4.6496e-01, 1.5275e-01, 1.3920e+00, 1.2508e-01, 6.4260e-02, 2.8516e-04,
        7.3196e-04, 2.3585e-04, 4.3563e-03, 2.5511e-04, 1.9779e-04, 1.0038e-03,
        3.9778e-02, 7.3416e-04, 5.4984e-04, 4.8179e-02, 1.6786e-02, 1.8530e+00,
        5.2475e-02, 1.3793e-01, 2.1125e-01, 4.2400e+00, 5.0654e-03, 3.8964e-03,
        3.8079e-04, 6.8941e-02, 4.7426e-05, 3.1931e-04, 1.1257e-04, 1.5981e-04,
        3.8171e-04, 6.6921e-02, 2.6787e+00, 1.5412e-01, 9.8643e-01, 3.4327e-02,
        1.0490e+00, 9.0301e-03, 3.6690e-01, 5.9816e-02, 2.1935e-02, 1.0413e-02,
        3.6616e-02, 5.6887e-01, 4.1848e-02, 4.3041e-01, 9.2113e-05, 1.4205e-02,
        2.1389e+00, 7.1090e-01, 4.0604e+00, 2.7253e-01, 1.1598e-01, 5.8071e-01,
        1.7527e-02, 8.1848e-03], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([4.2720e+00, 2.6724e-01, 3.7276e+00, 1.8158e-01, 1.5008e-02, 5.7048e-01,
        2.4936e-01, 4.5693e-01, 4.5019e-01, 3.1918e-01, 6.5475e-02, 4.1842e-01,
        2.7684e-02, 1.5677e-01, 3.8521e-01, 8.2249e-02, 2.4015e-01, 4.2250e-01,
        8.9760e-02, 3.9592e-01, 1.8232e-03, 1.6976e-03, 3.4963e-03, 2.7573e-02,
        3.2997e-01, 1.2341e-01, 1.3044e+00, 1.1017e-01, 7.3717e-02, 1.0924e-02,
        2.5973e-03, 1.5168e-03, 5.3762e-02, 9.9240e-03, 5.6734e-03, 5.3117e-03,
        2.5169e-02, 2.1844e-02, 2.1458e-03, 9.5766e-02, 2.6388e-02, 1.0003e+00,
        3.0827e-02, 6.7067e-02, 6.7375e-03, 1.5458e+00, 4.8741e-03, 2.4671e-03,
        8.7211e-04, 7.7971e-02, 1.2556e-04, 2.0619e-03, 2.9642e-03, 6.9134e-03,
        1.8043e-03, 2.5103e-02, 2.3293e+00, 1.3911e-01, 5.9279e-01, 3.8900e-03,
        7.8721e-01, 1.5802e-02, 4.8189e-02, 9.8786e-03, 1.9129e-03, 3.5118e-03,
        3.2463e-03, 5.6067e-01, 4.7256e-02, 2.4414e-01, 2.0610e-04, 6.7457e-03,
        2.2250e+00, 3.0217e-01, 7.0428e+00, 3.0227e-02, 6.1937e-03, 4.3857e-01,
        2.0031e-03, 2.3654e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.2262e+01, 2.0521e+01, 4.4030e+01, 1.1880e+01, 2.0056e+00, 3.5199e+01,
        4.3416e+01, 4.2722e+01, 2.0301e+01, 2.6490e+01, 1.4153e+01, 5.1107e+01,
        7.8545e+00, 9.8799e+00, 1.6372e+01, 3.3490e+00, 9.3638e+00, 1.6638e+01,
        3.4068e+00, 1.4645e+01, 1.0967e+00, 3.0980e-01, 3.7076e-01, 4.4292e+00,
        2.4572e+01, 1.7224e+01, 3.0301e+01, 1.3930e+01, 1.3706e+01, 1.1795e+00,
        2.8720e-01, 2.8448e-01, 1.0059e+00, 5.0717e+00, 6.4706e-01, 1.9519e-01,
        3.5012e+00, 1.8586e+00, 7.8506e-02, 2.2905e+00, 1.5126e-01, 2.2668e+00,
        4.7632e-02, 1.1914e-01, 7.5061e-03, 2.0498e+00, 7.3435e-01, 2.8918e-01,
        6.8813e-02, 6.4994e+00, 4.0673e-02, 3.4660e-01, 3.3535e-01, 5.0359e-01,
        3.2596e-01, 1.5052e+00, 1.0133e+01, 4.1763e+00, 4.7243e+00, 3.6743e-01,
        1.0320e+00, 8.5815e-01, 2.5973e-01, 2.2597e-01, 3.4216e-02, 1.6085e-01,
        4.4545e-02, 1.8908e+01, 1.0468e-01, 3.0111e+00, 2.6085e-02, 4.5188e-01,
        6.4941e+00, 2.1102e+00, 8.1472e+00, 3.0256e-02, 6.1940e-03, 4.3901e-01,
        3.5666e-03, 2.3220e-01], device='cuda:0')
Outer loop valEpocw Maximum [25/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 43.1
model_train val_loss valEpocw [25/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 32.7
model_train val_loss valEpocw [25/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 655.2
Sum_Val Meta Model:  tensor([2.5522e+00, 1.0275e-02, 5.0246e-01, 7.3637e-03, 1.6012e-03, 8.4277e-03,
        1.2475e-03, 3.1869e-02, 1.2010e-02, 4.4424e-03, 7.2835e-04, 2.1300e-03,
        1.7396e-04, 1.0833e-01, 2.0752e-02, 2.2719e-02, 6.7060e-02, 2.4604e-02,
        7.7443e-03, 1.4733e-02, 1.3643e-04, 1.8416e-03, 2.1600e-02, 1.9538e-03,
        4.0239e-02, 2.2741e-03, 4.5623e-01, 1.0732e-02, 7.3926e-04, 9.0857e-03,
        1.3576e-02, 7.8054e-03, 2.1382e-01, 4.8381e-05, 5.7624e-03, 3.4006e-02,
        3.5154e-02, 4.6253e-02, 2.3062e-03, 5.5013e-01, 4.6704e+00, 3.1459e+01,
        5.6886e+01, 6.8128e+01, 5.0836e+01, 6.4560e+01, 1.7079e-02, 2.1710e-02,
        1.8843e-01, 4.4869e-02, 1.8442e-02, 7.1548e-02, 1.7808e-01, 5.5194e-02,
        9.1204e-02, 7.7736e-01, 2.9118e+01, 1.3808e-01, 7.5133e-01, 4.5964e-03,
        9.0790e+01, 6.8287e-03, 1.6667e+00, 2.4562e-01, 4.1323e-01, 5.5186e-03,
        3.9340e-01, 1.1962e-01, 2.4516e+00, 7.7577e-02, 3.0015e-04, 3.3845e-02,
        1.3690e+00, 3.5004e+00, 8.7362e-01, 2.9625e+01, 1.1345e+01, 7.8397e-01,
        8.6239e-02, 3.0184e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.6476e-01, 2.4812e-03, 2.2934e-01, 3.0302e-03, 5.6423e-05, 8.1036e-04,
        1.5819e-04, 3.0241e-02, 4.2577e-03, 3.0026e-04, 1.0048e-04, 1.2316e-04,
        1.8665e-05, 8.3588e-02, 2.7310e-03, 1.2644e-02, 1.4219e-02, 8.3184e-04,
        2.4724e-03, 1.3427e-03, 1.9634e-05, 3.7503e-05, 8.7478e-05, 7.2862e-05,
        2.2887e-02, 1.5482e-03, 3.9891e-01, 8.2379e-03, 5.2084e-04, 5.7234e-04,
        1.0047e-03, 6.7608e-03, 2.1421e-01, 3.1644e-05, 3.3547e-03, 1.4162e-02,
        1.8523e-02, 3.5684e-02, 6.8800e-04, 8.8264e-01, 3.7774e+00, 3.4028e+01,
        5.0482e+01, 5.9091e+01, 7.6712e+01, 7.2689e+01, 1.4375e-02, 1.3699e-02,
        1.1434e-01, 2.7057e-02, 1.0897e-02, 6.6387e-02, 1.4565e-01, 1.0648e-01,
        6.3844e-02, 4.2866e-01, 2.4863e+01, 1.5637e-01, 8.2604e-01, 2.5782e-03,
        1.2750e+02, 2.1406e-03, 1.3000e+00, 1.9695e-01, 3.7421e-01, 2.2110e-02,
        4.0481e-01, 1.4049e-01, 2.6005e+00, 1.0813e-01, 4.3050e-04, 2.9018e-02,
        1.4549e+00, 3.1685e+00, 9.5674e-01, 2.4059e+01, 1.1513e+01, 5.6368e+00,
        3.8765e-02, 4.4125e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([1.7431e+01, 7.0752e-01, 5.0268e+00, 7.8673e-01, 3.7669e-02, 2.0209e-01,
        2.1028e-01, 1.3988e+01, 6.4552e-01, 1.0881e-01, 1.3029e-01, 7.7731e-02,
        4.0018e-02, 1.6121e+01, 4.0903e-01, 1.7107e+00, 1.4595e+00, 1.0835e-01,
        3.4546e-01, 1.7374e-01, 1.3781e-01, 4.0009e-02, 3.7175e-02, 5.5682e-02,
        4.7873e+00, 1.3034e+00, 2.3960e+01, 6.8092e+00, 6.6983e-01, 2.5622e-01,
        4.5117e-01, 6.8006e+00, 9.3691e+00, 1.3338e-01, 1.5761e+00, 1.2340e+00,
        1.2458e+01, 1.1311e+01, 8.9302e-02, 5.0014e+01, 2.5912e+01, 7.1505e+01,
        6.2678e+01, 8.2867e+01, 7.9725e+01, 8.4890e+01, 1.0801e+01, 7.4239e+00,
        3.5349e+01, 9.3269e+00, 2.0311e+01, 4.2240e+01, 6.1268e+01, 3.1158e+01,
        5.8715e+01, 7.2675e+01, 1.6470e+02, 1.4687e+01, 9.0916e+00, 1.1297e+00,
        1.4959e+02, 4.7408e-01, 9.7449e+00, 1.2245e+01, 1.4818e+01, 3.8064e+00,
        1.0818e+01, 1.4351e+01, 5.6592e+00, 2.7666e+00, 2.7503e-01, 8.1686e+00,
        4.7242e+00, 3.8373e+01, 1.0255e+00, 2.4061e+01, 1.1513e+01, 5.6373e+00,
        6.2625e-02, 7.7763e-01], device='cuda:0')
Outer loop valEpocw Maximum [25/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 456.7
model_train val_loss valEpocw [25/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 505.7
model_train val_loss valEpocw [25/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1456.6
Sum_Val Meta Model:  tensor([7.3211e+00, 3.6034e-02, 2.0083e+00, 2.1770e-02, 5.1330e-03, 1.7342e-01,
        2.5823e-02, 1.5575e-01, 4.2118e-02, 2.2995e-01, 2.0663e-02, 3.9231e-02,
        1.0095e-03, 2.9379e-01, 3.0594e-01, 5.1921e-02, 6.1908e-02, 3.9116e-02,
        1.4493e-02, 2.1167e-02, 1.6099e-03, 4.4787e-03, 6.1824e-03, 6.8506e-03,
        3.2353e-01, 1.6450e-02, 1.2902e+00, 1.5602e-02, 7.3581e-02, 6.6963e-03,
        1.4536e-02, 1.9944e-03, 3.1438e-01, 4.2869e-02, 8.1589e-02, 4.3030e-01,
        3.8843e-03, 2.4264e-02, 2.3786e-01, 1.0398e+00, 2.9966e+00, 1.5747e+01,
        7.5875e+00, 7.0781e+00, 1.0255e+01, 1.3624e+01, 5.9493e-03, 9.4943e-03,
        1.9437e-02, 1.0523e-02, 3.0060e-03, 4.1336e-03, 4.9269e-03, 1.3950e-01,
        7.4645e-03, 4.3898e-02, 6.2344e+00, 2.0123e-01, 2.2440e+00, 3.1808e-02,
        3.6655e+01, 2.0530e-02, 1.2985e+00, 4.9415e-01, 5.1834e-01, 5.7909e-02,
        7.1926e-01, 1.2941e+00, 8.5465e-01, 2.7846e-01, 1.3621e-03, 3.7931e-02,
        2.0138e+00, 1.7231e+00, 3.7868e+02, 1.4954e+01, 2.3337e+00, 1.0271e+01,
        8.9635e-02, 1.6398e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([2.6422e+00, 3.1542e-02, 1.2321e+00, 3.6785e-02, 6.5351e-03, 1.5175e-01,
        4.2266e-02, 1.4794e-01, 8.2891e-02, 1.7846e-01, 1.5975e-02, 8.1481e-02,
        1.7994e-03, 2.7664e-01, 3.9997e-01, 2.8843e-02, 1.6426e-02, 6.0261e-03,
        2.8884e-03, 3.7459e-03, 6.3910e-04, 2.9919e-04, 1.4772e-03, 5.6569e-03,
        2.7019e-01, 1.0792e-02, 1.1102e+00, 7.5637e-03, 9.5943e-02, 1.0133e-03,
        2.3862e-03, 1.4391e-03, 9.8436e-03, 5.0593e-04, 1.3084e-03, 2.1515e-03,
        4.0408e-03, 2.5441e-03, 1.6077e-03, 1.3234e-01, 1.7250e-01, 6.3081e-01,
        9.9455e-02, 1.7460e-01, 4.0189e-01, 5.9323e-01, 2.1122e-03, 1.2312e-03,
        5.9380e-04, 1.0668e-03, 7.0331e-05, 3.4323e-04, 7.8292e-04, 3.1127e-03,
        2.2498e-03, 2.1575e-02, 1.0052e+00, 1.9539e-02, 1.5446e+00, 3.7683e-03,
        1.4353e+01, 8.8796e-03, 1.0326e-01, 1.4551e-02, 9.9176e-03, 1.1537e-02,
        3.2432e-02, 2.0644e-01, 9.3693e-02, 2.9903e-02, 8.4742e-04, 9.6237e-03,
        1.3147e-01, 6.7557e-01, 9.5851e+01, 3.7857e+00, 2.0077e-02, 5.4774e+00,
        3.6308e-03, 7.4009e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.5119e+01, 2.5245e+00, 1.5418e+01, 2.6498e+00, 8.8651e-01, 1.0228e+01,
        9.1830e+00, 1.5825e+01, 4.1527e+00, 1.5750e+01, 3.6180e+00, 1.0968e+01,
        5.9281e-01, 1.6689e+01, 1.9530e+01, 1.4404e+00, 6.8878e-01, 2.5162e-01,
        1.3447e-01, 1.6854e-01, 4.4706e-01, 6.2191e-02, 1.7130e-01, 9.8481e-01,
        2.0401e+01, 1.6931e+00, 2.6717e+01, 1.1186e+00, 2.0121e+01, 1.1699e-01,
        2.7863e-01, 2.9998e-01, 1.7453e-01, 2.3615e-01, 1.4462e-01, 6.6575e-02,
        6.1624e-01, 2.1451e-01, 5.8032e-02, 2.8133e+00, 8.1077e-01, 1.3172e+00,
        1.4444e-01, 2.9672e-01, 4.4236e-01, 7.7100e-01, 3.7239e-01, 1.6997e-01,
        5.1051e-02, 1.0441e-01, 2.5104e-02, 6.2422e-02, 8.8561e-02, 2.1338e-01,
        4.3972e-01, 1.3386e+00, 4.3959e+00, 5.9430e-01, 1.1374e+01, 3.7714e-01,
        1.8358e+01, 4.9327e-01, 5.3337e-01, 3.1485e-01, 1.5465e-01, 5.2466e-01,
        3.7931e-01, 6.0326e+00, 2.0580e-01, 3.6764e-01, 1.1006e-01, 6.7325e-01,
        3.5615e-01, 4.6559e+00, 1.1126e+02, 3.7888e+00, 2.0078e-02, 5.4825e+00,
        7.1402e-03, 6.6510e-02], device='cuda:0')
Outer loop valEpocw Maximum [25/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 533.5
model_train val_loss valEpocw [25/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 132.6
model_train val_loss valEpocw [25/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 419.7
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.99478564 97.42815024 93.36630889 98.0080652  98.59772664 97.58409376
 98.31873393 95.25834237 98.08847358 97.00539711 98.62574774 98.97418404
 99.44932445 95.45205346 97.48784737 97.45860796 96.40964413 97.62917118
 98.87915596 98.43447326 98.70859273 99.3177471  99.52729621 99.01560654
 95.20961002 96.82508741 94.48106139 97.04072806 98.06167079 98.29071283
 98.40036062 98.59772664 97.08580548 98.56726892 98.63914913 98.85844471
 97.72419927 98.44665635 98.95103617 93.34803426 98.15182564 95.88820799
 98.37843106 97.64135427 98.59163509 97.59749516 98.2310157  98.67935332
 98.13964255 98.75488846 98.7317406  98.62331112 99.03753609 98.43081834
 98.77559971 97.73150912 92.80710518 96.96397461 96.72274948 97.57922053
 96.95544645 98.71955751 97.50368538 97.37941789 98.79631096 97.6316078
 98.59650833 95.9734896  99.10088815 98.37721275 99.81238045 97.54267126
 99.04971918 96.15136268 98.74514199 99.16789513 99.68567634 99.42373996
 99.84892972 99.18251483]
Accuracy th:0.7 is [86.1502662  97.32703062 92.88507694 98.02755814 98.42107187 97.51465016
 98.22248754 94.97082151 98.12867777 96.7946297  98.5782337  98.95712772
 99.42373996 95.34118736 97.40012914 97.30753768 96.35725686 97.52926987
 98.83407853 98.37599444 98.65255053 99.3031274  99.55531731 98.88281088
 95.24494097 96.72762271 94.27638552 96.93717182 98.02755814 98.22126923
 98.22492416 98.60138156 96.89209439 98.48564223 98.61478296 98.72321244
 97.56460082 98.35893812 98.74514199 93.27859066 98.03852292 95.33631413
 98.32482548 97.62673457 98.25903681 97.27707996 98.19690306 98.64524068
 98.03852292 98.69519134 98.66351531 98.61965619 99.02657131 98.40157893
 98.73905045 97.65841059 91.74473995 96.66792559 96.6009186  97.44033333
 96.04902474 98.62331112 97.18936173 97.21250959 98.69397303 97.49272061
 98.48686054 95.96861637 99.04119102 98.25050864 99.81603538 97.36114326
 98.98514882 95.93815865 98.49295208 99.17155005 99.64790877 99.47612724
 99.84527479 99.17033175]
Avg Prec: is [96.54384316 35.27957242 73.03807065 65.74803739 78.69995267 64.20614748
 73.972317   47.83467372 59.38770172 53.09599134 33.03891756 54.69552537
 26.23097963 29.72267096 34.57204195 57.79991899 29.52701536 43.45747389
 48.62584532 39.45827915 61.67630349 49.16439425 89.62269326 83.32909196
 26.14339425 34.5340421  38.73023198 39.92796248 24.73308394 39.35793598
 72.59046024 37.8327609  58.43883468 63.15815228 71.3399816  79.52409042
 59.51170461 75.19464339 87.07034776 46.44695893 46.96900377 80.71540873
 80.98900476 75.0499494  87.04816297 88.39115444 38.90504909 33.39522938
 42.17509049 46.55967857 61.77657328 37.60591537 25.92134352 73.08243191
 29.37959665 45.11638703 74.2293617  59.27640609 46.40997064 58.48560828
 93.78817598 83.24721448 73.38273389 50.30803232 60.57850515 45.29189779
 60.50187271 26.25817669 69.31576417 69.47412831 12.66573711 73.47128859
 82.70836799 51.74457627 91.47495761 92.88656064 86.01694626 90.22605345
 30.74823172 30.00079751]
Accuracy th:0.5 is [45.45753585 97.2137279  71.2978643  97.02489005 97.26733349 76.04317686
 76.35384559 75.75200107 77.24321097 96.46203141 77.52707691 98.52097319
 99.41399349 80.0745605  77.16402091 96.56680596 96.29512311 76.93376055
 98.65376884 98.30776915 80.27192651 78.00343563 98.38695922 79.9222719
 81.65105201 96.65086926 94.0778012  76.67182417 98.01293844 77.43448545
 97.30875598 98.57457877 96.36213009 98.02024829 88.29205297 77.03122525
 76.81558461 91.51813453 97.11504489 74.32657984 79.13646276 92.05906361
 76.36968361 75.92865584 96.9627563  93.87434364 98.02877645 98.57336046
 97.73638235 89.39340408 88.33225716 98.55508583 98.99976852 76.47080323
 98.70615611 76.81680291 71.71208928 93.61118895 96.24273583 96.9067141
 89.79300934 97.17717864 94.24714611 76.88868313 98.42838172 77.35285876
 98.20786784 76.07850782 77.98028776 97.55972759 78.47735773 95.99054592
 77.72931616 95.45083515 76.24785273 83.24825477 88.951158   77.50880228
 78.50537883 99.14718388]
Accuracy th:0.7 is [45.58302165 97.2137279  71.2978643  97.02489005 97.26733349 76.04317686
 76.35384559 76.05414164 77.24321097 96.47665111 77.52707691 98.52097319
 99.41399349 80.55457414 77.16402091 96.56680596 96.29512311 76.93376055
 98.65376884 98.30776915 80.66422193 78.00343563 98.38695922 80.83844008
 82.43564284 96.65086926 94.0778012  76.67182417 98.01293844 77.43448545
 97.30875598 98.57457877 96.36213009 98.02024829 88.49307391 77.03122525
 76.81558461 91.7508315  97.11504489 74.32657984 79.76754669 92.05906361
 76.36968361 75.92865584 96.9627563  93.87434364 98.02877645 98.57336046
 97.96176947 89.95626272 88.52840487 98.55508583 98.99976852 76.47080323
 98.70615611 76.81680291 71.71208928 94.0643998  96.24273583 96.9067141
 89.79300934 97.17717864 94.35801221 76.88868313 98.42838172 77.35285876
 98.20786784 76.07850782 77.98028776 97.55972759 78.4871042  95.99054592
 77.87185829 95.45083515 76.24785273 83.38105043 89.21431269 77.50880228
 78.50537883 99.14718388]
Avg Prec: is [55.92317613  2.94341538 11.14989661  3.34779343  2.25544963  3.70896773
  3.37508446  5.57637877  2.61704858  3.87123375  1.64291506  1.59538093
  0.59672765  5.22013091  2.65377383  3.1198474   3.63592611  2.68677151
  1.37872201  1.74513816  1.9302315   0.86079869  1.81383049  2.40668411
  5.16722986  3.57668745  6.58964313  3.17348764  2.06688386  1.81672505
  2.6614517   1.31771651  3.78540701  1.69247274  2.44649652  2.46135864
  3.10975172  2.66668658  2.84098309  7.25138759  2.27373153  8.23515355
  3.28345024  3.94866245  3.17609738  6.48592221  2.05421266  1.60865786
  2.16251484  1.57528943  1.89953395  1.62656461  1.11817082  3.01542877
  1.30593798  2.69107125 11.18157867  3.61359965  3.96624189  2.75974299
 10.858888    2.17601379  3.94842807  2.96097967  1.65878693  2.58735012
  1.79937265  4.11132637  1.18547798  2.38067473  0.1886424   3.30910934
  1.93289031  4.54603575  3.99900733  3.14569372  0.81985403  1.9234415
  0.147661    0.73995819]
mAP score regular 57.36, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [88.09826345 97.44126367 93.22071904 98.15880609 98.92866931 97.59573461
 98.49017116 95.39078656 98.07160475 97.11239006 98.6820141  98.97351571
 99.36716745 95.26621322 97.47116127 97.3341306  96.40730498 97.69290181
 98.96853278 98.37556369 98.88880584 99.33477838 99.57645066 99.22764531
 95.26870469 96.79597379 94.46396093 97.2593866  97.89221915 98.19617809
 98.67952263 98.55744077 96.91556419 98.73682637 98.89378877 99.16535865
 97.99187782 98.34068316 99.10307198 93.08368837 97.99436929 92.95662356
 97.31669034 96.32508658 97.01273139 94.78536014 98.37058076 98.79662157
 98.11395969 98.7642325  98.87136557 98.66208237 98.87634851 98.4054613
 98.73682637 97.76515435 90.92856965 97.1995914  96.52191245 97.61815781
 92.8644393  98.79163864 97.2369634  97.44624661 98.7642325  97.59324314
 98.57986397 95.86167377 98.75924957 98.28088796 99.81563146 97.63061514
 98.40047836 95.86914817 97.4238234  97.3640282  99.21767945 98.63218477
 99.82310586 99.16535865]
Accuracy th:0.7 is [87.11662556 97.3939258  93.00396143 98.30580263 98.86139971 97.64805541
 98.45529063 95.22385829 98.19119516 96.88566659 98.63965917 99.03331091
 99.36965892 95.15160575 97.40389167 97.29177567 96.32010365 97.58826021
 99.00341331 98.40047836 98.86389117 99.36218452 99.63375439 99.12798665
 95.45058176 96.66890899 94.49385853 97.11239006 97.84238981 98.22607569
 98.57238957 98.6969629  96.93549593 98.70194584 98.92368637 99.09061464
 97.97194608 98.25348182 98.88631437 93.35774971 97.95450582 93.21324464
 97.47116127 96.55430152 97.0501034  94.98467748 98.40296983 98.83399357
 98.00682662 98.72935197 98.75177517 98.6595909  98.90624611 98.4503077
 98.75177517 97.78259461 90.66945711 96.96041059 96.45464285 97.46119541
 92.73239156 98.82153624 97.1024242  97.31669034 98.6670653  97.56583701
 98.47522236 95.8218103  98.79662157 98.17126342 99.81563146 97.51600767
 98.46276503 95.87911403 97.36153674 97.50355034 99.24010265 98.7268605
 99.82559733 99.16785011]
Avg Prec: is [96.49768437 33.66591336 70.66256748 73.30207581 77.70926287 64.81280576
 80.20529239 47.86781663 63.3116673  56.2438417  39.10637932 57.27357983
 22.60269073 31.17660021 35.08628667 61.89379629 32.40960452 45.7765552
 51.27919928 37.18204865 70.16891906 58.39025746 92.16626713 87.94782845
 24.88804741 37.99022999 33.2680266  46.47221149 28.30883845 38.22447083
 76.8594496  38.26185459 55.23797934 64.85026178 74.87706493 82.88060169
 60.82920089 77.48348065 89.76159319 44.49675936 37.29014503 50.35330175
 51.65330742 41.15062352 31.2193474  52.67178011 39.69054404 27.97849213
 41.98950991 41.41517236 67.10921765 37.04499827 24.76251167 76.31630948
 29.50847373 42.10025561 55.92257164 57.13235173 39.90282048 61.34300847
 64.92561138 86.7927999  66.85941569 52.43588857 61.28709421 38.39099871
 62.0468709  26.71743058 42.42355365 65.50245775 11.54860148 74.07424937
 56.20510422 44.93715558 62.79089754 50.81934649 14.12654037 54.35653638
  2.00005066 23.12770841]
Accuracy th:0.5 is [45.27742482 97.22450607 69.55925954 96.96290206 97.90716795 74.915913
 75.028029   74.12611805 76.5104517  96.41976231 76.64997384 98.5325261
 99.34972718 77.8433864  76.5926701  96.31262924 96.21047911 76.00468396
 98.78167277 98.34068316 78.69795949 77.28529785 98.31327703 81.26915315
 78.04270374 96.52938685 94.3393876  76.10185116 97.81747515 76.60761891
 97.52597354 98.67204823 96.39983058 98.18870369 89.42372375 76.2338989
 76.2264245  92.68007076 97.0276802  73.4559135  77.25540025 92.37362035
 75.35690261 75.03052047 97.03764606 94.02795426 98.18621222 98.77668984
 97.92709968 89.47355308 86.94471435 98.55993223 98.87385704 75.34195381
 98.6969629  76.03956449 70.27181902 94.42658893 96.16314124 96.78102499
 90.13379176 97.04761193 94.36430226 76.09935969 98.32075143 76.95144131
 98.13139996 75.28215861 77.32765279 97.53593941 77.7163216  96.07843137
 77.11089518 95.44559882 75.21488901 84.20908389 91.03570272 76.67488851
 77.79604853 99.15040985]
Accuracy th:0.7 is [45.47923362 97.22450607 69.55925954 96.96290206 97.90716795 74.915913
 75.028029   74.32543538 76.5104517  96.41976231 76.64997384 98.5325261
 99.34972718 78.28188455 76.5926701  96.31262924 96.21047911 76.00468396
 98.78167277 98.34068316 79.00690136 77.28529785 98.31327703 81.88454543
 78.62321549 96.52938685 94.3393876  76.10185116 97.81747515 76.60761891
 97.52597354 98.67204823 96.39983058 98.18870369 89.59812642 76.2338989
 76.2264245  92.8644393  97.0276802  73.4559135  77.73376187 92.37362035
 75.35690261 75.03052047 97.03764606 94.02795426 98.18621222 98.77668984
 97.95948875 89.82484989 87.11662556 98.55993223 98.87385704 75.34195381
 98.6969629  76.03956449 70.27181902 94.76791987 96.16314124 96.78102499
 90.13379176 97.04761193 94.4515036  76.09935969 98.32075143 76.95144131
 98.13139996 75.28215861 77.32765279 97.53593941 77.7163216  96.07843137
 77.16321599 95.44559882 75.21488901 84.36853776 91.24747739 76.67488851
 77.79604853 99.15040985]
Avg Prec: is [54.53086615  3.74793779 14.86825806  4.55323658  1.50325751  4.2765013
 11.85908098  8.70230227  7.48971467  5.15319794  2.29270822  5.1777314
  1.55401822  5.88599664  3.06842871  3.9030602  25.48722646  6.34544107
  1.55533035  2.65142714  3.5743077   1.41616275  1.16952132  5.82996504
  5.73623076 11.23840824  8.15086963  4.49631033  3.87898628  6.99146393
  2.3420257   0.86690059  3.10907605  1.1073254   1.69066707  2.40829338
  2.01951131  2.22031302  2.24913337  6.21230274  1.730107    6.02039361
  2.19954846  2.73268121  2.37651235  4.8912798   1.77277861  1.04615072
  1.3867886   1.17252095  1.20735459  0.99575749  0.84466441  2.40077913
  0.85353933  1.829149   10.05531671  2.90672896  3.89743762  2.80484378
  7.87809327  2.0262032   3.2272664   2.53883193  1.35542728  1.83719556
  1.56463905  3.43297854  1.08185547  2.22582227  0.19482841  3.16373166
  1.57574527  3.94734198  3.52956746  2.32228581  0.60637063  1.56680809
  0.12699436  0.58859462]
mAP score regular 51.62, mAP score EMA 4.37
Train_data_mAP: current_mAP = 57.36, highest_mAP = 57.36
Val_data_mAP: current_mAP = 51.62, highest_mAP = 51.91
tensor([3.3782e-02, 3.8636e-03, 4.6997e-02, 4.1323e-03, 1.7126e-03, 4.3585e-03,
        8.6692e-04, 2.4123e-03, 7.1837e-03, 3.0487e-03, 8.8101e-04, 1.7339e-03,
        5.5142e-04, 5.6768e-03, 7.1539e-03, 7.5848e-03, 9.8041e-03, 7.7928e-03,
        7.2983e-03, 8.1268e-03, 1.8025e-04, 9.7897e-04, 2.1916e-03, 1.4191e-03,
        5.1917e-03, 1.3721e-03, 1.9934e-02, 1.3482e-03, 9.0863e-04, 2.1883e-03,
        2.1689e-03, 9.5039e-04, 2.2258e-02, 2.8349e-04, 2.2007e-03, 1.2515e-02,
        1.4254e-03, 3.3371e-03, 8.0454e-03, 2.1046e-02, 1.5652e-01, 4.7089e-01,
        7.9301e-01, 6.8891e-01, 9.6169e-01, 8.4769e-01, 1.2815e-03, 1.7313e-03,
        2.8886e-03, 2.6737e-03, 5.2055e-04, 1.5031e-03, 2.3466e-03, 3.6724e-03,
        1.1336e-03, 5.5329e-03, 1.5138e-01, 1.1619e-02, 1.0458e-01, 2.4544e-03,
        8.4074e-01, 5.0926e-03, 1.4207e-01, 1.6893e-02, 2.8856e-02, 6.4860e-03,
        4.2576e-02, 1.1823e-02, 5.1598e-01, 4.4697e-02, 1.7983e-03, 3.9190e-03,
        3.6634e-01, 8.7931e-02, 9.4110e-01, 9.9992e-01, 1.0000e+00, 9.9991e-01,
        6.4294e-01, 6.5049e-02], device='cuda:0')
Sum Train Loss:  tensor([7.6224e-01, 2.9415e-02, 1.2169e+00, 1.6471e-02, 9.1347e-03, 6.0589e-02,
        2.8875e-03, 3.5865e-02, 1.2814e-01, 4.8160e-02, 6.0562e-03, 7.3754e-03,
        2.6050e-04, 9.5069e-02, 4.7308e-02, 1.6696e-01, 1.2151e-01, 1.0858e-01,
        1.0039e-02, 3.1754e-02, 3.1400e-04, 3.0078e-03, 2.1827e-03, 3.3106e-03,
        1.2319e-01, 1.4146e-02, 4.1513e-01, 1.7722e-02, 9.4537e-03, 2.4224e-02,
        1.0393e-02, 2.1064e-03, 1.4568e-01, 2.1516e-03, 1.4206e-02, 4.3813e-02,
        1.0574e-02, 3.2501e-02, 4.5664e-02, 4.4913e-01, 8.7603e-01, 5.5421e+00,
        3.4485e+00, 7.9461e+00, 7.0549e+00, 5.7537e+00, 8.2848e-03, 2.7800e-02,
        7.8892e-03, 1.5816e-02, 5.1967e-03, 5.9332e-03, 1.2219e-02, 2.1018e-02,
        4.5911e-03, 3.4184e-02, 3.4826e+00, 2.3278e-01, 8.7639e-01, 2.4521e-02,
        1.1441e+01, 5.7299e-03, 1.1023e+00, 1.4209e-01, 3.5954e-02, 2.1107e-02,
        1.6890e-01, 1.3945e-01, 1.5742e+00, 3.1172e-01, 8.1507e-04, 5.2860e-02,
        1.1648e+00, 1.4841e+00, 6.6965e+00, 7.2673e+00, 4.0386e-01, 7.1641e-01,
        5.5066e-02, 6.3327e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [26/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 73.1
Sum Train Loss:  tensor([8.5143e-01, 3.1812e-02, 5.0037e-01, 2.0404e-02, 5.3978e-03, 2.6210e-02,
        5.6684e-03, 3.7789e-02, 5.8671e-02, 2.6405e-02, 4.9640e-03, 1.9232e-02,
        2.2845e-04, 8.5995e-02, 5.4232e-02, 2.9905e-02, 1.7201e-01, 8.2449e-02,
        3.4728e-02, 3.4490e-02, 7.4131e-04, 2.1846e-03, 4.7295e-03, 4.3559e-03,
        7.3619e-02, 1.3605e-02, 3.0859e-01, 1.2828e-02, 5.6489e-03, 1.9214e-02,
        1.7084e-02, 2.6264e-03, 2.6053e-01, 2.6098e-03, 8.0290e-03, 3.5891e-02,
        1.2355e-02, 2.8351e-02, 3.2148e-02, 2.7778e-01, 7.9140e-01, 5.2773e+00,
        4.7130e+00, 4.4771e+00, 2.2658e+00, 6.3361e+00, 7.6422e-03, 1.2841e-02,
        2.2840e-02, 5.3959e-03, 1.0412e-03, 6.4931e-03, 1.8851e-03, 1.7554e-02,
        9.0063e-03, 4.2016e-02, 4.2987e+00, 1.3660e-01, 1.0944e+00, 2.1906e-02,
        1.1698e+01, 2.4214e-02, 1.4523e+00, 1.3782e-01, 2.5424e-02, 1.1755e-01,
        3.0575e-01, 1.7273e-01, 2.0573e+00, 2.0121e-01, 4.2980e-03, 3.9203e-02,
        1.5037e+00, 1.4757e+00, 5.2169e+00, 7.7839e+00, 1.0962e+00, 7.1683e-01,
        1.7916e-01, 6.0089e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [26/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 67.0
Sum Train Loss:  tensor([9.0490e-01, 3.3273e-02, 1.0271e+00, 3.8882e-02, 4.7693e-03, 3.9771e-02,
        1.0659e-02, 3.6474e-02, 5.8386e-02, 2.3922e-02, 1.7206e-03, 9.6902e-03,
        1.1767e-03, 1.2749e-01, 6.0960e-02, 1.4693e-01, 1.5247e-01, 1.1353e-01,
        1.1903e-02, 1.8628e-01, 3.7777e-04, 5.7027e-04, 3.1156e-03, 1.6686e-02,
        1.3655e-01, 2.7448e-02, 3.3725e-01, 1.5749e-02, 1.3178e-03, 1.0035e-02,
        1.2218e-02, 3.2745e-03, 3.8750e-01, 2.0997e-03, 1.9434e-02, 2.8720e-02,
        1.2818e-02, 1.7607e-02, 1.8159e-02, 4.2280e-01, 1.0970e+00, 6.9631e+00,
        7.9358e-01, 2.4878e+00, 7.3395e-01, 3.8305e+00, 4.5923e-03, 3.5629e-03,
        2.1037e-02, 4.7093e-03, 1.3525e-03, 4.8816e-03, 6.8805e-03, 1.1011e-02,
        4.0653e-03, 3.5753e-02, 3.1452e+00, 5.8958e-02, 2.1886e+00, 1.2526e-02,
        3.2082e+00, 1.4126e-02, 6.9136e-01, 1.3568e-01, 1.3608e-02, 3.6229e-02,
        1.0586e-01, 2.1129e-01, 4.8993e+00, 1.2488e-01, 2.5003e-04, 3.7332e-02,
        1.6959e-01, 8.7733e-01, 9.9508e-01, 2.6716e+00, 1.9771e+00, 8.6557e+00,
        1.3470e-01, 5.8412e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [26/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 51.4
Sum Train Loss:  tensor([1.3091e+00, 1.2500e-02, 6.2478e-01, 2.0866e-02, 9.7571e-03, 3.8921e-02,
        1.1169e-02, 3.2571e-02, 3.0809e-02, 4.2315e-02, 1.2431e-02, 7.9402e-03,
        4.9464e-04, 1.0832e-01, 6.0357e-02, 5.6469e-02, 8.9143e-02, 6.1987e-02,
        9.8607e-02, 1.2619e-01, 1.5694e-03, 6.2452e-03, 1.0069e-02, 8.1795e-03,
        5.4666e-02, 1.0152e-02, 1.6778e-01, 1.2558e-02, 2.1624e-03, 1.2203e-02,
        9.0650e-03, 4.1872e-03, 1.8186e-01, 1.3848e-03, 2.1902e-02, 9.7513e-02,
        7.2187e-03, 2.4525e-02, 1.9100e-02, 3.2971e-01, 7.4073e-01, 5.2595e+00,
        4.6350e+00, 3.7076e+00, 4.0714e+00, 6.8286e+00, 5.5771e-03, 1.9560e-03,
        1.1148e-02, 6.4889e-03, 6.0352e-04, 1.7145e-02, 9.7589e-03, 1.7588e-02,
        1.7964e-02, 5.5837e-02, 2.4411e+00, 7.2190e-02, 5.7297e-01, 2.1502e-02,
        6.2561e+00, 1.2496e-02, 3.0597e-01, 2.4551e-01, 2.3571e-01, 6.5907e-02,
        1.2789e-01, 2.3833e-01, 4.8012e-01, 8.2001e-02, 2.1158e-04, 2.1232e-02,
        8.3239e-01, 9.4724e-01, 3.3638e+00, 7.0769e-01, 2.3990e-01, 5.3992e+00,
        8.1376e-02, 3.4554e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [26/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 52.2
Sum Train Loss:  tensor([1.2623e+00, 3.4076e-02, 6.0745e-01, 3.3704e-02, 1.1182e-02, 4.2586e-02,
        3.7883e-03, 2.4627e-02, 8.0789e-02, 2.1798e-02, 1.4558e-02, 2.3162e-03,
        6.0878e-04, 8.1977e-02, 9.1378e-02, 1.1305e-01, 1.5323e-01, 5.3283e-02,
        1.9402e-02, 4.9108e-02, 7.8068e-04, 6.1789e-04, 7.7613e-03, 4.4623e-03,
        1.1930e-01, 3.4643e-02, 4.2368e-01, 2.3388e-02, 1.0503e-02, 3.7269e-03,
        2.4417e-02, 9.8784e-03, 2.8568e-01, 5.3754e-04, 1.1332e-02, 2.5745e-02,
        6.8239e-03, 2.2894e-02, 1.5443e-02, 5.2059e-01, 9.8301e-01, 9.0171e+00,
        1.7899e+00, 2.8367e+00, 6.0221e+00, 8.4654e+00, 8.8467e-03, 1.5090e-02,
        2.2338e-02, 1.7987e-02, 3.2981e-03, 4.3159e-03, 2.4025e-03, 7.9185e-02,
        2.5724e-03, 3.4132e-02, 1.8512e+00, 1.2330e-01, 1.0140e+00, 2.1368e-02,
        3.8816e+00, 2.5296e-02, 1.9913e+00, 1.4539e-01, 2.1393e-01, 6.3121e-02,
        5.4662e-01, 7.0178e-02, 1.2932e+00, 1.9408e-01, 1.2009e-03, 5.0971e-02,
        5.7056e+00, 9.9422e-01, 2.5037e+00, 2.8037e+00, 3.3995e-01, 2.1026e+00,
        1.0470e-01, 2.4594e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [26/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 59.8
Sum Train Loss:  tensor([1.3151e+00, 5.0197e-02, 1.0023e+00, 5.5367e-02, 1.6186e-02, 5.1582e-02,
        1.1566e-02, 3.8308e-02, 9.4324e-02, 2.2982e-02, 9.3722e-03, 8.9809e-03,
        3.5829e-03, 1.9836e-01, 8.4186e-02, 7.1561e-02, 7.3421e-02, 6.1563e-02,
        1.7438e-02, 1.9323e-02, 1.0997e-03, 3.0649e-03, 1.0551e-02, 8.8511e-03,
        8.2855e-02, 1.0697e-02, 3.5395e-01, 9.4284e-03, 1.1290e-02, 2.4149e-02,
        6.2061e-03, 5.1055e-03, 1.3304e-01, 3.0090e-03, 5.4388e-03, 1.9601e-02,
        3.1206e-02, 1.1432e-02, 4.4425e-02, 4.4359e-01, 1.2026e+00, 2.9285e+00,
        2.6160e+00, 5.6978e+00, 1.7472e+00, 4.0316e+00, 4.3584e-03, 7.3355e-03,
        2.1899e-02, 9.6029e-03, 3.6586e-03, 1.1992e-02, 3.2947e-03, 1.8953e-02,
        3.5058e-03, 2.8287e-02, 4.1652e+00, 8.6598e-02, 1.0896e+00, 1.5104e-02,
        1.6245e+01, 2.4610e-03, 8.5137e-01, 1.4995e-01, 4.8225e-02, 1.0668e-01,
        7.9712e-02, 2.1173e-01, 2.6705e-01, 5.4134e-02, 3.7944e-04, 7.2930e-03,
        1.2761e+00, 1.1749e+00, 3.9597e+00, 1.4135e+00, 8.0788e-01, 5.4127e-01,
        1.0655e-01, 5.1952e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [26/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 55.5
Sum Train Loss:  tensor([1.3252e+00, 5.1292e-02, 8.1702e-01, 2.9311e-02, 8.6521e-03, 4.4194e-02,
        6.2679e-03, 2.8350e-02, 2.0932e-02, 3.5307e-02, 1.1004e-02, 7.4590e-03,
        2.3575e-03, 6.3255e-02, 2.3621e-02, 6.8548e-02, 1.9228e-01, 4.2680e-02,
        4.3269e-02, 2.1722e-02, 2.2990e-03, 1.8133e-03, 2.3449e-02, 2.0529e-03,
        1.8103e-01, 2.9849e-02, 3.0406e-01, 1.2526e-02, 6.2339e-03, 5.5261e-03,
        2.6685e-02, 1.0095e-03, 1.2594e-01, 2.0523e-03, 6.8213e-03, 1.5137e-02,
        1.3655e-02, 4.5916e-02, 1.6638e-02, 3.8194e-01, 1.5239e+00, 6.3221e+00,
        4.2050e+00, 4.4798e+00, 5.8191e+00, 1.2113e+01, 7.5301e-03, 4.5802e-03,
        1.2136e-02, 6.9176e-03, 3.5551e-03, 1.5062e-02, 6.8169e-03, 3.0138e-02,
        4.9089e-03, 5.1824e-02, 2.7929e+00, 4.4327e-02, 1.6202e+00, 2.7303e-02,
        1.4600e+01, 3.9049e-02, 5.2859e-01, 2.1830e-01, 6.9362e-02, 3.3075e-02,
        1.0358e-01, 3.5784e-01, 9.0486e-01, 3.2039e-01, 3.4941e-04, 4.4527e-02,
        1.8894e+00, 1.8049e+00, 1.3908e+01, 1.7237e+00, 6.7426e+00, 6.0753e-01,
        3.8871e+00, 4.5056e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [26/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 91.4
Sum_Val Meta Model:  tensor([1.7411e+00, 2.6380e-01, 3.9170e+00, 1.9092e-01, 2.8552e-03, 5.8094e-02,
        3.9886e-03, 4.5515e-02, 4.8091e-02, 2.9564e-02, 5.4106e-03, 2.0107e-02,
        4.3375e-03, 8.1961e-02, 5.6622e-02, 6.2213e-02, 4.9922e+00, 2.1976e-02,
        8.8475e-03, 8.6993e-03, 2.0495e-04, 3.7654e-04, 8.0235e-04, 1.0904e-03,
        1.4151e-01, 2.5711e-02, 5.9857e-01, 5.9605e-03, 1.0025e-02, 2.6967e-02,
        1.8437e-03, 6.0380e-04, 1.8274e-01, 2.3976e-04, 3.0476e-03, 2.2880e-02,
        4.1305e-03, 2.3773e-02, 1.5768e-02, 1.0352e+00, 1.8224e+00, 1.4719e+01,
        5.1924e+00, 1.3224e+01, 2.1451e+01, 1.4960e+01, 2.7104e-03, 8.8380e-03,
        3.5914e-03, 1.2729e-02, 1.6808e-04, 4.8557e-04, 1.6211e-02, 3.0881e-03,
        1.9177e-03, 8.2258e-03, 4.3685e+00, 6.9956e-02, 1.5667e+00, 8.6452e-03,
        1.2516e+01, 6.0518e-02, 8.4354e-01, 7.4551e-02, 1.9520e-02, 1.1136e-02,
        3.1612e-02, 8.6389e-02, 6.4750e+00, 1.0182e+00, 2.9982e-04, 6.2161e-02,
        8.3580e+00, 8.4908e-01, 1.8521e+01, 1.0049e+01, 1.4048e-01, 1.0861e+01,
        4.1928e-02, 5.3843e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.6610e+00, 1.6166e-01, 2.3828e+00, 9.2020e-02, 3.9563e-04, 5.7431e-02,
        3.4249e-03, 4.1345e-02, 4.8672e-02, 2.8857e-02, 6.2546e-03, 2.2850e-02,
        5.0981e-03, 7.7647e-02, 6.7006e-02, 2.5906e-01, 3.2981e+00, 4.6745e-02,
        4.9263e-03, 1.2830e-02, 1.0785e-04, 1.9060e-04, 2.7596e-04, 3.2324e-03,
        1.4027e-01, 2.4029e-02, 5.6067e-01, 9.3262e-03, 1.4160e-02, 3.3196e-02,
        2.1574e-03, 1.0437e-03, 1.6726e-01, 2.8792e-04, 4.0264e-03, 2.2230e-02,
        1.2920e-02, 1.6811e-02, 1.5967e-02, 9.7135e-01, 2.0001e+00, 1.8438e+01,
        4.9043e+00, 1.3308e+01, 2.4020e+01, 1.7666e+01, 1.9358e-03, 1.0702e-02,
        1.2343e-03, 1.0342e-02, 1.3984e-05, 1.0005e-04, 1.8899e-02, 5.3645e-04,
        8.5550e-04, 2.2759e-03, 3.5352e+00, 6.9520e-02, 1.3104e+00, 1.5591e-02,
        2.0428e+01, 2.8196e-02, 4.0266e-01, 1.0496e-01, 7.6550e-03, 1.8006e-02,
        1.7972e-02, 1.0455e-01, 6.4324e+00, 9.5683e-01, 4.4513e-04, 8.6725e-02,
        8.2021e+00, 9.4828e-01, 1.7432e+01, 1.4929e+01, 2.2811e-02, 9.8561e+00,
        5.2282e-02, 6.0953e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.9169e+01, 4.1842e+01, 5.0701e+01, 2.2268e+01, 2.3101e-01, 1.3177e+01,
        3.9506e+00, 1.7139e+01, 6.7754e+00, 9.4655e+00, 7.0994e+00, 1.3179e+01,
        9.2453e+00, 1.3678e+01, 9.3665e+00, 3.4156e+01, 3.3640e+02, 5.9985e+00,
        6.7499e-01, 1.5787e+00, 5.9830e-01, 1.9470e-01, 1.2592e-01, 2.2778e+00,
        2.7017e+01, 1.7512e+01, 2.8126e+01, 6.9177e+00, 1.5584e+01, 1.5169e+01,
        9.9471e-01, 1.0982e+00, 7.5147e+00, 1.0156e+00, 1.8296e+00, 1.7763e+00,
        9.0641e+00, 5.0376e+00, 1.9846e+00, 4.6154e+01, 1.2779e+01, 3.9156e+01,
        6.1844e+00, 1.9317e+01, 2.4977e+01, 2.0840e+01, 1.5106e+00, 6.1819e+00,
        4.2729e-01, 3.8681e+00, 2.6865e-02, 6.6560e-02, 8.0536e+00, 1.4608e-01,
        7.5466e-01, 4.1135e-01, 2.3354e+01, 5.9832e+00, 1.2530e+01, 6.3524e+00,
        2.4298e+01, 5.5367e+00, 2.8343e+00, 6.2129e+00, 2.6529e-01, 2.7761e+00,
        4.2211e-01, 8.8431e+00, 1.2466e+01, 2.1407e+01, 2.4753e-01, 2.2129e+01,
        2.2389e+01, 1.0784e+01, 1.8523e+01, 1.4930e+01, 2.2811e-02, 9.8570e+00,
        8.1317e-02, 9.3704e-01], device='cuda:0')
Outer loop valEpocw Maximum [26/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 161.2
model_train val_loss valEpocw [26/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 175.7
model_train val_loss valEpocw [26/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1214.0
Sum_Val Meta Model:  tensor([5.3630e+00, 2.0105e-01, 5.0337e+00, 1.2884e-01, 3.7391e-03, 5.7852e-01,
        3.1040e-01, 5.9804e-01, 7.8421e-01, 4.0550e-01, 7.7405e-02, 1.7640e+00,
        3.0139e-02, 8.5310e-02, 4.3730e-01, 5.1915e-01, 2.8508e-01, 4.4220e-01,
        1.8438e-01, 1.0634e+00, 6.6972e-05, 1.3408e-04, 2.2470e-03, 2.6274e-02,
        4.0673e-01, 1.4076e-01, 1.3513e+00, 1.1533e-01, 5.9149e-02, 5.9115e-04,
        6.8452e-04, 2.6349e-04, 6.0367e-03, 2.4868e-04, 2.9643e-04, 1.5786e-03,
        2.8871e-02, 1.1409e-03, 7.5728e-04, 6.7739e-02, 2.5876e-02, 2.0190e+00,
        8.2878e-02, 1.5784e-01, 2.3888e-01, 3.6687e+00, 6.0374e-03, 5.2988e-03,
        5.7791e-04, 6.5556e-02, 1.1491e-04, 6.6345e-04, 2.1561e-04, 4.4964e-04,
        5.8463e-04, 5.2198e-02, 2.6953e+00, 1.7479e-01, 1.0209e+00, 4.1991e-02,
        1.2986e+00, 1.0114e-02, 3.8247e-01, 6.9122e-02, 2.5888e-02, 1.1837e-02,
        3.8135e-02, 5.7674e-01, 6.4961e-02, 3.8323e-01, 9.7290e-05, 1.4688e-02,
        2.2391e+00, 6.8394e-01, 5.0053e+00, 3.1737e-01, 1.0808e-01, 6.7089e-01,
        2.3354e-02, 1.0118e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([4.4159e+00, 2.4437e-01, 3.6073e+00, 1.4805e-01, 1.7423e-02, 4.6904e-01,
        2.4272e-01, 3.8883e-01, 5.9895e-01, 3.0106e-01, 5.9470e-02, 4.5529e-01,
        2.3668e-02, 1.0668e-01, 3.6181e-01, 7.6899e-02, 2.2425e-01, 3.7961e-01,
        7.0948e-02, 4.6421e-01, 5.8497e-04, 5.5355e-04, 7.9262e-04, 2.7018e-02,
        2.9636e-01, 1.2445e-01, 1.2780e+00, 1.0172e-01, 5.9314e-02, 7.5201e-03,
        2.4444e-03, 1.3149e-03, 4.0182e-02, 6.4332e-03, 5.1125e-03, 1.1040e-02,
        1.4382e-02, 9.3757e-03, 3.1631e-03, 5.8848e-02, 4.2086e-02, 2.3551e+00,
        1.8884e-02, 3.7982e-01, 2.7409e-02, 1.1374e+00, 3.6391e-03, 2.2543e-03,
        2.6318e-03, 5.2457e-02, 6.8592e-05, 2.7443e-04, 1.6536e-03, 2.7221e-03,
        2.0917e-03, 1.3378e-02, 1.7946e+00, 1.7030e-01, 7.1130e-01, 6.2779e-03,
        5.2409e-01, 8.7797e-03, 1.7893e-01, 1.4198e-02, 4.1184e-03, 5.6756e-03,
        9.5200e-03, 5.6364e-01, 1.0027e-01, 2.2236e-01, 1.1309e-04, 1.4891e-02,
        2.2743e+00, 2.6330e-01, 6.2837e+00, 9.7933e-02, 2.0309e-01, 1.8808e-02,
        2.5186e-03, 3.0269e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.7130e+01, 2.1701e+01, 4.7100e+01, 1.0995e+01, 2.5627e+00, 3.1842e+01,
        4.7323e+01, 4.1745e+01, 2.9400e+01, 2.8144e+01, 1.5007e+01, 6.2319e+01,
        7.9459e+00, 7.5664e+00, 1.6527e+01, 3.3923e+00, 9.1837e+00, 1.5639e+01,
        2.9175e+00, 1.8218e+01, 3.8538e-01, 1.1551e-01, 9.0025e-02, 4.6220e+00,
        2.4639e+01, 1.9228e+01, 3.2524e+01, 1.4933e+01, 1.2152e+01, 8.9105e-01,
        2.8573e-01, 2.7662e-01, 8.0994e-01, 3.3913e+00, 6.4955e-01, 4.4460e-01,
        2.2222e+00, 8.4030e-01, 1.3236e-01, 1.5131e+00, 2.6072e-01, 5.3266e+00,
        2.9085e-02, 6.7805e-01, 3.0235e-02, 1.4882e+00, 6.3024e-01, 3.2158e-01,
        2.2907e-01, 4.9825e+00, 2.4165e-02, 5.1688e-02, 2.0140e-01, 2.1717e-01,
        4.0848e-01, 9.1686e-01, 8.2842e+00, 5.3419e+00, 6.0415e+00, 6.2450e-01,
        6.8109e-01, 5.3948e-01, 9.9683e-01, 3.6187e-01, 8.0646e-02, 2.7442e-01,
        1.4191e-01, 1.9917e+01, 2.2098e-01, 2.9687e+00, 1.7183e-02, 1.1788e+00,
        6.6473e+00, 1.9644e+00, 7.1598e+00, 9.8011e-02, 2.0310e-01, 1.8823e-02,
        4.7434e-03, 3.2270e-01], device='cuda:0')
Outer loop valEpocw Maximum [26/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 42.7
model_train val_loss valEpocw [26/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 32.2
model_train val_loss valEpocw [26/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 686.7
Sum_Val Meta Model:  tensor([3.5086e+00, 2.4541e-02, 6.4874e-01, 1.9069e-02, 3.5189e-03, 2.4505e-02,
        3.0179e-03, 4.7910e-02, 2.7384e-02, 9.9386e-03, 2.7677e-03, 5.7031e-03,
        6.0559e-04, 1.5277e-01, 4.0993e-02, 4.1918e-02, 1.2586e-01, 5.2547e-02,
        1.9868e-02, 3.3389e-02, 7.4520e-04, 4.3563e-03, 4.1577e-02, 6.0666e-03,
        6.0293e-02, 5.7425e-03, 5.5859e-01, 1.9297e-02, 2.1453e-03, 1.9457e-02,
        2.5172e-02, 1.4119e-02, 2.9133e-01, 2.5137e-04, 1.0578e-02, 6.0277e-02,
        5.7608e-02, 7.5036e-02, 6.6575e-03, 7.1983e-01, 4.5202e+00, 3.2097e+01,
        5.0639e+01, 5.0376e+01, 4.7740e+01, 5.7386e+01, 2.9699e-02, 3.1782e-02,
        2.7657e-01, 7.2887e-02, 3.8595e-02, 1.1215e-01, 2.5903e-01, 9.2039e-02,
        1.5357e-01, 8.1377e-01, 2.8435e+01, 2.0468e-01, 8.6411e-01, 1.1779e-02,
        6.2595e+01, 1.6367e-02, 1.7165e+00, 2.9423e-01, 4.5033e-01, 1.3659e-02,
        4.4584e-01, 1.7423e-01, 2.3282e+00, 1.4134e-01, 1.0498e-03, 4.6456e-02,
        1.5192e+00, 3.7838e+00, 1.3051e+00, 1.7496e+01, 1.0482e+01, 1.2892e+00,
        1.4201e-01, 5.5920e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.0002e+00, 3.6399e-03, 2.6496e-01, 1.2657e-03, 1.0184e-04, 8.2578e-04,
        2.1748e-04, 5.6131e-02, 1.6681e-03, 3.6598e-04, 8.0873e-05, 1.0896e-04,
        4.3833e-05, 1.2116e-01, 3.0690e-03, 1.2088e-02, 1.5356e-02, 3.2337e-03,
        1.0066e-03, 3.4575e-04, 1.6402e-05, 2.7371e-05, 3.1204e-05, 1.1157e-04,
        2.8853e-02, 2.7708e-03, 5.6079e-01, 1.8064e-02, 1.0669e-03, 8.2690e-04,
        3.4748e-03, 1.1788e-02, 2.8878e-01, 3.8302e-05, 6.3569e-03, 2.5607e-02,
        3.4075e-02, 6.4230e-02, 2.2958e-03, 7.7612e-01, 4.0924e+00, 3.6105e+01,
        5.0629e+01, 4.9227e+01, 8.0262e+01, 7.6483e+01, 1.8379e-02, 1.7030e-02,
        2.0557e-01, 4.3009e-02, 2.3439e-02, 1.4161e-01, 2.7064e-01, 8.4165e-02,
        1.0913e-01, 6.4632e-01, 2.0295e+01, 2.3867e-01, 9.3877e-01, 7.8865e-03,
        1.2599e+02, 4.4638e-03, 1.3816e+00, 2.5565e-01, 4.2459e-01, 4.9966e-02,
        4.9984e-01, 2.1643e-01, 2.4295e+00, 5.6557e-02, 2.1575e-04, 4.9264e-02,
        2.2489e+00, 3.7210e+00, 3.9769e+00, 2.5926e+01, 1.2386e+01, 5.3483e-02,
        2.7385e-02, 5.8263e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.3620e+01, 7.3307e-01, 5.1988e+00, 2.2421e-01, 3.9495e-02, 1.3327e-01,
        1.5485e-01, 1.6730e+01, 1.7608e-01, 8.4622e-02, 6.0939e-02, 4.1255e-02,
        5.3042e-02, 1.6881e+01, 3.1713e-01, 1.2127e+00, 1.1683e+00, 2.7131e-01,
        1.0165e-01, 3.2355e-02, 5.1443e-02, 1.6947e-02, 8.1774e-03, 5.2138e-02,
        4.5797e+00, 1.3610e+00, 2.6778e+01, 8.7374e+00, 7.7632e-01, 2.3043e-01,
        9.6167e-01, 6.7206e+00, 9.7100e+00, 7.1501e-02, 1.8869e+00, 1.6719e+00,
        1.3179e+01, 1.2287e+01, 2.0604e-01, 3.4260e+01, 2.7992e+01, 7.6607e+01,
        6.7188e+01, 7.4165e+01, 8.4472e+01, 9.1388e+01, 8.9989e+00, 6.5109e+00,
        3.8734e+01, 1.0071e+01, 2.3597e+01, 5.7023e+01, 6.8540e+01, 1.5465e+01,
        5.6640e+01, 8.1195e+01, 1.2370e+02, 1.5365e+01, 1.0134e+01, 2.0139e+00,
        1.5141e+02, 6.4922e-01, 9.6628e+00, 1.2306e+01, 1.3851e+01, 5.3853e+00,
        1.1604e+01, 1.5200e+01, 5.4944e+00, 1.2241e+00, 8.8306e-02, 9.5101e+00,
        7.0381e+00, 3.9947e+01, 4.3281e+00, 2.5930e+01, 1.2386e+01, 5.3494e-02,
        5.0373e-02, 9.0521e-01], device='cuda:0')
Outer loop valEpocw Maximum [26/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 385.2
model_train val_loss valEpocw [26/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 502.9
model_train val_loss valEpocw [26/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1457.6
Sum_Val Meta Model:  tensor([7.7164e+00, 2.4716e-02, 1.7204e+00, 1.0598e-02, 2.6985e-03, 1.6202e-01,
        2.7212e-02, 1.4414e-01, 2.7938e-02, 2.1347e-01, 1.7831e-02, 3.8446e-02,
        3.5254e-04, 2.2760e-01, 2.9391e-01, 2.8948e-02, 3.7493e-02, 2.5369e-02,
        5.7306e-03, 1.0309e-02, 6.4222e-04, 1.3457e-03, 3.0056e-03, 2.9954e-03,
        2.8515e-01, 8.3510e-03, 1.2100e+00, 8.2213e-03, 7.3125e-02, 3.7201e-03,
        7.9951e-03, 9.2912e-04, 2.0832e-01, 2.5484e-02, 4.0285e-02, 2.0332e-01,
        1.9505e-03, 1.6870e-02, 1.8823e-01, 1.1154e+00, 3.9487e+00, 1.7518e+01,
        8.7044e+00, 8.0148e+00, 1.2120e+01, 1.6933e+01, 2.6374e-03, 4.0036e-03,
        1.1005e-02, 5.7759e-03, 1.9470e-03, 2.2784e-03, 1.9698e-03, 8.5367e-02,
        2.9820e-03, 2.6718e-02, 5.8796e+00, 1.4887e-01, 2.1614e+00, 1.5922e-02,
        4.5279e+01, 9.2773e-03, 9.1720e-01, 4.2705e-01, 3.8177e-01, 3.3811e-02,
        5.7651e-01, 2.0663e+00, 5.8227e-01, 1.6748e-01, 4.1382e-04, 1.8655e-02,
        1.7985e+00, 1.7341e+00, 4.1182e+02, 1.1160e+01, 1.9226e+00, 8.3911e+00,
        3.6225e-02, 8.7224e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([2.4541e+00, 2.5096e-02, 1.1703e+00, 1.8189e-02, 7.5755e-03, 1.3004e-01,
        3.4388e-02, 1.3095e-01, 3.2583e-02, 1.5737e-01, 1.3328e-02, 5.6333e-02,
        2.0631e-03, 2.2008e-01, 3.5947e-01, 9.0012e-03, 1.4070e-02, 1.6625e-02,
        8.6132e-04, 8.7581e-04, 2.1427e-04, 1.4084e-04, 2.2013e-04, 2.3395e-03,
        2.7103e-01, 9.8716e-03, 1.1514e+00, 9.8397e-03, 7.5647e-02, 8.4303e-04,
        1.8408e-03, 1.0033e-03, 8.0536e-03, 2.4470e-04, 1.7686e-03, 4.5617e-03,
        2.0914e-03, 1.3009e-03, 1.3191e-03, 8.5847e-02, 1.9780e-01, 3.7459e-01,
        1.3549e-01, 8.7002e-02, 4.6396e-02, 3.8322e-01, 1.1903e-03, 1.1459e-03,
        7.5785e-04, 6.3672e-04, 3.7272e-05, 6.2968e-05, 2.4047e-04, 9.3342e-04,
        1.2127e-03, 1.1452e-02, 1.0631e+00, 1.7082e-02, 1.3786e+00, 6.8858e-03,
        1.5645e+01, 1.0525e-02, 1.9647e-01, 1.6295e-02, 1.2309e-02, 2.2280e-02,
        8.4103e-02, 2.0064e-01, 4.3625e-02, 7.3811e-03, 1.0749e-04, 1.5700e-02,
        1.1454e-02, 6.3843e-01, 4.9345e+01, 1.1278e+00, 4.1744e-03, 7.7037e+00,
        6.8096e-03, 6.9521e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.5611e+01, 2.4033e+00, 1.6809e+01, 1.5113e+00, 1.1314e+00, 1.0018e+01,
        8.5846e+00, 1.6270e+01, 1.8069e+00, 1.6282e+01, 3.6743e+00, 8.7639e+00,
        8.2461e-01, 1.5343e+01, 1.9585e+01, 5.0703e-01, 6.5404e-01, 7.3986e-01,
        4.7142e-02, 4.5881e-02, 1.7459e-01, 3.3716e-02, 2.9311e-02, 4.6625e-01,
        2.3308e+01, 1.8018e+00, 2.8897e+01, 1.6993e+00, 1.8627e+01, 1.1285e-01,
        2.4591e-01, 2.4718e-01, 1.5748e-01, 1.3149e-01, 2.2228e-01, 1.6057e-01,
        3.5610e-01, 1.2265e-01, 5.2609e-02, 2.0054e+00, 9.3589e-01, 7.7248e-01,
        1.9721e-01, 1.4495e-01, 5.0418e-02, 4.8555e-01, 2.5448e-01, 2.0249e-01,
        7.4619e-02, 7.6646e-02, 1.6196e-02, 1.3191e-02, 3.1871e-02, 7.2252e-02,
        2.7156e-01, 8.5251e-01, 4.9086e+00, 5.7084e-01, 1.1011e+01, 7.4790e-01,
        1.9651e+01, 6.6838e-01, 1.0597e+00, 3.8082e-01, 2.1186e-01, 1.1153e+00,
        1.0792e+00, 5.9811e+00, 9.5207e-02, 9.8432e-02, 1.7184e-02, 1.2829e+00,
        3.0891e-02, 4.6645e+00, 5.6315e+01, 1.1287e+00, 4.1745e-03, 7.7100e+00,
        1.4386e-02, 6.9819e-02], device='cuda:0')
Outer loop valEpocw Maximum [26/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 577.1
model_train val_loss valEpocw [26/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 85.3
model_train val_loss valEpocw [26/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 362.7
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.81569425 97.44398826 93.28590051 98.0494877  98.67326178 97.57312898
 98.37477614 95.20108186 98.04461447 97.0456013  98.6208745  98.97296573
 99.4420146  95.40819434 97.48419244 97.36845311 96.43522862 97.73150912
 98.90474044 98.37964937 98.66960685 99.34698651 99.57724687 99.06190227
 95.21204664 96.9067141  94.46278676 97.04803791 98.07507218 98.28949452
 98.35893812 98.57457877 97.0456013  98.60138156 98.68178994 98.85600809
 97.67546692 98.3833043  98.92910661 93.43331587 98.11405806 95.70667999
 98.480769   97.80948088 98.5087901  97.33677709 98.20664953 98.68544487
 98.14817071 98.74392369 98.76707155 98.64280406 99.02778962 98.35284658
 98.75123354 97.71079787 93.1311753  96.98346755 96.62650309 97.64135427
 96.89940425 98.76828986 97.6035867  97.41231223 98.83164191 97.67059368
 98.65255053 96.01856703 99.1228177  98.19933968 99.81603538 97.32703062
 98.85844471 96.11603172 98.85478978 99.14840219 99.67714818 99.24343027
 99.85380295 99.17398667]
Accuracy th:0.7 is [86.91414578 97.37088973 92.7961404  97.94105822 98.5368112  97.50124877
 98.28340298 94.94036379 97.92765683 96.8238691  98.57457877 98.94250801
 99.43348643 95.32291273 97.40378407 97.26733349 96.34751039 97.70957956
 98.85235316 98.33944518 98.55264921 99.27266968 99.52242297 99.03509947
 95.24615928 96.7946297  94.26420243 97.05412946 98.02755814 98.22492416
 98.16157211 98.60503649 96.72884102 98.45031128 98.61843788 98.8011842
 97.49637553 98.16279041 98.78290956 93.05807678 98.08360035 94.9428004
 98.216396   97.43302348 98.19933968 96.74467904 98.19568475 98.67935332
 98.15426225 98.69519134 98.65498715 98.5928534  99.02169808 98.18471997
 98.73417722 97.62551626 92.24180992 96.67523544 96.72153117 97.62551626
 96.02465857 98.66960685 97.29900951 97.34286863 98.79509265 97.54998112
 98.61843788 95.97714453 99.0107333  98.01537506 99.81603538 97.39647421
 98.68422656 95.93450372 98.83529684 99.11916278 99.63694399 99.12890925
 99.8464931  99.16058528]
Avg Prec: is [96.52663317 35.73911941 72.70812194 65.80184892 78.95006625 63.42979534
 74.86754638 47.3578345  57.50571782 52.52703861 33.19668888 54.83158909
 25.28199893 29.08366987 32.05768237 57.32673593 29.96685629 42.05857623
 49.30240576 37.13553829 60.87758576 52.7690009  90.36765106 83.37574598
 25.88268568 34.87732635 38.48261245 40.0592288  25.52866307 38.77506242
 73.4928902  37.7098214  57.79527965 63.18229879 71.2333485  78.83571786
 59.07615805 75.34623648 86.86981398 46.33218787 46.8693787  81.19059543
 82.32959246 77.09520073 87.23030776 87.93567782 36.82622674 35.24278688
 41.94546763 44.59454891 63.56352629 40.09215317 25.65325916 72.45524396
 27.11346905 44.03358406 74.34980451 59.47471338 47.19874124 59.79107498
 93.7401138  84.38292056 73.9204031  50.11751242 60.01295347 45.39550379
 60.7950714  26.37421156 71.56310132 66.81108813 12.52841738 71.58084532
 82.21599408 52.29807134 90.3830933  92.38621696 87.04839995 90.46173698
 32.80917499 29.04177284]
Accuracy th:0.5 is [45.64028216 97.2137279  71.39167408 97.02489005 97.26733349 76.13455002
 76.41354272 75.96642341 77.39793618 96.45959479 77.67205565 98.52097319
 99.41399349 80.44614466 77.3309292  96.56680596 96.29512311 77.010514
 98.65376884 98.30776915 80.5631023  78.16790731 98.38695922 80.7019895
 82.16152337 96.65086926 94.0778012  76.7169016  98.01293844 77.5672811
 97.30875598 98.57457877 96.36213009 98.02024829 88.28961635 77.22737296
 76.95812673 91.49864159 97.11504489 74.35216433 79.3094626  92.05906361
 76.41232441 75.98835297 96.9627563  93.87434364 98.02877645 98.57336046
 97.72054434 89.77838964 88.6721653  98.55508583 98.99976852 76.6060355
 98.70615611 76.93497886 71.83757508 93.493013   96.24273583 96.9067141
 89.79300934 97.17717864 94.36288544 77.04828158 98.42838172 77.44666853
 98.20786784 76.24785273 78.17156224 97.55972759 78.60284353 95.99054592
 77.97663284 95.45083515 76.38064838 83.51262777 89.23624225 77.63916132
 78.6674139  99.14718388]
Accuracy th:0.7 is [45.68292297 97.2137279  71.39167408 97.02489005 97.26733349 76.13455002
 76.41354272 76.27465552 77.39793618 96.4754328  77.67205565 98.52097319
 99.41399349 80.95783433 77.3309292  96.56680596 96.29512311 77.010514
 98.65376884 98.30776915 80.97001742 78.16790731 98.38695922 81.63155907
 82.98510008 96.65086926 94.0778012  76.7169016  98.01293844 77.5672811
 97.30875598 98.57457877 96.36213009 98.02024829 88.45896127 77.22737296
 76.95812673 91.75814135 97.11504489 74.35216433 79.96491271 92.05906361
 76.41232441 75.98835297 96.9627563  93.87434364 98.02877645 98.57336046
 97.96298778 90.3583046  88.84272852 98.55508583 98.99976852 76.6060355
 98.70615611 76.93619717 71.83757508 93.94135062 96.24273583 96.9067141
 89.79300934 97.17717864 94.45547691 77.04828158 98.42838172 77.44666853
 98.20786784 76.24785273 78.17156224 97.55972759 78.62233647 95.99054592
 78.11673834 95.45083515 76.38064838 83.65273328 89.49208708 77.63916132
 78.6674139  99.14718388]
Avg Prec: is [56.02188576  3.11194434 11.20362424  3.36789143  2.22138164  3.87738241
  3.33561466  5.67335877  2.55207965  3.92499827  1.56801341  1.56486823
  0.64469289  5.07410563  2.63049476  3.09734198  3.47811785  2.66257967
  1.38199507  1.73866776  2.10018345  0.88337293  1.83350348  2.41663085
  5.18292157  3.62761723  6.5073781   3.42633818  1.98868093  1.85767825
  2.61254715  1.36443531  3.82299017  1.62641876  2.39326434  2.4135005
  3.19910819  2.52179711  2.8385287   7.42222779  2.31233154  8.28195661
  3.34944685  4.06128805  3.25052951  6.36008509  2.04022524  1.49602114
  2.12740653  1.59605942  1.88938624  1.65601165  1.03283963  2.94565668
  1.39217117  2.69427024 11.18152882  3.71486878  3.81563231  2.8533211
 10.70035867  2.1103993   3.79570709  3.01709887  1.54998219  2.5310783
  1.77946042  4.08406535  1.20675024  2.4671547   0.20822429  3.31711797
  1.97921345  4.52830444  3.8794776   3.16838268  0.83653743  1.87949877
  0.14978709  0.77969878]
mAP score regular 57.32, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [87.98365598 97.38894287 93.14348357 98.31825996 98.94610957 97.65802128
 98.5250517  95.40075242 98.17375489 97.11239006 98.6894885  99.05075118
 99.36716745 95.21389242 97.39890874 97.29177567 96.46211725 97.73276528
 98.95856691 98.41044423 98.93863517 99.37713332 99.65368613 99.28245758
 95.21638389 96.85078606 94.52375614 97.26436953 97.92460822 98.17873782
 98.71689464 98.54249197 97.080001   98.78167277 98.82901064 99.11054638
 98.04170715 98.25597329 99.05324264 93.34030944 97.98191195 93.12355183
 97.38146847 96.48454045 96.96290206 94.77041134 98.32573436 98.75426664
 98.00931809 98.74679224 98.85392531 98.63965917 98.89628024 98.48518823
 98.73931784 97.71034208 90.98338192 97.17965967 96.19303884 97.55088821
 92.82706729 98.80409597 97.29925007 97.51600767 98.73931784 97.47863567
 98.60228717 95.8816055  98.84146797 98.14884022 99.81563146 97.3715026
 98.4204101  95.74457483 97.21204873 97.23198047 99.25255998 98.60976157
 99.82310586 99.13047811]
Accuracy th:0.7 is [87.93133518 97.43877221 92.96658943 98.29832823 98.90873757 97.61566634
 98.5101029  95.19396068 98.07658769 96.95542766 98.63467623 99.05324264
 99.37962479 95.13167402 97.43378927 97.30174154 96.29269751 97.76266288
 98.94610957 98.37805516 98.84395944 99.29989785 99.61880559 99.27249172
 95.45556469 96.73119565 94.4814012  97.2220146  97.84986422 98.23604156
 98.54249197 98.6595909  96.78102499 98.6745397  98.87883997 99.12549518
 97.86730448 98.10150235 98.88631437 93.15095797 98.02675835 93.2306849
 97.47116127 96.63651992 97.0575778  94.73553081 98.38303809 98.81157037
 98.17624636 98.76174104 98.74679224 98.5997957  98.89877171 98.37556369
 98.81157037 97.74273115 91.00082218 97.06006926 96.41477938 97.63310661
 92.67508782 98.8016045  97.14477913 97.39890874 98.6894885  97.62314074
 98.5624237  95.8367591  98.81406184 97.97194608 99.81563146 97.64307248
 98.37058076 95.9638239  97.4238234  97.39143434 99.26003438 98.56740663
 99.82310586 99.16535865]
Avg Prec: is [96.54123947 34.38807227 70.62540496 73.87120431 77.21443903 64.32776028
 80.48845191 48.05294782 62.68498468 56.16435298 40.52971639 58.00293028
 22.59431461 31.56500337 33.10582079 62.00992253 33.80134072 44.77077053
 49.37063052 37.21084071 70.55073335 58.1830517  92.36733238 88.24908108
 24.95000301 38.47989238 33.75718394 46.07978432 27.5114279  37.72334097
 77.56585431 36.89006232 55.12190693 66.87857105 73.07360328 81.75657408
 61.03795636 77.46793064 89.71736182 44.37033838 38.08375951 48.88075951
 50.95836822 39.69203371 29.83781358 49.39154986 38.31632453 28.5337038
 42.06795932 40.53511396 70.03423195 37.05885166 25.31685346 76.3624802
 31.19089311 40.20482383 56.41853913 58.1070029  39.05169869 61.74703556
 63.65528827 86.70392493 66.51306535 52.44663603 58.78638706 39.42832436
 60.29694071 27.26944128 43.05193643 64.53332361 11.53658837 74.86415844
 56.52040376 45.14191729 64.28810315 50.75404698 12.63167418 50.42113858
  2.18015987 23.29083891]
Accuracy th:0.5 is [45.28988215 97.22450607 69.43966913 96.96290206 97.90716795 74.77639086
 74.88850686 74.01898498 76.37092957 96.41976231 76.51543464 98.5325261
 99.34972718 77.8209632  76.46311384 96.31262924 96.21047911 75.87512769
 98.78167277 98.34068316 78.65311309 77.15574159 98.31327703 81.9244089
 78.09004161 96.52938685 94.3393876  75.97229489 97.81747515 76.46809677
 97.52597354 98.67204823 96.39983058 98.18870369 89.46607868 76.09935969
 76.09686823 92.70249396 97.0276802  73.33134016 77.18314772 92.37362035
 75.21738047 74.89598126 97.03764606 94.02795426 98.18621222 98.77668984
 97.93955702 89.64048135 87.13904876 98.55993223 98.87385704 75.2074146
 98.6969629  75.91000822 70.15721155 94.42409747 96.16314124 96.78102499
 90.13379176 97.04761193 94.46894387 75.96482049 98.32075143 76.81690211
 98.13139996 75.15260234 77.19809652 97.53593941 77.57679946 96.07843137
 76.99130478 95.44559882 75.09031567 84.29379376 91.23751152 76.5403493
 77.6565264  99.15040985]
Accuracy th:0.7 is [45.45431896 97.22450607 69.43966913 96.96290206 97.90716795 74.77639086
 74.88850686 74.24570845 76.37092957 96.41976231 76.51543464 98.5325261
 99.34972718 78.25946134 76.46311384 96.31262924 96.21047911 75.87512769
 98.78167277 98.34068316 78.97202083 77.15574159 98.31327703 82.55474998
 78.67304482 96.52938685 94.3393876  75.97229489 97.81747515 76.46809677
 97.52597354 98.67204823 96.39983058 98.18870369 89.63798988 76.09935969
 76.09686823 92.88437103 97.0276802  73.33134016 77.68393253 92.37362035
 75.21738047 74.89598126 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 90.00672696 87.3009941  98.55993223 98.87385704 75.2074146
 98.6969629  75.91000822 70.15721155 94.78286867 96.16314124 96.78102499
 90.13379176 97.04761193 94.5860428  75.96482049 98.32075143 76.81690211
 98.13139996 75.15260234 77.19809652 97.53593941 77.57679946 96.07843137
 77.04611705 95.44559882 75.09031567 84.46321349 91.45676059 76.5403493
 77.6565264  99.15040985]
Avg Prec: is [54.28397166  3.74813186 14.82137793  4.56682719  1.55718499  4.27755227
 11.73960976  8.71882666  7.45667817  5.14791505  2.27966866  5.19889723
  1.55874477  5.87232898  3.05461305  3.87990652 25.56189775  6.36689839
  1.53565826  2.6230147   3.59397728  1.40635803  1.27364461  5.83101791
  5.73146449 11.31923935  8.17212763  4.53946585  3.90404297  6.98708107
  2.33558758  0.86632211  2.97006224  1.11610482  1.74768698  2.45458491
  2.03525197  2.22654222  2.11612927  6.22611137  1.73192679  6.03329388
  2.19834873  2.72997805  2.35100727  4.89524584  1.78991908  1.0603489
  1.3928276   1.18524603  1.19980773  1.00980985  0.78667779  2.30167831
  0.91350276  1.85204065 10.18467794  3.07169384  3.97049029  2.85535071
  7.94272943  2.03904543  3.38507494  2.56119336  1.37325129  1.89700073
  1.57123725  3.52407012  1.06139086  2.20377077  0.19762812  3.17735961
  1.53503991  4.04858748  3.21760038  2.26384839  0.55910662  1.50795252
  0.12103994  0.59956292]
mAP score regular 51.44, mAP score EMA 4.37
Train_data_mAP: current_mAP = 57.32, highest_mAP = 57.36
Val_data_mAP: current_mAP = 51.44, highest_mAP = 51.91
tensor([2.6852e-02, 2.6379e-03, 3.7466e-02, 2.9588e-03, 1.2178e-03, 3.0924e-03,
        5.7983e-04, 1.6555e-03, 5.5463e-03, 2.0804e-03, 5.4561e-04, 1.2170e-03,
        3.4454e-04, 4.1865e-03, 5.3997e-03, 5.8426e-03, 7.5230e-03, 6.1157e-03,
        5.2908e-03, 5.9058e-03, 1.0961e-04, 6.6680e-04, 1.5285e-03, 9.8820e-04,
        4.0801e-03, 9.1546e-04, 1.7399e-02, 8.8280e-04, 5.9054e-04, 1.5347e-03,
        1.5648e-03, 6.1379e-04, 1.7288e-02, 1.7666e-04, 1.5022e-03, 9.5423e-03,
        9.5438e-04, 2.4656e-03, 5.9151e-03, 1.6596e-02, 1.5956e-01, 4.8160e-01,
        8.1844e-01, 7.2125e-01, 9.7338e-01, 8.7974e-01, 8.0285e-04, 1.0442e-03,
        2.0030e-03, 1.6657e-03, 3.1896e-04, 1.0799e-03, 1.6451e-03, 2.6410e-03,
        7.7840e-04, 4.0665e-03, 1.3549e-01, 9.0160e-03, 9.4830e-02, 1.8363e-03,
        8.6722e-01, 3.5524e-03, 1.3275e-01, 1.3567e-02, 2.3590e-02, 5.0415e-03,
        3.5676e-02, 9.1454e-03, 5.3958e-01, 3.8004e-02, 1.1496e-03, 2.6632e-03,
        3.8197e-01, 7.4771e-02, 9.5443e-01, 9.9995e-01, 1.0000e+00, 9.9995e-01,
        6.3127e-01, 5.3687e-02], device='cuda:0')
Sum Train Loss:  tensor([8.3495e-01, 1.5018e-02, 7.2047e-01, 1.6746e-02, 4.0399e-03, 2.8717e-02,
        4.3350e-03, 1.4738e-02, 1.4941e-02, 1.3275e-02, 6.4631e-03, 1.0497e-03,
        2.9149e-04, 7.2469e-02, 7.9105e-02, 5.8416e-02, 1.1518e-01, 1.0051e-01,
        8.6898e-03, 1.1764e-01, 3.0766e-04, 1.5750e-03, 2.0174e-03, 3.8899e-03,
        7.3401e-02, 2.6390e-02, 5.0246e-01, 9.2175e-03, 6.7277e-03, 1.1494e-02,
        6.0554e-03, 1.2929e-03, 1.8102e-01, 3.5710e-04, 4.0461e-03, 4.2287e-02,
        1.6873e-02, 1.3782e-02, 3.8643e-02, 3.4887e-01, 1.2780e+00, 3.7528e+00,
        2.8613e+00, 4.7369e+00, 7.2100e+00, 4.9379e+00, 9.6239e-03, 3.9510e-03,
        5.8485e-03, 1.1545e-02, 5.7467e-04, 2.1854e-03, 5.2115e-03, 1.0373e-02,
        2.0734e-03, 2.0277e-02, 2.4858e+00, 8.5186e-02, 1.4672e+00, 1.5710e-02,
        6.8278e+00, 3.9581e-02, 5.6378e-01, 1.9007e-01, 1.1187e-01, 4.9788e-02,
        7.9120e-02, 2.0752e-01, 1.6080e+00, 1.3901e-01, 2.7381e-04, 1.3459e-02,
        6.3342e-01, 5.0043e-01, 2.2938e+00, 7.2077e-01, 4.1007e-01, 5.6750e-01,
        6.7129e-02, 2.7683e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [27/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 47.7
Sum Train Loss:  tensor([8.9713e-01, 4.7660e-02, 9.9602e-01, 2.8369e-02, 8.9773e-03, 4.8365e-02,
        9.3381e-03, 3.4362e-02, 1.0566e-01, 1.2236e-02, 4.0766e-03, 5.8933e-03,
        9.7962e-04, 1.0851e-01, 1.0764e-01, 7.0194e-02, 1.0695e-01, 1.4419e-01,
        6.1035e-02, 4.9103e-02, 2.5729e-03, 8.4891e-04, 5.6380e-03, 1.0164e-02,
        7.4401e-02, 1.6280e-02, 3.4145e-01, 1.2003e-02, 1.5513e-03, 1.7075e-02,
        3.3008e-03, 2.4440e-03, 1.4979e-01, 2.0613e-03, 5.8088e-03, 2.9179e-02,
        7.9887e-03, 2.5659e-02, 3.4113e-02, 3.5242e-01, 8.0147e-01, 4.0573e+00,
        4.0957e+00, 7.5448e+00, 1.7198e+00, 1.1820e+01, 7.0333e-03, 6.3463e-03,
        7.9195e-03, 1.9485e-03, 1.4915e-03, 1.0243e-02, 4.0871e-03, 1.1835e-02,
        1.0469e-03, 4.3816e-02, 2.2715e+00, 3.9534e-02, 1.1368e+00, 7.1449e-03,
        1.1232e+01, 8.5822e-03, 6.0406e-01, 8.6797e-02, 5.6884e-02, 3.6276e-02,
        1.9379e-01, 1.3514e-01, 2.6424e-01, 3.1437e-01, 1.9648e-04, 3.1305e-02,
        1.3410e+00, 1.5030e+00, 8.0649e+00, 1.2015e+00, 5.3819e+00, 1.9128e+00,
        2.5246e-01, 6.2176e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [27/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 70.2
Sum Train Loss:  tensor([9.9107e-01, 3.2652e-02, 1.0059e+00, 7.7298e-03, 8.2600e-03, 1.1396e-02,
        1.4274e-03, 3.6001e-02, 3.4281e-02, 1.1945e-02, 9.4030e-04, 3.7219e-03,
        1.2162e-03, 9.5827e-02, 3.9668e-02, 4.8946e-02, 8.8447e-02, 1.0170e-01,
        1.0227e-02, 4.9702e-02, 4.2186e-04, 4.1995e-04, 2.5207e-04, 6.6551e-04,
        9.1591e-02, 1.3386e-02, 2.6145e-01, 1.1584e-02, 4.8269e-03, 7.2839e-03,
        8.0040e-03, 1.2850e-03, 2.3899e-01, 2.2203e-03, 1.1131e-02, 8.0389e-02,
        1.2516e-02, 2.2637e-02, 1.1832e-02, 4.1084e-01, 6.9739e-01, 7.6480e+00,
        1.1625e+01, 7.6518e+00, 6.3911e+00, 6.8475e+00, 6.0735e-03, 2.9968e-03,
        3.5232e-02, 7.5142e-03, 2.5016e-03, 6.8057e-03, 1.4511e-02, 2.4923e-02,
        6.3023e-03, 4.5315e-02, 3.4117e+00, 6.7343e-02, 1.1572e+00, 6.4505e-03,
        9.6069e+00, 4.2793e-03, 1.6968e+00, 3.7757e-02, 1.3785e-01, 3.4989e-02,
        1.4662e-01, 2.3554e-01, 8.5526e-01, 1.3116e-01, 2.7210e-04, 7.6767e-03,
        8.4461e-01, 8.5619e-01, 6.5443e+00, 2.2930e+00, 7.3872e-01, 1.4254e+00,
        2.5021e-01, 5.2400e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [27/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 75.8
Sum Train Loss:  tensor([1.1407e+00, 6.3069e-03, 7.1144e-01, 2.8421e-02, 4.7054e-03, 2.6375e-02,
        3.9785e-03, 3.0305e-02, 3.3984e-02, 2.5696e-02, 3.9296e-03, 1.9049e-02,
        2.5303e-04, 7.8274e-02, 9.0865e-02, 3.5972e-02, 5.4359e-02, 8.2690e-02,
        3.5949e-02, 6.6101e-02, 6.0011e-04, 8.7100e-04, 5.5555e-03, 2.8207e-03,
        7.3649e-02, 2.0197e-02, 3.9018e-01, 1.2814e-02, 5.6212e-03, 1.4286e-02,
        1.1310e-02, 3.8845e-03, 5.1527e-02, 1.1624e-03, 1.0694e-02, 6.0020e-02,
        3.7672e-03, 2.8968e-02, 8.2672e-03, 3.1677e-01, 5.4890e-01, 6.4851e+00,
        4.3269e+00, 5.1825e+00, 4.8087e+00, 7.8597e+00, 1.6658e-03, 1.9867e-03,
        8.0834e-03, 8.4633e-03, 1.5157e-03, 1.5609e-02, 6.1544e-03, 7.4907e-03,
        2.0169e-03, 5.1280e-02, 3.6912e+00, 4.5825e-02, 5.2788e-01, 7.8520e-03,
        1.1155e+01, 1.4135e-02, 2.6704e-01, 7.8501e-02, 8.0694e-02, 4.1975e-02,
        1.7534e-01, 1.2031e-01, 1.5575e+00, 3.6183e-01, 4.6028e-03, 3.5106e-02,
        8.4600e-01, 9.5505e-01, 1.3389e+01, 5.9657e+00, 9.1731e+00, 2.8305e-01,
        4.6438e-01, 9.0570e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [27/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 82.2
Sum Train Loss:  tensor([8.8166e-01, 2.7449e-02, 6.9092e-01, 4.1632e-02, 2.6267e-03, 1.9562e-02,
        1.1491e-03, 1.6578e-02, 4.5615e-02, 1.6508e-02, 8.8277e-03, 2.4594e-03,
        8.8138e-04, 4.6981e-02, 8.1629e-02, 7.0398e-02, 1.1406e-01, 3.4833e-02,
        2.1240e-02, 3.9837e-02, 1.4968e-03, 4.1794e-03, 7.9023e-04, 1.3149e-02,
        5.7683e-02, 1.8601e-02, 2.2693e-01, 4.4347e-03, 4.7585e-03, 1.9992e-02,
        7.5949e-03, 2.0137e-03, 1.5308e-01, 2.7124e-04, 1.0154e-02, 4.0243e-02,
        1.8146e-02, 2.3409e-02, 1.5904e-02, 4.1026e-01, 1.2319e+00, 3.7753e+00,
        3.9194e+00, 7.1708e+00, 1.4255e+00, 4.0231e+00, 4.5687e-03, 4.1940e-03,
        1.4492e-02, 1.3858e-02, 8.8349e-04, 4.7268e-03, 1.9974e-03, 7.8796e-03,
        4.8502e-03, 3.2074e-02, 1.9816e+00, 6.7666e-02, 9.3576e-01, 1.3657e-02,
        5.2436e+00, 8.3521e-03, 8.3925e-01, 2.2659e-01, 2.3182e-01, 5.8663e-02,
        3.0677e-01, 7.0148e-02, 5.1946e+00, 1.2824e-01, 2.8542e-04, 2.1295e-02,
        3.5576e+00, 1.1018e+00, 8.9134e+00, 2.7559e+00, 2.3853e-01, 4.2946e+00,
        1.3150e-01, 2.5756e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [27/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 61.4
Sum Train Loss:  tensor([7.9936e-01, 5.1759e-03, 5.9788e-01, 2.2371e-02, 1.5177e-02, 1.3085e-02,
        6.1076e-03, 1.5214e-02, 3.4377e-02, 2.1299e-02, 3.1317e-03, 1.6436e-03,
        3.1041e-03, 1.4636e-01, 2.3858e-02, 3.3083e-02, 2.0094e-01, 3.4623e-02,
        5.2161e-02, 5.9166e-02, 7.0633e-04, 1.6824e-03, 2.5572e-03, 2.9361e-03,
        5.0629e-02, 1.6142e-02, 4.5173e-01, 9.2040e-03, 8.6859e-03, 1.0141e-02,
        3.3929e-03, 9.9731e-04, 1.7372e-01, 9.5825e-04, 1.6860e-02, 4.5221e-02,
        1.6714e-02, 1.1287e-02, 7.0736e-03, 4.3358e-01, 1.3554e+00, 5.9129e+00,
        4.2558e+00, 9.9251e+00, 7.3549e+00, 5.1637e+00, 3.3922e-03, 4.8772e-03,
        1.6694e-02, 4.2347e-03, 5.0427e-04, 2.9645e-03, 1.3846e-03, 1.9356e-02,
        3.4310e-03, 1.4196e-02, 3.4766e+00, 1.1156e-01, 9.9546e-01, 1.4396e-02,
        1.4186e+01, 2.0830e-02, 5.8452e-01, 1.2877e-01, 3.5764e-02, 5.2768e-02,
        9.5658e-02, 2.2985e-01, 6.5405e-01, 2.3233e-01, 1.0439e-04, 2.4211e-02,
        3.2232e-01, 1.1305e+00, 1.9612e+00, 2.4060e+00, 2.2729e-01, 3.8730e+00,
        4.4307e+00, 3.2150e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [27/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 72.9
Sum Train Loss:  tensor([1.0912e+00, 5.1112e-02, 9.6661e-01, 2.3827e-02, 1.8451e-03, 2.9120e-02,
        4.8670e-03, 2.2489e-02, 2.4070e-02, 3.1468e-02, 4.1489e-03, 9.3375e-03,
        1.6147e-03, 5.4513e-02, 1.8618e-02, 3.2042e-02, 1.0255e-01, 4.1595e-02,
        5.2143e-02, 5.5014e-02, 4.7100e-04, 3.3726e-03, 3.4768e-03, 1.3545e-03,
        1.0026e-01, 8.1132e-03, 2.8576e-01, 6.9413e-03, 1.0316e-02, 8.2477e-03,
        4.2125e-03, 2.1015e-03, 1.7880e-01, 1.1928e-03, 2.5434e-02, 6.0780e-02,
        3.9414e-03, 7.0151e-03, 6.0468e-02, 4.4425e-01, 1.3621e+00, 5.7071e+00,
        1.0628e+01, 7.3265e+00, 5.2251e+00, 9.6960e+00, 4.0574e-03, 4.6159e-03,
        6.0683e-03, 1.3940e-02, 1.4169e-03, 8.2691e-03, 1.9859e-03, 2.2204e-02,
        5.0126e-03, 3.9428e-02, 2.5174e+00, 7.3704e-02, 1.6776e+00, 3.2460e-02,
        8.4627e+00, 2.2381e-02, 8.0736e-01, 1.9057e-01, 1.6381e-01, 5.0945e-02,
        3.1449e-01, 2.4746e-01, 7.1508e-01, 1.8104e-01, 8.4135e-03, 1.7754e-02,
        5.8110e-01, 1.4509e+00, 1.1209e+01, 6.6216e-01, 5.4431e+00, 4.9989e+00,
        2.0560e-01, 1.6317e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [27/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 84.1
Sum_Val Meta Model:  tensor([1.4098e+00, 1.7283e-01, 3.3782e+00, 1.6176e-01, 2.3012e-03, 3.8572e-02,
        2.3371e-03, 2.8390e-02, 3.4327e-02, 2.1712e-02, 3.2387e-03, 1.4963e-02,
        2.9981e-03, 6.0497e-02, 4.0881e-02, 4.5548e-02, 4.0913e+00, 1.8529e-02,
        5.6593e-03, 6.7984e-03, 1.0722e-04, 3.1180e-04, 2.2971e-04, 3.7055e-04,
        1.1983e-01, 1.8157e-02, 4.9735e-01, 3.1478e-03, 6.0344e-03, 2.1651e-02,
        2.7727e-03, 5.2619e-04, 1.3615e-01, 1.7453e-04, 9.4073e-04, 8.7435e-03,
        3.2719e-03, 1.5101e-02, 1.2101e-02, 7.6053e-01, 1.8725e+00, 1.2524e+01,
        5.5230e+00, 1.2741e+01, 1.5105e+01, 1.1335e+01, 9.8870e-04, 5.3227e-03,
        1.5554e-03, 7.9802e-03, 1.3687e-04, 6.7459e-04, 1.0828e-02, 4.9975e-03,
        8.7867e-04, 6.1534e-03, 4.1984e+00, 5.9635e-02, 1.3046e+00, 3.3612e-03,
        7.4577e+00, 4.3792e-02, 8.8131e-01, 5.7548e-02, 2.1048e-02, 7.0342e-03,
        2.2833e-02, 7.4173e-02, 5.4815e+00, 6.1167e-01, 1.1760e-04, 5.1920e-02,
        7.7800e+00, 7.1042e-01, 2.2487e+01, 8.1379e+00, 1.9621e-01, 9.5785e+00,
        5.8969e-02, 1.8227e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.3903e+00, 1.1138e-01, 2.2006e+00, 8.0288e-02, 4.4269e-04, 3.8666e-02,
        1.6201e-03, 2.7228e-02, 2.9850e-02, 1.9089e-02, 3.4797e-03, 1.6351e-02,
        3.5611e-03, 6.5672e-02, 4.4724e-02, 1.7382e-01, 2.6963e+00, 3.2213e-02,
        6.4737e-03, 1.0399e-02, 1.0627e-04, 2.4638e-04, 7.7671e-05, 6.4799e-04,
        1.0434e-01, 1.7054e-02, 5.6834e-01, 4.9610e-03, 1.1002e-02, 2.4580e-02,
        1.8072e-03, 6.2478e-04, 1.3742e-01, 2.3705e-04, 7.9079e-04, 6.3485e-03,
        6.7103e-03, 1.1288e-02, 1.1174e-02, 7.2171e-01, 2.0225e+00, 1.5927e+01,
        6.0888e+00, 1.3379e+01, 1.7631e+01, 1.0513e+01, 5.8453e-04, 6.2119e-03,
        4.0363e-04, 6.2260e-03, 1.3899e-05, 1.8575e-04, 1.1348e-02, 8.7310e-04,
        4.4261e-04, 2.1597e-03, 3.7977e+00, 5.6664e-02, 1.2541e+00, 3.3532e-03,
        1.0019e+01, 2.7563e-02, 5.4041e-01, 7.9916e-02, 1.7978e-02, 9.7827e-03,
        1.7864e-02, 9.4250e-02, 6.6118e+00, 7.4189e-01, 8.3938e-05, 6.0652e-02,
        7.4941e+00, 7.0136e-01, 3.4477e+01, 1.1450e+01, 2.2257e-02, 8.6633e+00,
        1.1019e-01, 2.6013e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.1776e+01, 4.2221e+01, 5.8735e+01, 2.7135e+01, 3.6350e-01, 1.2504e+01,
        2.7941e+00, 1.6447e+01, 5.3820e+00, 9.1754e+00, 6.3776e+00, 1.3435e+01,
        1.0336e+01, 1.5687e+01, 8.2826e+00, 2.9750e+01, 3.5841e+02, 5.2673e+00,
        1.2236e+00, 1.7608e+00, 9.6950e-01, 3.6949e-01, 5.0816e-02, 6.5573e-01,
        2.5573e+01, 1.8629e+01, 3.2666e+01, 5.6197e+00, 1.8630e+01, 1.6015e+01,
        1.1549e+00, 1.0179e+00, 7.9488e+00, 1.3418e+00, 5.2644e-01, 6.6530e-01,
        7.0310e+00, 4.5780e+00, 1.8892e+00, 4.3486e+01, 1.2675e+01, 3.3071e+01,
        7.4395e+00, 1.8550e+01, 1.8113e+01, 1.1950e+01, 7.2806e-01, 5.9489e+00,
        2.0151e-01, 3.7378e+00, 4.3575e-02, 1.7202e-01, 6.8980e+00, 3.3059e-01,
        5.6862e-01, 5.3110e-01, 2.8030e+01, 6.2848e+00, 1.3225e+01, 1.8261e+00,
        1.1553e+01, 7.7590e+00, 4.0709e+00, 5.8905e+00, 7.6208e-01, 1.9404e+00,
        5.0074e-01, 1.0306e+01, 1.2254e+01, 1.9521e+01, 7.3014e-02, 2.2774e+01,
        1.9619e+01, 9.3801e+00, 3.6124e+01, 1.1450e+01, 2.2257e-02, 8.6637e+00,
        1.7455e-01, 4.8454e-01], device='cuda:0')
Outer loop valEpocw Maximum [27/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 139.5
model_train val_loss valEpocw [27/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 160.4
model_train val_loss valEpocw [27/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1219.5
Sum_Val Meta Model:  tensor([4.5652e+00, 1.5018e-01, 4.1856e+00, 8.9348e-02, 4.0733e-03, 4.5430e-01,
        2.5980e-01, 4.2640e-01, 4.8906e-01, 3.3058e-01, 6.2624e-02, 1.2806e+00,
        2.0927e-02, 7.3824e-02, 4.5207e-01, 4.2161e-01, 2.1944e-01, 3.8115e-01,
        1.9971e-01, 1.2409e+00, 5.6425e-05, 1.5720e-04, 1.8243e-03, 2.3550e-02,
        4.1127e-01, 1.1213e-01, 1.2356e+00, 7.7433e-02, 4.2418e-02, 3.9135e-04,
        8.3823e-04, 2.5794e-04, 5.4204e-03, 3.2667e-04, 2.6620e-04, 1.4977e-03,
        2.9321e-02, 9.9876e-04, 8.7282e-04, 6.1373e-02, 3.3016e-02, 2.1398e+00,
        9.6996e-02, 2.1926e-01, 3.7596e-01, 3.7546e+00, 4.1221e-03, 4.3543e-03,
        7.2300e-04, 4.7634e-02, 1.2730e-04, 7.9120e-04, 1.8013e-04, 3.2352e-04,
        6.4301e-04, 4.1842e-02, 2.5274e+00, 1.2669e-01, 1.0078e+00, 3.3011e-02,
        1.5174e+00, 1.0323e-02, 4.0097e-01, 6.8981e-02, 2.8605e-02, 1.4078e-02,
        4.7295e-02, 4.4099e-01, 8.3225e-02, 3.1585e-01, 1.4556e-04, 1.3212e-02,
        2.1066e+00, 6.2512e-01, 4.5072e+00, 4.5055e-01, 1.6256e-01, 5.1274e-01,
        3.3240e-02, 1.1509e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([4.3078e+00, 1.9499e-01, 2.8271e+00, 1.0350e-01, 1.5477e-02, 4.2705e-01,
        1.9889e-01, 3.5716e-01, 4.3920e-01, 2.1915e-01, 4.6882e-02, 4.6929e-01,
        1.9936e-02, 9.6088e-02, 3.1667e-01, 7.6424e-02, 1.9160e-01, 3.4831e-01,
        5.2881e-02, 4.0451e-01, 7.3516e-04, 8.6495e-04, 3.7120e-04, 1.9194e-02,
        2.6030e-01, 9.4329e-02, 1.0123e+00, 7.2051e-02, 4.7551e-02, 2.8875e-03,
        1.4063e-03, 8.8709e-04, 4.4260e-02, 6.1400e-03, 1.4381e-03, 3.7650e-03,
        1.1598e-02, 1.1042e-02, 3.6509e-03, 6.3057e-02, 2.6594e-02, 2.5242e+00,
        1.0020e-02, 2.1034e-01, 1.0165e-02, 8.1800e-01, 1.5377e-03, 1.4475e-03,
        1.0677e-03, 4.8323e-02, 8.9695e-05, 5.7908e-04, 1.3481e-03, 6.1330e-03,
        1.3338e-03, 1.4625e-02, 1.9448e+00, 9.3638e-02, 7.2021e-01, 1.7476e-03,
        1.8149e+00, 2.8486e-03, 2.0918e-01, 9.9985e-03, 9.0493e-03, 4.1715e-03,
        8.7626e-03, 4.3601e-01, 1.0442e-01, 2.1173e-01, 2.5176e-05, 2.1531e-03,
        2.5911e+00, 2.8498e-01, 1.1687e+01, 5.5893e-02, 1.4455e-02, 1.6093e-01,
        3.5190e-03, 7.8648e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([7.5625e+01, 2.2658e+01, 4.3370e+01, 9.7337e+00, 2.7400e+00, 3.5062e+01,
        4.9891e+01, 4.7454e+01, 2.5512e+01, 2.6324e+01, 1.5481e+01, 7.9542e+01,
        8.9025e+00, 8.2383e+00, 1.6615e+01, 3.9787e+00, 9.2203e+00, 1.6420e+01,
        2.5422e+00, 1.9457e+01, 6.6835e-01, 2.2946e-01, 5.3390e-02, 4.1625e+00,
        2.4108e+01, 1.8567e+01, 2.8701e+01, 1.3784e+01, 1.2759e+01, 4.3139e-01,
        2.0251e-01, 2.4342e-01, 1.0455e+00, 4.4483e+00, 2.2623e-01, 1.7114e-01,
        2.1941e+00, 1.2072e+00, 1.8436e-01, 1.9156e+00, 1.6097e-01, 5.4232e+00,
        1.4541e-02, 3.5532e-01, 1.0963e-02, 1.0260e+00, 3.5403e-01, 2.7333e-01,
        1.1365e-01, 6.1090e+00, 4.1945e-02, 1.2875e-01, 2.0495e-01, 5.8773e-01,
        3.2144e-01, 1.2005e+00, 9.4358e+00, 3.3945e+00, 6.1563e+00, 2.0275e-01,
        2.2812e+00, 2.2275e-01, 1.2296e+00, 2.8800e-01, 1.9604e-01, 2.3228e-01,
        1.4137e-01, 1.8826e+01, 2.2679e-01, 3.2784e+00, 4.9089e-03, 2.1041e-01,
        7.6531e+00, 2.2444e+00, 1.3273e+01, 5.5925e-02, 1.4456e-02, 1.6102e-01,
        6.7953e-03, 8.9555e-02], device='cuda:0')
Outer loop valEpocw Maximum [27/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 39.1
model_train val_loss valEpocw [27/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 36.8
model_train val_loss valEpocw [27/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 720.2
Sum_Val Meta Model:  tensor([2.8977e+00, 1.1972e-02, 4.8806e-01, 1.1784e-02, 2.4454e-03, 1.3447e-02,
        2.0742e-03, 3.9170e-02, 1.8044e-02, 6.8085e-03, 1.4513e-03, 3.7769e-03,
        3.2989e-04, 1.1932e-01, 2.9158e-02, 3.3420e-02, 8.3760e-02, 3.9232e-02,
        1.1645e-02, 1.8511e-02, 3.8017e-04, 2.0448e-03, 2.3440e-02, 3.4526e-03,
        4.8122e-02, 3.0109e-03, 4.8823e-01, 1.4449e-02, 1.4029e-03, 1.1578e-02,
        1.4716e-02, 1.0089e-02, 2.2969e-01, 1.1123e-04, 5.6123e-03, 4.3483e-02,
        4.2015e-02, 6.4107e-02, 3.7356e-03, 6.1162e-01, 5.5286e+00, 3.1782e+01,
        5.5125e+01, 5.7499e+01, 4.9663e+01, 7.3989e+01, 3.0507e-02, 3.2456e-02,
        2.5468e-01, 8.3875e-02, 2.8201e-02, 8.9768e-02, 2.1300e-01, 7.9986e-02,
        1.4751e-01, 8.1273e-01, 2.8918e+01, 1.7850e-01, 8.2698e-01, 9.0479e-03,
        9.8219e+01, 1.0718e-02, 1.5285e+00, 2.6047e-01, 4.1719e-01, 9.0012e-03,
        4.1118e-01, 1.3941e-01, 2.3996e+00, 8.7025e-02, 5.0013e-04, 3.7549e-02,
        1.4743e+00, 3.5511e+00, 1.0820e+00, 3.0426e+01, 1.0822e+01, 9.0331e-01,
        8.3451e-02, 3.4869e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([7.4302e-01, 2.1385e-03, 1.8153e-01, 4.9423e-04, 8.3009e-05, 4.1741e-04,
        1.4841e-04, 5.1551e-02, 1.5018e-03, 1.9510e-04, 7.2782e-05, 5.7336e-05,
        1.1776e-05, 9.4801e-02, 2.0452e-03, 9.5397e-03, 1.2374e-02, 2.3246e-03,
        9.0838e-04, 3.2118e-04, 1.4332e-05, 3.3068e-05, 1.5388e-05, 4.1682e-05,
        2.3415e-02, 1.7487e-03, 4.7427e-01, 1.0883e-02, 1.1473e-03, 3.2784e-04,
        1.4321e-03, 1.1395e-02, 2.1330e-01, 4.4425e-05, 2.4536e-03, 1.6983e-02,
        2.8067e-02, 5.1273e-02, 1.0017e-03, 9.1310e-01, 4.0009e+00, 4.3882e+01,
        5.9510e+01, 5.8705e+01, 7.1761e+01, 6.9398e+01, 1.5335e-02, 1.3676e-02,
        1.6852e-01, 3.7767e-02, 1.6940e-02, 9.9885e-02, 2.0834e-01, 1.2844e-01,
        9.0961e-02, 5.4952e-01, 1.6458e+01, 2.0094e-01, 9.3791e-01, 1.7932e-03,
        9.0781e+01, 5.4718e-04, 1.2915e+00, 1.9558e-01, 3.1954e-01, 3.6652e-02,
        4.1011e-01, 1.9186e-01, 2.7125e+00, 8.7209e-02, 7.6622e-05, 3.0799e-02,
        9.6962e-01, 3.1106e+00, 3.0431e+00, 3.7273e+01, 1.5292e+01, 4.9850e+00,
        4.2699e-02, 1.3330e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.0252e+01, 5.7066e-01, 4.1540e+00, 1.1174e-01, 3.8366e-02, 8.0999e-02,
        1.3290e-01, 1.8603e+01, 1.8459e-01, 5.7016e-02, 7.2218e-02, 2.6488e-02,
        1.8341e-02, 1.6101e+01, 2.4892e-01, 1.1313e+00, 1.1093e+00, 2.1346e-01,
        1.1053e-01, 3.7551e-02, 6.1866e-02, 2.6346e-02, 5.1229e-03, 2.4480e-02,
        4.1501e+00, 1.0752e+00, 2.6026e+01, 6.7976e+00, 1.0553e+00, 1.1349e-01,
        4.7670e-01, 8.3166e+00, 8.2808e+00, 1.1195e-01, 9.2687e-01, 1.2875e+00,
        1.3306e+01, 1.1761e+01, 1.0790e-01, 4.7046e+01, 2.6591e+01, 8.8975e+01,
        7.5495e+01, 8.5231e+01, 7.4616e+01, 8.0641e+01, 9.3646e+00, 6.5467e+00,
        3.8444e+01, 1.0898e+01, 2.2883e+01, 4.8203e+01, 6.6272e+01, 2.7808e+01,
        5.8361e+01, 8.3364e+01, 1.0617e+02, 1.4957e+01, 1.0384e+01, 5.2173e-01,
        1.0671e+02, 1.0107e-01, 9.5379e+00, 1.0639e+01, 1.1656e+01, 4.4291e+00,
        1.0556e+01, 1.6283e+01, 6.0515e+00, 2.2468e+00, 4.0831e-02, 7.3813e+00,
        3.1021e+00, 3.6146e+01, 3.3058e+00, 3.7278e+01, 1.5292e+01, 4.9857e+00,
        8.2663e-02, 2.2165e-01], device='cuda:0')
Outer loop valEpocw Maximum [27/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 462.6
model_train val_loss valEpocw [27/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 489.8
model_train val_loss valEpocw [27/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1416.0
Sum_Val Meta Model:  tensor([9.4829e+00, 1.4494e-02, 1.5038e+00, 7.5648e-03, 1.7536e-03, 1.2381e-01,
        1.5467e-02, 1.1182e-01, 2.2934e-02, 1.6761e-01, 1.2619e-02, 2.6852e-02,
        1.8969e-04, 1.6318e-01, 2.2380e-01, 2.1636e-02, 2.5658e-02, 1.9556e-02,
        3.9211e-03, 8.6156e-03, 3.5874e-04, 7.6764e-04, 1.6486e-03, 2.6815e-03,
        2.4281e-01, 3.7888e-03, 1.0136e+00, 4.0285e-03, 5.0909e-02, 2.4278e-03,
        5.1222e-03, 7.3671e-04, 1.6262e-01, 4.1610e-02, 5.1042e-02, 1.9126e-01,
        1.5972e-03, 1.0746e-02, 1.0743e-01, 6.4682e-01, 2.2732e+00, 1.6177e+01,
        8.6033e+00, 6.9917e+00, 9.8110e+00, 1.5003e+01, 1.4693e-03, 2.5653e-03,
        6.8552e-03, 3.4121e-03, 1.1396e-03, 1.0771e-03, 1.0751e-03, 5.4828e-02,
        2.2857e-03, 1.5371e-02, 4.7272e+00, 9.6883e-02, 1.6529e+00, 7.4649e-03,
        4.2578e+01, 5.8783e-03, 7.3344e-01, 2.8711e-01, 3.0894e-01, 2.2993e-02,
        3.9194e-01, 7.8718e-01, 3.7070e-01, 9.3239e-02, 2.0908e-04, 1.0632e-02,
        9.9369e-01, 1.3573e+00, 4.3433e+02, 1.1905e+01, 8.3987e-01, 8.3769e+00,
        2.1256e-02, 6.4783e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([2.1836e+00, 1.9386e-02, 1.0655e+00, 6.5723e-03, 5.0981e-03, 1.0068e-01,
        2.0874e-02, 1.1210e-01, 3.8041e-02, 1.1767e-01, 8.9203e-03, 4.2110e-02,
        8.9799e-04, 1.7458e-01, 2.9368e-01, 7.2180e-03, 1.1066e-02, 1.6904e-02,
        9.7466e-04, 1.2394e-03, 2.3182e-04, 1.3770e-04, 1.7898e-04, 1.2423e-03,
        2.1321e-01, 5.4075e-03, 7.3822e-01, 7.6705e-03, 5.4937e-02, 3.4828e-04,
        1.3212e-03, 8.3367e-04, 1.0826e-02, 2.8186e-04, 6.1144e-04, 2.0274e-03,
        2.2562e-03, 1.4741e-03, 3.6895e-03, 9.5226e-02, 1.5337e-01, 8.9396e-01,
        6.1621e-02, 1.5073e-01, 3.0880e-02, 2.3978e+00, 7.7641e-04, 8.2171e-04,
        3.7009e-04, 1.5864e-03, 6.1447e-05, 1.5009e-04, 2.9705e-04, 1.9143e-03,
        9.3235e-04, 4.0540e-03, 9.9049e-01, 2.7270e-02, 1.4569e+00, 1.9994e-03,
        1.2679e+01, 4.5475e-03, 2.4482e-01, 1.1246e-02, 3.4659e-02, 1.3823e-02,
        5.6008e-02, 1.5621e-01, 6.2704e-02, 1.9473e-02, 3.5510e-05, 2.8429e-03,
        2.3399e-02, 5.3209e-01, 5.2157e+01, 4.9732e+00, 1.2995e-02, 4.8819e+00,
        1.3341e-02, 2.2825e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.6224e+01, 2.6735e+00, 1.8264e+01, 7.6105e-01, 1.0619e+00, 1.0277e+01,
        7.5074e+00, 1.9239e+01, 2.6313e+00, 1.7041e+01, 3.7113e+00, 9.0941e+00,
        5.4393e-01, 1.6375e+01, 2.0511e+01, 5.2194e-01, 6.4440e-01, 9.1525e-01,
        6.8944e-02, 8.4485e-02, 3.0955e-01, 4.7779e-02, 3.3384e-02, 3.4990e-01,
        2.2647e+01, 1.3967e+00, 2.3579e+01, 1.9374e+00, 2.0051e+01, 6.3040e-02,
        2.3075e-01, 3.0965e-01, 2.5513e-01, 2.1287e-01, 1.0129e-01, 8.4862e-02,
        5.4794e-01, 1.8283e-01, 1.9532e-01, 2.8121e+00, 7.5205e-01, 1.7971e+00,
        8.5538e-02, 2.4232e-01, 3.2956e-02, 2.9405e+00, 2.3847e-01, 2.0240e-01,
        5.0737e-02, 2.6423e-01, 4.1186e-02, 4.2987e-02, 5.5034e-02, 2.0195e-01,
        2.9208e-01, 4.0223e-01, 4.9322e+00, 1.1362e+00, 1.2720e+01, 2.9256e-01,
        1.5561e+01, 4.2676e-01, 1.4361e+00, 3.2371e-01, 7.0053e-01, 8.8115e-01,
        8.6211e-01, 6.3321e+00, 1.3829e-01, 3.3368e-01, 8.5234e-03, 3.2585e-01,
        6.6303e-02, 4.3541e+00, 5.8909e+01, 4.9753e+00, 1.2995e-02, 4.8842e+00,
        2.9673e-02, 2.5502e-02], device='cuda:0')
Outer loop valEpocw Maximum [27/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 583.4
model_train val_loss valEpocw [27/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 87.4
model_train val_loss valEpocw [27/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 370.8
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.5988353  97.42449532 93.14579501 98.07507218 98.62209281 97.67546692
 98.36381136 95.1097087  98.07629049 97.03829144 98.64524068 98.92788831
 99.43226813 95.42525067 97.4915023  97.44276995 96.44375678 97.79607948
 98.91936014 98.42960003 98.70981104 99.33845835 99.50658496 98.98880374
 95.22301142 96.83970712 94.28491368 97.06387593 98.05192432 98.24563541
 98.44178312 98.57579708 97.09311534 98.6342759  98.67082516 98.89621228
 97.64013596 98.40645216 98.97540235 93.44306234 98.14451578 96.01369379
 98.50148025 98.02755814 98.80727574 97.9374033  98.24929034 98.69275472
 98.13720593 98.76585324 98.78169126 98.65255053 99.04728256 98.41498032
 98.74514199 97.69861478 93.54296366 97.00661542 96.73249595 97.61089655
 97.40134745 98.70615611 97.70348802 97.4646995  98.76463493 97.67546692
 98.66107869 95.99420085 99.2190641  98.34066349 99.81603538 97.43546009
 99.08261352 96.11603172 98.8718461  98.95956433 99.71978899 99.47490893
 99.86354942 99.18129652]
Accuracy th:0.7 is [85.08424605 97.36479819 92.2807958  97.93374837 98.5514309  97.5341431
 98.24685372 94.86726526 97.98735396 96.85676344 98.57336046 98.87428272
 99.41764842 95.34484229 97.37210804 97.2137279  96.34385546 97.69617817
 98.8572264  98.36259305 98.6342759  99.29094431 99.45419768 98.81093067
 95.24372266 96.73127764 94.42867411 96.98590417 98.06167079 98.17741012
 98.26878328 98.60138156 96.90305917 98.50757179 98.52462811 98.77072648
 97.40134745 98.28340298 98.85113485 93.23229493 98.05314263 96.12577819
 98.15669887 97.80948088 98.66473362 97.72298096 98.17862843 98.63671252
 98.04095954 98.75123354 98.70128288 98.60381818 99.01926146 98.37112121
 98.73417722 97.58043883 92.85096429 96.90793241 96.65817912 97.39769252
 96.82752403 98.50757179 97.54145296 97.33555878 98.85113485 97.54388957
 98.66838854 95.97592622 99.13743741 98.23832556 99.81603538 97.1576857
 98.93154323 95.90526431 98.92910661 99.18251483 99.68323973 99.4700357
 99.85867619 99.15693035]
Avg Prec: is [96.51678983 35.46187384 73.08598929 67.4871978  79.24438861 64.87541765
 74.52694875 47.9454421  57.6425345  53.42401015 33.88325175 54.55913198
 24.59889361 28.91012491 34.03789489 58.82495943 30.63810921 43.56931544
 50.83358819 39.47339594 61.91984326 50.34915639 90.04988296 83.57031965
 26.19154069 35.15279184 38.35559278 40.61922642 24.20885346 39.28906152
 73.41879994 37.82552159 58.87728669 64.4093641  73.84678682 80.36196392
 58.64799    74.81305299 87.11508218 46.7806416  49.07074092 82.93394428
 85.11735061 80.6501001  88.48623543 90.74703183 40.05952563 34.64359476
 42.26352283 46.11393766 62.47963982 39.24593709 26.32237238 73.11622986
 26.89874923 45.80399224 75.05758772 59.42518625 46.4834538  61.85012106
 94.89724587 83.75517198 74.53440456 52.16658581 60.95972422 45.4706037
 61.67344211 26.67792686 74.02363457 68.47565844 11.22674231 72.47431798
 84.86048969 52.26797182 91.46867168 93.28748626 88.10700215 91.86417736
 32.0665872  30.52119143]
Accuracy th:0.5 is [45.57693011 97.2137279  71.30639247 97.02489005 97.26733349 75.95424032
 76.35993714 75.90307136 77.25417575 96.46446803 77.55509801 98.52097319
 99.41399349 80.37426445 77.21397156 96.56680596 96.29512311 76.79609167
 98.65376884 98.30776915 80.60574311 78.08749893 98.38695922 81.08210183
 82.34914292 96.65086926 94.0778012  76.70228189 98.01293844 77.49661919
 97.30875598 98.57457877 96.36213009 98.02024829 88.17875026 77.09092238
 76.92888732 91.38412056 97.11504489 74.14505184 79.37646958 92.05906361
 76.39039485 75.94936709 96.9627563  93.87434364 98.02877645 98.57336046
 97.77780485 89.94895286 88.91704536 98.55508583 98.99976852 76.45740183
 98.70615611 76.86675357 71.57929362 93.39798492 96.24273583 96.9067141
 89.79300934 97.17717864 94.43720228 76.88868313 98.42838172 77.41499251
 98.20786784 76.14307818 77.98394269 97.55972759 78.48101266 95.99054592
 77.74393587 95.45083515 76.18328237 83.32378991 89.30324923 77.4978375
 78.55289287 99.14718388]
Accuracy th:0.7 is [45.57083856 97.2137279  71.30639247 97.02489005 97.26733349 75.95424032
 76.35993714 76.19668376 77.25417575 96.4754328  77.55509801 98.52097319
 99.41399349 80.89813721 77.21397156 96.56680596 96.29512311 76.79609167
 98.65376884 98.30776915 81.02362301 78.08749893 98.38695922 82.0396925
 83.17150132 96.65086926 94.0778012  76.70228189 98.01293844 77.49661919
 97.30875598 98.57457877 96.36213009 98.02024829 88.38586275 77.09092238
 76.92888732 91.63265555 97.11504489 74.14505184 80.04288447 92.05906361
 76.39039485 75.94936709 96.9627563  93.87434364 98.02877645 98.57336046
 97.96176947 90.52886783 89.11928461 98.55508583 98.99976852 76.45740183
 98.70615611 76.86675357 71.57929362 93.88043518 96.24273583 96.9067141
 89.79300934 97.17717864 94.52492051 76.88868313 98.42838172 77.41499251
 98.20786784 76.14307818 77.98394269 97.55972759 78.5005056  95.99054592
 77.91206247 95.45083515 76.18328237 83.47120527 89.55787576 77.4978375
 78.55289287 99.14718388]
Avg Prec: is [56.05091459  3.03937117 11.31636149  3.36175479  2.2460676   3.82672727
  3.30993987  5.71156233  2.45713345  3.94262509  1.58143097  1.65828664
  0.58480543  5.13277259  2.67028668  3.14864313  3.6925126   2.77625973
  1.34710032  1.71734623  2.06369606  0.85673068  1.90856901  2.52081371
  5.06498445  3.71644421  6.52989041  3.23889776  2.02433727  1.8447341
  2.64472347  1.37313348  3.76260461  1.67177292  2.28063394  2.41116145
  3.02769831  2.68614962  2.74967748  7.4049197   2.28596249  8.24013948
  3.32525785  3.98927074  3.16314919  6.41089169  2.10000194  1.51419583
  2.14814856  1.58236061  1.77752068  1.56012484  1.09567846  2.97447844
  1.36169603  2.74477658 11.17661188  3.52045627  4.04860304  2.88138962
 10.82007774  2.10751872  3.77891182  3.03152945  1.57706937  2.40528959
  1.82499914  4.18415694  1.25220336  2.35590519  0.18898474  3.38903286
  1.91008266  4.59605394  3.93561845  3.12590539  0.81365041  1.83161039
  0.12813581  0.72501317]
mAP score regular 58.04, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [87.87901438 97.45870394 93.11607743 98.31327703 98.96105838 97.68293594
 98.47522236 95.35092309 98.17624636 97.14228766 98.67952263 99.04327678
 99.36716745 95.23382415 97.45372101 97.38146847 96.45215138 97.82494955
 98.99593891 98.38802103 98.88631437 99.33976132 99.62378852 99.23761118
 95.35341456 96.77604206 93.89590652 97.31419887 97.91464235 98.24849889
 98.71689464 98.58235543 97.0276802  98.7343349  98.91621197 99.14044398
 98.01180955 98.33819169 99.12798665 93.24314224 98.01180955 92.26648728
 97.51102474 96.36993298 96.86573486 94.73054787 98.41044423 98.82153624
 98.10648529 98.68699704 98.90375464 98.6446421  98.91122904 98.41791863
 98.7642325  97.81000075 90.95348432 97.11239006 96.32508658 97.59822608
 92.81710143 98.81406184 97.2593866  97.55088821 98.70692877 97.53344794
 98.66208237 95.81184443 98.76921544 98.26593916 99.81563146 97.59573461
 98.45279916 95.98375564 96.85576899 96.61409672 99.21767945 98.61225303
 99.81314    99.18030745]
Accuracy th:0.7 is [86.52365648 97.42631487 92.51812542 98.21860129 98.92866931 97.60570048
 98.4876797  95.06191295 98.11894262 96.91058126 98.63965917 98.98348158
 99.36218452 95.15160575 97.3715026  97.17467673 96.30017191 97.78259461
 98.99095598 98.37556369 98.88880584 99.34225278 99.55402746 99.12051225
 95.46054762 96.67887485 94.53621347 97.19460847 97.91464235 98.16378902
 98.61972743 98.69945437 96.96290206 98.71440317 98.7268605  99.03580238
 97.84986422 98.19617809 99.03580238 93.2531081  97.98938635 92.97904676
 97.45870394 96.63402845 97.03266313 94.82273214 98.37805516 98.83150211
 98.00931809 98.73682637 98.7791813  98.61225303 98.90375464 98.46027356
 98.73184344 97.70535914 91.12788699 97.18962553 96.46959165 97.41385754
 92.92174303 98.6969629  97.24942073 97.39143434 98.81157037 97.61815781
 98.65460797 95.83925057 98.82153624 98.15382316 99.81563146 97.37648554
 98.43785036 95.8965543  97.17467673 96.98034233 99.24010265 98.66457383
 99.82310586 99.16535865]
Avg Prec: is [96.44655001 34.2327691  70.89447544 73.69186462 77.2693498  65.02172202
 79.83460235 48.194956   62.35507736 55.37476197 38.08680881 56.91462851
 21.93803317 30.77514945 34.49727326 62.61802381 33.85497541 45.52870955
 51.75882593 37.08925288 69.39759694 58.26794867 92.15656255 88.42775392
 24.69349252 37.30087788 33.05932959 46.89346719 26.96802341 37.73893512
 77.33111695 38.49291648 56.19199456 65.39905067 74.68831123 82.41342478
 59.73589747 76.46984675 90.16416816 44.96231436 38.7702281  50.7758473
 52.63518772 39.84589835 30.42550073 51.6121611  40.34548968 28.8189389
 42.00030099 41.43357614 68.77179648 35.8946541  26.5699115  75.91206677
 29.21957174 43.03666944 57.24500597 58.55933887 39.74489862 63.20448411
 65.00088613 87.07928983 65.96082643 52.89504551 61.04402555 39.56695782
 62.32889907 27.52233324 43.6130331  66.01661591 12.19243055 74.50382192
 55.93812301 45.85746799 63.63569756 48.85234935 13.91705134 55.74641895
  2.44346687 24.31596885]
Accuracy th:0.5 is [45.30233949 97.22450607 69.31011286 96.96290206 97.90716795 74.63686872
 74.75396766 73.93178364 76.23639036 96.41976231 76.3759125  98.5325261
 99.34972718 77.77860827 76.3310661  96.31262924 96.21047911 75.74557142
 98.78167277 98.34068316 78.59331789 77.01621945 98.31327703 82.47004011
 78.14983681 96.52938685 94.3393876  75.83277275 97.81747515 76.32857463
 97.52597354 98.67204823 96.39983058 98.18870369 89.51341655 75.95983756
 75.96232902 92.72990009 97.0276802  73.21174976 77.08348905 92.37362035
 75.08284127 74.76144206 97.03764606 94.02795426 98.18621222 98.77668984
 97.94952288 89.84478162 87.27857089 98.55993223 98.87385704 75.06789247
 98.6969629  75.77546902 70.02267235 94.42658893 96.16314124 96.78102499
 90.13379176 97.04761193 94.56112814 75.83028129 98.32075143 76.68734584
 98.13139996 75.028029   77.06355732 97.53593941 77.43727733 96.07843137
 76.86174851 95.44559882 74.97072527 84.36604629 91.41191419 76.41577597
 77.51700426 99.15040985]
Accuracy th:0.7 is [45.55397763 97.22450607 69.31011286 96.96290206 97.90716795 74.63686872
 74.75396766 74.15103271 76.23639036 96.41976231 76.3759125  98.5325261
 99.34972718 78.20963201 76.3310661  96.31262924 96.21047911 75.74557142
 98.78167277 98.34068316 78.94461469 77.01621945 98.31327703 83.17263373
 78.76024616 96.52938685 94.3393876  75.83277275 97.81747515 76.32857463
 97.52597354 98.67204823 96.39983058 98.18870369 89.67287042 75.95983756
 75.96232902 92.8943369  97.0276802  73.21174976 77.60918853 92.37362035
 75.08284127 74.76144206 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 90.24839923 87.47041383 98.55993223 98.87385704 75.06789247
 98.6969629  75.77546902 70.02267235 94.77290281 96.16314124 96.78102499
 90.13379176 97.04761193 94.69566734 75.83028129 98.32075143 76.68734584
 98.13139996 75.028029   77.06355732 97.53593941 77.43727733 96.07843137
 76.92652665 95.44559882 74.97072527 84.52799163 91.61621447 76.41577597
 77.51700426 99.15040985]
Avg Prec: is [54.54557919  3.74872883 14.85197487  4.57099003  1.56074305  4.28819139
 11.69235654  8.72477948  7.411882    5.15601115  2.27780414  5.19933407
  1.55914944  5.9090135   3.06252305  3.90414821 25.60848283  6.36207135
  1.52659089  2.61976041  3.60399735  1.41303658  1.1650638   5.79426175
  5.73746941 11.50256974  8.18900286  4.56625067  3.89781041  6.82514159
  2.33514248  0.86665486  3.00535377  1.15187145  1.70277622  2.45287639
  2.04166131  2.23239438  2.20170226  6.23736432  1.73395097  6.03904756
  2.20656313  2.73137643  2.35681072  4.87191705  1.74763923  1.03725694
  1.44683794  1.21024876  1.22891099  0.99825378  0.75295276  2.30889172
  0.9171466   1.87484114 10.18078486  3.02573016  3.95979458  2.84750379
  7.94481562  2.05194585  3.38262581  2.5976492   1.35442237  1.89532722
  1.61192807  3.53105723  1.06196044  2.19836731  0.19643137  3.18213616
  1.53327948  4.08364711  3.18916494  2.29588834  0.65600396  1.48577357
  0.12125794  0.60801971]
mAP score regular 51.75, mAP score EMA 4.37
Train_data_mAP: current_mAP = 58.04, highest_mAP = 58.04
Val_data_mAP: current_mAP = 51.75, highest_mAP = 51.91
tensor([2.4702e-02, 1.9841e-03, 3.1996e-02, 2.2585e-03, 9.8338e-04, 2.5787e-03,
        4.6781e-04, 1.3396e-03, 4.8469e-03, 1.6703e-03, 4.0986e-04, 9.9138e-04,
        2.6058e-04, 3.3386e-03, 4.5587e-03, 4.8435e-03, 6.4666e-03, 5.5588e-03,
        4.4151e-03, 4.8702e-03, 8.0880e-05, 5.2775e-04, 1.2404e-03, 7.9086e-04,
        3.4730e-03, 7.4272e-04, 1.4418e-02, 6.8789e-04, 4.6011e-04, 1.2632e-03,
        1.3281e-03, 4.5773e-04, 1.5157e-02, 1.4843e-04, 1.2698e-03, 8.9178e-03,
        7.7191e-04, 2.1141e-03, 4.9926e-03, 1.3953e-02, 1.5971e-01, 4.8726e-01,
        8.3047e-01, 7.3081e-01, 9.7774e-01, 8.9295e-01, 6.3684e-04, 8.5911e-04,
        1.5956e-03, 1.3588e-03, 2.3720e-04, 8.6099e-04, 1.3200e-03, 2.1766e-03,
        6.3533e-04, 3.2583e-03, 1.2611e-01, 7.7841e-03, 8.7165e-02, 1.5376e-03,
        8.7347e-01, 2.6308e-03, 1.2352e-01, 1.1742e-02, 2.1190e-02, 4.3675e-03,
        3.1313e-02, 8.1140e-03, 5.3276e-01, 2.9977e-02, 8.3867e-04, 2.1096e-03,
        3.7314e-01, 6.9775e-02, 9.5304e-01, 9.9996e-01, 1.0000e+00, 9.9997e-01,
        6.0207e-01, 5.0082e-02], device='cuda:0')
Sum Train Loss:  tensor([8.3503e-01, 2.4803e-02, 1.1693e+00, 2.0612e-02, 6.2635e-03, 2.0937e-02,
        2.3223e-03, 2.8505e-02, 1.7651e-02, 4.0225e-02, 5.4166e-03, 7.2759e-03,
        1.2195e-03, 5.0740e-02, 1.0370e-01, 3.4542e-02, 9.3979e-02, 2.1538e-02,
        1.1155e-02, 1.4673e-02, 4.9810e-04, 3.2111e-03, 3.3808e-04, 3.0569e-03,
        4.6217e-02, 8.2428e-03, 4.3590e-01, 4.7945e-03, 8.4540e-03, 1.0907e-02,
        4.6247e-03, 6.3313e-04, 2.3751e-01, 4.6957e-04, 1.5650e-02, 4.6563e-02,
        1.2504e-02, 6.0899e-03, 2.5879e-02, 2.4019e-01, 4.9803e-01, 4.6086e+00,
        5.5739e+00, 6.5958e+00, 1.9298e+00, 4.1108e+00, 2.1085e-03, 5.8173e-03,
        1.0725e-02, 3.3743e-03, 1.9123e-03, 3.0480e-03, 4.1855e-03, 1.1421e-02,
        6.8588e-04, 1.2163e-02, 2.5885e+00, 5.1895e-02, 7.7266e-01, 2.0847e-03,
        4.0433e+00, 5.4282e-03, 8.0290e-01, 3.9832e-02, 8.5081e-02, 1.2962e-02,
        3.2903e-02, 9.6907e-02, 8.9722e-01, 6.4951e-02, 1.6188e-04, 2.8488e-02,
        1.3904e+00, 1.4773e+00, 1.0780e+00, 1.2606e+00, 3.8714e-01, 1.4871e-01,
        5.7514e-02, 2.0210e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [28/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 42.3
Sum Train Loss:  tensor([1.1379e+00, 2.0465e-02, 5.3898e-01, 1.5610e-02, 5.2224e-03, 8.2154e-03,
        2.7470e-03, 1.2749e-02, 1.0778e-01, 1.5881e-02, 8.6783e-04, 5.0726e-03,
        2.5518e-03, 3.0884e-02, 6.3449e-02, 3.7522e-02, 1.2122e-01, 3.7318e-02,
        9.5732e-03, 9.0209e-03, 3.3619e-04, 6.1504e-03, 1.1373e-03, 1.7968e-03,
        8.6175e-02, 3.4056e-03, 1.6761e-01, 9.3878e-03, 4.2269e-03, 1.1450e-02,
        1.5579e-02, 1.9178e-03, 1.8734e-01, 9.4962e-04, 4.6061e-03, 2.9004e-02,
        6.5731e-03, 8.0799e-03, 1.4663e-02, 3.7970e-01, 1.5391e+00, 6.0911e+00,
        3.9681e+00, 3.4168e+00, 5.1157e+00, 4.3673e+00, 3.5341e-03, 1.1577e-02,
        9.7339e-03, 1.9554e-02, 1.6208e-03, 7.4556e-03, 4.8215e-03, 6.3926e-03,
        7.1998e-03, 5.2556e-02, 3.0867e+00, 1.5369e-01, 1.1951e+00, 1.8944e-02,
        9.1739e+00, 2.2651e-02, 1.6257e+00, 1.4051e-01, 9.0586e-02, 4.2902e-02,
        4.9464e-02, 1.3871e-01, 1.8544e+00, 2.4266e-01, 1.7987e-04, 4.0037e-02,
        3.6095e-01, 9.2963e-01, 4.8674e+00, 6.2942e+00, 5.7553e-01, 2.7829e+00,
        6.0383e-01, 6.0410e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [28/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 62.6
Sum Train Loss:  tensor([8.3285e-01, 2.6990e-02, 5.7707e-01, 3.0698e-02, 2.7074e-03, 1.1432e-02,
        1.3024e-03, 2.1897e-02, 3.6490e-02, 1.0596e-02, 5.5644e-04, 4.8095e-03,
        1.5172e-03, 7.9372e-02, 5.0063e-02, 7.0104e-02, 6.3764e-02, 5.5526e-02,
        1.7713e-02, 2.8768e-02, 4.7293e-04, 6.7593e-03, 6.1775e-03, 2.3992e-03,
        2.9497e-02, 1.4390e-02, 2.2565e-01, 7.1119e-03, 9.4138e-04, 2.1064e-02,
        4.7940e-03, 1.1470e-03, 8.1729e-02, 2.2168e-04, 1.3139e-03, 2.3693e-02,
        9.5486e-03, 2.7790e-02, 3.2714e-03, 3.3211e-01, 1.0622e+00, 5.4773e+00,
        3.2006e+00, 3.7301e+00, 2.8244e+00, 1.2023e+01, 4.1596e-03, 8.7065e-03,
        2.6243e-03, 1.6047e-03, 3.2030e-04, 5.5855e-03, 6.0250e-03, 1.2732e-02,
        5.2779e-03, 2.0804e-02, 2.7487e+00, 8.4461e-02, 1.6890e+00, 1.2674e-02,
        7.1226e+00, 4.9785e-03, 4.5774e-01, 1.2011e-01, 2.1786e-01, 3.6067e-02,
        3.1290e-01, 1.9574e-01, 4.7598e-01, 9.6356e-02, 1.5330e-04, 1.2647e-02,
        4.3110e-01, 1.5953e+00, 1.1432e+01, 1.2543e+00, 6.6758e+00, 1.9949e-01,
        1.3023e-01, 1.5294e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [28/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 66.6
Sum Train Loss:  tensor([1.0422e+00, 1.6540e-02, 6.3410e-01, 2.9947e-02, 1.4707e-02, 1.7861e-02,
        3.9702e-03, 1.8237e-02, 6.0975e-02, 2.5950e-02, 5.1083e-04, 3.6763e-03,
        1.8987e-03, 5.0554e-02, 8.6606e-02, 4.0050e-02, 1.4957e-01, 3.5356e-02,
        7.7718e-03, 3.8589e-02, 3.4769e-04, 1.1716e-03, 3.4352e-03, 4.9949e-04,
        4.6890e-02, 9.1587e-03, 3.0361e-01, 1.2487e-02, 5.3335e-03, 7.1371e-03,
        9.5281e-03, 3.5604e-03, 1.6843e-01, 2.0601e-04, 7.4213e-03, 1.7155e-02,
        4.6827e-03, 5.7250e-03, 4.1547e-02, 1.8109e-01, 7.1317e-01, 3.5373e+00,
        3.8239e+00, 1.7747e+00, 3.9707e+00, 7.0749e+00, 6.2221e-03, 2.2852e-03,
        1.1064e-02, 7.2611e-03, 2.8139e-03, 2.1742e-03, 1.1758e-02, 1.9366e-02,
        3.4178e-03, 3.7730e-02, 3.5442e+00, 4.8783e-02, 8.9005e-01, 6.1334e-03,
        1.3546e+01, 4.2910e-02, 1.0667e+00, 7.3575e-02, 9.1723e-02, 4.1615e-02,
        1.5741e-01, 1.0139e-01, 1.0316e+00, 2.1076e-01, 2.4214e-03, 3.3798e-02,
        1.0582e+00, 1.0468e+00, 1.2637e+00, 6.6650e-01, 3.3551e-01, 3.9244e+00,
        5.0788e-01, 4.7312e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [28/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 53.8
Sum Train Loss:  tensor([9.8807e-01, 1.3944e-02, 6.2391e-01, 1.7808e-02, 1.1553e-02, 4.2086e-02,
        3.6778e-03, 1.7559e-02, 3.6441e-02, 1.3056e-02, 2.8110e-03, 7.4793e-03,
        1.4318e-04, 6.4228e-02, 5.7317e-02, 3.4329e-02, 1.4665e-01, 9.1733e-02,
        2.4198e-02, 3.8549e-02, 6.5912e-04, 3.6118e-04, 1.2103e-03, 7.8344e-04,
        3.7348e-02, 7.6665e-03, 3.7304e-01, 1.1989e-02, 3.2622e-03, 1.1689e-02,
        3.7344e-03, 1.1448e-03, 1.5105e-01, 6.3034e-04, 1.0578e-02, 7.7772e-02,
        1.0335e-02, 1.3408e-02, 5.0247e-03, 2.6250e-01, 3.9962e-01, 1.1257e+01,
        2.5767e+00, 7.5047e+00, 1.2445e+00, 5.8207e+00, 3.6583e-03, 3.6878e-03,
        7.1851e-03, 5.2230e-03, 1.6538e-04, 4.8273e-03, 4.5433e-03, 4.9762e-03,
        5.6449e-03, 2.3105e-02, 3.3574e+00, 4.7463e-02, 1.2046e+00, 1.4053e-02,
        3.9227e+00, 1.8680e-02, 9.8144e-01, 8.1685e-02, 3.6392e-02, 4.2948e-02,
        1.7549e-01, 2.4120e-01, 1.0017e+00, 4.1775e-02, 4.5413e-04, 2.5314e-02,
        5.7670e-01, 3.8664e-01, 1.1565e+01, 2.3198e+00, 1.6455e-01, 2.8287e+00,
        1.0073e-01, 2.3484e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [28/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 61.5
Sum Train Loss:  tensor([8.2993e-01, 2.1975e-02, 5.4736e-01, 1.0581e-02, 3.6218e-03, 3.3252e-02,
        4.8538e-03, 2.4115e-02, 2.5351e-02, 1.5868e-02, 7.8387e-04, 1.5320e-03,
        1.6399e-04, 7.3631e-02, 4.7800e-02, 1.1414e-01, 1.8455e-01, 8.3211e-02,
        3.1625e-02, 3.1410e-02, 1.7045e-04, 1.0238e-03, 1.1349e-02, 1.7457e-03,
        4.3283e-02, 8.1629e-03, 3.5516e-01, 9.3110e-03, 5.6497e-03, 7.4656e-03,
        8.9790e-03, 3.0826e-03, 1.0711e-01, 6.3667e-04, 6.7176e-03, 2.8319e-02,
        6.5615e-03, 1.7937e-02, 3.5621e-03, 4.0012e-01, 1.8948e+00, 9.9223e+00,
        2.4160e+00, 6.8937e+00, 3.7131e+00, 4.8418e+00, 3.0699e-03, 7.8029e-03,
        9.0924e-03, 5.4861e-03, 5.8292e-04, 5.2482e-03, 4.7537e-03, 7.1227e-03,
        1.4888e-03, 2.1855e-02, 3.3928e+00, 7.8860e-02, 4.7975e-01, 1.1444e-02,
        4.8483e+00, 1.6208e-02, 5.8886e-01, 1.4607e-01, 1.1682e-01, 2.6236e-02,
        3.6949e-01, 1.5631e-01, 1.2187e+00, 8.5279e-02, 3.4838e-04, 2.4441e-02,
        1.7110e+00, 5.5274e-01, 3.3153e+00, 8.0084e+00, 2.3356e-01, 1.9272e+00,
        1.1915e-01, 4.8486e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [28/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 60.3
Sum Train Loss:  tensor([8.9369e-01, 3.0578e-02, 9.5583e-01, 1.5454e-02, 7.3542e-03, 3.0777e-02,
        5.9440e-03, 2.6630e-02, 1.1628e-02, 2.9710e-02, 3.3702e-03, 3.6839e-03,
        4.5260e-04, 6.5745e-02, 4.1175e-02, 3.9809e-02, 7.8413e-02, 3.3890e-02,
        3.9513e-02, 3.5125e-02, 1.9027e-04, 1.0263e-03, 5.1166e-04, 4.9502e-04,
        7.4393e-02, 1.2902e-02, 4.2398e-01, 1.1059e-02, 6.2270e-03, 2.2868e-02,
        1.1529e-02, 8.5722e-04, 5.6754e-02, 7.8234e-04, 4.7852e-03, 2.9326e-02,
        1.1664e-02, 3.3240e-02, 1.3592e-02, 2.8026e-01, 8.8869e-01, 3.0405e+00,
        3.5478e+00, 7.3964e+00, 4.5220e+00, 3.9250e+00, 4.8326e-03, 8.4517e-03,
        9.3273e-03, 1.0633e-02, 5.0884e-04, 9.3347e-03, 4.9232e-03, 5.6576e-03,
        4.6853e-03, 3.2928e-02, 2.4373e+00, 1.0077e-01, 1.3855e+00, 2.2535e-02,
        9.7317e+00, 3.5672e-03, 7.2372e-01, 1.1492e-01, 4.1066e-02, 4.4635e-02,
        9.2606e-02, 1.2998e-01, 3.9940e-01, 8.4863e-02, 6.6859e-05, 1.1369e-02,
        2.8652e+00, 1.2575e+00, 2.0310e+00, 1.3571e+00, 1.5173e+00, 4.9152e+00,
        1.8903e+00, 7.6584e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [28/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 58.0
Sum_Val Meta Model:  tensor([1.2808e+00, 1.2202e-01, 2.5115e+00, 1.0729e-01, 1.4220e-03, 3.3279e-02,
        2.1604e-03, 2.6245e-02, 3.4272e-02, 1.5016e-02, 2.6008e-03, 1.1764e-02,
        2.1169e-03, 4.6709e-02, 4.0830e-02, 4.5743e-02, 3.3173e+00, 1.7289e-02,
        4.3337e-03, 6.6695e-03, 5.5863e-05, 2.1961e-04, 6.3111e-04, 5.4438e-04,
        9.5966e-02, 1.4277e-02, 4.0310e-01, 2.3782e-03, 4.8132e-03, 1.7091e-02,
        2.6976e-03, 3.2820e-04, 1.1061e-01, 1.9233e-04, 1.1279e-03, 7.3424e-03,
        3.9312e-03, 1.2995e-02, 7.4380e-03, 6.9602e-01, 1.6658e+00, 1.4346e+01,
        6.1120e+00, 1.6510e+01, 1.8354e+01, 1.9658e+01, 1.2500e-03, 4.6722e-03,
        1.0415e-03, 6.4894e-03, 5.6593e-05, 4.5380e-04, 8.2793e-03, 2.9760e-03,
        3.6227e-04, 5.2859e-03, 3.4853e+00, 5.0793e-02, 1.1328e+00, 7.3006e-03,
        8.8145e+00, 2.9371e-02, 6.1309e-01, 5.5770e-02, 6.2966e-03, 6.8565e-03,
        1.4092e-02, 6.0468e-02, 6.0216e+00, 5.5189e-01, 1.9528e-04, 3.5612e-02,
        6.2086e+00, 6.1511e-01, 2.4884e+01, 8.2909e+00, 1.3549e-01, 9.7061e+00,
        1.6838e-02, 2.3457e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.2451e+00, 7.9378e-02, 1.7227e+00, 5.6250e-02, 2.4835e-04, 3.3110e-02,
        1.5933e-03, 2.8047e-02, 3.1306e-02, 1.5793e-02, 3.1362e-03, 1.4009e-02,
        2.4602e-03, 5.0532e-02, 3.7437e-02, 2.0725e-01, 2.0277e+00, 3.6904e-02,
        3.2199e-03, 1.1975e-02, 4.7935e-05, 1.9890e-04, 3.2038e-04, 1.1912e-03,
        9.3757e-02, 1.3433e-02, 3.9199e-01, 3.2524e-03, 6.6004e-03, 1.8134e-02,
        1.1824e-03, 2.4805e-04, 1.1186e-01, 2.1327e-04, 1.0483e-03, 5.5677e-03,
        7.4831e-03, 8.8231e-03, 8.5201e-03, 6.0642e-01, 1.8908e+00, 1.5256e+01,
        4.5648e+00, 1.4208e+01, 1.5875e+01, 1.8403e+01, 1.1262e-03, 5.2778e-03,
        5.9197e-04, 4.7304e-03, 1.8618e-05, 2.4618e-04, 7.5124e-03, 1.2862e-03,
        3.3257e-04, 4.0017e-03, 3.7566e+00, 5.8038e-02, 1.0309e+00, 7.3410e-03,
        1.1932e+01, 1.2328e-02, 4.6574e-01, 8.7379e-02, 4.0325e-03, 9.4444e-03,
        1.2872e-02, 6.2420e-02, 5.1076e+00, 4.4945e-01, 4.2292e-04, 4.3009e-02,
        4.3928e+00, 5.7817e-01, 2.6619e+01, 9.4739e+00, 9.4787e-02, 9.4413e+00,
        3.5634e-02, 6.3440e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.0403e+01, 4.0007e+01, 5.3841e+01, 2.4906e+01, 2.5255e-01, 1.2840e+01,
        3.4059e+00, 2.0937e+01, 6.4591e+00, 9.4548e+00, 7.6520e+00, 1.4130e+01,
        9.4414e+00, 1.5136e+01, 8.2122e+00, 4.2790e+01, 3.1357e+02, 6.6389e+00,
        7.2929e-01, 2.4589e+00, 5.9266e-01, 3.7689e-01, 2.5828e-01, 1.5062e+00,
        2.6996e+01, 1.8086e+01, 2.7187e+01, 4.7280e+00, 1.4345e+01, 1.4355e+01,
        8.9029e-01, 5.4190e-01, 7.3803e+00, 1.4369e+00, 8.2555e-01, 6.2433e-01,
        9.6943e+00, 4.1735e+00, 1.7066e+00, 4.3461e+01, 1.1839e+01, 3.1310e+01,
        5.4967e+00, 1.9442e+01, 1.6237e+01, 2.0609e+01, 1.7684e+00, 6.1434e+00,
        3.7100e-01, 3.4813e+00, 7.8489e-02, 2.8593e-01, 5.6910e+00, 5.9091e-01,
        5.2346e-01, 1.2281e+00, 2.9787e+01, 7.4560e+00, 1.1827e+01, 4.7743e+00,
        1.3661e+01, 4.6861e+00, 3.7706e+00, 7.4415e+00, 1.9030e-01, 2.1624e+00,
        4.1107e-01, 7.6929e+00, 9.5871e+00, 1.4993e+01, 5.0428e-01, 2.0387e+01,
        1.1772e+01, 8.2863e+00, 2.7931e+01, 9.4742e+00, 9.4787e-02, 9.4416e+00,
        5.9185e-02, 1.2667e+00], device='cuda:0')
Outer loop valEpocw Maximum [28/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 156.5
model_train val_loss valEpocw [28/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 150.9
model_train val_loss valEpocw [28/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1155.2
Sum_Val Meta Model:  tensor([3.9011e+00, 1.0319e-01, 3.6380e+00, 7.8244e-02, 4.2085e-03, 3.6635e-01,
        2.1203e-01, 3.3355e-01, 5.5517e-01, 2.4036e-01, 4.3308e-02, 9.4114e-01,
        1.4923e-02, 5.5099e-02, 2.5914e-01, 3.7424e-01, 2.0571e-01, 3.2904e-01,
        1.5866e-01, 9.9099e-01, 5.0679e-05, 1.7241e-04, 1.5394e-03, 1.7398e-02,
        3.1675e-01, 7.9418e-02, 1.0336e+00, 6.0654e-02, 3.3085e-02, 3.9136e-04,
        6.1862e-04, 1.4518e-04, 4.4504e-03, 2.1315e-04, 2.1561e-04, 1.2397e-03,
        1.8797e-02, 7.0466e-04, 7.6328e-04, 4.3476e-02, 3.1521e-02, 1.8189e+00,
        1.4371e-01, 2.1054e-01, 3.3656e-01, 4.0153e+00, 3.2439e-03, 3.1790e-03,
        5.4838e-04, 3.8369e-02, 9.7366e-05, 7.1605e-04, 1.6158e-04, 2.6887e-04,
        6.1615e-04, 2.8055e-02, 2.3558e+00, 1.0888e-01, 9.2880e-01, 3.0557e-02,
        1.5386e+00, 8.0312e-03, 3.7497e-01, 5.4549e-02, 2.0959e-02, 1.2515e-02,
        3.6427e-02, 3.8220e-01, 6.6422e-02, 2.2116e-01, 8.3988e-05, 1.0179e-02,
        1.9231e+00, 5.6317e-01, 5.4264e+00, 4.4676e-01, 1.4872e-01, 7.3604e-01,
        3.2132e-02, 1.1564e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([3.2372e+00, 1.3339e-01, 2.6624e+00, 7.8831e-02, 1.1530e-02, 3.2140e-01,
        1.3540e-01, 2.1989e-01, 3.6924e-01, 1.6782e-01, 3.1929e-02, 2.6899e-01,
        1.5289e-02, 8.8099e-02, 2.1772e-01, 6.0627e-02, 1.6272e-01, 2.9148e-01,
        5.2596e-02, 2.2397e-01, 3.9474e-04, 9.6588e-04, 1.9247e-03, 1.3153e-02,
        2.2772e-01, 7.3394e-02, 9.8035e-01, 5.1444e-02, 3.3770e-02, 8.8772e-03,
        1.4137e-03, 3.6326e-04, 2.1846e-02, 4.7420e-03, 2.4814e-03, 4.0708e-03,
        1.2951e-02, 7.7949e-03, 1.7389e-03, 6.7559e-02, 7.8000e-02, 1.9669e+00,
        5.3760e-02, 2.4054e-01, 2.4951e-01, 7.5649e-01, 2.7179e-03, 1.6363e-03,
        1.4336e-03, 3.6415e-02, 1.0938e-04, 8.9499e-04, 1.9143e-03, 5.4640e-03,
        7.3195e-04, 9.4679e-03, 1.9863e+00, 9.2835e-02, 5.4703e-01, 3.3079e-03,
        8.3879e+00, 6.0309e-03, 1.1021e-01, 1.5924e-02, 2.0206e-03, 3.3874e-03,
        4.4355e-03, 3.6949e-01, 2.2371e-01, 1.7998e-01, 1.8391e-04, 8.1861e-03,
        2.7729e+00, 2.9457e-01, 6.5995e+00, 1.0824e-01, 3.0017e-01, 2.8876e-02,
        1.3062e-03, 2.0408e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.5437e+01, 2.0554e+01, 4.6532e+01, 9.8648e+00, 2.7141e+00, 3.4066e+01,
        4.5606e+01, 3.8639e+01, 2.5500e+01, 2.7157e+01, 1.5178e+01, 5.9703e+01,
        9.7035e+00, 9.4581e+00, 1.5184e+01, 3.9601e+00, 9.4907e+00, 1.6070e+01,
        3.1105e+00, 1.2879e+01, 5.4097e-01, 3.5003e-01, 3.6153e-01, 3.7479e+00,
        2.5033e+01, 1.9275e+01, 3.0478e+01, 1.3746e+01, 1.2543e+01, 1.7094e+00,
        2.5906e-01, 1.4366e-01, 5.9621e-01, 4.6155e+00, 4.9186e-01, 2.1179e-01,
        3.3250e+00, 1.0687e+00, 1.0728e-01, 2.5433e+00, 5.0025e-01, 4.2642e+00,
        7.4336e-02, 3.9331e-01, 2.6510e-01, 9.2510e-01, 8.6563e-01, 4.0486e-01,
        2.0341e-01, 6.0016e+00, 7.4646e-02, 2.5746e-01, 3.7233e-01, 6.6475e-01,
        2.3100e-01, 9.8954e-01, 1.0449e+01, 4.0662e+00, 5.0982e+00, 4.8251e-01,
        1.0358e+01, 6.3942e-01, 6.9080e-01, 5.5655e-01, 4.9402e-02, 2.2795e-01,
        8.0441e-02, 1.8221e+01, 4.9552e-01, 3.6058e+00, 4.9647e-02, 1.0677e+00,
        8.4841e+00, 2.5594e+00, 7.4100e+00, 1.0828e-01, 3.0017e-01, 2.8886e-02,
        2.3780e-03, 2.5902e-01], device='cuda:0')
Outer loop valEpocw Maximum [28/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 36.5
model_train val_loss valEpocw [28/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 35.7
model_train val_loss valEpocw [28/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 683.7
Sum_Val Meta Model:  tensor([2.3140e+00, 1.0266e-02, 4.0497e-01, 8.3473e-03, 1.7312e-03, 1.0127e-02,
        1.4873e-03, 2.9209e-02, 1.4494e-02, 4.3547e-03, 9.3294e-04, 2.2857e-03,
        1.8926e-04, 9.5245e-02, 2.3409e-02, 2.4038e-02, 6.8838e-02, 2.9682e-02,
        9.3192e-03, 1.5817e-02, 1.6811e-04, 1.5004e-03, 1.6653e-02, 2.8060e-03,
        3.9016e-02, 2.2440e-03, 4.3167e-01, 8.8263e-03, 1.0221e-03, 8.1973e-03,
        1.0119e-02, 7.6383e-03, 1.9736e-01, 8.2608e-05, 3.9040e-03, 3.5361e-02,
        2.9693e-02, 4.7899e-02, 3.2421e-03, 4.7374e-01, 4.8887e+00, 3.3207e+01,
        5.4020e+01, 5.8964e+01, 5.1918e+01, 6.6771e+01, 2.0917e-02, 2.3541e-02,
        2.0545e-01, 5.4992e-02, 1.7587e-02, 6.6189e-02, 1.6421e-01, 6.2282e-02,
        1.0343e-01, 6.3541e-01, 2.6163e+01, 1.4334e-01, 7.3117e-01, 5.5664e-03,
        9.4251e+01, 7.0017e-03, 1.4860e+00, 2.2231e-01, 3.9660e-01, 6.7590e-03,
        3.6995e-01, 1.2388e-01, 2.2169e+00, 6.3535e-02, 2.9303e-04, 2.6998e-02,
        1.2921e+00, 3.1949e+00, 1.1149e+00, 3.0005e+01, 1.1065e+01, 8.4054e-01,
        9.3573e-02, 3.1382e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([7.5722e-01, 2.3112e-03, 1.8105e-01, 5.9720e-04, 5.8029e-05, 3.1230e-04,
        1.4699e-04, 2.7543e-02, 9.0301e-04, 2.3940e-04, 6.0001e-05, 6.9594e-05,
        2.0010e-05, 7.0799e-02, 2.3484e-03, 1.5258e-02, 1.1555e-02, 2.2968e-03,
        7.0373e-04, 4.2292e-04, 6.7597e-06, 2.6388e-05, 4.4297e-05, 6.1888e-05,
        2.1036e-02, 1.3346e-03, 4.1584e-01, 6.8393e-03, 4.4026e-04, 7.7130e-04,
        6.6939e-04, 7.3602e-03, 1.9787e-01, 3.4522e-05, 1.7190e-03, 5.9073e-03,
        1.7741e-02, 3.9534e-02, 5.6323e-04, 6.5768e-01, 4.9307e+00, 4.2582e+01,
        5.6671e+01, 5.7088e+01, 7.5851e+01, 8.2047e+01, 1.1741e-02, 9.8499e-03,
        1.1439e-01, 2.4380e-02, 9.7742e-03, 7.3086e-02, 1.4759e-01, 1.1636e-01,
        7.4678e-02, 3.7796e-01, 1.4370e+01, 1.5936e-01, 7.6731e-01, 2.3442e-03,
        8.5631e+01, 1.9087e-03, 1.1883e+00, 2.0586e-01, 3.8752e-01, 2.7935e-02,
        3.6591e-01, 1.4338e-01, 2.9405e+00, 2.5272e-01, 3.4828e-04, 2.5980e-02,
        1.7119e+00, 2.9393e+00, 4.9982e-01, 2.6958e+01, 1.3308e+01, 1.1373e-01,
        1.5586e-02, 3.4219e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.4857e+01, 8.3214e-01, 4.8448e+00, 1.8526e-01, 3.7400e-02, 8.0580e-02,
        1.9186e-01, 1.3492e+01, 1.4198e-01, 9.6749e-02, 8.7170e-02, 4.4683e-02,
        4.6323e-02, 1.5599e+01, 3.8173e-01, 2.3354e+00, 1.2652e+00, 2.6810e-01,
        1.0739e-01, 6.1203e-02, 4.6718e-02, 3.0369e-02, 2.0266e-02, 4.8913e-02,
        4.5064e+00, 1.1409e+00, 2.6008e+01, 6.2397e+00, 6.0525e-01, 3.5388e-01,
        2.9442e-01, 8.3423e+00, 9.0969e+00, 1.2182e-01, 8.5010e-01, 5.2103e-01,
        1.1980e+01, 1.1686e+01, 7.7027e-02, 4.3077e+01, 3.2929e+01, 8.5865e+01,
        6.8940e+01, 7.9536e+01, 7.8128e+01, 9.3189e+01, 1.0199e+01, 6.4139e+00,
        3.6330e+01, 9.5500e+00, 2.0753e+01, 4.7957e+01, 6.1490e+01, 3.3541e+01,
        6.5630e+01, 7.5295e+01, 1.0335e+02, 1.4651e+01, 9.1997e+00, 8.8948e-01,
        9.9065e+01, 5.0074e-01, 9.5367e+00, 1.3948e+01, 1.5984e+01, 4.2707e+00,
        1.0586e+01, 1.4265e+01, 6.5750e+00, 8.6291e+00, 2.6439e-01, 8.5793e+00,
        5.6607e+00, 3.7753e+01, 5.3772e-01, 2.6960e+01, 1.3308e+01, 1.1374e-01,
        2.7531e-02, 6.4363e-01], device='cuda:0')
Outer loop valEpocw Maximum [28/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 449.4
model_train val_loss valEpocw [28/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 474.6
model_train val_loss valEpocw [28/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1401.0
Sum_Val Meta Model:  tensor([6.0139e+00, 2.0543e-02, 1.5825e+00, 1.2282e-02, 3.0821e-03, 1.0097e-01,
        1.5875e-02, 9.7742e-02, 2.6791e-02, 1.2930e-01, 9.9504e-03, 2.2599e-02,
        5.1667e-04, 1.6717e-01, 1.9301e-01, 3.8522e-02, 5.1734e-02, 3.0479e-02,
        9.4785e-03, 1.5933e-02, 7.5202e-04, 2.3397e-03, 4.3341e-03, 5.2154e-03,
        2.2837e-01, 6.6861e-03, 9.5686e-01, 7.8649e-03, 3.7556e-02, 4.9900e-03,
        8.4069e-03, 1.1455e-03, 2.3257e-01, 3.3732e-02, 4.2953e-02, 2.1883e-01,
        1.9163e-03, 1.2490e-02, 1.4628e-01, 8.6776e-01, 3.2060e+00, 1.6738e+01,
        8.5675e+00, 8.5090e+00, 1.1845e+01, 1.5874e+01, 3.1159e-03, 4.3555e-03,
        1.0667e-02, 6.3626e-03, 2.0230e-03, 2.8940e-03, 2.6813e-03, 7.6675e-02,
        3.6104e-03, 2.6660e-02, 5.7545e+00, 1.4078e-01, 2.2046e+00, 1.9585e-02,
        3.9980e+01, 1.3197e-02, 1.0483e+00, 3.3725e-01, 4.0244e-01, 4.8085e-02,
        5.8924e-01, 6.4305e-01, 8.5824e-01, 1.8373e-01, 7.1852e-04, 1.8889e-02,
        1.8861e+00, 1.5121e+00, 4.2244e+02, 1.4570e+01, 1.7471e+00, 7.4679e+00,
        6.3275e-02, 1.3595e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.8536e+00, 2.6549e-02, 1.0031e+00, 1.0134e-02, 4.0913e-03, 9.2795e-02,
        2.3139e-02, 9.8684e-02, 3.8714e-02, 1.0987e-01, 8.8828e-03, 3.7350e-02,
        2.0136e-03, 1.5159e-01, 2.3117e-01, 1.1664e-02, 1.3736e-02, 2.4654e-02,
        1.1563e-03, 2.5829e-03, 1.3478e-04, 1.4312e-04, 1.2250e-03, 2.4854e-03,
        1.9699e-01, 9.9494e-03, 7.6543e-01, 5.3782e-03, 4.7420e-02, 1.4200e-03,
        1.1050e-03, 5.5252e-04, 4.3967e-03, 4.3675e-04, 6.1202e-04, 1.3593e-03,
        4.1863e-03, 1.2923e-03, 1.6286e-03, 8.9939e-02, 2.5394e-01, 6.6573e-01,
        2.8814e-01, 5.6497e-01, 3.4915e-01, 2.7859e+00, 1.1989e-03, 7.7526e-04,
        6.2474e-04, 7.8087e-04, 8.5120e-05, 3.1609e-04, 4.5560e-04, 4.5972e-03,
        5.8855e-04, 8.8335e-03, 1.3488e+00, 2.3122e-02, 1.4469e+00, 3.3784e-03,
        1.6371e+01, 7.7285e-03, 3.5462e-01, 1.8652e-02, 7.9159e-03, 1.1037e-02,
        2.2036e-02, 1.3950e-01, 1.7308e-01, 6.9955e-02, 3.1587e-04, 9.3124e-03,
        2.1565e-01, 6.0110e-01, 7.4262e+01, 1.1794e+00, 1.0780e-02, 9.7691e+00,
        3.6080e-03, 6.3228e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.3194e+01, 4.0391e+00, 1.8124e+01, 1.3314e+00, 9.4427e-01, 1.0598e+01,
        9.5950e+00, 1.8088e+01, 2.9078e+00, 1.8048e+01, 4.2593e+00, 8.8940e+00,
        1.3604e+00, 1.5039e+01, 1.8108e+01, 9.2001e-01, 8.5983e-01, 1.4359e+00,
        8.4628e-02, 1.8603e-01, 2.0896e-01, 5.6097e-02, 2.5419e-01, 7.4778e-01,
        2.1491e+01, 2.7974e+00, 2.4557e+01, 1.5963e+00, 1.9773e+01, 2.7278e-01,
        2.0175e-01, 2.2935e-01, 1.1311e-01, 3.5257e-01, 1.1355e-01, 5.6301e-02,
        1.1351e+00, 1.7457e-01, 9.4517e-02, 2.9432e+00, 1.2520e+00, 1.3303e+00,
        3.8347e-01, 8.6433e-01, 3.6831e-01, 3.3369e+00, 4.2005e-01, 2.1418e-01,
        9.8176e-02, 1.4419e-01, 6.5709e-02, 9.4332e-02, 9.1464e-02, 5.3936e-01,
        1.9706e-01, 9.3241e-01, 7.0064e+00, 1.0686e+00, 1.2915e+01, 5.2543e-01,
        1.9819e+01, 8.2984e-01, 2.1248e+00, 5.9531e-01, 1.6093e-01, 7.5339e-01,
        3.3683e-01, 5.9840e+00, 3.7896e-01, 1.3889e+00, 8.2055e-02, 1.1973e+00,
        6.0754e-01, 5.1857e+00, 8.3666e+01, 1.1798e+00, 1.0780e-02, 9.7726e+00,
        7.3878e-03, 7.4272e-02], device='cuda:0')
Outer loop valEpocw Maximum [28/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 578.3
model_train val_loss valEpocw [28/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 115.9
model_train val_loss valEpocw [28/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 411.2
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.7109197  97.44398826 93.38336521 98.03243138 98.61478296 97.55607266
 98.3138607  95.13651149 98.05192432 97.03098159 98.62452943 98.97905727
 99.4566343  95.42768728 97.46835443 97.38429113 96.41451737 97.76196684
 98.8718461  98.44787466 98.64280406 99.31896541 99.50658496 99.07043043
 95.24981421 96.87016484 94.44573044 97.06387593 98.06776233 98.2724382
 98.44178312 98.59650833 96.9907774  98.58067031 98.72443075 98.83286022
 97.74003728 98.43447326 98.9693108  93.3821469  98.21395938 96.28415833
 98.64280406 98.06167079 98.84138838 97.79729779 98.24563541 98.69153641
 98.16888196 98.74757861 98.79874758 98.67448009 99.03753609 98.35406489
 98.73783214 97.75831191 93.51737917 96.89209439 96.75564382 97.67912184
 97.36845311 98.75488846 97.73272743 97.48541075 98.80362081 97.6316078
 98.662297   95.96861637 99.17276836 98.28340298 99.81359876 97.48175583
 99.09357829 96.13065143 98.8998672  99.22271902 99.735627   99.39693717
 99.85014802 99.17885991]
Accuracy th:0.7 is [85.30475993 97.36845311 92.67796445 97.89232587 98.46858591 97.30266444
 98.29680438 95.10849039 97.99953704 96.87747469 98.56361399 98.9693108
 99.44079629 95.33509582 97.44033333 97.35870664 96.35969347 97.69374155
 98.80727574 98.41254371 98.46614929 99.29825416 99.51633143 98.92179676
 95.23763112 96.76417198 94.28004045 96.95057321 98.04217785 98.2297974
 98.23710725 98.58432524 96.60701015 98.49782532 98.58067031 98.6208745
 97.5475445  98.26391004 98.86331794 93.16772456 98.08969189 95.82241932
 98.37477614 98.00928351 98.85844471 97.93618499 98.19812137 98.65986038
 98.05923417 98.71712089 98.67935332 98.62818435 99.02169808 98.43081834
 98.72199413 97.66084721 92.93015436 96.8372705  96.61919324 97.5195234
 97.46957274 98.70615611 97.49759384 97.38429113 98.66351531 97.53657972
 98.55508583 95.96008821 99.22271902 98.38208599 99.81603538 97.31972076
 99.1362191  95.99054592 98.78534618 99.20078946 99.70882421 99.29825416
 99.8464931  99.16545851]
Avg Prec: is [96.46889012 35.62436587 72.32803394 66.8465343  78.25208506 64.06572509
 74.51975928 46.29780723 56.41458669 52.57269555 33.16720429 55.25283756
 25.40110131 29.05136349 31.89787988 57.70604993 29.44880898 42.56660106
 48.39572236 40.38655392 63.385104   51.24804208 89.63568201 83.66433717
 25.62048599 34.13926214 38.40478328 39.86187027 26.11458957 38.3103656
 74.04357903 37.40877049 58.56584171 62.10387981 73.24417279 79.43473175
 59.37382796 74.58717958 87.58458543 46.8938722  50.90431218 83.95825791
 85.43561916 81.51899608 89.48445366 90.72591446 40.54897657 34.37341664
 42.95360996 46.1572093  63.60106088 40.4330183  25.2349519  72.92954436
 28.26131557 46.51914862 74.68773046 58.52648883 47.75127532 60.71556765
 94.77798532 83.85693012 74.7033759  50.89226576 60.25604939 44.32756425
 62.03689672 25.29616293 76.6160944  68.67667642 12.74099321 72.30848611
 85.52352135 52.77088444 91.61098896 93.36193142 86.72571175 91.48341034
 40.68766304 29.72823032]
Accuracy th:0.5 is [45.60738782 97.2137279  71.0883152  97.02489005 97.26733349 75.75321938
 76.08825429 75.69108563 77.01904217 96.4608131  77.34920384 98.52097319
 99.41399349 80.23415894 76.93132394 96.56680596 96.29512311 76.63649322
 98.65376884 98.30776915 80.38644753 77.79144991 98.38695922 81.5937915
 82.46853718 96.65086926 94.0778012  76.38673993 98.01293844 77.2468659
 97.30875598 98.57457877 96.36213009 98.02024829 88.26159525 76.83385924
 76.71324667 91.43772615 97.11504489 74.06586177 79.1839768  92.05906361
 76.03830363 75.67037439 96.9627563  93.87434364 98.02877645 98.57336046
 97.77414993 90.03910771 88.94141153 98.55508583 98.99976852 76.26856398
 98.70615611 76.74857762 71.60244149 93.47595668 96.24273583 96.9067141
 89.79300934 97.17717864 94.52248389 76.67182417 98.42838172 77.25539406
 98.20786784 75.87870518 77.69520352 97.55972759 78.24344245 95.99054592
 77.60383036 95.45083515 76.01881069 83.44318417 89.51158002 77.30168979
 78.27877341 99.14718388]
Accuracy th:0.7 is [45.65490187 97.2137279  71.0883152  97.02489005 97.26733349 75.75321938
 76.08825429 76.02368392 77.01904217 96.4742145  77.34920384 98.52097319
 99.41399349 80.7580317  76.93132394 96.56680596 96.29512311 76.63649322
 98.65376884 98.30776915 80.81041898 77.79144991 98.38695922 82.60620606
 83.27383926 96.65086926 94.0778012  76.38673993 98.01293844 77.2468659
 97.30875598 98.57457877 96.36213009 98.02024829 88.45165142 76.83385924
 76.71324667 91.70088084 97.11504489 74.06586177 79.80531426 92.05906361
 76.03830363 75.67037439 96.9627563  93.87434364 98.02877645 98.57336046
 97.97029763 90.58369172 89.12903108 98.55508583 98.99976852 76.26856398
 98.70615611 76.74857762 71.60244149 93.97180834 96.24273583 96.9067141
 89.79300934 97.17717864 94.64187814 76.67182417 98.42838172 77.25539406
 98.20786784 75.87870518 77.69520352 97.55972759 78.27024525 95.99054592
 77.74759079 95.45083515 76.01881069 83.59181784 89.78569949 77.30168979
 78.27877341 99.14718388]
Avg Prec: is [55.77505832  3.05013734 11.26314582  3.29902747  2.22874329  3.9033865
  3.40459817  5.52452282  2.55095578  3.82916073  1.53702805  1.65996736
  0.64541163  5.11243308  2.66323689  3.14783961  3.67442654  2.62953463
  1.41160028  1.73770453  2.07460224  0.90762638  1.83551693  2.40205304
  5.05044243  3.62718459  6.46572973  3.37718653  2.04806104  1.81735749
  2.68976003  1.31462247  3.79336721  1.66228903  2.39781759  2.42525706
  3.01781989  2.54019013  2.87732233  7.32269715  2.29060731  8.36095315
  3.54916236  4.16548125  3.33436095  6.36296703  2.08351039  1.52787324
  2.17733534  1.62989236  1.7665167   1.56909764  1.01795491  3.03640721
  1.28151645  2.72101841 11.27322491  3.57992201  3.99053843  2.76538677
 10.89609591  2.15453624  3.68680115  3.0067115   1.56994453  2.42374179
  1.8158083   4.2097278   1.2624586   2.34617987  0.21270503  3.39338615
  1.8579283   4.53170165  3.9789007   3.0137234   0.84078035  1.85783464
  0.13397282  0.732444  ]
mAP score regular 58.04, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [88.15805865 97.40389167 93.1484665  98.32324289 98.95607544 97.64307248
 98.42290156 95.12419962 98.15133169 97.11239006 98.67204823 99.04576824
 99.36965892 95.28116202 97.42880634 97.34409647 96.42225378 97.78010315
 98.96105838 98.38054663 98.88382291 99.34225278 99.58143359 99.29242345
 95.35839749 96.84580312 94.43406333 97.30174154 97.91962528 98.15133169
 98.75426664 98.67952263 96.93549593 98.7343349  98.89378877 99.12798665
 98.03921569 98.35314049 99.11802078 93.26307397 97.96945462 92.83703316
 97.2892842  95.9712983  96.71126392 93.93078705 98.45279916 98.79662157
 98.09402795 98.79163864 98.93365224 98.63467623 98.89378877 98.34815756
 98.7642325  97.78757755 90.64205098 97.0650522  96.46959165 97.67297008
 92.34621422 98.78167277 97.30672447 97.48611007 98.7941301  97.56085407
 98.63467623 95.85419937 98.65460797 98.18621222 99.81812293 97.47614421
 98.34068316 95.86914817 97.3565538  97.16720233 99.15290131 98.6296933
 99.82310586 99.15040985]
Accuracy th:0.7 is [86.97959489 97.42133194 92.87689663 98.25099036 98.88133144 97.46119541
 98.51508583 95.38580362 98.13887436 96.95542766 98.6371677  99.04327678
 99.38211625 95.15160575 97.42880634 97.34409647 96.35498418 97.76515435
 98.92866931 98.40047836 98.75177517 99.33976132 99.62627999 99.17781598
 95.46303909 96.71624685 94.53123054 97.12235593 97.85733862 98.24102449
 98.5997957  98.69447144 96.63402845 98.71440317 98.82901064 98.97102424
 97.93955702 98.20365249 98.99843038 93.1559409  97.97692902 93.25061664
 97.45870394 96.40481351 96.92303859 94.5187732  98.38802103 98.82153624
 98.04170715 98.77668984 98.8016045  98.63965917 98.89877171 98.4353589
 98.72935197 97.79505195 90.98089045 97.16720233 96.49201485 97.50355034
 92.73986596 98.78416424 97.3191818  97.52348207 98.6595909  97.62064928
 98.55993223 95.7993871  98.78167277 98.28587089 99.81563146 97.62812368
 98.45529063 95.90901164 97.47614421 97.3191818  99.20273065 98.60228717
 99.82559733 99.17034158]
Avg Prec: is [96.48014542 34.41780401 70.7605013  74.04354834 77.16291193 65.15316887
 80.55159385 47.20148904 62.45488106 55.88772394 38.7598585  57.5729387
 24.24047637 30.72147629 33.50097339 62.57640531 32.90075693 45.62352228
 49.16759434 37.85896346 70.33424344 57.47332505 91.96202465 88.09095089
 24.67048298 36.17821874 33.14413133 46.07947239 27.552849   37.64973065
 77.69015191 36.5520447  56.3184553  65.22012901 74.47280504 82.22191548
 60.02411147 77.35405994 89.70854648 44.40048067 37.67449404 49.8742967
 50.13910938 39.06507486 29.91864728 50.27196446 40.3020026  29.72425078
 40.61124459 42.41170479 69.52792792 36.5067029  25.71082398 75.86216963
 30.26235341 42.73678368 56.58921801 58.279573   40.64996388 63.39047881
 64.48412635 86.39975494 67.49339867 53.03453596 62.09513408 39.61549299
 61.95536566 26.15057281 41.78818271 65.52742169  9.93661457 74.49887847
 56.08571899 45.46697968 63.16601688 50.60787922 12.59337921 51.18625158
  2.31470213 22.56756518]
Accuracy th:0.5 is [45.26995042 97.22450607 69.18304806 96.96290206 97.90716795 74.47990632
 74.60198819 73.8146847  76.08939383 96.41976231 76.2189501  98.5325261
 99.34972718 77.75120213 76.17410369 96.31262924 96.21047911 75.58860901
 98.78167277 98.34068316 78.52355682 76.85925704 98.31327703 83.06051773
 78.24949548 96.52938685 94.3393876  75.68577622 97.81747515 76.18656103
 97.52597354 98.67204823 96.39983058 98.18870369 89.55826295 75.80785809
 75.81034955 92.73986596 97.0276802  73.07970202 76.98383038 92.37362035
 74.9308618  74.62441139 97.03764606 94.02795426 98.18621222 98.77668984
 97.94952288 90.1238259  87.47041383 98.55993223 98.87385704 74.92089593
 98.6969629  75.61850661 69.91055634 94.41662307 96.16314124 96.78102499
 90.13379176 97.04761193 94.77041134 75.68577622 98.32075143 76.54533224
 98.13139996 74.8860154  76.91656078 97.53593941 77.28031492 96.07843137
 76.72720931 95.44559882 74.8336946  84.48065376 91.5464534  76.25881356
 77.36004186 99.15040985]
Accuracy th:0.7 is [45.50663976 97.22450607 69.18304806 96.96290206 97.90716795 74.47990632
 74.60198819 74.02396791 76.08939383 96.41976231 76.2189501  98.5325261
 99.34972718 78.13737948 76.17659516 96.31262924 96.21047911 75.58860901
 98.78167277 98.34068316 78.86239629 76.85925704 98.31327703 83.80795774
 78.89727683 96.52938685 94.3393876  75.68577622 97.81747515 76.18656103
 97.52597354 98.67204823 96.39983058 98.18870369 89.69778509 75.80785809
 75.81034955 92.90430276 97.0276802  73.07970202 77.50703839 92.37362035
 74.9308618  74.62441139 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 90.47761417 87.69962877 98.55993223 98.87385704 74.92089593
 98.6969629  75.61850661 69.91055634 94.78037721 96.16314124 96.78102499
 90.13379176 97.04761193 94.88751028 75.69075915 98.32075143 76.54533224
 98.13139996 74.8860154  76.91656078 97.53593941 77.28031492 96.07843137
 76.80444478 95.44559882 74.8336946  84.68495403 91.78065127 76.25881356
 77.36004186 99.15040985]
Avg Prec: is [54.62004836  3.73711792 14.92919919  4.55179026  1.49831707  4.28132222
 11.62975344  8.74192052  7.35485574  5.17143216  2.28658237  5.22757527
  1.56278428  5.93177854  3.09662423  3.96512962 25.59268128  6.33357869
  1.53142681  2.61646308  3.63198186  1.42866332  1.2316228   5.77458835
  5.76384555 11.56384688  8.18107644  4.52141962  3.88783892  6.88269104
  2.3360041   0.86625426  3.13318165  1.09074142  1.71138764  2.43925077
  2.04003401  2.23868674  2.25911274  6.21256607  1.73567834  6.03141214
  2.2189939   2.74306126  2.37447143  4.86785338  1.75421971  1.02439214
  1.3993367   1.16735858  1.18232     0.96723281  0.74128576  2.33754993
  0.860394    1.8322733  10.09227701  2.96228015  3.86366937  2.78759711
  7.88095233  2.05530448  3.22870405  2.57820038  1.35380712  1.85915582
  1.57916255  3.47127926  1.06933926  2.20617366  0.19512306  3.18059458
  1.56269432  4.0013498   3.52118898  2.31983447  0.65441027  1.53146249
  0.12614714  0.58520932]
mAP score regular 51.51, mAP score EMA 4.37
Train_data_mAP: current_mAP = 58.04, highest_mAP = 58.04
Val_data_mAP: current_mAP = 51.51, highest_mAP = 51.91
tensor([2.3844e-02, 1.7801e-03, 2.9989e-02, 1.9975e-03, 9.2470e-04, 2.3846e-03,
        4.2480e-04, 1.3001e-03, 4.4188e-03, 1.5269e-03, 3.7328e-04, 9.2168e-04,
        2.4240e-04, 3.2217e-03, 4.0729e-03, 4.4830e-03, 6.0012e-03, 5.2814e-03,
        4.2426e-03, 4.5008e-03, 7.4981e-05, 4.8514e-04, 1.1376e-03, 7.6146e-04,
        3.3227e-03, 7.0153e-04, 1.3735e-02, 6.2847e-04, 4.1374e-04, 1.2233e-03,
        1.2949e-03, 4.3122e-04, 1.4324e-02, 1.4587e-04, 1.1925e-03, 8.8046e-03,
        7.2653e-04, 1.9903e-03, 4.7344e-03, 1.2700e-02, 1.5906e-01, 5.0595e-01,
        8.4640e-01, 7.4511e-01, 9.8114e-01, 9.0379e-01, 5.7522e-04, 7.8433e-04,
        1.4624e-03, 1.2697e-03, 2.1261e-04, 8.2551e-04, 1.2457e-03, 1.9853e-03,
        6.0450e-04, 3.1066e-03, 1.2366e-01, 7.0819e-03, 8.3690e-02, 1.5288e-03,
        8.7976e-01, 2.3732e-03, 1.1792e-01, 1.1179e-02, 2.1694e-02, 4.0934e-03,
        3.2017e-02, 8.4640e-03, 5.2952e-01, 2.5153e-02, 8.0123e-04, 1.9390e-03,
        3.6825e-01, 6.7659e-02, 9.5180e-01, 9.9997e-01, 1.0000e+00, 9.9997e-01,
        6.4602e-01, 4.7756e-02], device='cuda:0')
Sum Train Loss:  tensor([9.4709e-01, 2.0244e-02, 7.0365e-01, 1.6840e-02, 3.0776e-03, 2.2237e-02,
        1.7689e-03, 3.6793e-02, 1.8627e-02, 1.2522e-02, 5.0235e-04, 3.8645e-03,
        1.6640e-04, 3.1611e-02, 5.6267e-02, 8.8031e-02, 9.6991e-02, 3.7383e-02,
        6.4511e-03, 5.1945e-02, 1.4940e-04, 3.5209e-03, 4.1317e-03, 1.4536e-03,
        4.3894e-02, 8.8693e-03, 3.1289e-01, 7.3824e-03, 4.9913e-03, 7.4345e-03,
        1.6471e-02, 4.3896e-04, 1.1055e-01, 9.4847e-04, 3.3835e-03, 2.8335e-02,
        6.1153e-03, 2.7979e-03, 7.9183e-03, 1.8799e-01, 1.2310e+00, 5.1989e+00,
        5.6212e+00, 4.2392e+00, 4.8503e+00, 1.1923e+01, 4.0960e-03, 6.1910e-03,
        1.3601e-02, 1.5724e-03, 8.9676e-04, 4.1525e-03, 1.7142e-03, 1.2023e-02,
        7.0026e-04, 2.0640e-02, 3.7095e+00, 8.6339e-02, 7.5719e-01, 1.4643e-02,
        4.5832e+00, 8.9051e-03, 1.1160e+00, 9.0789e-02, 1.0779e-01, 3.9184e-02,
        3.8036e-02, 1.1619e-01, 2.6612e+00, 1.7905e-01, 4.2649e-04, 1.7305e-02,
        2.1170e+00, 1.6624e+00, 3.3272e+00, 2.7253e+00, 1.7216e+00, 4.7723e+00,
        2.9433e-02, 8.6241e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [29/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 66.0
Sum Train Loss:  tensor([7.9598e-01, 1.4021e-02, 5.9844e-01, 1.4448e-02, 4.7541e-03, 2.2023e-02,
        3.4353e-03, 2.7152e-02, 2.4308e-02, 1.6525e-02, 1.5366e-03, 2.7983e-03,
        1.0914e-03, 8.3741e-02, 5.9642e-02, 3.8779e-02, 2.6605e-02, 8.1392e-02,
        2.3113e-02, 2.6877e-02, 6.1094e-04, 8.3728e-04, 2.9275e-03, 1.2201e-03,
        6.7623e-02, 9.3047e-03, 2.2993e-01, 1.0490e-02, 1.8389e-03, 1.0929e-02,
        3.5653e-03, 1.6673e-03, 2.1261e-01, 1.8256e-04, 1.6856e-03, 6.7547e-02,
        5.4840e-03, 8.6095e-03, 1.4399e-02, 1.8213e-01, 2.1181e+00, 4.5128e+00,
        1.3617e+00, 2.6890e+00, 4.8942e+00, 1.8310e+00, 3.4606e-03, 1.5535e-03,
        1.2055e-02, 6.0233e-03, 3.0117e-04, 6.4541e-03, 8.7963e-03, 1.5887e-02,
        1.1731e-03, 2.4938e-02, 1.6374e+00, 8.7870e-02, 9.5028e-01, 2.0908e-02,
        6.0102e+00, 3.6554e-03, 8.6569e-01, 1.3338e-01, 1.5447e-01, 1.0921e-02,
        1.5185e-01, 6.8564e-02, 1.5240e+00, 1.7223e-01, 5.3150e-05, 9.0286e-03,
        2.4007e+00, 7.8687e-01, 9.0426e+00, 8.5970e+00, 4.4214e-01, 2.9075e+00,
        4.7331e-02, 5.1169e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [29/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 56.3
Sum Train Loss:  tensor([8.3723e-01, 1.9920e-02, 5.9754e-01, 1.3312e-02, 9.9132e-03, 2.9237e-02,
        9.7689e-04, 2.7080e-02, 1.4152e-02, 1.5942e-02, 1.5789e-03, 4.0376e-03,
        1.1695e-04, 7.6064e-02, 5.0446e-02, 1.1497e-02, 1.0347e-01, 6.0256e-02,
        4.5172e-02, 8.5492e-03, 1.3862e-04, 1.5276e-03, 9.8556e-04, 1.0088e-03,
        5.7504e-02, 9.5775e-03, 3.5546e-01, 9.3575e-03, 2.6963e-03, 1.9835e-02,
        4.7429e-03, 2.0372e-03, 2.0368e-01, 1.2460e-03, 9.8101e-03, 2.6973e-02,
        8.8169e-03, 3.9851e-03, 2.1108e-02, 2.7408e-01, 5.1546e-01, 8.0525e+00,
        5.9217e+00, 3.6762e+00, 3.4422e+00, 9.0230e+00, 2.8833e-03, 9.3682e-03,
        1.7300e-02, 1.0948e-02, 1.9393e-04, 1.2477e-03, 4.3459e-03, 2.3808e-02,
        1.5692e-03, 2.6172e-02, 3.0409e+00, 6.6352e-02, 1.1142e+00, 2.0533e-02,
        3.6652e+00, 1.9975e-02, 1.3817e+00, 8.0332e-02, 6.0321e-02, 7.5370e-02,
        8.0736e-02, 8.5707e-02, 1.8254e+00, 9.7327e-02, 1.2803e-04, 3.2831e-02,
        1.5494e+00, 1.5202e+00, 3.9327e+00, 4.9664e+00, 2.9815e+00, 2.0697e+00,
        4.5144e-02, 1.3828e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [29/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 62.5
Sum Train Loss:  tensor([1.0100e+00, 2.5584e-02, 7.0648e-01, 9.4482e-03, 1.5886e-03, 1.7473e-02,
        2.9678e-03, 2.0839e-02, 1.6323e-02, 3.0982e-02, 4.0542e-03, 6.5487e-03,
        1.0369e-03, 7.4125e-02, 7.8662e-02, 6.4226e-02, 1.3124e-01, 7.9110e-02,
        1.2542e-02, 1.1993e-02, 1.2674e-04, 1.6788e-03, 4.4796e-03, 2.0312e-03,
        6.4145e-02, 6.1439e-03, 3.3206e-01, 6.1982e-03, 2.5983e-03, 6.6512e-03,
        5.9205e-03, 2.8698e-03, 8.8110e-02, 4.2734e-04, 6.3597e-03, 1.7204e-02,
        4.3907e-03, 2.1499e-02, 4.7671e-03, 2.1719e-01, 1.2897e+00, 9.7978e+00,
        1.7631e+00, 4.1407e+00, 1.0220e+01, 1.2303e+01, 9.5212e-03, 2.4225e-03,
        1.0850e-02, 2.8378e-03, 3.4634e-04, 3.8908e-03, 1.6253e-03, 1.7575e-02,
        7.9734e-03, 2.3006e-02, 1.9382e+00, 1.0334e-01, 1.1162e+00, 1.3663e-02,
        5.3359e+00, 8.5562e-03, 8.2119e-01, 1.3795e-01, 4.3389e-02, 2.1465e-02,
        2.8474e-01, 1.0749e-01, 1.3972e+00, 4.6585e-02, 2.1543e-03, 1.0814e-02,
        4.0886e-01, 1.1976e+00, 9.8296e+00, 2.7280e+00, 5.6432e-01, 1.1945e+00,
        9.3423e-02, 3.0709e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [29/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 70.4
Sum Train Loss:  tensor([8.7395e-01, 1.6187e-02, 5.7546e-01, 1.4160e-02, 2.7654e-03, 1.3092e-02,
        5.0862e-03, 2.1413e-02, 6.2392e-02, 8.7511e-03, 1.8518e-03, 6.9337e-03,
        1.6768e-04, 8.0814e-02, 4.5415e-02, 3.3944e-02, 1.3610e-01, 8.1406e-02,
        4.7141e-03, 1.3041e-02, 2.4419e-04, 2.7161e-04, 9.2327e-04, 9.6403e-04,
        6.8131e-02, 1.3919e-02, 2.7166e-01, 4.7554e-03, 8.3858e-03, 7.0370e-03,
        6.5952e-03, 1.3182e-03, 1.0696e-01, 1.0319e-03, 7.3280e-03, 6.5837e-02,
        3.5775e-03, 6.5119e-03, 1.5606e-02, 3.6244e-01, 8.8782e-01, 1.1269e+01,
        6.1995e+00, 9.0032e+00, 1.1819e+01, 2.0968e+01, 7.3542e-03, 2.4810e-03,
        1.2602e-02, 3.1815e-03, 8.3948e-04, 7.9468e-03, 1.7355e-03, 1.7017e-02,
        4.1155e-03, 4.1124e-02, 2.2412e+00, 6.4710e-02, 1.3684e+00, 2.9723e-02,
        5.9126e+00, 2.2175e-02, 9.7644e-01, 1.1635e-01, 3.1263e-02, 1.1655e-02,
        6.8910e-02, 1.4638e-01, 2.1093e+00, 1.3118e-01, 3.0252e-04, 2.6884e-02,
        1.4671e+00, 9.1213e-01, 7.7974e+00, 5.6499e-01, 4.7581e-01, 7.0786e+00,
        1.6958e-01, 1.1834e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [29/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 95.0
Sum Train Loss:  tensor([7.2152e-01, 2.7549e-02, 4.2870e-01, 7.2035e-03, 2.8919e-03, 7.2948e-03,
        1.2689e-03, 1.4307e-02, 2.0353e-02, 1.0599e-02, 3.8404e-03, 1.6936e-03,
        1.4080e-04, 2.6093e-02, 9.6077e-02, 7.1762e-02, 6.8288e-02, 1.2646e-02,
        3.8085e-02, 3.8401e-02, 1.0714e-04, 5.3003e-04, 7.0803e-03, 2.8178e-03,
        3.3266e-02, 6.6146e-03, 3.9338e-01, 8.0548e-03, 5.0504e-03, 1.5564e-02,
        3.1151e-03, 3.7624e-04, 1.6666e-01, 7.0587e-04, 6.6521e-03, 1.6073e-02,
        1.1855e-02, 5.3723e-03, 2.8899e-02, 1.9405e-01, 1.0282e+00, 1.0529e+01,
        4.1874e+00, 5.9926e+00, 8.4402e+00, 3.0976e+00, 5.8011e-03, 3.7597e-03,
        1.3608e-02, 9.1680e-03, 4.4993e-04, 8.2381e-03, 2.1933e-03, 1.2873e-02,
        4.2815e-03, 2.5848e-02, 2.9147e+00, 5.2932e-02, 1.1355e+00, 2.4508e-02,
        4.2299e+00, 1.0831e-02, 9.5581e-01, 1.2972e-01, 1.8218e-01, 1.7402e-02,
        2.3741e-01, 2.0211e-01, 1.8102e+00, 1.0891e-01, 8.8171e-05, 9.8435e-03,
        1.7350e-01, 9.2609e-01, 4.8863e+00, 2.9290e+00, 1.1106e+00, 1.0795e+00,
        1.9026e+00, 2.3095e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [29/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 61.1
Sum Train Loss:  tensor([8.9752e-01, 2.4306e-02, 8.0967e-01, 6.7586e-03, 6.3885e-03, 2.9555e-02,
        1.2501e-03, 2.4002e-02, 4.7916e-02, 1.6028e-02, 3.7500e-03, 6.5674e-03,
        2.0195e-04, 4.6664e-02, 6.2956e-02, 6.4199e-02, 8.3108e-02, 2.8517e-02,
        4.8129e-03, 1.2437e-02, 2.9459e-04, 2.3493e-03, 1.0333e-03, 2.6395e-03,
        8.8268e-02, 6.9412e-03, 1.9629e-01, 8.7598e-03, 3.6587e-03, 8.9582e-03,
        1.0664e-02, 1.2633e-03, 9.8386e-02, 3.0345e-03, 3.5861e-03, 3.5171e-02,
        1.0293e-02, 2.2757e-02, 5.1351e-03, 1.9603e-01, 4.3324e-01, 3.1302e+00,
        1.5584e+00, 3.9644e+00, 2.2329e+00, 6.8082e+00, 5.0066e-03, 2.8538e-03,
        6.6129e-03, 4.4237e-03, 4.8718e-04, 3.1383e-03, 2.0816e-03, 1.6673e-02,
        4.1241e-03, 1.2266e-02, 2.2480e+00, 9.3399e-02, 5.0762e-01, 1.6843e-02,
        2.7592e+00, 8.9012e-03, 7.0073e-01, 9.0981e-02, 8.8004e-02, 3.2635e-02,
        2.5296e-02, 1.0254e-01, 7.6664e-01, 2.4646e-01, 3.9422e-04, 1.6267e-02,
        1.3726e+00, 1.6499e+00, 2.1892e+00, 5.8233e-01, 1.8508e+00, 4.6258e-01,
        7.3750e-02, 1.7709e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [29/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 37.1
Sum_Val Meta Model:  tensor([1.1791e+00, 1.1114e-01, 2.6524e+00, 9.5495e-02, 1.2337e-03, 3.1438e-02,
        1.6274e-03, 2.3134e-02, 2.7527e-02, 1.5039e-02, 2.4179e-03, 1.1423e-02,
        2.1073e-03, 4.8666e-02, 3.3724e-02, 3.7856e-02, 2.8587e+00, 8.3122e-03,
        6.1585e-03, 6.0675e-03, 8.6557e-05, 1.6834e-04, 1.3920e-04, 4.6928e-04,
        9.3940e-02, 1.3618e-02, 3.8362e-01, 2.4497e-03, 4.7646e-03, 1.5387e-02,
        1.9959e-03, 4.1185e-04, 1.0526e-01, 1.4248e-04, 1.4521e-03, 9.5345e-03,
        2.0147e-03, 1.1198e-02, 5.9754e-03, 6.3888e-01, 1.8690e+00, 1.4706e+01,
        7.2661e+00, 1.5144e+01, 1.9303e+01, 1.5015e+01, 7.1176e-04, 4.2007e-03,
        8.1829e-04, 6.0634e-03, 1.1006e-04, 5.6700e-04, 9.0540e-03, 2.0415e-03,
        8.3499e-04, 4.1033e-03, 3.6247e+00, 5.6474e-02, 1.2204e+00, 6.2848e-03,
        8.8367e+00, 2.7141e-02, 7.3203e-01, 4.8295e-02, 1.5729e-02, 1.0030e-02,
        3.1906e-02, 5.6523e-02, 6.2241e+00, 4.8072e-01, 2.5078e-04, 3.5935e-02,
        6.9008e+00, 6.2215e-01, 2.6711e+01, 7.2243e+00, 2.2808e-01, 1.3775e+01,
        7.7298e-02, 2.0561e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.1135e+00, 7.6344e-02, 1.6795e+00, 4.7495e-02, 3.5030e-04, 3.0644e-02,
        1.9012e-03, 2.0799e-02, 2.8194e-02, 1.4876e-02, 2.7242e-03, 1.3162e-02,
        2.3071e-03, 4.7906e-02, 3.4317e-02, 1.4488e-01, 1.6723e+00, 1.7402e-02,
        4.9124e-03, 1.0044e-02, 7.0162e-05, 9.7542e-05, 3.2489e-05, 1.1168e-03,
        8.5985e-02, 1.2552e-02, 3.7456e-01, 3.2516e-03, 9.6366e-03, 1.7084e-02,
        1.0039e-03, 4.7345e-04, 9.3859e-02, 1.3558e-04, 1.0325e-03, 6.9202e-03,
        5.3064e-03, 7.3301e-03, 1.5978e-03, 5.7796e-01, 2.3782e+00, 2.3392e+01,
        6.6869e+00, 1.5952e+01, 2.2709e+01, 1.8798e+01, 3.2920e-04, 4.2549e-03,
        3.2080e-04, 4.7762e-03, 2.4462e-05, 2.4191e-04, 9.1649e-03, 4.3981e-04,
        5.3224e-04, 1.6957e-03, 3.0067e+00, 4.8552e-02, 1.1180e+00, 8.3059e-03,
        1.4874e+01, 1.5613e-02, 5.2574e-01, 6.3801e-02, 9.5072e-03, 1.1072e-02,
        1.9298e-02, 6.4428e-02, 6.7833e+00, 5.8294e-01, 3.1864e-04, 3.6315e-02,
        7.4046e+00, 6.8332e-01, 3.0244e+01, 1.9454e+01, 3.7559e-02, 1.3246e+01,
        1.3817e-01, 2.6084e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.6699e+01, 4.2888e+01, 5.6004e+01, 2.3777e+01, 3.7883e-01, 1.2850e+01,
        4.4756e+00, 1.5998e+01, 6.3805e+00, 9.7425e+00, 7.2982e+00, 1.4281e+01,
        9.5177e+00, 1.4870e+01, 8.4258e+00, 3.2318e+01, 2.7866e+02, 3.2949e+00,
        1.1579e+00, 2.2317e+00, 9.3573e-01, 2.0106e-01, 2.8560e-02, 1.4666e+00,
        2.5878e+01, 1.7892e+01, 2.7271e+01, 5.1738e+00, 2.3291e+01, 1.3965e+01,
        7.7523e-01, 1.0979e+00, 6.5527e+00, 9.2951e-01, 8.6587e-01, 7.8598e-01,
        7.3037e+00, 3.6829e+00, 3.3750e-01, 4.5509e+01, 1.4952e+01, 4.6233e+01,
        7.9003e+00, 2.1409e+01, 2.3146e+01, 2.0799e+01, 5.7229e-01, 5.4249e+00,
        2.1936e-01, 3.7619e+00, 1.1505e-01, 2.9305e-01, 7.3571e+00, 2.2154e-01,
        8.8046e-01, 5.4584e-01, 2.4314e+01, 6.8558e+00, 1.3359e+01, 5.4328e+00,
        1.6907e+01, 6.5787e+00, 4.4583e+00, 5.7071e+00, 4.3824e-01, 2.7048e+00,
        6.0276e-01, 7.6120e+00, 1.2810e+01, 2.3176e+01, 3.9769e-01, 1.8729e+01,
        2.0108e+01, 1.0099e+01, 3.1775e+01, 1.9455e+01, 3.7559e-02, 1.3246e+01,
        2.1387e-01, 5.4619e-01], device='cuda:0')
Outer loop valEpocw Maximum [29/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 158.7
model_train val_loss valEpocw [29/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 194.5
model_train val_loss valEpocw [29/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1174.6
Sum_Val Meta Model:  tensor([3.5880e+00, 9.4949e-02, 3.3914e+00, 5.8328e-02, 1.6077e-03, 3.4715e-01,
        1.5273e-01, 2.9353e-01, 3.7015e-01, 2.1516e-01, 3.5891e-02, 1.0342e+00,
        1.3257e-02, 5.0915e-02, 2.9619e-01, 2.8162e-01, 1.5305e-01, 2.7951e-01,
        1.2696e-01, 4.6752e-01, 1.0335e-05, 5.3161e-05, 7.5530e-04, 1.2252e-02,
        2.0752e-01, 7.1440e-02, 9.1030e-01, 5.3063e-02, 2.4317e-02, 1.0632e-04,
        2.2530e-04, 4.5543e-05, 2.0922e-03, 8.8787e-05, 7.5925e-05, 5.4067e-04,
        2.0547e-02, 2.7558e-04, 2.8211e-04, 2.8585e-02, 1.8296e-02, 2.0578e+00,
        6.5524e-02, 1.4765e-01, 1.7495e-01, 4.1175e+00, 2.0876e-03, 1.5789e-03,
        1.8904e-04, 3.3865e-02, 2.0319e-05, 1.4336e-04, 4.2748e-05, 7.1800e-05,
        2.1025e-04, 3.6620e-02, 2.3641e+00, 8.4645e-02, 9.3610e-01, 1.4453e-02,
        9.2673e-01, 3.4209e-03, 2.1447e-01, 2.7714e-02, 1.2145e-02, 3.4911e-03,
        1.9258e-02, 3.7270e-01, 2.8769e-02, 2.5944e-01, 2.9023e-05, 7.3491e-03,
        3.0698e+00, 4.7894e-01, 5.9552e+00, 3.3354e-01, 8.0584e-02, 3.6281e-01,
        1.6125e-02, 4.6312e-03], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([3.0473e+00, 1.0617e-01, 2.4210e+00, 6.4620e-02, 1.0425e-02, 2.7567e-01,
        1.4785e-01, 1.9434e-01, 2.8434e-01, 1.4835e-01, 2.8480e-02, 2.6268e-01,
        1.1951e-02, 8.6811e-02, 2.0105e-01, 5.1042e-02, 1.3251e-01, 2.5574e-01,
        5.7464e-02, 2.2970e-01, 3.8241e-04, 6.0297e-04, 1.2735e-04, 1.4273e-02,
        1.9896e-01, 6.0641e-02, 8.5346e-01, 4.3249e-02, 3.0369e-02, 4.4127e-03,
        1.4983e-03, 8.1105e-04, 2.5681e-02, 1.8084e-03, 1.7191e-03, 3.2204e-03,
        6.4087e-03, 5.0122e-03, 1.1589e-03, 4.2701e-02, 3.7072e-02, 1.2551e+00,
        2.7296e-01, 1.0424e-01, 6.5213e-02, 1.5181e-01, 1.0440e-03, 1.8291e-03,
        1.2078e-03, 3.4224e-02, 1.5852e-04, 8.7720e-04, 1.3625e-03, 2.8703e-03,
        1.4041e-03, 7.6258e-03, 2.2272e+00, 8.5204e-02, 4.6978e-01, 3.9875e-03,
        7.3809e+00, 6.3691e-03, 8.9507e-02, 1.0793e-02, 2.7904e-03, 3.8457e-03,
        4.7821e-03, 3.6196e-01, 1.6678e-02, 2.1152e-01, 9.4299e-05, 1.5117e-03,
        2.4326e+00, 2.4230e-01, 9.2269e+00, 3.2344e-02, 6.7913e+00, 1.6714e-01,
        5.3128e-03, 1.1112e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.6338e+01, 1.9456e+01, 4.5927e+01, 9.9576e+00, 2.9453e+00, 3.4329e+01,
        6.3299e+01, 3.9819e+01, 2.2978e+01, 2.7849e+01, 1.6865e+01, 6.9224e+01,
        9.1705e+00, 1.0539e+01, 1.6168e+01, 3.8150e+00, 8.9329e+00, 1.6279e+01,
        3.9604e+00, 1.5773e+01, 6.9050e-01, 2.6835e-01, 2.9332e-02, 4.8207e+00,
        2.4806e+01, 1.9494e+01, 3.0696e+01, 1.4326e+01, 1.4289e+01, 9.9619e-01,
        3.0978e-01, 3.8564e-01, 7.9378e-01, 2.1339e+00, 4.1997e-01, 1.8618e-01,
        2.0346e+00, 7.7885e-01, 8.4570e-02, 1.8506e+00, 2.3067e-01, 2.6511e+00,
        3.6593e-01, 1.6233e-01, 6.8711e-02, 1.8329e-01, 4.1256e-01, 5.6764e-01,
        2.0646e-01, 6.7488e+00, 1.4029e-01, 2.9548e-01, 3.1504e-01, 4.2353e-01,
        5.3180e-01, 8.9099e-01, 1.2536e+01, 4.4943e+00, 4.4191e+00, 6.8409e-01,
        9.0095e+00, 8.1087e-01, 6.0755e-01, 4.2605e-01, 7.2524e-02, 3.1713e-01,
        9.2099e-02, 1.9031e+01, 3.5855e-02, 5.0069e+00, 3.0745e-02, 2.3756e-01,
        7.3017e+00, 2.2788e+00, 1.0296e+01, 3.2352e-02, 6.7914e+00, 1.6718e-01,
        9.1408e-03, 1.5535e-01], device='cuda:0')
Outer loop valEpocw Maximum [29/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 34.8
model_train val_loss valEpocw [29/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 41.0
model_train val_loss valEpocw [29/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 722.1
Sum_Val Meta Model:  tensor([2.1275e+00, 8.7937e-03, 4.0628e-01, 8.4830e-03, 2.1965e-03, 1.3088e-02,
        2.0763e-03, 2.4964e-02, 1.9319e-02, 5.9365e-03, 1.2161e-03, 3.4337e-03,
        2.2715e-04, 8.7529e-02, 2.5886e-02, 2.3930e-02, 7.4805e-02, 4.8959e-02,
        1.2093e-02, 1.9133e-02, 3.2183e-04, 1.4427e-03, 2.6967e-02, 3.5839e-03,
        4.0982e-02, 3.1938e-03, 3.8401e-01, 7.9359e-03, 8.9987e-04, 1.3912e-02,
        1.2498e-02, 6.1930e-03, 1.8976e-01, 1.0606e-04, 8.4289e-03, 5.9153e-02,
        2.3827e-02, 4.5093e-02, 3.4228e-03, 4.1087e-01, 4.9169e+00, 3.3127e+01,
        5.6360e+01, 6.7800e+01, 5.3364e+01, 6.2715e+01, 2.2304e-02, 3.1571e-02,
        1.2093e-01, 6.0934e-02, 1.4737e-02, 5.6727e-02, 1.3855e-01, 5.0962e-02,
        7.6887e-02, 4.4350e-01, 2.4379e+01, 1.1511e-01, 7.7684e-01, 6.7087e-03,
        8.7576e+01, 7.4703e-03, 1.4194e+00, 1.9380e-01, 3.6147e-01, 7.5717e-03,
        3.2694e-01, 1.1397e-01, 2.4079e+00, 7.0161e-02, 4.5872e-04, 2.3660e-02,
        1.3685e+00, 2.8733e+00, 1.2446e+00, 1.5732e+01, 1.0940e+01, 8.9055e-01,
        1.5255e-01, 3.7335e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.9272e-01, 1.2223e-03, 1.7272e-01, 7.4342e-04, 4.9129e-05, 2.9024e-04,
        3.4548e-05, 2.7526e-02, 1.8471e-03, 1.6884e-04, 6.0008e-05, 5.1056e-05,
        7.4891e-06, 6.0389e-02, 1.9996e-03, 8.5997e-03, 1.7476e-02, 7.7455e-04,
        7.9626e-04, 4.5652e-04, 1.1676e-05, 1.8375e-05, 8.2027e-06, 3.9472e-05,
        2.0058e-02, 1.5728e-03, 3.5418e-01, 5.5868e-03, 8.7578e-04, 4.9154e-04,
        8.6266e-04, 5.7878e-03, 1.6360e-01, 3.0485e-05, 1.2621e-03, 5.1429e-03,
        1.4437e-02, 3.5450e-02, 2.4090e-04, 4.7273e-01, 3.6054e+00, 4.3664e+01,
        5.4003e+01, 7.0888e+01, 1.0124e+02, 7.6504e+01, 8.8722e-03, 9.6888e-03,
        9.3606e-02, 2.0613e-02, 7.7008e-03, 6.0763e-02, 1.2255e-01, 4.8926e-02,
        5.2736e-02, 3.8655e-01, 1.2723e+01, 1.3202e-01, 6.4731e-01, 3.7953e-03,
        1.0959e+02, 2.0604e-03, 1.0248e+00, 1.4578e-01, 3.0570e-01, 2.5162e-02,
        3.2457e-01, 1.2809e-01, 3.2146e+00, 3.1441e-02, 1.7966e-04, 2.1455e-02,
        1.6357e+00, 2.6749e+00, 5.9074e+00, 3.8361e+01, 1.3792e+01, 1.6007e+00,
        1.4801e-01, 2.0703e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.0756e+01, 5.4492e-01, 5.1119e+00, 2.8683e-01, 3.9096e-02, 8.9535e-02,
        5.6971e-02, 1.5827e+01, 3.2658e-01, 8.0871e-02, 1.0936e-01, 3.7510e-02,
        2.2405e-02, 1.5086e+01, 3.8032e-01, 1.5018e+00, 2.2236e+00, 9.7423e-02,
        1.4082e-01, 7.7595e-02, 1.0334e-01, 2.5942e-02, 4.3972e-03, 3.6832e-02,
        4.7567e+00, 1.6134e+00, 2.5119e+01, 6.2102e+00, 1.5417e+00, 2.5638e-01,
        4.4317e-01, 8.0251e+00, 8.6999e+00, 1.3059e-01, 7.3779e-01, 4.6422e-01,
        1.2556e+01, 1.2298e+01, 3.9129e-02, 3.6582e+01, 2.3328e+01, 8.4505e+01,
        6.3898e+01, 9.5544e+01, 1.0379e+02, 8.6387e+01, 9.5331e+00, 7.4502e+00,
        3.7660e+01, 9.5162e+00, 2.0547e+01, 4.5892e+01, 6.2111e+01, 1.7195e+01,
        5.5299e+01, 8.4708e+01, 9.8880e+01, 1.4563e+01, 7.7270e+00, 1.6960e+00,
        1.2542e+02, 6.6179e-01, 8.9328e+00, 1.1088e+01, 1.3195e+01, 4.7077e+00,
        9.6448e+00, 1.3387e+01, 6.8537e+00, 1.2786e+00, 1.6845e-01, 8.6606e+00,
        5.2113e+00, 3.7575e+01, 6.3299e+00, 3.8363e+01, 1.3792e+01, 1.6008e+00,
        2.4612e-01, 4.2646e-01], device='cuda:0')
Outer loop valEpocw Maximum [29/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 434.5
model_train val_loss valEpocw [29/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 545.1
model_train val_loss valEpocw [29/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1420.2
Sum_Val Meta Model:  tensor([7.5657e+00, 3.8300e-02, 1.6666e+00, 3.2012e-02, 9.8880e-03, 1.1622e-01,
        2.0861e-02, 1.1412e-01, 5.2353e-02, 1.4800e-01, 1.5576e-02, 3.2670e-02,
        2.0567e-03, 2.0202e-01, 2.1839e-01, 6.4045e-02, 9.4961e-02, 7.9988e-02,
        2.3937e-02, 3.9181e-02, 2.5634e-03, 5.4058e-03, 1.1772e-02, 1.1470e-02,
        2.4749e-01, 1.9239e-02, 1.0638e+00, 2.0375e-02, 4.4434e-02, 1.1499e-02,
        1.4795e-02, 2.9560e-03, 3.8901e-01, 5.2386e-02, 1.0330e-01, 2.2764e-01,
        6.1076e-03, 3.5710e-02, 2.3083e-01, 7.5792e-01, 3.7080e+00, 1.7106e+01,
        1.1748e+01, 9.0538e+00, 1.2570e+01, 1.5113e+01, 7.5459e-03, 8.3902e-03,
        2.8391e-02, 1.3327e-02, 4.3842e-03, 7.7953e-03, 8.7517e-03, 1.6993e-01,
        6.3297e-03, 5.8629e-02, 6.9322e+00, 2.2896e-01, 2.1546e+00, 5.3620e-02,
        4.5198e+01, 2.4025e-02, 1.5045e+00, 5.2890e-01, 6.6578e-01, 7.2031e-02,
        8.0307e-01, 1.2542e+00, 1.2422e+00, 2.3922e-01, 2.5650e-03, 3.9661e-02,
        2.3717e+00, 2.0220e+00, 3.1463e+02, 1.7391e+01, 2.4498e+00, 9.7651e+00,
        1.7728e-01, 2.6730e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([2.4192e+00, 1.1821e-02, 9.8611e-01, 8.7574e-03, 7.3193e-03, 9.2633e-02,
        1.9589e-02, 1.0638e-01, 5.2903e-02, 1.2802e-01, 9.7962e-03, 5.0121e-02,
        8.2032e-04, 1.8190e-01, 2.7484e-01, 1.4147e-02, 2.3466e-02, 1.1634e-02,
        1.9509e-03, 2.2807e-03, 2.6142e-04, 2.2349e-04, 1.3058e-04, 2.1183e-03,
        2.3089e-01, 1.0893e-02, 7.7119e-01, 5.8465e-03, 5.2680e-02, 1.0582e-03,
        1.8743e-03, 1.0126e-03, 7.1572e-03, 3.2246e-04, 2.2841e-03, 4.2407e-03,
        2.1942e-03, 1.2744e-03, 1.2060e-03, 6.6901e-02, 1.2999e-01, 2.8060e-01,
        1.7126e-01, 8.9260e-02, 7.7713e-03, 8.0153e-01, 5.6971e-04, 1.3439e-03,
        4.5312e-04, 7.0479e-04, 1.5824e-04, 3.4708e-04, 4.6625e-04, 2.1656e-03,
        1.7739e-03, 6.4046e-03, 1.3631e+00, 2.2896e-02, 1.4220e+00, 7.5162e-03,
        1.6652e+01, 4.4124e-03, 2.9871e-01, 1.8971e-02, 2.5307e-02, 1.6755e-02,
        5.8986e-02, 1.8260e-01, 1.1968e-02, 8.4986e-03, 2.0162e-04, 2.2224e-03,
        1.5650e-02, 6.2505e-01, 4.9686e+01, 6.5002e-01, 4.5029e-03, 5.1737e+00,
        1.2930e-02, 3.8084e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.8465e+01, 1.5689e+00, 1.7139e+01, 1.0161e+00, 1.4014e+00, 8.9548e+00,
        6.4678e+00, 1.6764e+01, 3.4582e+00, 1.7247e+01, 3.7385e+00, 9.6087e+00,
        4.4498e-01, 1.5661e+01, 1.9007e+01, 9.7317e-01, 1.3370e+00, 5.7294e-01,
        1.2899e-01, 1.4542e-01, 2.9807e-01, 6.9839e-02, 2.2464e-02, 5.1379e-01,
        2.3575e+01, 2.3930e+00, 2.3455e+01, 1.3570e+00, 1.7343e+01, 1.6329e-01,
        2.8698e-01, 3.3206e-01, 1.6147e-01, 1.8197e-01, 3.3103e-01, 1.5816e-01,
        4.8367e-01, 1.4038e-01, 5.7567e-02, 2.0421e+00, 6.2212e-01, 5.4965e-01,
        2.2968e-01, 1.3874e-01, 8.2673e-03, 9.8270e-01, 1.5957e-01, 2.9055e-01,
        5.5093e-02, 9.8477e-02, 8.8191e-02, 8.2310e-02, 7.5844e-02, 2.0300e-01,
        4.6497e-01, 5.7105e-01, 6.7327e+00, 8.8858e-01, 1.2007e+01, 8.9827e-01,
        2.0366e+01, 3.9943e-01, 1.7496e+00, 5.4062e-01, 4.4377e-01, 9.6122e-01,
        8.0525e-01, 6.2923e+00, 2.6189e-02, 1.6688e-01, 4.4311e-02, 2.3619e-01,
        4.4400e-02, 4.9102e+00, 5.6674e+01, 6.5031e-01, 4.5030e-03, 5.1759e+00,
        2.6568e-02, 4.4163e-02], device='cuda:0')
Outer loop valEpocw Maximum [29/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 493.4
model_train val_loss valEpocw [29/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 83.3
model_train val_loss valEpocw [29/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 362.2
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.84127874 97.41596715 93.25178787 98.05679755 98.59650833 97.5901853
 98.31264239 95.32047612 98.10431159 96.95179152 98.64036744 98.94738125
 99.42739489 95.41672251 97.48662906 97.42815024 96.44010185 97.77536823
 98.86697287 98.4259451  98.70006457 99.31287387 99.39815548 98.99976852
 95.20961002 96.87503807 94.46522338 97.09067872 98.06654402 98.28340298
 98.43690988 98.55021259 97.08336887 98.57336046 98.68666317 98.86453625
 97.63526273 98.38939584 98.88159257 93.42844264 98.15669887 96.10384864
 98.70250119 98.04705108 98.72686736 98.034868   98.24319879 98.68057163
 98.17984674 98.76463493 98.83286022 98.66351531 99.04484594 98.4539662
 98.73905045 97.72298096 93.2846822  96.97737601 96.66548897 97.61455148
 97.57191067 98.76463493 97.80826257 97.50124877 98.86088132 97.57678391
 98.66595193 95.99420085 99.09114168 98.26878328 99.81359876 97.39647421
 98.92545169 96.23298936 98.93519816 99.21540917 99.74171855 99.50171172
 99.85258464 99.17398667]
Accuracy th:0.7 is [86.01868886 97.33555878 92.4343027  97.97273425 98.51366333 97.41109392
 98.10431159 95.01955386 97.97395256 96.69229176 98.58919847 98.89012073
 99.42008504 95.35093383 97.39891083 97.26977011 96.40111597 97.65110074
 98.87671934 98.39426908 98.59529002 99.28485277 99.29703585 98.83286022
 95.24250436 96.75564382 94.32633618 96.97737601 98.07385388 98.22248754
 98.24685372 98.60381818 96.73127764 98.38817753 98.58310693 98.7463603
 97.3660165  98.19690306 98.66107869 93.02152752 98.05070601 95.45205346
 98.59407171 97.6316078  98.38817753 97.77414993 98.20421291 98.66960685
 98.06898064 98.69762795 98.71590258 98.60503649 99.03997271 98.28827621
 98.7463603  97.58165714 93.27615404 96.88843947 96.50345391 97.5755656
 97.53536141 98.69640964 97.61942471 97.39281929 98.84626162 97.59993177
 98.67326178 95.96617975 98.93154323 98.05801586 99.81725369 97.03098159
 98.68909979 96.06120783 99.06555719 99.17520498 99.72466222 99.48100048
 99.84771141 99.15571204]
Avg Prec: is [96.4741581  35.13801156 72.64514283 65.60305882 77.75965968 63.93164046
 73.96052964 48.1175078  57.44679256 53.1940376  32.12439028 55.27793726
 27.87229061 28.89065263 32.41647964 58.28400109 30.1207143  44.64711892
 49.65006791 39.42498013 61.98544379 51.13980654 89.23680946 82.84908401
 25.79777064 33.39243644 38.77161509 40.34842067 24.94037263 39.66120746
 74.54238377 37.29697541 58.13405555 62.92023887 71.83760379 79.52268962
 59.00303673 75.09029439 87.24637794 47.52448997 50.48453448 84.19023007
 86.00020027 82.96409102 89.89375707 91.12188059 40.75453694 34.70635213
 42.88950778 45.99893977 62.22281906 40.1178365  26.71943468 73.76028465
 26.95861626 44.94255204 75.06151489 59.06797914 47.04445263 59.70479924
 95.23847967 83.2444562  75.65772094 51.39817316 62.11874505 44.82881662
 61.53753046 26.51617005 77.03359695 68.36109091 12.57575865 72.12243417
 85.89771022 53.03779865 92.82124097 93.5874893  87.91086856 91.71701864
 31.26985938 28.97332142]
Accuracy th:0.5 is [45.51966959 97.2137279  70.96161109 97.02489005 97.26733349 75.63626174
 76.02977547 75.48762807 76.90939438 96.4608131  77.15914767 98.52097319
 99.41399349 80.01364506 76.77903534 96.56680596 96.29512311 76.53902852
 98.65376884 98.30776915 80.22197585 77.65012609 98.38695922 82.12131918
 82.65859334 96.65086926 94.0778012  76.22104994 98.01293844 77.06168297
 97.30875598 98.57457877 96.36213009 98.02024829 88.26281356 76.68278895
 76.51588065 91.59001474 97.11504489 73.83681973 79.05239946 92.05906361
 75.93352907 75.5193041  96.9627563  93.87434364 98.02877645 98.57336046
 97.81800904 90.14266395 89.08760858 98.55508583 98.99976852 76.10774723
 98.70615611 76.43669059 71.2272024  93.56367491 96.24273583 96.9067141
 89.79300934 97.17717864 94.68451895 76.50613418 98.42838172 77.03366187
 98.20786784 75.63260682 77.54900647 97.55972759 78.07044261 95.99054592
 77.39549957 95.45083515 75.86286717 83.46998696 89.72722067 77.07021113
 78.12039327 99.14718388]
Accuracy th:0.7 is [45.56718364 97.2137279  70.96161109 97.02489005 97.26733349 75.63626174
 76.02977547 75.81778974 76.90939438 96.4754328  77.15914767 98.52097319
 99.41399349 80.51071503 76.78025365 96.56680596 96.29512311 76.53902852
 98.65376884 98.30776915 80.66544024 77.65012609 98.38695922 83.20805058
 83.58938122 96.65086926 94.0778012  76.22104994 98.01293844 77.06168297
 97.30875598 98.57457877 96.36213009 98.02024829 88.46139789 76.68278895
 76.51588065 91.8629159  97.11504489 73.83681973 79.67373692 92.05906361
 75.93352907 75.5193041  96.9627563  93.87434364 98.02877645 98.57336046
 97.97638918 90.734762   89.28375629 98.55508583 98.99976852 76.10774723
 98.70615611 76.43790889 71.2272024  94.01810407 96.24273583 96.9067141
 89.79300934 97.17717864 94.78076534 76.50735249 98.42838172 77.03366187
 98.20786784 75.63260682 77.54900647 97.55972759 78.10577357 95.99054592
 77.58677404 95.45083515 75.86286717 83.6137474  90.02692462 77.07021113
 78.12039327 99.14718388]
Avg Prec: is [55.94720676  3.13554918 11.25836301  3.44969542  2.21151444  3.76724229
  3.36879237  5.70680367  2.44480971  3.92052036  1.55916298  1.62444118
  0.66009512  5.21451273  2.69431811  3.09526479  3.60679342  2.62271167
  1.3666555   1.69594055  2.09457941  0.83241575  1.80278564  2.41241365
  5.12791775  3.68019651  6.4776235   3.37117742  2.07409994  1.92557241
  2.50980389  1.26064994  3.73552311  1.66609593  2.30661645  2.34793348
  3.05455079  2.58575543  2.79251628  7.36757942  2.25386955  8.34758967
  3.56882779  4.23551292  3.36449137  6.48392765  2.08404834  1.46548882
  2.03579156  1.57617576  1.77047762  1.58541824  1.04086651  3.00123237
  1.2925585   2.7863572  11.37251331  3.59315332  4.07722423  2.79434947
 10.84727805  2.17367818  3.88489345  3.06837685  1.58112031  2.46847974
  1.83681474  4.26050742  1.27935183  2.43473164  0.1946043   3.35507603
  1.93329844  4.46864622  3.94404626  3.09202722  0.86841168  1.86481233
  0.14373494  0.72953926]
mAP score regular 58.06, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [88.02601091 97.43877221 93.22570197 98.27839649 98.96604131 97.71283355
 98.47771383 95.29860229 98.13139996 97.03764606 98.6969629  99.02832798
 99.37464185 95.31853402 97.42880634 97.4014002  96.36993298 97.79256048
 98.93863517 98.39798689 98.91372051 99.37464185 99.47180905 99.19027331
 95.32351695 96.80095672 94.27460946 97.2818098  97.81249221 98.20365249
 98.6969629  98.57238957 97.07501806 98.77419837 98.88880584 99.11802078
 98.00682662 98.27341356 99.00590478 93.27553131 97.95450582 93.0363505
 97.33662207 96.44965991 97.00525699 94.68071854 98.40795276 98.7268605
 98.15133169 98.76174104 98.91870344 98.68699704 98.85392531 98.50761143
 98.72935197 97.79754341 90.1910955  97.1397962  96.46211725 97.63559808
 92.77225503 98.76672397 97.31419887 97.56583701 98.81904477 97.36901114
 98.63218477 95.81682737 98.82153624 98.09402795 99.81812293 97.52099061
 98.37556369 95.78942123 96.87819219 97.34658794 99.17781598 98.69197997
 99.81064853 99.15539278]
Accuracy th:0.7 is [87.49034557 97.378977   92.71744276 98.26095622 98.88382291 97.52099061
 98.37556369 95.36836336 98.11645115 96.81341406 98.65709943 99.03081944
 99.36716745 95.16157162 97.3864514  97.192117   96.37740738 97.71034208
 98.98846451 98.39549543 98.83897651 99.34225278 99.37962479 99.07815731
 95.46802202 96.70378952 94.50133293 97.1622194  97.93457408 98.21860129
 98.5698981  98.6670653  96.79597379 98.6222189  98.8090789  99.05075118
 97.81747515 98.06662182 98.8090789  92.96409796 97.95450582 93.1036201
 97.51102474 96.54931858 97.05508633 94.81027481 98.38054663 98.79911304
 98.04170715 98.75177517 98.84894237 98.6296933  98.92119491 98.41542716
 98.78416424 97.72279941 90.93106112 97.13481326 96.36245858 97.59822608
 93.02389317 98.77419837 97.31669034 97.50604181 98.81157037 97.59822608
 98.6222189  95.81184443 98.7866557  97.90716795 99.81563146 97.3490794
 98.30580263 95.98375564 97.18962553 97.46119541 99.21269651 98.69197997
 99.82310586 99.16535865]
Avg Prec: is [96.37610398 34.18583709 70.34014296 73.48679114 77.44027082 64.54539891
 80.34148919 48.76055064 62.2113694  55.90744511 38.22011008 57.08592453
 23.86376335 31.45139048 33.11424562 62.10458947 32.86479687 45.643777
 50.70408847 38.18907029 69.89859304 59.39773822 91.87318065 86.7783848
 24.75056394 37.00327219 32.81155617 46.05924826 27.34547255 38.04750746
 77.53702998 37.81099146 55.32728031 65.6284697  73.65584779 81.89921349
 60.5328019  76.43063076 89.02574101 44.31158425 36.45825576 48.07615656
 52.80075166 36.9472658  28.01541389 49.71935011 40.50867009 28.76891702
 42.10710306 41.22971467 68.68634162 37.82639726 25.21849498 76.96089544
 31.66279779 42.8995044  56.18247511 57.37063769 39.30477968 62.58206807
 65.0073478  85.48683368 67.23550878 54.23369881 63.26340753 38.75634887
 62.68536144 26.17596792 42.93786129 64.9468146  10.95694808 72.88569212
 54.36635499 45.69807965 65.21538374 49.20544094 11.9782675  53.36091402
  2.84178669 22.67123416]
Accuracy th:0.5 is [45.27244189 97.22450607 69.01113686 96.96290206 97.90716795 74.28806338
 74.41014525 73.66270523 75.90253382 96.41976231 76.02710716 98.5325261
 99.34972718 77.6565264  75.99222662 96.31262924 96.21047911 75.39178314
 98.78167277 98.34068316 78.41144082 76.66243117 98.31327703 83.68587588
 78.25447841 96.52938685 94.3393876  75.48895035 97.81747515 75.99471809
 97.52597354 98.67204823 96.39983058 98.18870369 89.59065202 75.62099808
 75.63345542 92.77474649 97.0276802  72.92273962 76.84679971 92.37362035
 74.74400179 74.44253432 97.03764606 94.02795426 98.18621222 98.77668984
 97.95201435 90.2583651  87.66225677 98.55993223 98.87385704 74.72407006
 98.6969629  75.42666368 69.78349154 94.41911453 96.16314124 96.78102499
 90.13379176 97.04761193 94.89996761 75.50140768 98.32075143 76.3684381
 98.13139996 74.70413833 76.72970077 97.53593941 77.08847198 96.07843137
 76.5403493  95.44559882 74.64683459 84.62266736 91.73082193 76.08191943
 77.16321599 99.15040985]
Accuracy th:0.7 is [45.51909709 97.22450607 69.01113686 96.96290206 97.90716795 74.28806338
 74.41014525 73.87946284 75.90253382 96.41976231 76.02710716 98.5325261
 99.34972718 78.04519521 75.99222662 96.31262924 96.21047911 75.39178314
 98.78167277 98.34068316 78.74031442 76.66243117 98.31327703 84.48314523
 78.92966589 96.52938685 94.3393876  75.48895035 97.81747515 75.99471809
 97.52597354 98.67204823 96.39983058 98.18870369 89.72020829 75.62099808
 75.63345542 92.95164063 97.0276802  72.92273962 77.37000772 92.37362035
 74.74400179 74.44253432 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 90.64205098 87.86904851 98.55993223 98.87385704 74.72407006
 98.6969629  75.42666368 69.78349154 94.78286867 96.16314124 96.78102499
 90.13379176 97.04761193 94.96225428 75.50140768 98.32075143 76.3684381
 98.13139996 74.70413833 76.72970077 97.53593941 77.08847198 96.07843137
 76.63502504 95.44559882 74.64683459 84.8369335  92.06218701 76.08191943
 77.16321599 99.15040985]
Avg Prec: is [54.56057548  3.75509286 14.97324208  4.55088797  1.52140109  4.29631252
 11.23175139  8.76855312  7.2685948   5.18323008  2.25952759  5.28433088
  1.55930972  5.91249234  3.10629754  4.16958691 26.5845547   6.25900126
  1.54405302  2.60062903  3.64882237  1.43674597  1.23305847  5.80425399
  5.77658644 11.95165274  8.20328073  4.51692398  3.8969185   7.24429597
  2.32383693  0.87474291  3.08572104  1.10786368  1.69594682  2.42389669
  2.02741801  2.18731296  2.24356744  6.24780809  1.73109173  6.03112114
  2.20843541  2.73640663  2.36125053  4.84683323  1.73053638  1.04490057
  1.39558874  1.19270925  1.2117303   0.97377295  0.73547152  2.36053826
  0.86234846  1.8451651  10.10472141  2.97409793  3.8384099   2.83466769
  7.86891489  2.02138047  3.25070542  2.60489565  1.37478045  1.85997382
  1.59693158  3.44391646  1.0712408   2.20313729  0.19193569  3.16453002
  1.57256954  3.9795079   3.51932228  2.32226422  0.59751291  1.5401367
  0.12773023  0.6035    ]
mAP score regular 51.45, mAP score EMA 4.39
Train_data_mAP: current_mAP = 58.06, highest_mAP = 58.06
Val_data_mAP: current_mAP = 51.45, highest_mAP = 51.91
tensor([2.3979e-02, 1.6977e-03, 2.7990e-02, 1.9133e-03, 9.2592e-04, 2.3476e-03,
        4.1649e-04, 1.2619e-03, 4.4739e-03, 1.5257e-03, 3.7259e-04, 9.4604e-04,
        2.3512e-04, 3.1632e-03, 4.0352e-03, 4.5617e-03, 5.7567e-03, 5.4717e-03,
        4.0261e-03, 4.4693e-03, 7.6102e-05, 4.8555e-04, 1.1526e-03, 7.7601e-04,
        3.1002e-03, 7.3508e-04, 1.2770e-02, 6.4248e-04, 4.0256e-04, 1.2566e-03,
        1.3006e-03, 4.3265e-04, 1.4689e-02, 1.8440e-04, 1.3266e-03, 9.5796e-03,
        7.2290e-04, 2.1026e-03, 4.9097e-03, 1.2046e-02, 1.5667e-01, 5.1442e-01,
        8.5315e-01, 7.4645e-01, 9.8055e-01, 8.9875e-01, 5.9491e-04, 8.4465e-04,
        1.5642e-03, 1.4073e-03, 2.4599e-04, 8.7015e-04, 1.2532e-03, 2.1309e-03,
        6.5241e-04, 3.0743e-03, 1.1755e-01, 7.3830e-03, 8.4832e-02, 1.6303e-03,
        8.7998e-01, 2.3811e-03, 1.1789e-01, 1.0875e-02, 2.2598e-02, 4.2428e-03,
        3.3411e-02, 8.9384e-03, 5.1318e-01, 2.2166e-02, 7.5252e-04, 1.9527e-03,
        3.6229e-01, 6.7915e-02, 9.5093e-01, 9.9997e-01, 1.0000e+00, 9.9997e-01,
        5.9112e-01, 4.4767e-02], device='cuda:0')
Sum Train Loss:  tensor([1.1426e+00, 7.8337e-03, 4.5617e-01, 9.5224e-03, 2.6068e-03, 1.1766e-02,
        1.7565e-03, 1.4466e-02, 5.1357e-02, 1.9885e-02, 2.5630e-03, 6.2940e-03,
        1.8104e-03, 3.4962e-02, 6.4465e-02, 2.3041e-02, 1.6167e-01, 2.2603e-02,
        2.2486e-02, 4.5952e-02, 8.3358e-04, 2.9669e-04, 8.6895e-04, 2.3735e-03,
        6.5811e-02, 2.0257e-02, 2.9693e-01, 7.9500e-03, 4.7671e-03, 4.2180e-03,
        2.9938e-03, 2.2534e-03, 1.4269e-01, 5.3967e-04, 5.8672e-03, 4.5262e-02,
        1.1379e-02, 1.0394e-02, 2.0933e-02, 2.8837e-01, 2.5154e+00, 8.5658e+00,
        2.0390e+00, 5.7204e+00, 4.0009e+00, 1.0399e+01, 1.1365e-02, 8.3954e-03,
        2.2896e-03, 1.5195e-02, 8.2866e-04, 1.5666e-03, 5.0044e-03, 1.6260e-02,
        4.8140e-03, 3.2241e-02, 3.4227e+00, 9.6754e-02, 1.0135e+00, 1.3648e-02,
        1.0765e+01, 6.6338e-03, 1.1367e+00, 1.4177e-01, 1.6946e-01, 2.2519e-02,
        3.8019e-01, 2.0535e-01, 2.1429e+00, 6.8769e-02, 2.4778e-03, 1.6838e-02,
        9.9750e-01, 1.3685e+00, 4.5580e+00, 6.4099e+00, 3.8716e-01, 6.1027e+00,
        1.7017e-01, 3.9581e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [30/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 76.0
Sum Train Loss:  tensor([1.2144e+00, 1.3305e-02, 4.6692e-01, 1.0566e-02, 1.3766e-02, 8.6591e-03,
        1.3325e-03, 2.5687e-02, 2.2171e-02, 9.6213e-03, 4.0761e-03, 8.0047e-04,
        1.4269e-03, 7.1038e-02, 5.7663e-02, 3.6626e-02, 9.6347e-02, 6.0409e-02,
        5.0653e-02, 3.1326e-02, 1.7317e-04, 2.4851e-04, 7.6885e-04, 3.6916e-03,
        6.3935e-02, 8.9476e-03, 1.6741e-01, 9.2664e-03, 1.9278e-03, 9.5572e-03,
        2.0447e-02, 4.4834e-03, 1.1476e-01, 3.0569e-03, 6.0829e-03, 6.4871e-02,
        7.5943e-03, 1.6973e-02, 1.0140e-02, 3.1381e-01, 4.5483e-01, 8.9653e+00,
        1.7473e+00, 5.1621e+00, 6.2404e+00, 7.9152e+00, 5.8446e-03, 5.2480e-03,
        2.3112e-03, 7.6492e-03, 1.2224e-04, 6.7954e-04, 1.2212e-03, 1.0835e-02,
        7.2522e-03, 2.1943e-02, 3.2032e+00, 5.9452e-02, 6.5770e-01, 1.2051e-02,
        7.2885e+00, 4.5830e-03, 9.5909e-01, 8.2285e-02, 3.2522e-02, 1.1958e-01,
        1.7137e-01, 1.3362e-01, 2.3119e+00, 2.9451e-01, 2.1638e-04, 1.4945e-02,
        2.3484e+00, 7.3422e-01, 9.3045e+00, 9.5985e-01, 9.1835e-01, 4.5311e-01,
        5.8901e-02, 4.9785e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [30/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 63.7
Sum Train Loss:  tensor([9.1888e-01, 1.1556e-02, 6.2403e-01, 8.7620e-03, 4.2819e-03, 3.1107e-02,
        4.5309e-03, 3.6608e-02, 6.4742e-02, 2.0737e-02, 9.6026e-04, 1.9992e-03,
        1.6431e-03, 7.7088e-02, 1.0093e-01, 4.6033e-02, 1.2770e-01, 4.2444e-02,
        4.8775e-02, 6.6976e-02, 3.7288e-04, 1.1302e-03, 7.1559e-04, 9.9931e-04,
        9.0527e-02, 1.3464e-02, 3.0679e-01, 5.1847e-03, 3.4014e-03, 8.6738e-03,
        1.3556e-02, 1.2810e-03, 9.8955e-02, 1.0234e-03, 4.7907e-03, 2.5347e-02,
        4.1277e-03, 5.0137e-03, 1.8399e-02, 2.4673e-01, 8.2863e-01, 6.8958e+00,
        5.2451e-01, 2.3851e+00, 2.5873e+00, 1.2176e+01, 4.3547e-03, 3.2631e-03,
        4.7233e-03, 6.7254e-03, 3.2535e-04, 3.4639e-03, 1.3310e-03, 4.9608e-03,
        3.7991e-03, 6.1663e-03, 3.2385e+00, 8.7276e-02, 1.0277e+00, 6.8302e-03,
        5.6185e+00, 3.1726e-02, 1.0156e+00, 4.2771e-02, 3.6981e-02, 4.7515e-02,
        4.9941e-02, 7.2485e-02, 5.3682e-01, 1.3315e-01, 2.2405e-03, 2.4549e-02,
        5.5897e-01, 6.4947e-01, 2.2678e+00, 8.2712e-01, 1.9733e+00, 3.2616e-01,
        1.0838e-01, 4.2770e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [30/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 47.6
Sum Train Loss:  tensor([8.3764e-01, 1.7742e-02, 5.6766e-01, 7.4653e-03, 1.2122e-03, 2.2450e-02,
        2.8989e-03, 1.8637e-02, 3.5370e-02, 1.7896e-02, 2.8883e-03, 2.3484e-03,
        5.4837e-04, 2.5316e-02, 4.0353e-02, 4.1960e-02, 7.5156e-02, 7.1051e-02,
        7.3301e-03, 3.0218e-02, 1.6641e-04, 3.2917e-03, 6.5930e-04, 1.5635e-03,
        4.2907e-02, 9.0962e-03, 3.3547e-01, 1.2747e-02, 3.2064e-03, 7.9170e-03,
        6.0682e-03, 1.7525e-03, 7.3255e-02, 6.5320e-04, 4.9631e-03, 3.3142e-02,
        2.9491e-03, 9.3959e-03, 1.0819e-02, 2.7767e-01, 7.5250e-01, 5.4105e+00,
        2.9692e+00, 5.3442e+00, 1.1229e+01, 3.5646e+00, 7.8325e-03, 8.2056e-03,
        1.0760e-02, 9.7044e-03, 3.0131e-03, 9.3468e-03, 4.1234e-03, 1.5912e-02,
        2.0530e-03, 4.4820e-02, 2.8540e+00, 7.6514e-02, 1.6421e+00, 2.0022e-02,
        8.9200e+00, 3.8118e-03, 4.0831e-01, 5.1972e-02, 1.5760e-01, 6.3177e-02,
        3.1967e-01, 1.6291e-01, 1.4006e-01, 1.8784e-01, 5.4942e-05, 1.4340e-02,
        4.4381e-01, 9.5463e-01, 5.6370e+00, 4.7498e-01, 6.9580e-01, 2.7079e+00,
        3.7704e+00, 1.9877e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [30/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 62.0
Sum Train Loss:  tensor([1.1655e+00, 2.0260e-02, 6.9279e-01, 9.5595e-03, 2.0334e-03, 3.0491e-02,
        2.4530e-03, 2.5384e-02, 3.8226e-02, 1.0578e-02, 4.8983e-04, 4.7874e-03,
        4.5228e-04, 6.0793e-02, 3.4713e-02, 8.7570e-02, 3.0311e-02, 7.1603e-02,
        2.8272e-02, 9.1796e-03, 4.1020e-04, 1.8215e-04, 1.0587e-03, 8.7056e-04,
        7.2487e-02, 7.7049e-03, 3.1460e-01, 7.7234e-03, 6.4109e-03, 1.0068e-02,
        1.2689e-03, 4.8763e-03, 4.8212e-02, 3.8607e-04, 6.2973e-03, 1.0322e-01,
        6.3988e-03, 1.1513e-02, 7.8260e-03, 3.3662e-01, 1.6602e+00, 7.8448e+00,
        2.8352e+00, 4.9749e+00, 2.3654e+00, 1.5080e+01, 2.0756e-03, 6.0056e-03,
        1.7482e-02, 5.7623e-03, 3.0863e-03, 5.9931e-03, 4.8199e-03, 1.3722e-02,
        3.8849e-03, 3.9795e-02, 3.4869e+00, 2.8100e-02, 8.9487e-01, 4.5573e-03,
        8.2306e+00, 1.5045e-02, 4.9429e-01, 4.1558e-02, 1.2562e-01, 1.2624e-02,
        1.8370e-01, 1.4175e-01, 2.2697e+00, 5.0952e-02, 3.1666e-04, 2.3313e-02,
        2.7797e+00, 6.9617e-01, 6.9852e+00, 7.6902e-01, 1.8512e-01, 3.3456e-01,
        1.0215e-01, 5.6433e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [30/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 66.0
Sum Train Loss:  tensor([8.9209e-01, 2.0937e-02, 5.5088e-01, 1.8354e-02, 7.3241e-03, 2.4008e-02,
        1.4708e-03, 2.3151e-02, 2.2938e-02, 1.8444e-02, 1.6584e-03, 6.9335e-03,
        1.1498e-03, 6.6981e-02, 2.4594e-02, 5.8583e-02, 8.0630e-02, 1.2419e-01,
        1.9384e-02, 2.6244e-02, 2.5798e-04, 5.8972e-04, 5.4682e-03, 1.4352e-03,
        7.2790e-02, 4.9501e-03, 1.4977e-01, 3.5720e-03, 2.3799e-03, 2.9990e-03,
        1.3359e-02, 1.4365e-03, 6.4809e-02, 1.1616e-03, 2.0573e-03, 1.0719e-02,
        1.0476e-02, 3.0819e-02, 1.3302e-02, 2.4236e-01, 3.4372e-01, 7.1662e+00,
        2.4142e+00, 5.4718e+00, 9.5699e-01, 5.4000e+00, 8.0007e-03, 5.1223e-03,
        9.5264e-03, 1.2740e-02, 4.8582e-04, 2.1970e-03, 1.4183e-02, 7.2089e-03,
        4.1343e-03, 8.6366e-03, 1.6924e+00, 1.2773e-01, 6.2912e-01, 4.7648e-03,
        4.9704e+00, 1.6978e-02, 8.7656e-01, 1.7695e-02, 1.6689e-02, 1.1702e-02,
        4.4233e-02, 1.3110e-01, 4.5682e-01, 1.5559e-01, 6.5449e-05, 1.0766e-02,
        1.8777e+00, 9.5717e-01, 5.0812e+00, 5.8345e+00, 3.0150e-01, 7.9878e-01,
        2.1934e-02, 5.9400e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [30/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 48.5
Sum Train Loss:  tensor([1.1046e+00, 2.5414e-02, 7.3871e-01, 6.0574e-03, 9.5611e-03, 3.2985e-02,
        6.2062e-03, 1.2623e-02, 2.9844e-02, 1.1423e-02, 2.1495e-03, 4.1646e-03,
        3.6392e-04, 3.3514e-02, 1.1149e-01, 3.6719e-02, 4.4365e-02, 6.6769e-02,
        6.2504e-02, 5.4040e-02, 2.6511e-04, 2.0367e-03, 8.4732e-04, 1.2232e-03,
        4.7798e-02, 1.1070e-02, 1.8356e-01, 8.9494e-03, 1.1080e-02, 7.2983e-03,
        9.7494e-03, 9.7082e-04, 9.8376e-02, 1.6064e-03, 2.4846e-03, 3.5222e-02,
        5.9551e-03, 2.8844e-02, 2.9091e-02, 2.3022e-01, 8.5923e-01, 6.6595e+00,
        1.6967e+00, 7.6124e+00, 1.7151e+00, 2.0785e+00, 5.3299e-03, 4.2278e-03,
        6.5337e-03, 1.0084e-02, 7.3034e-04, 1.3843e-03, 1.2964e-03, 9.7752e-03,
        5.7258e-03, 3.2022e-02, 3.4625e+00, 3.1751e-02, 7.2742e-01, 2.3913e-02,
        1.1390e+01, 1.0075e-02, 1.1893e+00, 9.6468e-02, 2.4160e-02, 2.2453e-02,
        3.6936e-02, 1.9108e-01, 4.5723e-01, 1.1211e-01, 1.9974e-03, 8.9418e-03,
        1.0075e+00, 1.6929e+00, 8.6528e-01, 1.8757e+00, 1.2986e-01, 3.9723e-01,
        2.1802e+00, 2.3193e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [30/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 49.8
Sum_Val Meta Model:  tensor([1.3177e+00, 1.1269e-01, 2.4874e+00, 8.4248e-02, 1.1909e-03, 3.1066e-02,
        1.8143e-03, 2.1727e-02, 2.7513e-02, 1.5481e-02, 2.3606e-03, 1.1219e-02,
        2.0665e-03, 4.7469e-02, 3.4790e-02, 4.0941e-02, 2.7476e+00, 1.1427e-02,
        6.6402e-03, 6.4907e-03, 7.2295e-05, 1.5540e-04, 2.5523e-04, 7.0428e-04,
        8.6211e-02, 1.4244e-02, 3.6950e-01, 2.8499e-03, 4.5461e-03, 1.6274e-02,
        1.6214e-03, 4.6249e-04, 1.1166e-01, 1.9848e-04, 8.8736e-04, 6.0412e-03,
        2.5656e-03, 1.2210e-02, 1.1656e-02, 5.5940e-01, 1.7314e+00, 1.4424e+01,
        7.0109e+00, 1.4951e+01, 1.7251e+01, 1.4072e+01, 1.2856e-03, 4.4531e-03,
        1.6294e-03, 7.2950e-03, 1.3733e-04, 6.8781e-04, 9.1896e-03, 1.9092e-03,
        9.5687e-04, 5.3178e-03, 3.3327e+00, 5.3160e-02, 1.2935e+00, 5.6492e-03,
        1.4529e+01, 2.6025e-02, 6.4522e-01, 4.6341e-02, 7.5273e-03, 6.7508e-03,
        2.1174e-02, 7.3236e-02, 5.9344e+00, 3.5891e-01, 2.2823e-04, 3.5044e-02,
        6.4042e+00, 6.2456e-01, 2.8540e+01, 7.0802e+00, 1.7984e-01, 1.1512e+01,
        2.4813e-02, 3.0757e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.2649e+00, 7.4297e-02, 1.5043e+00, 4.6946e-02, 2.2382e-04, 3.1140e-02,
        1.6443e-03, 2.2203e-02, 2.6740e-02, 1.4991e-02, 2.8026e-03, 1.3133e-02,
        2.1347e-03, 5.0932e-02, 3.3594e-02, 1.8936e-01, 1.6549e+00, 2.5996e-02,
        4.5019e-03, 1.0678e-02, 5.5037e-05, 1.1297e-04, 1.2670e-04, 2.0677e-03,
        7.9865e-02, 1.3089e-02, 3.3614e-01, 3.2658e-03, 6.5976e-03, 1.7224e-02,
        7.7130e-04, 3.7622e-04, 1.0249e-01, 2.5983e-04, 6.1065e-04, 4.8061e-03,
        4.1819e-03, 1.2047e-02, 4.9977e-03, 4.7904e-01, 1.7079e+00, 1.6846e+01,
        4.7001e+00, 1.2735e+01, 1.6399e+01, 1.8307e+01, 9.4594e-04, 4.9618e-03,
        1.2038e-03, 6.0662e-03, 4.3918e-05, 4.3942e-04, 8.2576e-03, 7.2202e-04,
        7.3884e-04, 6.7282e-03, 3.7872e+00, 4.4281e-02, 1.0767e+00, 7.1625e-03,
        2.0999e+01, 1.1652e-02, 3.9764e-01, 6.7368e-02, 3.9167e-03, 7.4689e-03,
        1.3960e-02, 8.3022e-02, 6.4535e+00, 3.2132e-01, 5.4905e-04, 3.9081e-02,
        5.2148e+00, 5.9517e-01, 3.6462e+01, 5.7636e+00, 2.5523e-02, 1.0432e+01,
        2.1446e-02, 5.1191e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.2748e+01, 4.3763e+01, 5.3743e+01, 2.4536e+01, 2.4173e-01, 1.3265e+01,
        3.9480e+00, 1.7596e+01, 5.9769e+00, 9.8258e+00, 7.5220e+00, 1.3882e+01,
        9.0789e+00, 1.6101e+01, 8.3251e+00, 4.1511e+01, 2.8747e+02, 4.7510e+00,
        1.1182e+00, 2.3892e+00, 7.2320e-01, 2.3266e-01, 1.0993e-01, 2.6645e+00,
        2.5761e+01, 1.7807e+01, 2.6322e+01, 5.0831e+00, 1.6389e+01, 1.3707e+01,
        5.9301e-01, 8.6958e-01, 6.9769e+00, 1.4091e+00, 4.6030e-01, 5.0170e-01,
        5.7849e+00, 5.7297e+00, 1.0179e+00, 3.9768e+01, 1.0902e+01, 3.2747e+01,
        5.5090e+00, 1.7061e+01, 1.6725e+01, 2.0369e+01, 1.5900e+00, 5.8744e+00,
        7.6956e-01, 4.3106e+00, 1.7853e-01, 5.0499e-01, 6.5895e+00, 3.3883e-01,
        1.1325e+00, 2.1885e+00, 3.2218e+01, 5.9977e+00, 1.2692e+01, 4.3933e+00,
        2.3863e+01, 4.8934e+00, 3.3730e+00, 6.1946e+00, 1.7332e-01, 1.7604e+00,
        4.1783e-01, 9.2882e+00, 1.2575e+01, 1.4497e+01, 7.2962e-01, 2.0014e+01,
        1.4394e+01, 8.7635e+00, 3.8343e+01, 5.7638e+00, 2.5523e-02, 1.0432e+01,
        3.6281e-02, 1.1435e+00], device='cuda:0')
Outer loop valEpocw Maximum [30/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 158.5
model_train val_loss valEpocw [30/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 168.6
model_train val_loss valEpocw [30/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1148.5
Sum_Val Meta Model:  tensor([3.6446e+00, 8.2632e-02, 2.8087e+00, 5.3050e-02, 2.0271e-03, 2.5003e-01,
        1.2591e-01, 2.1896e-01, 3.1680e-01, 1.6090e-01, 2.9100e-02, 6.6857e-01,
        9.9306e-03, 4.1266e-02, 2.2208e-01, 2.7793e-01, 1.5641e-01, 2.6543e-01,
        8.6219e-02, 7.5411e-01, 3.9344e-05, 2.1409e-04, 1.2410e-03, 1.3432e-02,
        2.5324e-01, 6.0009e-02, 7.8044e-01, 4.1057e-02, 1.9353e-02, 3.3761e-04,
        6.2133e-04, 1.3034e-04, 5.1360e-03, 1.7904e-04, 2.5868e-04, 1.3771e-03,
        1.0885e-02, 7.9083e-04, 8.2871e-04, 4.1269e-02, 3.2000e-02, 1.8781e+00,
        1.2338e-01, 2.1795e-01, 3.3241e-01, 4.3238e+00, 2.7211e-03, 2.9776e-03,
        5.9925e-04, 3.2346e-02, 9.1248e-05, 5.2988e-04, 2.0237e-04, 2.8546e-04,
        5.4956e-04, 2.4994e-02, 2.0989e+00, 8.3723e-02, 8.2182e-01, 1.9947e-02,
        1.5960e+00, 5.3408e-03, 3.3009e-01, 4.2058e-02, 2.2038e-02, 1.0940e-02,
        3.6515e-02, 2.9492e-01, 6.9066e-02, 1.6691e-01, 1.0640e-04, 8.6708e-03,
        2.0840e+00, 5.1482e-01, 4.7172e+00, 4.5239e-01, 1.9355e-01, 5.7173e-01,
        2.9378e-02, 1.0394e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([2.8920e+00, 9.5302e-02, 2.0675e+00, 5.9916e-02, 4.4254e-03, 2.1544e-01,
        1.2725e-01, 1.6998e-01, 2.6422e-01, 1.1579e-01, 2.0988e-02, 2.2708e-01,
        9.5529e-03, 7.2505e-02, 1.5855e-01, 3.6467e-02, 1.2361e-01, 2.3530e-01,
        3.9257e-02, 2.2810e-01, 2.2857e-04, 3.6367e-04, 4.1012e-04, 1.0638e-02,
        1.7729e-01, 4.9094e-02, 7.9751e-01, 3.3941e-02, 2.1532e-02, 2.1335e-03,
        9.0482e-04, 4.0607e-04, 2.1144e-02, 2.5420e-03, 9.3378e-04, 1.6252e-03,
        4.0808e-03, 6.6207e-03, 1.9943e-03, 5.2179e-02, 6.9191e-02, 1.6413e+00,
        2.3315e-01, 2.2325e-01, 1.5695e+00, 2.3841e-01, 1.8653e-03, 1.1851e-03,
        1.4798e-03, 2.9260e-02, 1.2027e-04, 6.9110e-04, 8.5080e-04, 1.7971e-03,
        1.2349e-03, 1.2033e-02, 1.6285e+00, 7.4567e-02, 3.5101e-01, 2.1198e-03,
        9.8695e+00, 4.4475e-03, 2.8295e-01, 7.7019e-03, 2.6959e-03, 2.2687e-03,
        6.8959e-03, 2.8982e-01, 1.2763e-01, 1.2980e-01, 8.7776e-05, 2.4690e-03,
        5.0145e-01, 2.3349e-01, 3.6325e+00, 3.4487e-02, 2.4010e-02, 4.6909e-01,
        6.0474e-04, 9.2573e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.9327e+01, 2.1242e+01, 4.4609e+01, 1.1173e+01, 1.5279e+00, 3.2902e+01,
        6.7707e+01, 4.1850e+01, 2.5126e+01, 2.7692e+01, 1.5648e+01, 7.0825e+01,
        9.5411e+00, 1.0244e+01, 1.5582e+01, 3.0700e+00, 9.8722e+00, 1.7002e+01,
        3.3924e+00, 1.7639e+01, 5.3825e-01, 1.9811e-01, 1.1211e-01, 4.2028e+00,
        2.6110e+01, 1.8016e+01, 3.2466e+01, 1.3700e+01, 1.2654e+01, 5.6372e-01,
        2.3070e-01, 2.3704e-01, 7.3024e-01, 3.0963e+00, 2.5168e-01, 9.3965e-02,
        1.5755e+00, 1.1798e+00, 1.6551e-01, 2.5997e+00, 4.7058e-01, 3.4002e+00,
        3.0843e-01, 3.4567e-01, 1.6487e+00, 2.8605e-01, 8.9315e-01, 4.2732e-01,
        2.9824e-01, 6.5017e+00, 1.2311e-01, 2.8740e-01, 2.3612e-01, 3.0814e-01,
        5.3757e-01, 1.6924e+00, 1.0300e+01, 4.5181e+00, 3.5309e+00, 4.0974e-01,
        1.1979e+01, 6.6760e-01, 2.0126e+00, 3.5812e-01, 7.4619e-02, 2.1642e-01,
        1.4426e-01, 1.7238e+01, 2.8744e-01, 3.8025e+00, 3.6477e-02, 4.6621e-01,
        1.5455e+00, 2.3225e+00, 3.9969e+00, 3.4494e-02, 2.4010e-02, 4.6918e-01,
        1.2161e-03, 1.4626e-01], device='cuda:0')
Outer loop valEpocw Maximum [30/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 32.6
model_train val_loss valEpocw [30/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 30.1
model_train val_loss valEpocw [30/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 717.0
Sum_Val Meta Model:  tensor([1.7985e+00, 5.6146e-03, 3.0236e-01, 4.3640e-03, 7.6772e-04, 5.8947e-03,
        5.3542e-04, 1.8414e-02, 8.2700e-03, 3.0286e-03, 4.7764e-04, 1.4548e-03,
        8.0908e-05, 6.3651e-02, 1.4281e-02, 1.4197e-02, 4.3401e-02, 2.6053e-02,
        5.5891e-03, 9.0862e-03, 7.8830e-05, 5.0212e-04, 1.3647e-02, 1.3909e-03,
        2.9135e-02, 1.7750e-03, 3.1193e-01, 5.4107e-03, 2.9071e-04, 8.2382e-03,
        5.7621e-03, 4.2154e-03, 1.2635e-01, 4.5089e-05, 5.9324e-03, 4.4587e-02,
        1.7208e-02, 4.1551e-02, 1.3540e-03, 3.2540e-01, 4.4660e+00, 4.1233e+01,
        6.0505e+01, 7.1878e+01, 5.2171e+01, 5.9956e+01, 1.4459e-02, 1.2771e-02,
        9.4952e-02, 3.3709e-02, 1.0886e-02, 4.2071e-02, 1.0949e-01, 3.6032e-02,
        7.3298e-02, 4.0559e-01, 1.9401e+01, 9.4752e-02, 6.6625e-01, 3.7481e-03,
        1.0747e+02, 3.4010e-03, 1.2433e+00, 1.5229e-01, 3.2897e-01, 3.7715e-03,
        2.6466e-01, 8.9744e-02, 2.2548e+00, 3.4404e-02, 1.5595e-04, 1.6149e-02,
        1.3112e+00, 2.6887e+00, 1.0533e+00, 2.2877e+01, 1.1014e+01, 7.6282e-01,
        7.1931e-02, 2.0251e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.0520e-01, 9.7787e-04, 1.2605e-01, 1.3085e-03, 1.8064e-05, 2.7681e-04,
        1.3729e-05, 1.8163e-02, 7.1084e-04, 1.8362e-04, 2.6235e-05, 3.9824e-05,
        6.3430e-06, 5.0318e-02, 9.9175e-04, 6.6001e-03, 9.5609e-03, 7.0276e-04,
        5.6900e-04, 1.5879e-04, 3.3815e-06, 7.8063e-06, 9.3020e-06, 2.9908e-05,
        1.4892e-02, 1.2473e-03, 3.3011e-01, 4.3983e-03, 2.3026e-04, 2.9747e-04,
        6.8204e-04, 4.2354e-03, 1.5075e-01, 2.7854e-05, 1.3910e-03, 7.0943e-03,
        1.1828e-02, 2.7549e-02, 3.9859e-04, 5.0947e-01, 4.2336e+00, 4.7085e+01,
        5.9196e+01, 7.1749e+01, 8.1859e+01, 1.0019e+02, 7.2338e-03, 6.4229e-03,
        7.7345e-02, 1.5599e-02, 5.6775e-03, 3.8856e-02, 9.7572e-02, 4.2430e-02,
        3.8904e-02, 2.5319e-01, 1.0364e+01, 1.0135e-01, 6.6701e-01, 1.1591e-03,
        7.4216e+01, 1.1720e-03, 1.0540e+00, 1.1357e-01, 2.9936e-01, 1.6103e-02,
        2.9930e-01, 1.1542e-01, 3.2368e+00, 1.2613e-01, 2.3501e-04, 1.5953e-02,
        2.9685e+00, 2.3268e+00, 9.2877e-01, 3.6883e+01, 1.2917e+01, 1.0113e+00,
        3.2473e-02, 3.1642e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.1655e+01, 5.9253e-01, 4.5541e+00, 6.7248e-01, 1.9797e-02, 1.1650e-01,
        3.2027e-02, 1.4387e+01, 1.6908e-01, 1.2466e-01, 6.9075e-02, 3.9746e-02,
        2.7694e-02, 1.6116e+01, 2.5679e-01, 1.4418e+00, 1.5809e+00, 1.0845e-01,
        1.3583e-01, 3.3682e-02, 4.9419e-02, 1.5530e-02, 6.7568e-03, 3.6996e-02,
        4.4884e+00, 1.7048e+00, 2.8785e+01, 6.8425e+00, 6.1527e-01, 2.0183e-01,
        4.7494e-01, 8.2362e+00, 9.6600e+00, 1.3877e-01, 9.8876e-01, 6.6742e-01,
        1.4264e+01, 1.2163e+01, 8.1322e-02, 4.9288e+01, 3.0417e+01, 9.0317e+01,
        6.9295e+01, 9.4531e+01, 8.3660e+01, 1.1192e+02, 1.0511e+01, 6.8119e+00,
        4.0647e+01, 9.0439e+00, 1.9924e+01, 4.0138e+01, 6.6552e+01, 1.9569e+01,
        5.2703e+01, 7.4260e+01, 9.4852e+01, 1.4049e+01, 8.9881e+00, 6.5134e-01,
        8.4297e+01, 5.0237e-01, 9.8322e+00, 1.1147e+01, 1.4659e+01, 3.8127e+00,
        1.0229e+01, 1.4827e+01, 7.2978e+00, 6.9737e+00, 3.1999e-01, 8.7429e+00,
        9.9055e+00, 3.6445e+01, 9.8173e-01, 3.6884e+01, 1.2917e+01, 1.0113e+00,
        6.3482e-02, 7.7867e-01], device='cuda:0')
Outer loop valEpocw Maximum [30/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 466.1
model_train val_loss valEpocw [30/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 514.4
model_train val_loss valEpocw [30/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1411.3
Sum_Val Meta Model:  tensor([7.8486e+00, 1.1366e-02, 1.2356e+00, 4.7137e-03, 1.1251e-03, 1.1475e-01,
        1.5233e-02, 1.0477e-01, 1.3488e-02, 1.2696e-01, 1.1588e-02, 2.3766e-02,
        1.4326e-04, 1.7129e-01, 2.3116e-01, 1.6838e-02, 1.8749e-02, 1.7618e-02,
        2.6166e-03, 6.3969e-03, 2.6089e-04, 3.6410e-04, 9.0645e-04, 1.9480e-03,
        2.1523e-01, 4.0421e-03, 8.9754e-01, 2.5844e-03, 4.6136e-02, 2.2017e-03,
        2.6293e-03, 3.4346e-04, 8.4387e-02, 4.7641e-02, 3.3117e-02, 1.8356e-01,
        5.0862e-04, 4.7322e-03, 8.5708e-02, 5.0191e-01, 2.1680e+00, 1.4217e+01,
        7.1933e+00, 6.6808e+00, 7.9627e+00, 1.5969e+01, 1.1609e-03, 2.0686e-03,
        3.6851e-03, 2.2075e-03, 4.8926e-04, 6.5986e-04, 8.8834e-04, 4.2311e-02,
        8.5327e-04, 1.2473e-02, 2.9392e+00, 7.2744e-02, 1.3777e+00, 7.6295e-03,
        3.7370e+01, 2.5423e-03, 6.1857e-01, 2.0269e-01, 3.0437e-01, 1.9172e-02,
        3.9416e-01, 6.7960e-01, 2.9656e-01, 4.9218e-02, 1.0324e-04, 7.5593e-03,
        7.8793e-01, 1.0278e+00, 4.5712e+02, 9.4673e+00, 1.1144e+00, 9.5154e+00,
        1.3242e-02, 4.9338e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.9134e+00, 1.5930e-02, 8.5784e-01, 1.7483e-02, 4.9779e-03, 8.3483e-02,
        1.8698e-02, 8.9030e-02, 4.5168e-02, 1.0995e-01, 8.1222e-03, 3.8011e-02,
        1.4243e-03, 1.7656e-01, 2.3540e-01, 1.3581e-02, 2.6657e-02, 1.2165e-02,
        2.1119e-03, 2.1354e-03, 1.5817e-04, 1.1852e-04, 1.7513e-03, 4.7840e-03,
        2.0918e-01, 1.2120e-02, 8.1948e-01, 6.9776e-03, 4.1261e-02, 6.4117e-04,
        1.0859e-03, 6.7988e-04, 4.8162e-03, 4.8928e-04, 5.9625e-04, 1.4496e-03,
        2.5036e-03, 1.9869e-03, 1.9042e-03, 1.0769e-01, 1.1840e-01, 1.3956e+00,
        5.5673e-01, 2.6178e-01, 9.6831e-02, 3.4176e+00, 1.8512e-03, 1.0512e-03,
        1.4369e-03, 5.7161e-04, 2.5075e-04, 6.6198e-04, 3.1170e-04, 1.8660e-03,
        1.8543e-03, 1.1761e-02, 1.6205e+00, 2.7750e-02, 1.1882e+00, 4.0808e-03,
        1.8415e+01, 4.3664e-03, 4.3522e-01, 1.9504e-02, 2.0905e-02, 1.3151e-02,
        6.1987e-02, 1.4336e-01, 1.7892e-01, 3.6272e-02, 2.4296e-04, 4.1106e-03,
        8.5306e-02, 6.3155e-01, 1.1337e+02, 1.2184e+00, 6.5565e-03, 9.2459e+00,
        3.2957e-03, 5.4824e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.4739e+01, 2.5576e+00, 1.6870e+01, 2.3833e+00, 1.1297e+00, 9.6341e+00,
        7.3990e+00, 1.6422e+01, 3.5033e+00, 1.8352e+01, 3.7839e+00, 8.4026e+00,
        9.4230e-01, 1.7447e+01, 1.9303e+01, 1.0618e+00, 1.7702e+00, 6.5694e-01,
        1.6057e-01, 1.5220e-01, 2.4034e-01, 4.3016e-02, 3.4507e-01, 1.3544e+00,
        2.3832e+01, 3.1331e+00, 2.7834e+01, 1.9216e+00, 1.7042e+01, 1.1539e-01,
        1.9473e-01, 2.6979e-01, 1.2322e-01, 3.1788e-01, 1.0296e-01, 5.4553e-02,
        6.4803e-01, 2.8306e-01, 1.0367e-01, 3.7140e+00, 5.9773e-01, 2.6779e+00,
        7.3701e-01, 3.9882e-01, 1.0252e-01, 4.1372e+00, 6.0606e-01, 2.7082e-01,
        2.0375e-01, 9.0790e-02, 1.6232e-01, 2.0140e-01, 6.1533e-02, 2.0680e-01,
        5.6579e-01, 1.2529e+00, 9.0802e+00, 1.2932e+00, 1.0943e+01, 5.6616e-01,
        2.2359e+01, 4.6106e-01, 2.7362e+00, 6.4190e-01, 4.0522e-01, 8.8571e-01,
        9.5267e-01, 5.5735e+00, 4.0047e-01, 8.3663e-01, 6.7167e-02, 5.1836e-01,
        2.4113e-01, 5.4680e+00, 1.2777e+02, 1.2189e+00, 6.5566e-03, 9.2493e+00,
        7.6197e-03, 6.7637e-02], device='cuda:0')
Outer loop valEpocw Maximum [30/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 589.8
model_train val_loss valEpocw [30/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 157.5
model_train val_loss valEpocw [30/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 462.4
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.56594096 97.45617134 93.31026669 98.05070601 98.53802951 97.59749516
 98.21030446 95.28149024 98.06654402 97.03463652 98.65376884 98.93763478
 99.43835967 95.43499714 97.43546009 97.43789671 96.42304553 97.77780485
 98.86453625 98.45518451 98.65742376 99.31896541 99.55288069 99.07408535
 95.19742693 96.89574932 94.41405441 97.06265762 98.10309329 98.25294526
 98.32726209 98.58188862 97.11138997 98.65498715 98.68422656 98.82189544
 97.59384023 98.45762113 98.98758543 93.27980897 98.18715659 96.40720751
 98.73417722 98.10674821 98.99002205 97.90450896 98.27974805 98.70737442
 98.15548056 98.76828986 98.79021942 98.61721958 99.03753609 98.42350849
 98.75610677 97.76074853 93.18843581 97.00052387 96.74955227 97.64379089
 97.42815024 98.76585324 97.67668523 97.40622068 98.83895177 97.61820641
 98.69153641 96.00272901 99.14840219 98.31142408 99.80263398 97.4634812
 99.16058528 96.14405283 99.04240933 99.10332476 99.74537347 99.54435253
 99.85623957 99.19591623]
Accuracy th:0.7 is [85.76649895 97.34530525 92.56709835 98.02268491 98.30655085 97.52561494
 97.95445962 94.97325812 97.95933285 96.86650991 98.60503649 98.8718461
 99.42373996 95.36189861 97.3672348  97.29413628 96.39502443 97.66206552
 98.88037426 98.36259305 98.50391686 99.28485277 99.51633143 98.9827122
 95.24494097 96.80315786 94.27638552 96.97250277 98.04339616 98.18471997
 98.08969189 98.61721958 96.75564382 98.51122672 98.5514309  98.64524068
 97.35383341 98.35284658 98.82555037 93.30904838 98.15791718 96.37431318
 98.85113485 98.18715659 98.96200095 98.07263557 98.20543122 98.68178994
 98.12136792 98.69640964 98.80362081 98.63305759 99.01560654 98.31264239
 98.72443075 97.68155846 93.26518926 96.78610153 96.6277214  97.47079105
 97.70348802 98.67448009 97.55729097 97.30875598 98.72321244 97.49881215
 98.6062548  95.98932762 99.22759226 98.36015643 99.81603538 97.35748833
 99.17885991 95.99541916 98.9132686  99.26779644 99.75755656 99.54922576
 99.84892972 99.1776416 ]
Avg Prec: is [96.35314574 36.5231746  72.59028823 66.15231083 79.02111281 64.0541755
 74.74574974 47.62425566 56.85894495 53.31683593 34.46592469 54.40776897
 25.65911412 29.13763671 32.04428457 57.35846273 29.53332745 44.21552226
 49.94387767 40.05514577 61.34894839 52.29982107 89.22454991 83.30607056
 25.51226885 34.60226014 37.53961191 40.52078467 26.24816334 39.26018681
 73.23282248 37.81380609 58.70172267 63.71755962 72.8833134  79.68715842
 58.28536587 75.16136494 86.59206132 46.54855228 50.38463924 85.42407882
 88.63517084 83.53581365 91.44579772 91.79415367 40.34794871 35.63780119
 41.56874994 48.17116439 62.87372749 37.71992944 27.56531076 73.64046766
 29.90587228 46.16439857 74.78751948 59.13071988 47.16383841 60.2910664
 95.53892983 84.11323029 74.51688348 49.96972479 62.39330759 44.42454485
 62.16036277 27.39580868 76.29660709 68.48947036 11.51858546 72.5308568
 86.45693448 52.37997648 93.11764563 94.46105809 90.49745186 93.5162665
 42.36362277 31.81715665]
Accuracy th:0.5 is [45.71459899 97.2137279  70.7252592  97.02489005 97.26733349 75.37798029
 75.69352225 75.491283   76.61700028 96.45959479 76.93010563 98.52097319
 99.41399349 79.99780704 76.48298632 96.56680596 96.29512311 76.25638089
 98.65376884 98.30776915 80.24999695 77.3723517  98.38695922 82.45391747
 82.94124097 96.65086926 94.0778012  75.97251495 98.01293844 76.81802122
 97.30875598 98.57457877 96.36213009 98.02024829 88.23235584 76.53902852
 76.21130347 91.61559923 97.11504489 73.5955946  78.88670947 92.05906361
 75.65088145 75.24152971 96.9627563  93.87434364 98.02877645 98.57336046
 97.80948088 90.33150181 89.1180663  98.55508583 98.99976852 75.84702915
 98.70615611 76.18328237 71.05663917 93.51128763 96.24273583 96.9067141
 89.79300934 97.17717864 94.77711042 76.35140897 98.42838172 76.68035233
 98.20786784 75.32559301 77.29072502 97.55972759 77.80485131 95.99054592
 77.16158429 95.45083515 75.66550115 83.46755035 89.86367125 76.79000012
 77.87673152 99.14718388]
Accuracy th:0.7 is [45.73043701 97.2137279  70.7252592  97.02489005 97.26733349 75.37798029
 75.69352225 75.76662078 76.61700028 96.4754328  76.93010563 98.52097319
 99.41399349 80.55701076 76.48298632 96.56680596 96.29512311 76.25638089
 98.65376884 98.30776915 80.6471656  77.3723517  98.38695922 83.52115593
 83.86350069 96.65086926 94.0778012  75.97251495 98.01293844 76.81802122
 97.30875598 98.57457877 96.36213009 98.02024829 88.39195429 76.53902852
 76.21130347 91.88728208 97.11504489 73.5955946  79.55312435 92.05906361
 75.65088145 75.24152971 96.9627563  93.87434364 98.02877645 98.57336046
 97.97760749 90.8322267  89.36050974 98.55508583 98.99976852 75.84702915
 98.70615611 76.18328237 71.05663917 93.95718863 96.24273583 96.9067141
 89.79300934 97.17717864 94.87457511 76.35262728 98.42838172 76.68035233
 98.20786784 75.32559301 77.29072502 97.55972759 77.8596752  95.99054592
 77.357732   95.45083515 75.66550115 83.63567695 90.18530476 76.79000012
 77.87673152 99.14718388]
Avg Prec: is [55.81594503  3.12910515 11.14400886  3.33147854  2.26562002  3.76263692
  3.35132367  5.55803781  2.58952747  3.84659245  1.51465681  1.61473483
  0.64537229  5.09950195  2.79593722  3.22447611  3.72660858  2.6505509
  1.40757186  1.66394761  1.95657978  0.9059591   1.79180602  2.41237547
  5.21474833  3.69764391  6.60074437  3.34481591  2.02655765  1.90039494
  2.60776282  1.35896311  3.60459012  1.57235318  2.32870181  2.32946659
  3.06437858  2.59973024  2.67138789  7.382396    2.26883542  8.23427992
  3.35009925  4.03379039  3.17313098  6.46171955  2.0288495   1.59301372
  2.07803442  1.73577311  1.81751282  1.65483849  1.01437052  3.08582232
  1.34880576  2.70313764 11.11018589  3.70972137  3.9811929   2.84685125
 10.72149566  2.159416    3.78987064  2.8526768   1.53902115  2.53898954
  1.69692461  4.13167789  1.25372901  2.42476023  0.24312241  3.38288164
  1.91358064  4.55155105  3.86256683  3.13384841  0.84364657  1.91576958
  0.12153105  0.81186978]
mAP score regular 58.46, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [87.89894611 97.45870394 93.15095797 98.25846476 98.84395944 97.68542741
 98.46027356 95.39078656 98.16129756 97.02269726 98.67952263 99.01587064
 99.37464185 95.31604255 97.47614421 97.3640282  96.33505245 97.78010315
 98.97600718 98.42290156 98.89628024 99.33976132 99.65368613 99.23262825
 95.33846575 96.86573486 94.44652067 97.27184393 97.94204848 98.21610982
 98.6521165  98.58982983 97.06256073 98.7791813  98.92866931 99.12798665
 97.97942048 98.34068316 99.12798665 93.07621397 97.96696315 92.01484914
 97.0351546  95.74457483 96.75860179 94.01798839 98.4204101  98.78416424
 98.06163889 98.78167277 98.87385704 98.55744077 98.92119491 98.4727309
 98.75177517 97.73525675 90.00672696 97.16471087 96.47457458 97.68293594
 92.04723821 98.82651917 97.27184393 97.51102474 98.87883997 97.48611007
 98.69197997 95.84672497 98.57488103 98.28587089 99.81563146 97.52846501
 98.26593916 95.79440417 97.266861   97.03017166 99.11552931 98.69197997
 99.82310586 99.14542691]
Accuracy th:0.7 is [87.19635249 97.42133194 92.68754516 98.30081969 98.7717069  97.60819194
 98.24600742 95.23880709 98.09651942 96.88815806 98.6521165  98.98846451
 99.36467598 95.20143508 97.38894287 97.26187807 96.42972818 97.69539328
 99.00590478 98.40296983 98.76921544 99.31733812 99.62627999 99.20023918
 95.44809029 96.74614446 94.48638413 97.18215113 97.88474475 98.16877196
 98.5026285  98.67703117 96.82088846 98.72187757 98.79163864 98.91122904
 97.71283355 98.28337943 99.02583651 93.29795451 97.99686075 92.79467823
 97.2892842  96.21047911 96.91307273 94.45648653 98.41542716 98.82901064
 98.11146822 98.74679224 98.94361811 98.66955677 98.90873757 98.39549543
 98.76672397 97.76515435 91.03071978 97.09245833 96.47955752 97.53095647
 92.51812542 98.77668984 97.3565538  97.44873807 98.73931784 97.52099061
 98.6222189  95.83925057 98.72935197 98.27590503 99.81563146 97.57331141
 98.39300396 95.95136657 97.33662207 97.23198047 99.16535865 98.72935197
 99.82559733 99.16286718]
Avg Prec: is [96.43029547 35.14464997 69.94216067 73.4184524  76.47579419 65.48674757
 80.69720981 47.94178173 62.29024394 54.31960298 39.23711313 55.92888276
 23.43037299 31.68578385 33.87746378 62.04311541 32.4366186  45.44045283
 51.82181085 37.60340191 70.44110427 57.14351798 92.1971575  86.8967639
 24.16518569 37.29354979 32.6369709  45.85049662 28.06392161 36.56380524
 77.80611576 36.17671809 56.27635427 66.41291073 74.03805575 82.1634166
 60.15585625 76.81482695 90.11798789 44.94101982 38.16842106 49.01563103
 51.92234848 37.11207689 29.04666506 49.30606602 40.04617104 28.30936624
 41.89310947 42.16461309 70.68400632 35.97266757 26.65622021 76.35516097
 29.70620687 41.03822455 56.94645024 57.8017941  40.56630138 63.25742921
 63.96417644 85.8242256  67.1569209  53.36263842 63.49369703 36.39854384
 62.93726892 27.02675061 43.27357552 66.22600816  8.66700328 74.47735805
 54.96329944 45.48509483 62.16994693 50.14570228 12.54250366 53.87192097
  2.69240699 22.45762802]
Accuracy th:0.5 is [45.27493335 97.22450607 68.87659765 96.96290206 97.90716795 74.14854125
 74.27062312 73.5680295  75.76301168 96.41976231 75.88260209 98.5325261
 99.34972718 77.59423973 75.85270449 96.31262924 96.21047911 75.24727807
 98.78167277 98.34068316 78.38403468 76.52290904 98.31327703 84.16922042
 78.38154321 96.52938685 94.3393876  75.35939408 97.81747515 75.85021302
 97.52597354 98.67204823 96.39983058 98.18870369 89.62802402 75.47649301
 75.50888208 92.78969529 97.0276802  72.78321748 76.77454718 92.37362035
 74.61444552 74.32294392 97.03764606 94.02795426 98.18621222 98.77668984
 97.95201435 90.51747764 87.98614745 98.55993223 98.87385704 74.57956499
 98.6969629  75.29710741 69.69379874 94.41662307 96.16314124 96.78102499
 90.13379176 97.04761193 95.02703241 75.37185141 98.32075143 76.22393303
 98.13139996 74.57956499 76.5851957  97.53593941 76.94396691 96.07843137
 76.40830157 95.44559882 74.50232952 84.74973217 91.98246007 75.95236316
 77.01871092 99.15040985]
Accuracy th:0.7 is [45.55646909 97.22450607 68.87659765 96.96290206 97.90716795 74.14854125
 74.27062312 73.80222737 75.76301168 96.41976231 75.88260209 98.5325261
 99.34972718 78.02028054 75.85519595 96.31262924 96.21047911 75.24727807
 98.78167277 98.34068316 78.69048509 76.52290904 98.31327703 84.97396417
 79.09659416 96.52938685 94.3393876  75.35939408 97.81747515 75.85021302
 97.52597354 98.67204823 96.39983058 98.18870369 89.75259735 75.47649301
 75.50888208 92.9690809  97.0276802  72.78321748 77.30522959 92.37362035
 74.61444552 74.32294392 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 90.94102698 88.25273438 98.55993223 98.87385704 74.57956499
 98.6969629  75.29710741 69.69379874 94.78286867 96.16314124 96.78102499
 90.13379176 97.04761193 95.10177642 75.37434288 98.32075143 76.22393303
 98.13139996 74.57956499 76.5851957  97.53593941 76.94396691 96.07843137
 76.51543464 95.44559882 74.50232952 84.97396417 92.28143608 75.95236316
 77.01871092 99.15040985]
Avg Prec: is [54.26180263  3.76604017 14.88347094  4.5756686   1.60423187  4.29843052
 10.96705493  8.77756028  7.23894598  5.15882883  2.26097733  5.30030936
  1.55409839  5.88944739  3.08667377  4.2930827  26.54739383  6.21669242
  1.53298653  2.58612656  3.63619268  1.43178061  1.35219447  5.81255297
  5.75103613 12.17515809  8.18712995  4.53558055  3.90616821  6.90722346
  2.33986402  0.87034783  3.0075905   1.1062808   1.73017374  2.4544895
  2.04434315  2.20957428  2.11054571  6.20782379  1.70143125  6.03986701
  2.19975421  2.73037788  2.37447089  4.88203948  1.77506789  1.0655887
  1.44896857  1.15693398  1.21146936  0.99995637  0.77604935  2.31775619
  0.92291478  1.88267653 10.19682739  3.04351774  3.91281597  2.97265822
  7.96009195  2.04874067  3.33175123  2.63802708  1.38554134  1.92847055
  1.58785066  3.53531106  1.05659243  2.18499873  0.19300138  3.15993493
  1.53542689  4.02094246  3.1876388   2.28176342  0.56539632  1.49920635
  0.12166334  0.63104635]
mAP score regular 51.44, mAP score EMA 4.39
Train_data_mAP: current_mAP = 58.46, highest_mAP = 58.46
Val_data_mAP: current_mAP = 51.44, highest_mAP = 51.91
tensor([2.1087e-02, 1.4920e-03, 2.5937e-02, 1.7225e-03, 8.2027e-04, 2.0775e-03,
        3.8057e-04, 1.1506e-03, 3.9671e-03, 1.2910e-03, 3.3909e-04, 8.7866e-04,
        2.1185e-04, 2.9505e-03, 3.5186e-03, 4.1345e-03, 5.1492e-03, 5.3299e-03,
        3.6774e-03, 4.2417e-03, 6.3371e-05, 4.5676e-04, 1.0929e-03, 7.2639e-04,
        3.0678e-03, 6.6591e-04, 1.2446e-02, 5.8227e-04, 3.4573e-04, 1.1713e-03,
        1.2241e-03, 3.8883e-04, 1.3419e-02, 1.5932e-04, 1.1735e-03, 9.4763e-03,
        6.6379e-04, 1.6737e-03, 4.6520e-03, 1.0930e-02, 1.5262e-01, 5.2819e-01,
        8.6191e-01, 7.6175e-01, 9.8186e-01, 9.0380e-01, 5.4944e-04, 7.6476e-04,
        1.4435e-03, 1.3472e-03, 2.3263e-04, 7.3512e-04, 1.1078e-03, 1.9011e-03,
        6.0652e-04, 2.8022e-03, 1.0420e-01, 6.4859e-03, 7.9445e-02, 1.5149e-03,
        8.8083e-01, 2.1403e-03, 1.1188e-01, 9.8569e-03, 2.1268e-02, 3.8259e-03,
        3.0422e-02, 8.5282e-03, 5.0535e-01, 1.9357e-02, 6.5071e-04, 1.7179e-03,
        3.6567e-01, 6.3164e-02, 9.6239e-01, 9.9997e-01, 1.0000e+00, 9.9998e-01,
        5.6619e-01, 4.3970e-02], device='cuda:0')
Sum Train Loss:  tensor([6.5039e-01, 1.0349e-02, 4.2997e-01, 2.1432e-02, 2.4356e-03, 1.7543e-02,
        3.4467e-03, 1.1328e-02, 9.2096e-03, 1.3923e-02, 3.7837e-03, 9.5795e-04,
        5.5298e-04, 8.4217e-02, 6.1891e-02, 5.6955e-02, 8.5623e-02, 3.7281e-02,
        4.9576e-03, 1.9956e-02, 2.3409e-04, 4.4520e-03, 1.2288e-03, 5.0566e-03,
        7.2208e-02, 7.5594e-03, 1.4238e-01, 8.7600e-03, 2.4027e-03, 7.4897e-03,
        3.9260e-03, 1.2374e-03, 8.2622e-02, 5.5531e-04, 2.3367e-03, 4.7074e-03,
        2.5065e-03, 7.3231e-03, 1.3365e-02, 2.3534e-01, 1.6970e+00, 9.4034e+00,
        2.6419e+00, 2.5619e+00, 1.2662e+00, 1.0387e+01, 2.3282e-03, 5.0843e-03,
        3.6264e-03, 6.6667e-03, 2.9667e-03, 8.5418e-03, 8.2506e-04, 1.2873e-02,
        3.4322e-03, 4.1767e-02, 1.8636e+00, 8.4047e-02, 9.5615e-01, 8.9968e-03,
        5.2654e+00, 7.4386e-03, 4.1159e-01, 7.4406e-02, 2.4532e-02, 3.2837e-02,
        7.4550e-02, 2.0675e-01, 6.0500e-01, 5.9242e-02, 8.6074e-04, 2.5211e-02,
        4.9427e-01, 5.8291e-01, 2.7418e+00, 7.6455e+00, 5.9889e+00, 4.4113e-01,
        1.8277e+00, 6.9942e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [31/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 59.7
Sum Train Loss:  tensor([6.2275e-01, 8.6069e-03, 3.8849e-01, 1.1112e-02, 3.0718e-03, 1.2848e-02,
        1.6085e-03, 1.5606e-02, 1.9406e-02, 7.3393e-03, 3.1692e-03, 4.0957e-03,
        5.7591e-04, 4.6947e-02, 3.1142e-02, 3.5093e-02, 7.2983e-02, 5.4608e-02,
        5.7299e-02, 1.0165e-02, 3.7512e-04, 9.5230e-04, 4.0005e-04, 1.0855e-03,
        5.9787e-02, 7.9290e-03, 1.6342e-01, 6.6410e-03, 1.7531e-03, 8.2777e-03,
        9.3833e-03, 9.4755e-04, 1.1847e-01, 1.0622e-03, 2.9221e-03, 2.7026e-02,
        4.0763e-03, 6.2239e-03, 2.1722e-02, 2.4212e-01, 7.9631e-01, 4.6105e+00,
        1.7295e+00, 4.4996e+00, 3.1518e+00, 6.4881e+00, 4.1899e-03, 9.3681e-03,
        9.8373e-03, 1.8467e-03, 8.2485e-04, 2.5805e-03, 5.1880e-03, 6.7145e-03,
        3.1275e-03, 3.2339e-02, 1.6800e+00, 1.7484e-01, 9.6063e-01, 2.1459e-02,
        3.1888e+00, 3.8555e-03, 7.8196e-01, 1.3586e-01, 1.2973e-01, 3.2600e-02,
        1.1591e-01, 9.6722e-02, 6.6496e-01, 1.2885e-01, 8.2021e-05, 1.1909e-02,
        1.1784e+00, 8.9380e-01, 4.6642e+00, 3.5051e+00, 7.3530e+00, 3.5566e-01,
        2.6517e-02, 1.6086e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [31/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 49.7
Sum Train Loss:  tensor([8.5990e-01, 2.2918e-02, 3.3214e-01, 1.8454e-02, 6.3497e-03, 1.2438e-02,
        2.4818e-03, 1.5847e-02, 2.9212e-02, 6.9640e-03, 5.0809e-04, 2.9347e-03,
        8.5192e-05, 6.8272e-02, 1.5944e-02, 2.5660e-02, 6.5968e-02, 3.8519e-02,
        1.3613e-02, 1.4379e-02, 3.8666e-04, 8.8583e-04, 4.2119e-04, 7.9968e-04,
        9.7907e-02, 9.5618e-03, 2.7732e-01, 3.4947e-03, 1.2340e-03, 2.6411e-02,
        2.8753e-02, 6.3613e-03, 9.0542e-02, 3.4080e-04, 2.4142e-03, 9.6785e-02,
        6.8502e-03, 6.9605e-03, 4.0847e-03, 3.2695e-01, 1.4354e+00, 9.4640e+00,
        6.8476e+00, 3.3551e+00, 5.1271e+00, 1.1684e+01, 2.8196e-03, 3.0893e-03,
        1.1612e-02, 4.7582e-03, 1.9365e-03, 5.2849e-03, 1.2057e-03, 1.7114e-02,
        2.9341e-03, 3.3713e-02, 2.8251e+00, 8.6863e-02, 1.1545e+00, 1.7887e-02,
        3.4829e+00, 8.9296e-03, 6.6311e-01, 5.3670e-02, 1.2788e-02, 6.5963e-02,
        2.0617e-02, 1.5875e-01, 3.0118e+00, 1.5534e-01, 2.6115e-03, 1.6591e-02,
        4.7419e+00, 1.0261e+00, 9.2920e-01, 6.3980e+00, 9.8438e-02, 2.5317e+00,
        1.1611e-01, 1.5177e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [31/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 68.3
Sum Train Loss:  tensor([9.7962e-01, 1.6047e-02, 9.1696e-01, 9.3799e-03, 2.8430e-03, 1.5453e-02,
        2.1773e-03, 1.6807e-02, 1.7730e-02, 1.9947e-02, 3.4079e-03, 1.7570e-03,
        2.9295e-04, 2.5224e-02, 5.4487e-02, 5.3933e-02, 1.2876e-01, 5.4923e-02,
        1.4361e-02, 1.9570e-02, 4.4592e-04, 4.6674e-04, 9.6825e-04, 4.0916e-03,
        4.6608e-02, 5.4413e-03, 2.6333e-01, 6.7622e-03, 4.0151e-03, 4.8794e-03,
        2.5959e-03, 4.4659e-04, 9.7167e-02, 6.3578e-04, 7.1110e-03, 4.2879e-02,
        4.3577e-03, 2.1351e-02, 1.7568e-02, 3.0628e-01, 1.6676e+00, 6.8236e+00,
        5.7181e+00, 5.5604e+00, 8.6424e+00, 6.2829e+00, 5.0983e-03, 5.2215e-03,
        1.9919e-02, 1.1834e-02, 1.3766e-03, 1.0624e-03, 2.8573e-03, 3.3656e-02,
        5.0584e-03, 4.1349e-02, 2.2499e+00, 7.7884e-02, 6.3756e-01, 1.2647e-02,
        1.1413e+01, 1.2444e-02, 1.2116e+00, 9.1051e-02, 1.0786e-01, 2.3450e-02,
        3.6481e-01, 2.4028e-01, 3.4939e+00, 1.2871e-01, 5.1949e-05, 1.7230e-02,
        1.4604e+00, 8.6141e-01, 1.3957e+00, 1.5405e+00, 4.2017e+00, 1.4613e+00,
        1.2360e-01, 7.8573e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [31/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 69.2
Sum Train Loss:  tensor([5.5746e-01, 1.1665e-02, 4.1262e-01, 1.3458e-02, 5.1323e-03, 2.5887e-02,
        1.6412e-03, 1.6291e-02, 1.1027e-02, 1.1168e-02, 5.7821e-03, 5.5172e-03,
        7.1292e-04, 4.4453e-02, 5.1581e-02, 2.3805e-02, 7.5446e-02, 6.7164e-02,
        5.7965e-03, 6.4177e-02, 8.3901e-05, 3.3098e-03, 6.0293e-04, 1.5967e-03,
        4.4827e-02, 6.5616e-03, 2.0866e-01, 4.8215e-03, 5.4521e-03, 4.9238e-03,
        1.7825e-03, 1.0843e-03, 2.3649e-01, 7.2687e-04, 1.2380e-02, 7.3874e-02,
        2.9890e-03, 5.0582e-03, 4.0473e-02, 2.0174e-01, 3.6968e-01, 7.4984e+00,
        1.3348e+00, 3.0877e+00, 2.8987e+00, 5.6668e+00, 5.0514e-03, 5.1651e-03,
        3.3624e-03, 1.1895e-02, 3.3670e-04, 3.2211e-03, 1.1378e-03, 5.2339e-03,
        1.3151e-03, 2.6900e-02, 2.5768e+00, 3.1842e-02, 1.3983e+00, 1.0551e-02,
        5.5676e+00, 6.8953e-03, 7.7776e-01, 1.0618e-01, 3.7400e-02, 2.0277e-02,
        6.7711e-02, 1.5160e-01, 5.1691e+00, 1.1396e-01, 9.4449e-05, 1.2037e-02,
        3.8610e-01, 4.0465e-01, 5.6554e+00, 3.4590e-01, 9.4436e+00, 1.6184e+00,
        4.6205e+00, 2.2180e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [31/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 61.9
Sum Train Loss:  tensor([8.2870e-01, 2.4784e-02, 6.1319e-01, 4.5235e-03, 7.9761e-03, 3.4739e-02,
        4.5229e-03, 8.6869e-03, 2.9548e-02, 7.4650e-03, 3.4584e-03, 2.1330e-03,
        1.5020e-04, 2.8364e-02, 5.0568e-02, 3.1681e-02, 5.3185e-02, 1.5149e-02,
        1.4617e-02, 3.6192e-03, 3.0996e-04, 1.5349e-03, 1.0227e-03, 1.2969e-03,
        5.0379e-02, 8.1956e-03, 2.8770e-01, 3.9456e-03, 2.9231e-03, 7.7000e-03,
        6.7783e-03, 3.3594e-03, 6.4930e-02, 9.7371e-04, 7.5788e-03, 7.9509e-02,
        5.6125e-03, 9.1323e-03, 9.0925e-03, 2.2136e-01, 2.3057e+00, 5.4102e+00,
        6.4134e+00, 1.8733e+00, 7.3555e+00, 1.0194e+01, 4.0002e-03, 1.8766e-03,
        1.0328e-02, 2.5446e-03, 1.3113e-03, 4.6994e-03, 2.8887e-03, 8.1459e-03,
        2.2997e-03, 2.2373e-02, 1.9934e+00, 4.0116e-02, 1.7481e+00, 1.0202e-02,
        7.2779e+00, 3.3519e-02, 6.7398e-01, 9.5425e-02, 2.1936e-02, 6.9463e-03,
        1.5370e-01, 1.1877e-01, 3.9070e-01, 1.9262e-01, 1.2746e-04, 2.3494e-02,
        2.0991e+00, 8.3466e-01, 1.1261e+00, 3.2312e-01, 3.6391e+00, 1.6081e-01,
        4.4928e-01, 3.3527e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [31/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 57.6
Sum Train Loss:  tensor([7.9374e-01, 8.1091e-03, 4.2768e-01, 8.2037e-03, 2.9744e-03, 1.6079e-02,
        3.4547e-03, 1.0741e-02, 7.4076e-03, 8.1294e-03, 1.2467e-03, 2.8622e-03,
        4.4326e-04, 3.7161e-02, 8.5865e-02, 4.9900e-02, 8.5814e-02, 5.9228e-02,
        4.5670e-03, 2.4817e-02, 5.0006e-04, 1.5510e-03, 3.3648e-03, 4.8548e-03,
        6.8156e-02, 1.0177e-02, 2.5434e-01, 1.0203e-02, 4.7353e-03, 9.6342e-03,
        7.8459e-03, 1.4204e-03, 4.0368e-02, 6.5605e-04, 2.6857e-03, 1.9539e-02,
        7.9085e-03, 3.7371e-03, 5.3561e-02, 2.6234e-01, 1.7228e+00, 6.8736e+00,
        5.6886e+00, 3.0171e+00, 2.9926e+00, 5.7636e+00, 5.2478e-03, 4.2022e-03,
        1.8942e-02, 1.4301e-02, 9.2484e-04, 1.7398e-03, 6.2428e-03, 1.8134e-02,
        6.2079e-03, 2.7279e-02, 2.3100e+00, 3.7477e-02, 7.2061e-01, 1.5839e-02,
        1.1807e+01, 4.4975e-03, 8.9723e-01, 6.3471e-02, 5.3258e-02, 6.0056e-02,
        4.7034e-02, 9.5863e-02, 5.4682e-01, 7.6805e-02, 2.1212e-03, 9.6669e-03,
        1.3823e+00, 1.0527e+00, 1.3531e+00, 1.1151e+00, 1.1750e-01, 4.6419e-01,
        2.2956e-01, 2.2967e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [31/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 51.3
Sum_Val Meta Model:  tensor([1.0978e+00, 1.0591e-01, 2.1486e+00, 7.8082e-02, 1.1053e-03, 3.0714e-02,
        1.8255e-03, 1.9470e-02, 2.4248e-02, 1.2779e-02, 2.0009e-03, 1.0610e-02,
        1.6547e-03, 4.2684e-02, 2.9491e-02, 4.5521e-02, 2.3946e+00, 1.0952e-02,
        2.9388e-03, 3.7323e-03, 9.5391e-05, 1.1842e-04, 4.6794e-04, 3.7012e-04,
        8.4445e-02, 1.2535e-02, 3.7148e-01, 1.9004e-03, 3.7353e-03, 1.4339e-02,
        1.9870e-03, 1.8044e-04, 9.7473e-02, 1.3753e-04, 1.0756e-03, 1.2079e-02,
        3.1877e-03, 9.1477e-03, 6.0239e-03, 5.5150e-01, 1.6415e+00, 1.6653e+01,
        6.8873e+00, 1.5452e+01, 1.9044e+01, 1.7031e+01, 1.0604e-03, 4.1631e-03,
        9.3025e-04, 6.3972e-03, 9.7450e-05, 5.9816e-04, 8.2996e-03, 2.4025e-03,
        7.6070e-04, 5.1931e-03, 2.9162e+00, 4.0955e-02, 9.8263e-01, 5.7767e-03,
        1.3207e+01, 2.9875e-02, 5.3106e-01, 4.3234e-02, 1.5431e-02, 6.1508e-03,
        3.2113e-02, 6.6074e-02, 5.6594e+00, 3.0607e-01, 1.6866e-04, 2.8975e-02,
        7.1862e+00, 6.6517e-01, 2.6431e+01, 7.0151e+00, 1.6238e-01, 9.1390e+00,
        3.1330e-02, 3.1670e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.0034e+00, 6.5099e-02, 1.4259e+00, 4.2538e-02, 2.8101e-04, 2.9827e-02,
        1.1979e-03, 1.9398e-02, 2.2175e-02, 1.3564e-02, 2.3389e-03, 1.2506e-02,
        1.8167e-03, 4.1239e-02, 2.8491e-02, 1.9813e-01, 1.5783e+00, 1.7952e-02,
        1.7040e-03, 3.5721e-03, 8.2716e-05, 5.9987e-05, 1.9540e-04, 1.1594e-03,
        8.4864e-02, 1.2418e-02, 3.4144e-01, 2.6427e-03, 5.1549e-03, 1.6239e-02,
        8.1267e-04, 1.4273e-04, 9.4512e-02, 1.6145e-04, 1.1072e-03, 1.2221e-02,
        9.4226e-03, 7.2109e-03, 6.0390e-03, 4.6146e-01, 1.9484e+00, 2.1901e+01,
        1.0579e+01, 1.9962e+01, 2.9497e+01, 3.0216e+01, 7.2939e-04, 4.3292e-03,
        1.8681e-04, 3.6401e-03, 1.2813e-05, 1.4727e-04, 8.5108e-03, 3.5889e-04,
        2.4229e-04, 1.1968e-03, 2.4129e+00, 4.3255e-02, 8.4438e-01, 7.0418e-03,
        2.3327e+01, 1.3547e-02, 4.6041e-01, 5.9861e-02, 6.7724e-03, 1.0464e-02,
        1.0039e-02, 8.7787e-02, 7.3631e+00, 3.1048e-01, 2.9984e-04, 3.2482e-02,
        5.7667e+00, 7.5754e-01, 3.0420e+01, 1.2984e+01, 4.2280e-02, 9.7905e+00,
        7.3715e-02, 1.0134e-01], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.7584e+01, 4.3632e+01, 5.4976e+01, 2.4695e+01, 3.4258e-01, 1.4357e+01,
        3.1477e+00, 1.6858e+01, 5.5897e+00, 1.0506e+01, 6.8978e+00, 1.4233e+01,
        8.5752e+00, 1.3977e+01, 8.0973e+00, 4.7921e+01, 3.0651e+02, 3.3681e+00,
        4.6336e-01, 8.4214e-01, 1.3053e+00, 1.3133e-01, 1.7879e-01, 1.5961e+00,
        2.7663e+01, 1.8648e+01, 2.7434e+01, 4.5386e+00, 1.4910e+01, 1.3864e+01,
        6.6390e-01, 3.6707e-01, 7.0433e+00, 1.0133e+00, 9.4354e-01, 1.2896e+00,
        1.4195e+01, 4.3083e+00, 1.2982e+00, 4.2218e+01, 1.2767e+01, 4.1464e+01,
        1.2273e+01, 2.6205e+01, 3.0042e+01, 3.3432e+01, 1.3275e+00, 5.6608e+00,
        1.2942e-01, 2.7019e+00, 5.5080e-02, 2.0033e-01, 7.6827e+00, 1.8878e-01,
        3.9947e-01, 4.2710e-01, 2.3156e+01, 6.6692e+00, 1.0628e+01, 4.6483e+00,
        2.6483e+01, 6.3295e+00, 4.1153e+00, 6.0730e+00, 3.1843e-01, 2.7351e+00,
        3.2998e-01, 1.0294e+01, 1.4570e+01, 1.6040e+01, 4.6079e-01, 1.8908e+01,
        1.5770e+01, 1.1993e+01, 3.1608e+01, 1.2985e+01, 4.2280e-02, 9.7908e+00,
        1.3019e-01, 2.3047e+00], device='cuda:0')
Outer loop valEpocw Maximum [31/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 158.5
model_train val_loss valEpocw [31/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 214.6
model_train val_loss valEpocw [31/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1227.5
Sum_Val Meta Model:  tensor([2.6613e+00, 6.2611e-02, 2.5798e+00, 3.8789e-02, 1.5807e-03, 2.0791e-01,
        8.5889e-02, 1.7604e-01, 2.4730e-01, 1.1594e-01, 2.1726e-02, 5.8675e-01,
        7.9638e-03, 3.4394e-02, 1.6841e-01, 2.1796e-01, 1.1194e-01, 2.2238e-01,
        7.5440e-02, 5.8486e-01, 1.6719e-05, 7.4358e-05, 7.6220e-04, 1.0626e-02,
        2.0842e-01, 4.5950e-02, 6.7569e-01, 3.0468e-02, 1.5469e-02, 1.6615e-04,
        3.9250e-04, 6.6985e-05, 2.5241e-03, 9.0810e-05, 1.2649e-04, 8.8542e-04,
        8.9437e-03, 4.1934e-04, 4.5538e-04, 3.3128e-02, 1.8646e-02, 1.8685e+00,
        9.4566e-02, 1.9630e-01, 3.0086e-01, 3.8833e+00, 1.7219e-03, 1.5308e-03,
        2.9745e-04, 2.2861e-02, 3.3331e-05, 2.1118e-04, 1.1063e-04, 1.5573e-04,
        2.7844e-04, 1.9889e-02, 1.7267e+00, 6.9766e-02, 7.5758e-01, 1.9040e-02,
        1.5359e+00, 3.5452e-03, 2.9631e-01, 3.6744e-02, 1.6229e-02, 7.6798e-03,
        2.3717e-02, 2.7191e-01, 4.4936e-02, 1.3611e-01, 3.3209e-05, 5.5856e-03,
        1.8968e+00, 4.8945e-01, 4.6371e+00, 3.0207e-01, 1.3194e-01, 6.1198e-01,
        2.1739e-02, 5.5807e-03], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([2.3399e+00, 7.2033e-02, 1.9013e+00, 4.9848e-02, 5.6361e-03, 1.7629e-01,
        6.4896e-02, 1.2277e-01, 2.4575e-01, 1.0368e-01, 1.7690e-02, 1.7008e-01,
        6.0454e-03, 4.8503e-02, 1.2611e-01, 3.1993e-02, 8.1122e-02, 2.1214e-01,
        2.4531e-02, 2.1762e-01, 2.5155e-04, 2.0515e-04, 5.9764e-04, 9.5693e-03,
        1.4040e-01, 3.8597e-02, 6.7524e-01, 2.6574e-02, 1.6701e-02, 2.2962e-03,
        8.0735e-04, 2.0251e-04, 1.7896e-02, 1.4632e-03, 1.3884e-03, 4.9021e-03,
        9.2380e-03, 4.8266e-03, 1.3150e-03, 4.2308e-02, 2.4774e-02, 3.2877e+00,
        7.5349e-03, 2.4571e-01, 4.6583e-01, 3.7727e-01, 1.3582e-03, 5.5561e-04,
        4.5189e-04, 1.8605e-02, 8.0791e-05, 5.4803e-04, 6.4367e-04, 1.3335e-03,
        4.1451e-04, 4.9902e-03, 1.5493e+00, 6.3778e-02, 4.1457e-01, 2.5145e-03,
        2.8432e+00, 2.5707e-03, 3.0994e-01, 7.5620e-03, 3.9409e-03, 1.9412e-03,
        4.7003e-03, 2.9010e-01, 9.1945e-02, 9.2046e-02, 6.4144e-05, 3.3213e-03,
        1.5287e+00, 2.1911e-01, 5.0695e+00, 6.1984e-02, 1.5917e+00, 1.1053e-02,
        2.2849e-03, 2.8040e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.7840e+01, 2.0714e+01, 4.8204e+01, 1.1599e+01, 2.5173e+00, 3.3740e+01,
        4.4248e+01, 3.8288e+01, 2.8452e+01, 3.2327e+01, 1.7479e+01, 6.6170e+01,
        8.0733e+00, 8.7195e+00, 1.5525e+01, 3.3810e+00, 7.9806e+00, 1.7626e+01,
        2.5616e+00, 2.0248e+01, 8.6018e-01, 1.4288e-01, 2.0408e-01, 4.6987e+00,
        2.5256e+01, 1.8189e+01, 3.1911e+01, 1.3894e+01, 1.3100e+01, 7.6406e-01,
        2.5886e-01, 1.5924e-01, 7.5111e-01, 2.6290e+00, 4.7274e-01, 3.4485e-01,
        4.6020e+00, 1.2230e+00, 1.2846e-01, 2.4549e+00, 1.9330e-01, 6.7650e+00,
        9.6463e-03, 3.6897e-01, 4.8592e-01, 4.4861e-01, 8.1779e-01, 2.5388e-01,
        1.1198e-01, 5.0756e+00, 1.0575e-01, 2.9763e-01, 2.3127e-01, 2.8294e-01,
        2.2685e-01, 8.6292e-01, 1.1067e+01, 4.6830e+00, 4.6413e+00, 6.0181e-01,
        3.4078e+00, 4.8896e-01, 2.3481e+00, 4.0698e-01, 1.2514e-01, 2.2110e-01,
        1.1306e-01, 1.9017e+01, 2.1491e-01, 3.2995e+00, 3.6323e-02, 7.9613e-01,
        4.8590e+00, 2.4369e+00, 5.4437e+00, 6.1992e-02, 1.5917e+00, 1.1055e-02,
        4.8961e-03, 4.9189e-01], device='cuda:0')
Outer loop valEpocw Maximum [31/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 28.7
model_train val_loss valEpocw [31/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 25.6
model_train val_loss valEpocw [31/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 700.6
Sum_Val Meta Model:  tensor([1.8147e+00, 6.5410e-03, 3.1320e-01, 6.8827e-03, 1.2707e-03, 8.9690e-03,
        1.2550e-03, 1.9693e-02, 1.2529e-02, 3.4900e-03, 9.0328e-04, 2.6697e-03,
        1.5313e-04, 6.8760e-02, 1.7683e-02, 2.2336e-02, 6.2960e-02, 4.2829e-02,
        8.4339e-03, 1.4106e-02, 1.6929e-04, 1.5160e-03, 2.1913e-02, 2.7576e-03,
        3.4406e-02, 2.8294e-03, 3.5079e-01, 7.1252e-03, 6.1619e-04, 1.2196e-02,
        9.1445e-03, 4.4713e-03, 1.6030e-01, 9.2195e-05, 6.2081e-03, 4.7651e-02,
        2.0540e-02, 3.5160e-02, 3.0612e-03, 3.4704e-01, 4.1170e+00, 3.3482e+01,
        6.1238e+01, 5.8441e+01, 5.2761e+01, 7.5977e+01, 1.6759e-02, 2.1107e-02,
        1.0830e-01, 5.3647e-02, 1.2649e-02, 4.6649e-02, 1.1265e-01, 4.4862e-02,
        6.8989e-02, 4.2667e-01, 1.8866e+01, 1.0510e-01, 7.1598e-01, 6.2465e-03,
        8.5212e+01, 6.0669e-03, 1.2606e+00, 1.6910e-01, 3.2558e-01, 6.7199e-03,
        3.1666e-01, 1.1968e-01, 2.2169e+00, 5.5844e-02, 3.1235e-04, 1.8187e-02,
        1.3408e+00, 2.5799e+00, 1.1026e+00, 2.9793e+01, 1.0520e+01, 1.1041e+00,
        1.1182e-01, 3.6782e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.2575e-01, 9.7830e-04, 1.6483e-01, 1.1904e-03, 3.1648e-05, 3.9197e-04,
        7.2132e-05, 2.1613e-02, 6.2458e-04, 3.8975e-04, 4.1752e-05, 5.9197e-05,
        7.2510e-06, 5.5828e-02, 1.3145e-03, 1.4911e-02, 1.0352e-02, 9.8458e-04,
        2.9939e-04, 1.3718e-04, 6.3002e-06, 7.6506e-06, 2.4717e-05, 3.5409e-05,
        1.9867e-02, 1.5342e-03, 3.5563e-01, 5.4270e-03, 3.2606e-04, 3.6848e-04,
        6.2684e-04, 4.8588e-03, 1.5830e-01, 2.4159e-05, 1.5060e-03, 1.3425e-02,
        1.1157e-02, 2.9425e-02, 4.1264e-04, 5.2040e-01, 4.0100e+00, 4.1582e+01,
        8.2226e+01, 6.9860e+01, 9.5480e+01, 1.0004e+02, 8.1356e-03, 7.4152e-03,
        9.0915e-02, 2.4893e-02, 6.8446e-03, 4.9569e-02, 1.0917e-01, 4.8982e-02,
        5.4664e-02, 2.9053e-01, 1.5140e+01, 1.2356e-01, 7.2719e-01, 3.1887e-03,
        1.3105e+02, 1.2043e-03, 1.0978e+00, 1.2708e-01, 2.6199e-01, 2.4136e-02,
        3.3425e-01, 1.3710e-01, 2.3169e+00, 1.0868e-01, 1.5670e-04, 1.8493e-02,
        1.5103e+00, 2.4577e+00, 1.3364e+00, 3.5917e+01, 1.8520e+01, 2.8185e-01,
        3.4924e-02, 4.8561e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.1834e+01, 5.4450e-01, 5.8868e+00, 5.4429e-01, 3.0234e-02, 1.4687e-01,
        1.4092e-01, 1.5368e+01, 1.3552e-01, 2.4301e-01, 9.3847e-02, 4.8750e-02,
        2.7262e-02, 1.6447e+01, 3.1229e-01, 3.1932e+00, 1.6712e+00, 1.2777e-01,
        6.4296e-02, 2.6565e-02, 7.4995e-02, 1.2435e-02, 1.5331e-02, 3.7348e-02,
        5.4528e+00, 1.7405e+00, 2.7446e+01, 7.0965e+00, 7.2124e-01, 2.1703e-01,
        3.6502e-01, 8.1846e+00, 9.7330e+00, 1.0900e-01, 9.5320e-01, 1.2187e+00,
        1.1405e+01, 1.3362e+01, 7.2650e-02, 4.6725e+01, 3.0594e+01, 7.9540e+01,
        9.6346e+01, 9.2810e+01, 9.7789e+01, 1.1313e+02, 1.0189e+01, 6.5139e+00,
        4.0285e+01, 1.2241e+01, 1.9531e+01, 4.7561e+01, 6.8075e+01, 1.9837e+01,
        6.2561e+01, 7.9990e+01, 1.4227e+02, 1.5943e+01, 1.0111e+01, 1.5520e+00,
        1.5021e+02, 4.5988e-01, 1.0164e+01, 1.1492e+01, 1.2343e+01, 5.0793e+00,
        1.1203e+01, 1.4964e+01, 5.4506e+00, 5.9603e+00, 1.9166e-01, 9.0416e+00,
        5.0410e+00, 3.7836e+01, 1.4025e+00, 3.5918e+01, 1.8520e+01, 2.8186e-01,
        7.4306e-02, 1.1431e+00], device='cuda:0')
Outer loop valEpocw Maximum [31/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 446.4
model_train val_loss valEpocw [31/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 607.4
model_train val_loss valEpocw [31/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1585.5
Sum_Val Meta Model:  tensor([4.4627e+00, 1.1567e-02, 1.1341e+00, 7.8699e-03, 1.9563e-03, 6.9854e-02,
        1.0595e-02, 6.9287e-02, 2.0345e-02, 8.6397e-02, 6.0924e-03, 1.7165e-02,
        3.3035e-04, 1.1996e-01, 1.2860e-01, 2.4475e-02, 3.1657e-02, 2.9434e-02,
        7.6466e-03, 1.1268e-02, 5.5846e-04, 1.3940e-03, 3.3095e-03, 3.7674e-03,
        1.6650e-01, 7.2557e-03, 7.5421e-01, 5.0626e-03, 2.4334e-02, 3.3807e-03,
        3.9188e-03, 7.4786e-04, 1.9422e-01, 2.1770e-02, 3.9537e-02, 2.0640e-01,
        1.2110e-03, 1.0654e-02, 1.2354e-01, 5.8389e-01, 2.5634e+00, 1.7636e+01,
        8.2973e+00, 8.9320e+00, 1.1352e+01, 1.7275e+01, 2.0266e-03, 2.9050e-03,
        7.7283e-03, 4.1579e-03, 9.1234e-04, 1.3257e-03, 1.9342e-03, 5.8599e-02,
        2.0837e-03, 1.8279e-02, 5.1199e+00, 9.3802e-02, 1.5505e+00, 1.5322e-02,
        4.2154e+01, 7.5402e-03, 1.0721e+00, 3.1890e-01, 4.0918e-01, 3.1040e-02,
        5.7906e-01, 6.9502e-01, 7.8101e-01, 1.0994e-01, 3.7121e-04, 1.4172e-02,
        1.9142e+00, 1.3353e+00, 4.0265e+02, 1.3924e+01, 1.7880e+00, 8.1980e+00,
        6.0880e-02, 1.2152e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.3553e+00, 8.6512e-03, 6.8543e-01, 8.7809e-03, 4.1233e-03, 6.6812e-02,
        1.3728e-02, 6.2433e-02, 1.0993e-02, 7.5173e-02, 4.7943e-03, 2.4846e-02,
        7.2273e-04, 1.1609e-01, 1.6578e-01, 8.6758e-03, 9.7642e-03, 7.9378e-03,
        3.0024e-04, 4.8568e-04, 1.2698e-04, 4.6212e-05, 6.5178e-04, 1.8758e-03,
        1.5911e-01, 7.4833e-03, 7.0349e-01, 3.0892e-03, 3.1009e-02, 4.0188e-04,
        5.8631e-04, 1.8603e-04, 3.6660e-03, 1.5905e-04, 5.2431e-04, 2.5406e-03,
        1.9832e-03, 8.9497e-04, 5.3936e-04, 6.5377e-02, 6.2369e-02, 4.2629e-01,
        8.7506e-02, 3.0498e-01, 1.7677e-02, 3.4797e+00, 7.0740e-04, 3.9081e-04,
        2.7657e-04, 6.9866e-04, 8.3844e-05, 3.4971e-04, 1.6842e-04, 9.5529e-04,
        4.4555e-04, 3.4470e-03, 8.4118e-01, 1.0730e-02, 9.2657e-01, 3.5512e-03,
        2.1161e+01, 3.5082e-03, 1.7403e-01, 1.2757e-02, 1.2117e-02, 9.6343e-03,
        2.2749e-02, 1.3658e-01, 3.3456e-02, 1.9190e-02, 9.5344e-05, 3.1719e-03,
        6.6966e-02, 5.2472e-01, 6.7872e+01, 9.1397e-01, 2.6831e-02, 6.9681e+00,
        6.9322e-03, 9.8066e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.1442e+01, 1.9737e+00, 1.6338e+01, 1.6493e+00, 1.3713e+00, 1.0782e+01,
        8.2747e+00, 1.6604e+01, 1.1510e+00, 1.8219e+01, 3.4820e+00, 7.7454e+00,
        7.6042e-01, 1.5748e+01, 1.8537e+01, 9.3250e-01, 8.8243e-01, 5.3974e-01,
        3.1214e-02, 4.6344e-02, 3.3244e-01, 2.5332e-02, 1.8438e-01, 7.6894e-01,
        2.3068e+01, 2.8551e+00, 2.7922e+01, 1.2780e+00, 1.9846e+01, 1.0002e-01,
        1.4732e-01, 1.1626e-01, 1.1961e-01, 1.7108e-01, 1.2372e-01, 1.1809e-01,
        7.8128e-01, 1.7975e-01, 3.7985e-02, 2.8661e+00, 3.4905e-01, 8.1595e-01,
        1.1140e-01, 4.5054e-01, 1.8500e-02, 4.1532e+00, 3.4395e-01, 1.3909e-01,
        5.6091e-02, 1.5827e-01, 8.0442e-02, 1.5458e-01, 4.7794e-02, 1.4888e-01,
        1.9776e-01, 4.9320e-01, 5.5387e+00, 6.5748e-01, 9.5727e+00, 7.1015e-01,
        2.5300e+01, 5.3205e-01, 1.1812e+00, 5.2847e-01, 2.7937e-01, 8.6457e-01,
        4.0018e-01, 6.2384e+00, 7.6472e-02, 5.7136e-01, 4.0505e-02, 5.7964e-01,
        1.9475e-01, 5.2257e+00, 7.3703e+01, 9.1415e-01, 2.6831e-02, 6.9694e+00,
        1.7030e-02, 1.4256e-01], device='cuda:0')
Outer loop valEpocw Maximum [31/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 557.5
model_train val_loss valEpocw [31/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 107.8
model_train val_loss valEpocw [31/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 385.5
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.64391272 97.41353054 93.25909772 98.11527637 98.63305759 97.58409376
 98.37843106 95.23763112 98.01902998 96.94448167 98.662297   98.98393051
 99.4420146  95.41672251 97.4915023  97.32215738 96.44132016 97.72663588
 98.86331794 98.40279724 98.68544487 99.32871188 99.554099   99.08383182
 95.18524384 96.91280564 94.43598397 97.07971394 98.06654402 98.26878328
 98.4259451  98.61234634 97.03950975 98.57092384 98.72321244 98.87062779
 97.60480501 98.44543804 98.82311375 93.49423131 98.18593828 96.69594669
 98.55021259 98.33944518 99.0521558  97.80460764 98.27609313 98.70493781
 98.11527637 98.76341663 98.88768412 98.64036744 99.04484594 98.46614929
 98.74026876 97.70348802 93.27980897 97.02976328 96.83361557 97.68765
 97.62551626 98.74392369 97.74734713 97.41353054 98.89743059 97.63891765
 98.70737442 95.98079945 99.3031274  98.37599444 99.81481707 97.48419244
 99.0801769  96.11846834 98.9827122  99.25074012 99.73684531 99.50902158
 99.86964096 99.18495145]
Accuracy th:0.7 is [86.57545595 97.35870664 92.79857702 98.0775088  98.55021259 97.52317832
 98.28949452 95.0670679  97.85699492 97.03341821 98.60381818 98.93519816
 99.42739489 95.33631413 97.40256576 97.32581231 96.35847516 97.60480501
 98.79509265 98.33700856 98.60016325 99.2617049  99.53095113 98.93641647
 95.24494097 96.81655925 94.26298413 96.97737601 98.04461447 98.21517769
 98.15426225 98.57336046 96.81046771 98.41254371 98.61600127 98.83773346
 97.62186133 98.40036062 98.60259987 93.26275265 98.08725527 96.38771457
 98.1883749  98.12502284 98.93519816 98.0775088  98.22005093 98.68178994
 98.02999476 98.74392369 98.8024025  98.63549421 99.01195161 98.33091702
 98.72686736 97.58653038 92.25521132 96.88600285 96.70081992 97.54876281
 96.97859432 98.61965619 97.62307964 97.37210804 98.83773346 97.56947406
 98.64280406 95.98932762 99.18738807 98.35406489 99.81603538 97.41353054
 98.92667    96.09410217 98.95712772 99.2617049  99.75999318 99.43714136
 99.86233111 99.19835285]
Avg Prec: is [96.38360566 35.07004977 72.31644882 67.38832143 79.01729029 64.15497005
 75.12619497 47.36849691 58.10244901 53.46502771 34.17853985 55.10245045
 27.06076659 28.66960377 33.29044618 57.30891075 30.21491554 43.00354765
 48.99904967 39.44828993 60.27425399 51.62388877 89.78539439 84.96346064
 25.66408559 34.39593776 38.05595083 40.5770114  25.61896739 39.27875293
 72.0087014  38.78578639 57.02937559 63.28527309 72.55977542 79.20363742
 56.87414334 75.33546543 86.65094914 47.62209968 50.95915255 86.96724215
 88.82078548 85.28622166 91.23172414 91.99998524 40.75513094 35.98060965
 42.56027353 46.3818325  64.51624946 38.57986319 27.54777613 74.13958528
 27.69966749 43.31266389 75.40616758 59.18861637 49.4054348  60.56018482
 95.85597704 83.55808802 75.12047267 51.24041453 62.34578209 44.93392775
 61.51678073 26.55281898 80.35111538 68.5799185  12.32071214 72.86175614
 86.13064326 52.50044864 92.10404847 94.41928055 90.32005505 93.37295837
 44.0110096  32.60291406]
Accuracy th:0.5 is [45.79135244 97.2137279  70.46941436 97.02489005 97.26733349 75.08314957
 75.52052241 75.12213545 76.41476103 96.46934126 76.63283829 98.52097319
 99.41399349 79.9222719  76.27709214 96.56680596 96.29512311 76.02733885
 98.65376884 98.30776915 80.07090557 77.13112657 98.38695922 82.87057906
 83.10814927 96.65086926 94.0778012  75.7702757  98.01293844 76.58654256
 97.30875598 98.57457877 96.36213009 98.02024829 88.41875708 76.23445133
 76.13820494 91.56930349 97.11504489 73.49569328 78.68325191 92.05906361
 75.37798029 75.09776928 96.9627563  93.87434364 98.02877645 98.57336046
 97.85577661 90.33759335 89.16801696 98.55508583 98.99976852 75.58387447
 98.70615611 75.9786065  71.06638564 93.43331587 96.24273583 96.9067141
 89.79300934 97.17717864 94.85751879 76.08947259 98.42838172 76.47323985
 98.20786784 75.33533948 77.08361253 97.55972759 77.58921066 95.99054592
 77.04828158 95.45083515 75.43402249 83.64055019 90.20236108 76.62187352
 77.65012609 99.14718388]
Accuracy th:0.7 is [45.87541575 97.2137279  70.46941436 97.02489005 97.26733349 75.08314957
 75.52052241 75.49006469 76.41476103 96.4754328  76.63283829 98.52097319
 99.41399349 80.38035599 76.27709214 96.56680596 96.29512311 76.02733885
 98.65376884 98.30776915 80.46320098 77.13112657 98.38695922 83.95243723
 84.05355685 96.65086926 94.0778012  75.7702757  98.01293844 76.58654256
 97.30875598 98.57457877 96.36213009 98.02024829 88.61490479 76.23445133
 76.13820494 91.8629159  97.11504489 73.49569328 79.33748371 92.05906361
 75.37798029 75.09776928 96.9627563  93.87434364 98.02877645 98.57336046
 97.98248072 90.9016703  89.38974915 98.55508583 98.99976852 75.58387447
 98.70615611 75.98104312 71.06638564 93.90723797 96.24273583 96.9067141
 89.79300934 97.17717864 94.95985673 76.08947259 98.42838172 76.47323985
 98.20786784 75.33533948 77.08361253 97.55972759 77.62210499 95.99054592
 77.25904899 95.45083515 75.43402249 83.79893032 90.50571996 76.62187352
 77.65012609 99.14718388]
Avg Prec: is [55.64240658  3.06611194 11.14421117  3.3991644   2.21435586  3.6716298
  3.3276959   5.44322838  2.44800825  4.04622946  1.66087672  1.62964004
  0.64533444  5.29565833  2.75179333  3.14589117  3.72263749  2.76615922
  1.38636435  1.67049691  2.00957757  0.90370885  1.95448315  2.42421994
  5.12234907  3.47629396  6.49435125  3.28921768  2.04660383  1.92955626
  2.60100656  1.26429718  3.66493258  1.62355546  2.30485374  2.36326363
  2.9640414   2.55676571  2.86578233  7.38756575  2.25051149  8.29196031
  3.45232597  4.04308832  3.31108272  6.41688082  2.08883597  1.5225618
  2.20919058  1.61183667  1.90362643  1.57875746  1.10684787  3.16299489
  1.26542084  2.66790191 11.14865496  3.63919942  3.96911706  2.75967732
 10.84398136  2.18818319  3.77305786  2.9630911   1.55734177  2.51098333
  1.71649286  4.16756639  1.19920561  2.27993483  0.18214872  3.36036279
  1.86729059  4.44644765  3.877916    3.07856362  0.8417504   1.91319522
  0.12492046  0.70772219]
mAP score regular 58.59, mAP score EMA 3.75
starting validation
Accuracy th:0.5 is [87.68717144 97.42880634 93.1409921  98.25846476 98.91621197 97.63061514
 98.44034183 95.30109375 98.13389142 96.74116152 98.6745397  99.00092184
 99.38460772 95.27368762 97.46866981 97.24692927 96.41976231 97.76266288
 99.00092184 98.40296983 98.87385704 99.36467598 99.63624586 99.24259412
 95.14911428 96.91307273 94.49136707 97.30921594 97.91962528 98.23105862
 98.6894885  98.68699704 97.03017166 98.7642325  98.92866931 99.08812318
 97.84488128 98.35064903 99.02085358 93.2381593  98.00433515 92.23160675
 97.48611007 96.19054738 96.72372125 93.95071879 98.31078556 98.75924957
 98.08157062 98.6969629  98.95358397 98.60477863 98.90624611 98.4802053
 98.76921544 97.74023968 90.87624885 97.10491566 96.48952338 97.69041034
 92.87689663 98.83150211 97.29177567 97.47365274 98.82651917 97.44873807
 98.68699704 95.91399457 98.7567581  98.22109276 99.81812293 97.54590527
 98.35314049 95.46303909 97.30672447 97.416349   99.09559758 98.6820141
 99.80815706 99.02583651]
Accuracy th:0.7 is [87.63983357 97.4014002  92.93420036 98.29583676 98.88631437 97.59822608
 98.52256023 95.36836336 98.01679249 97.09245833 98.64713357 99.00839624
 99.37215038 95.14413135 97.4088746  97.3191818  96.36245858 97.67297008
 98.95109251 98.36808929 98.83150211 99.30238932 99.62129706 99.16286718
 95.42815856 96.77604206 94.46894387 97.17467673 97.85484715 98.19617809
 98.54996637 98.67952263 96.98034233 98.6222189  98.88382291 99.14044398
 97.97942048 98.28836236 98.85641677 93.24065077 97.96945462 92.84201609
 97.46119541 96.47955752 96.87320926 94.458978   98.38303809 98.82402771
 97.99935222 98.72935197 98.92119491 98.66955677 98.90375464 98.44034183
 98.72436904 97.75020555 90.80399631 97.12733886 96.47457458 97.63808954
 92.79966116 98.75426664 97.2743354  97.48112714 98.83399357 97.59573461
 98.59730423 95.85669083 98.81904477 98.26843063 99.81563146 97.54341381
 98.38054663 95.93641777 97.37648554 97.50604181 99.18778185 98.65460797
 99.8206144  99.15040985]
Avg Prec: is [96.36259759 34.06058585 70.31129392 73.28943504 76.90166226 64.66608571
 81.00290357 48.20016112 62.78955594 55.78313551 37.45737189 56.2693644
 25.18173443 30.75400532 33.9371657  61.7986741  32.75219165 44.60180135
 50.97749716 37.91681256 68.87108026 59.17722706 92.1482901  87.96877392
 24.18329682 38.53300886 32.01480124 46.37214233 27.50745984 37.8247899
 77.85481335 38.52873114 56.43717725 66.49058205 74.10167422 82.35076812
 60.07103445 76.75675859 89.54684424 44.73689229 36.66354931 45.76928305
 50.39128506 38.37490281 29.5005275  51.03475229 39.28670899 28.71926491
 41.9736479  41.95038022 70.12821113 36.30195075 25.97800533 76.66638088
 29.48628476 40.33463944 55.11127349 57.17073385 40.63763719 63.68235662
 63.25190325 86.03137594 66.2546781  53.25554289 62.28222826 38.79378734
 61.91616859 27.59224923 40.58422572 64.66628544 12.10274162 73.80025061
 51.20485517 45.45341764 63.27320356 48.47122074 13.14841189 51.81854103
  2.88220284 22.65640404]
Accuracy th:0.5 is [45.28489922 97.22450607 68.77195605 96.96290206 97.90716795 74.02396791
 74.13608391 73.4932855  75.62847248 96.41976231 75.75304582 98.5325261
 99.34972718 77.56185066 75.74058848 96.31262924 96.21047911 75.1177218
 98.78167277 98.34068316 78.34417121 76.38836983 98.31327703 84.65505643
 78.54847149 96.52938685 94.3393876  75.23482074 97.81747515 75.71567382
 97.52597354 98.67204823 96.39983058 98.18870369 89.66539602 75.35191968
 75.38929168 92.80464409 97.0276802  72.67359294 76.69731171 92.37362035
 74.47990632 74.19837058 97.03764606 94.02795426 98.18621222 98.77668984
 97.95201435 90.72925231 88.31003812 98.55993223 98.87385704 74.44502579
 98.6969629  75.17253407 69.57919127 94.41164013 96.16314124 96.78102499
 90.13379176 97.04761193 95.15160575 75.26720981 98.32075143 76.10932556
 98.13139996 74.46994045 76.4656053  97.53593941 76.80942771 96.07843137
 76.2936941  95.44559882 74.37277325 84.92413484 92.17181155 75.82778982
 76.88417171 99.15040985]
Accuracy th:0.7 is [45.57141789 97.22450607 68.77195605 96.96290206 97.90716795 74.02396791
 74.13608391 73.7175175  75.62847248 96.41976231 75.75304582 98.5325261
 99.34972718 77.98290854 75.73809702 96.31262924 96.21047911 75.1177218
 98.78167277 98.34068316 78.63567282 76.38836983 98.31327703 85.45980018
 79.26103097 96.52938685 94.3393876  75.23482074 97.81747515 75.71567382
 97.52597354 98.67204823 96.39983058 98.18870369 89.81239256 75.35191968
 75.38929168 92.98901263 97.0276802  72.67359294 77.22051972 92.37362035
 74.47990632 74.19837058 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 91.12290405 88.62894586 98.55993223 98.87385704 74.44502579
 98.6969629  75.17502554 69.57919127 94.78536014 96.16314124 96.78102499
 90.13379176 97.04761193 95.20392655 75.27468421 98.32075143 76.10932556
 98.13139996 74.46994045 76.4656053  97.53593941 76.81191918 96.07843137
 76.40331863 95.44559882 74.37277325 85.18573884 92.49570222 75.82778982
 76.88417171 99.15040985]
Avg Prec: is [54.43643288  3.76378701 14.89465209  4.58408474  1.61093219  4.29998437
 10.78754921  8.77614982  7.19275954  5.13404114  2.2590254   5.29914528
  1.55108911  5.89322907  3.05548831  4.23021592 26.35908598  6.21931782
  1.53506684  2.6108429   3.64833791  1.43608945  1.35975992  5.73069015
  5.74740143 12.49796591  8.22806207  4.52034852  3.91097322  6.94595242
  2.31841696  0.87153441  2.9807823   1.15468911  1.73720846  2.42419499
  2.05348207  2.16964504  2.20278063  6.20459471  1.73711278  6.05303496
  2.20568846  2.7438401   2.33864488  4.93737645  1.79352138  1.05760559
  1.46348686  1.21008152  1.19389041  1.00213223  0.74933166  2.32223815
  0.96459391  1.90556985 10.2374786   3.09364201  3.91746399  2.8923932
  7.96843689  2.0086721   3.29815817  2.61947899  1.35320749  1.88857222
  1.57372542  3.52795142  1.05131767  2.18535226  0.19306142  3.15603812
  1.57061514  4.00389885  3.19699976  2.34539758  0.56578982  1.52372695
  0.12182005  0.60846697]
mAP score regular 51.31, mAP score EMA 4.39
Train_data_mAP: current_mAP = 58.59, highest_mAP = 58.59
Val_data_mAP: current_mAP = 51.31, highest_mAP = 51.91
tensor([1.9633e-02, 1.3146e-03, 2.3330e-02, 1.5762e-03, 7.4022e-04, 1.8527e-03,
        3.4323e-04, 1.0156e-03, 3.5314e-03, 1.1240e-03, 2.9134e-04, 8.1035e-04,
        1.8130e-04, 2.6302e-03, 3.1265e-03, 3.5357e-03, 4.4192e-03, 5.2436e-03,
        3.2513e-03, 3.7599e-03, 5.5649e-05, 4.1036e-04, 9.7598e-04, 6.4601e-04,
        2.7823e-03, 6.0698e-04, 1.2177e-02, 5.2390e-04, 3.0613e-04, 1.0918e-03,
        1.0955e-03, 3.3004e-04, 1.2401e-02, 1.4746e-04, 1.0988e-03, 8.7341e-03,
        5.7977e-04, 1.4535e-03, 4.4664e-03, 1.0183e-02, 1.4660e-01, 5.3159e-01,
        8.7110e-01, 7.6611e-01, 9.8272e-01, 9.0110e-01, 4.8788e-04, 7.2870e-04,
        1.3015e-03, 1.2460e-03, 2.2071e-04, 6.3812e-04, 9.7787e-04, 1.7443e-03,
        5.4169e-04, 2.4631e-03, 9.5934e-02, 6.0626e-03, 7.4123e-02, 1.3457e-03,
        8.8394e-01, 1.8984e-03, 1.0995e-01, 9.1834e-03, 2.0516e-02, 3.5958e-03,
        2.8951e-02, 8.8299e-03, 4.9689e-01, 1.6821e-02, 5.5535e-04, 1.5041e-03,
        3.5441e-01, 6.1098e-02, 9.6613e-01, 9.9998e-01, 1.0000e+00, 9.9998e-01,
        5.1866e-01, 4.0826e-02], device='cuda:0')
Sum Train Loss:  tensor([5.5450e-01, 1.9104e-02, 5.5560e-01, 1.9020e-02, 2.2431e-03, 1.9033e-02,
        2.9404e-03, 1.1637e-02, 4.4129e-02, 1.2999e-02, 2.3818e-03, 1.0981e-03,
        8.7713e-05, 5.5803e-02, 1.6018e-02, 3.6737e-02, 4.9864e-02, 2.4620e-02,
        1.0380e-02, 2.6418e-02, 2.3592e-04, 8.3874e-04, 1.2954e-03, 5.8811e-03,
        4.3716e-02, 7.8314e-03, 2.9220e-01, 4.8743e-03, 5.6841e-04, 1.6562e-03,
        1.3594e-02, 2.4582e-03, 1.4004e-01, 7.1482e-04, 1.3356e-02, 4.2627e-02,
        4.4397e-03, 6.8388e-03, 5.8808e-03, 2.0895e-01, 1.3787e+00, 6.5518e+00,
        4.1203e+00, 1.4630e+01, 1.9902e+00, 4.7454e+00, 1.0898e-03, 8.9213e-04,
        4.3209e-03, 2.3713e-03, 5.9529e-04, 4.5200e-03, 5.4354e-03, 2.7118e-02,
        1.5210e-03, 3.3782e-02, 3.1184e+00, 8.2666e-02, 9.2501e-01, 4.4433e-03,
        1.2859e+01, 1.6656e-03, 9.1622e-01, 9.0666e-02, 3.4669e-02, 2.2739e-02,
        1.5960e-01, 1.2080e-01, 1.3498e+00, 2.0624e-01, 5.0116e-03, 1.3586e-02,
        2.5676e+00, 1.2541e+00, 1.6477e+00, 3.1285e+00, 1.4573e+00, 1.5597e+00,
        6.3574e-02, 4.0298e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [32/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 67.4
Sum Train Loss:  tensor([7.2429e-01, 1.6639e-02, 4.5106e-01, 1.6429e-02, 6.8364e-03, 9.5673e-03,
        2.5625e-03, 1.4249e-02, 2.3344e-02, 1.2549e-02, 7.7789e-04, 1.3911e-02,
        1.1265e-03, 4.7521e-02, 6.6608e-02, 4.3284e-02, 9.2533e-02, 4.4660e-02,
        1.3011e-02, 4.7904e-02, 2.7277e-04, 8.2187e-04, 3.1791e-04, 2.4188e-03,
        3.7242e-02, 7.4911e-03, 2.7872e-01, 6.9741e-03, 3.4479e-03, 1.7575e-03,
        3.9536e-03, 1.1570e-03, 1.7285e-01, 1.5636e-04, 1.5625e-02, 2.7197e-02,
        6.3339e-03, 2.0653e-02, 7.8602e-03, 1.3574e-01, 1.1943e+00, 3.7001e+00,
        2.8815e+00, 1.8170e+00, 1.2707e+00, 2.1191e+00, 4.5714e-03, 7.3385e-03,
        3.2553e-03, 4.7193e-03, 3.1952e-04, 1.0089e-03, 1.6119e-03, 2.0066e-02,
        2.7907e-03, 5.7811e-03, 2.3744e+00, 1.1852e-01, 7.8295e-01, 1.0880e-02,
        8.8402e+00, 1.6869e-03, 6.6529e-01, 6.3903e-02, 1.7100e-01, 2.3378e-02,
        1.0095e-01, 1.1528e-01, 3.0413e-01, 5.0310e-02, 5.8757e-05, 2.3000e-02,
        5.8995e-01, 6.1754e-01, 5.3446e+00, 4.7789e-01, 5.6079e-01, 7.8425e-01,
        6.6935e-02, 8.7575e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [32/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 37.6
Sum Train Loss:  tensor([8.6707e-01, 9.8596e-03, 5.6401e-01, 1.6401e-02, 2.7690e-03, 9.9538e-03,
        8.6581e-04, 1.6965e-02, 2.5770e-02, 1.3254e-02, 8.8393e-04, 7.7849e-03,
        1.3662e-04, 4.1155e-02, 1.3580e-02, 2.9678e-02, 8.4404e-02, 2.7837e-02,
        2.4963e-02, 1.4652e-02, 1.7400e-04, 1.0977e-03, 4.2261e-04, 2.2755e-03,
        4.5411e-02, 4.8800e-03, 1.9896e-01, 1.0748e-02, 2.1046e-03, 7.5249e-03,
        5.0223e-03, 8.6482e-04, 1.3551e-01, 7.6550e-04, 7.2793e-03, 5.2009e-02,
        6.8195e-03, 3.5898e-03, 1.4539e-02, 2.4126e-01, 6.4688e-01, 5.0854e+00,
        6.2618e+00, 6.0753e+00, 3.1333e+00, 3.4877e+00, 3.3278e-03, 7.8958e-04,
        8.1704e-03, 8.5883e-03, 1.3198e-03, 6.5248e-03, 1.4024e-03, 1.7735e-02,
        3.2491e-03, 3.9540e-02, 1.4854e+00, 6.1511e-02, 1.4609e+00, 2.4569e-02,
        7.9699e+00, 1.0921e-02, 5.0850e-01, 7.0643e-02, 9.2703e-02, 5.5290e-02,
        2.5079e-01, 3.0169e-02, 1.2217e+00, 6.6782e-02, 1.0144e-04, 1.2958e-02,
        1.2283e+00, 6.3109e-01, 4.6884e+00, 1.4774e+00, 1.8510e-01, 8.7578e-01,
        3.9746e-02, 2.4495e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [32/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 49.8
Sum Train Loss:  tensor([9.0933e-01, 8.9141e-03, 5.1634e-01, 1.9660e-02, 7.5328e-03, 7.9431e-03,
        1.8505e-03, 1.7888e-02, 5.6114e-02, 1.5141e-02, 6.5581e-04, 3.6309e-03,
        3.9751e-04, 3.6661e-02, 5.6674e-02, 3.7008e-02, 5.1313e-02, 4.3331e-02,
        3.3721e-02, 4.1279e-02, 5.6606e-04, 3.0589e-03, 3.6961e-04, 9.5874e-04,
        4.2423e-02, 8.4957e-03, 1.7317e-01, 2.3552e-03, 9.4360e-04, 4.7462e-03,
        2.5942e-03, 1.8526e-03, 2.5250e-01, 4.5485e-04, 6.4462e-03, 2.8520e-02,
        3.8747e-03, 9.7853e-03, 2.6916e-02, 2.2685e-01, 1.7579e+00, 4.8273e+00,
        1.6286e+00, 2.1099e+00, 1.8621e+00, 2.2585e+00, 6.4114e-03, 9.6173e-03,
        1.5452e-02, 5.8405e-03, 1.0208e-03, 4.8933e-03, 9.0232e-03, 6.6163e-03,
        2.6972e-03, 1.8741e-02, 3.1626e+00, 4.1752e-02, 1.3809e+00, 1.0465e-02,
        3.9322e+00, 5.3852e-03, 9.3067e-01, 7.8941e-02, 8.6007e-02, 4.1970e-02,
        2.2469e-01, 1.0424e-01, 1.5330e+00, 4.9695e-02, 1.1988e-04, 8.2148e-03,
        4.8066e-01, 9.3410e-01, 1.3845e+00, 4.1674e+00, 1.5271e+00, 4.3299e+00,
        1.0086e-01, 2.2241e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [32/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 41.9
Sum Train Loss:  tensor([9.0612e-01, 1.9731e-02, 3.4549e-01, 9.2373e-03, 2.5686e-03, 1.5207e-02,
        1.0573e-03, 2.2244e-02, 2.1360e-02, 9.4554e-03, 1.5753e-03, 5.0331e-03,
        1.4176e-04, 3.7536e-02, 5.3354e-02, 6.0436e-02, 5.0389e-02, 3.6216e-02,
        6.3502e-03, 8.4579e-03, 5.8345e-04, 1.0238e-03, 1.2994e-03, 5.1418e-03,
        5.2588e-02, 8.9867e-03, 1.4669e-01, 3.8085e-03, 1.6510e-03, 3.9893e-03,
        3.4964e-03, 1.0951e-03, 1.0386e-01, 7.7320e-04, 1.0555e-02, 3.3898e-02,
        7.7884e-03, 1.0334e-02, 6.2277e-03, 1.4841e-01, 6.3753e-01, 7.5796e+00,
        8.6058e+00, 6.3966e+00, 2.4156e+00, 7.8992e+00, 3.4679e-03, 5.0483e-03,
        8.3269e-03, 2.9007e-03, 1.1090e-03, 2.5501e-03, 1.7234e-03, 1.3862e-02,
        1.6575e-03, 3.5507e-02, 2.6821e+00, 7.9481e-02, 7.5727e-01, 1.5665e-02,
        5.7341e+00, 1.8915e-02, 1.2196e+00, 1.3744e-01, 2.0634e-01, 3.8353e-02,
        3.1655e-01, 1.2409e-01, 2.0315e+00, 1.0649e-01, 9.4735e-05, 8.0013e-03,
        1.3674e+00, 8.3558e-01, 7.7251e+00, 1.0953e+00, 1.9874e+00, 1.4969e+00,
        4.0774e-02, 4.8834e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [32/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 63.8
Sum Train Loss:  tensor([8.0138e-01, 9.0308e-03, 3.1025e-01, 2.8227e-03, 4.0233e-03, 1.9686e-02,
        1.4843e-03, 1.0849e-02, 1.4015e-02, 7.0777e-03, 1.7383e-03, 1.3047e-03,
        7.1706e-05, 3.8125e-02, 6.9123e-02, 1.5826e-02, 3.3412e-02, 3.7118e-02,
        1.9264e-02, 1.1807e-02, 8.7999e-05, 1.5654e-03, 1.9350e-03, 2.5251e-03,
        6.8639e-02, 8.4808e-03, 3.1070e-01, 5.4577e-03, 2.6229e-03, 1.5374e-02,
        3.5005e-03, 8.4074e-04, 9.3834e-02, 7.4898e-04, 1.5102e-02, 4.0987e-02,
        3.7956e-03, 1.0067e-02, 9.6026e-03, 3.0591e-01, 4.4156e-01, 1.0273e+01,
        3.6169e+00, 5.3543e+00, 6.0876e+00, 3.6546e+00, 3.3736e-03, 2.2628e-03,
        7.8612e-03, 4.3668e-03, 6.4693e-04, 2.2936e-03, 8.9857e-03, 6.6676e-03,
        5.3663e-03, 1.4978e-02, 2.4186e+00, 5.6111e-02, 7.0463e-01, 1.4754e-02,
        7.1248e+00, 3.5109e-03, 9.5307e-01, 5.8637e-02, 1.4567e-01, 1.4172e-02,
        1.2456e-01, 1.5170e-01, 3.8424e+00, 1.4548e-01, 2.5800e-03, 2.7533e-02,
        4.2463e+00, 1.0398e+00, 5.1908e+00, 3.5758e+00, 6.4488e+00, 1.9289e+00,
        2.9811e-01, 4.4588e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [32/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 70.4
Sum Train Loss:  tensor([4.5847e-01, 1.2628e-02, 4.1102e-01, 6.6163e-03, 6.6153e-03, 1.1289e-02,
        1.3057e-03, 1.8725e-02, 5.1031e-03, 1.1836e-02, 1.4272e-03, 6.7332e-04,
        9.5498e-05, 7.3298e-02, 7.4930e-02, 4.6135e-02, 8.7819e-02, 4.6777e-02,
        1.3491e-02, 5.4227e-03, 1.1740e-04, 9.5867e-04, 7.9827e-04, 2.9956e-03,
        6.2102e-02, 6.8526e-03, 1.8599e-01, 2.6336e-03, 9.1027e-04, 3.0101e-03,
        1.3379e-02, 1.2328e-03, 1.2661e-01, 1.9013e-04, 7.6626e-03, 3.7548e-02,
        6.5598e-03, 1.1669e-02, 7.6650e-02, 2.1363e-01, 7.9342e-01, 2.4918e+00,
        5.6038e+00, 2.7541e+00, 1.7231e+00, 1.3687e+01, 3.9396e-03, 1.5524e-03,
        1.2046e-02, 3.6181e-03, 1.1950e-03, 1.7269e-03, 9.8376e-04, 6.6473e-03,
        7.4067e-04, 2.0758e-02, 1.8774e+00, 5.2447e-02, 1.7858e+00, 1.9564e-02,
        4.8056e+00, 2.2580e-02, 5.6358e-01, 7.7527e-02, 1.1182e-01, 2.3932e-02,
        1.9608e-01, 1.1654e-01, 3.8572e-01, 2.6984e-02, 5.1851e-05, 1.8677e-02,
        9.5542e-01, 5.7257e-01, 4.3246e+00, 3.6875e+00, 4.2804e-01, 3.6972e+00,
        1.6507e-01, 2.0359e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [32/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 53.3
Sum_Val Meta Model:  tensor([1.0695e+00, 9.2649e-02, 1.7583e+00, 8.1878e-02, 1.0116e-03, 2.5457e-02,
        1.2353e-03, 1.7321e-02, 2.3081e-02, 9.5869e-03, 1.9172e-03, 9.5445e-03,
        1.4553e-03, 4.0340e-02, 2.9605e-02, 3.3273e-02, 2.3102e+00, 1.3009e-02,
        3.9831e-03, 5.5275e-03, 8.2615e-05, 1.7297e-04, 5.1122e-04, 3.1018e-04,
        8.0439e-02, 1.2026e-02, 3.6277e-01, 2.2335e-03, 3.4049e-03, 1.3898e-02,
        1.7394e-03, 3.3537e-04, 9.2524e-02, 1.9502e-04, 9.7234e-04, 1.1740e-02,
        2.2959e-03, 7.9927e-03, 2.3296e-03, 5.2954e-01, 1.5110e+00, 1.5269e+01,
        5.9481e+00, 1.5177e+01, 2.0284e+01, 2.2747e+01, 8.8139e-04, 4.3625e-03,
        7.7089e-04, 5.9042e-03, 7.4480e-05, 5.1213e-04, 6.9056e-03, 1.9827e-03,
        6.0879e-04, 5.5681e-03, 2.7507e+00, 4.4409e-02, 1.0663e+00, 2.9175e-03,
        1.4451e+01, 2.1507e-02, 8.1285e-01, 3.9923e-02, 1.6850e-02, 8.1330e-03,
        3.2319e-02, 6.6640e-02, 4.8360e+00, 2.7992e-01, 7.1965e-05, 2.7027e-02,
        6.4626e+00, 5.7481e-01, 2.6115e+01, 7.1477e+00, 1.8661e-01, 9.8369e+00,
        2.2455e-02, 3.4448e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.0123e+00, 6.1165e-02, 1.1952e+00, 4.1733e-02, 1.9223e-04, 2.3455e-02,
        9.6745e-04, 1.7511e-02, 2.2592e-02, 9.3444e-03, 2.3083e-03, 1.1438e-02,
        1.7966e-03, 5.0989e-02, 2.8524e-02, 1.4097e-01, 1.5860e+00, 2.2327e-02,
        2.0961e-03, 7.5709e-03, 5.9727e-05, 9.7175e-05, 8.2203e-05, 7.8794e-04,
        7.4354e-02, 1.0786e-02, 3.6214e-01, 3.0766e-03, 4.9557e-03, 1.5966e-02,
        9.6518e-04, 1.7933e-04, 8.8610e-02, 1.9742e-04, 6.1248e-04, 1.1061e-02,
        3.8518e-03, 6.8630e-03, 6.3820e-04, 4.4940e-01, 1.7644e+00, 1.9683e+01,
        5.2147e+00, 1.6869e+01, 2.0362e+01, 2.5463e+01, 4.1243e-04, 4.2561e-03,
        3.5320e-04, 4.4313e-03, 1.4127e-05, 1.6035e-04, 6.0100e-03, 5.7121e-04,
        3.5130e-04, 4.2555e-03, 3.3170e+00, 3.8486e-02, 8.3707e-01, 4.1596e-03,
        1.7800e+01, 1.4835e-02, 7.1609e-01, 6.0034e-02, 7.9416e-03, 7.6152e-03,
        2.0894e-02, 8.0619e-02, 5.8831e+00, 3.0878e-01, 5.6758e-05, 2.8038e-02,
        5.3955e+00, 5.3779e-01, 3.2985e+01, 1.7879e+01, 1.8218e-02, 8.6541e+00,
        2.3251e-02, 7.5501e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.1560e+01, 4.6529e+01, 5.1233e+01, 2.6477e+01, 2.5970e-01, 1.2660e+01,
        2.8186e+00, 1.7242e+01, 6.3973e+00, 8.3136e+00, 7.9229e+00, 1.4114e+01,
        9.9096e+00, 1.9386e+01, 9.1233e+00, 3.9870e+01, 3.5889e+02, 4.2580e+00,
        6.4468e-01, 2.0136e+00, 1.0733e+00, 2.3680e-01, 8.4226e-02, 1.2197e+00,
        2.6724e+01, 1.7770e+01, 2.9738e+01, 5.8725e+00, 1.6188e+01, 1.4623e+01,
        8.8105e-01, 5.4336e-01, 7.1454e+00, 1.3388e+00, 5.5742e-01, 1.2664e+00,
        6.6437e+00, 4.7217e+00, 1.4289e-01, 4.4130e+01, 1.2036e+01, 3.7027e+01,
        5.9863e+00, 2.2019e+01, 2.0721e+01, 2.8257e+01, 8.4536e-01, 5.8408e+00,
        2.7138e-01, 3.5563e+00, 6.4007e-02, 2.5128e-01, 6.1460e+00, 3.2748e-01,
        6.4854e-01, 1.7277e+00, 3.4576e+01, 6.3482e+00, 1.1293e+01, 3.0911e+00,
        2.0137e+01, 7.8145e+00, 6.5131e+00, 6.5373e+00, 3.8710e-01, 2.1178e+00,
        7.2169e-01, 9.1302e+00, 1.1840e+01, 1.8356e+01, 1.0220e-01, 1.8641e+01,
        1.5224e+01, 8.8020e+00, 3.4142e+01, 1.7879e+01, 1.8218e-02, 8.6542e+00,
        4.4829e-02, 1.8494e+00], device='cuda:0')
Outer loop valEpocw Maximum [32/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 162.5
model_train val_loss valEpocw [32/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 189.3
model_train val_loss valEpocw [32/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1260.5
Sum_Val Meta Model:  tensor([2.8584e+00, 4.6418e-02, 2.2173e+00, 3.6348e-02, 1.7769e-03, 1.5381e-01,
        6.8872e-02, 1.3930e-01, 1.9633e-01, 8.6331e-02, 1.4240e-02, 4.4017e-01,
        5.1467e-03, 2.9359e-02, 1.3265e-01, 1.6784e-01, 1.1244e-01, 1.8037e-01,
        7.4883e-02, 4.4357e-01, 4.1323e-05, 1.6903e-04, 1.5978e-03, 1.3542e-02,
        1.8603e-01, 3.3578e-02, 6.5984e-01, 2.1597e-02, 1.1671e-02, 3.6318e-04,
        7.6958e-04, 1.6239e-04, 7.0957e-03, 1.7147e-04, 3.8255e-04, 2.0408e-03,
        7.7178e-03, 8.7157e-04, 1.1927e-03, 4.2126e-02, 4.7477e-02, 2.3655e+00,
        2.1613e-01, 4.2615e-01, 6.4505e-01, 4.1958e+00, 2.4570e-03, 1.9291e-03,
        6.8416e-04, 1.7262e-02, 9.2545e-05, 5.2301e-04, 2.5458e-04, 3.8433e-04,
        4.5697e-04, 1.4339e-02, 1.8112e+00, 7.1217e-02, 7.2801e-01, 2.1137e-02,
        2.6834e+00, 6.7688e-03, 4.1500e-01, 4.8467e-02, 2.8242e-02, 8.6622e-03,
        4.0681e-02, 2.4908e-01, 1.2684e-01, 1.0923e-01, 1.0611e-04, 8.4516e-03,
        1.8175e+00, 5.2102e-01, 4.7767e+00, 8.7391e-01, 2.7651e-01, 1.1899e+00,
        5.4651e-02, 1.4680e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.9907e+00, 5.4805e-02, 1.6386e+00, 3.6942e-02, 2.8448e-03, 1.4068e-01,
        6.3809e-02, 9.7296e-02, 1.7752e-01, 6.7673e-02, 1.2542e-02, 1.2612e-01,
        4.3046e-03, 6.1953e-02, 1.0158e-01, 2.8854e-02, 7.4130e-02, 1.7192e-01,
        2.1989e-02, 1.3652e-01, 1.6386e-04, 1.4738e-04, 3.9247e-04, 9.4701e-03,
        1.1894e-01, 3.0904e-02, 5.9481e-01, 2.0252e-02, 1.1911e-02, 1.6478e-03,
        5.0569e-04, 1.8537e-04, 7.1587e-03, 1.3148e-03, 7.4416e-04, 4.0693e-03,
        3.7048e-03, 2.3747e-03, 6.3389e-04, 2.6272e-02, 4.8876e-02, 2.7642e+00,
        1.5613e-01, 1.8555e-01, 6.3091e-02, 4.9366e-01, 6.9530e-04, 8.2050e-04,
        4.9765e-04, 1.9217e-02, 3.7398e-05, 4.5472e-04, 9.8806e-04, 2.1966e-03,
        4.7137e-04, 5.4123e-03, 1.3431e+00, 4.9474e-02, 4.7868e-01, 8.1734e-04,
        6.3823e+00, 1.5322e-03, 4.0483e-01, 4.4344e-03, 4.1297e-03, 1.7988e-03,
        7.6198e-03, 2.6901e-01, 2.3839e-02, 8.7027e-02, 8.6137e-06, 1.0437e-03,
        1.6079e+00, 2.0795e-01, 3.9348e+00, 7.3140e-02, 1.0201e+00, 3.5074e-02,
        3.8932e-04, 1.6738e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.3381e+01, 1.9780e+01, 4.8619e+01, 1.0444e+01, 1.6525e+00, 3.3416e+01,
        5.6976e+01, 3.8459e+01, 2.5377e+01, 2.7627e+01, 1.5911e+01, 6.0279e+01,
        7.6960e+00, 1.3268e+01, 1.5806e+01, 3.8675e+00, 9.0547e+00, 1.6349e+01,
        2.8746e+00, 1.5491e+01, 7.8023e-01, 1.3512e-01, 1.6923e-01, 5.5342e+00,
        2.4937e+01, 1.8775e+01, 3.1212e+01, 1.3744e+01, 1.2662e+01, 6.7141e-01,
        2.0574e-01, 1.9699e-01, 3.4757e-01, 2.9969e+00, 3.0057e-01, 3.1270e-01,
        2.4156e+00, 7.5732e-01, 7.4336e-02, 1.7986e+00, 3.8780e-01, 5.6578e+00,
        1.9301e-01, 2.6604e-01, 6.5371e-02, 5.7894e-01, 5.3104e-01, 4.5255e-01,
        1.5179e-01, 6.4046e+00, 6.0028e-02, 3.1270e-01, 4.5101e-01, 5.7520e-01,
        3.3393e-01, 1.1708e+00, 1.0906e+01, 4.1466e+00, 6.0124e+00, 2.4807e-01,
        7.5640e+00, 3.6776e-01, 3.1991e+00, 2.7618e-01, 1.4510e-01, 2.4235e-01,
        1.9818e-01, 1.9547e+01, 5.5417e-02, 3.7396e+00, 6.3851e-03, 3.1692e-01,
        5.2973e+00, 2.4849e+00, 4.1933e+00, 7.3147e-02, 1.0201e+00, 3.5077e-02,
        8.1514e-04, 3.2193e-01], device='cuda:0')
Outer loop valEpocw Maximum [32/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 32.2
model_train val_loss valEpocw [32/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 25.5
model_train val_loss valEpocw [32/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 692.3
Sum_Val Meta Model:  tensor([1.7856e+00, 6.2582e-03, 3.0329e-01, 5.8742e-03, 1.4665e-03, 8.0518e-03,
        1.0648e-03, 1.7190e-02, 1.1787e-02, 3.6169e-03, 8.6406e-04, 2.3339e-03,
        1.5646e-04, 6.0416e-02, 1.7441e-02, 2.4279e-02, 5.2172e-02, 4.3723e-02,
        8.5965e-03, 1.4195e-02, 1.2769e-04, 1.7038e-03, 2.3094e-02, 2.4221e-03,
        3.3929e-02, 2.9753e-03, 3.2400e-01, 6.1053e-03, 6.4308e-04, 9.0336e-03,
        7.9156e-03, 4.1458e-03, 1.5493e-01, 1.1054e-04, 7.6312e-03, 4.5673e-02,
        1.7991e-02, 3.1169e-02, 2.8758e-03, 3.0935e-01, 4.2649e+00, 3.6083e+01,
        6.2251e+01, 7.2578e+01, 5.2655e+01, 7.7800e+01, 1.3306e-02, 1.8517e-02,
        8.5492e-02, 4.0327e-02, 1.1777e-02, 3.8396e-02, 9.8802e-02, 3.7760e-02,
        6.1527e-02, 3.6551e-01, 1.8099e+01, 9.1201e-02, 5.9091e-01, 4.8456e-03,
        9.6831e+01, 6.2790e-03, 1.2078e+00, 1.5135e-01, 2.9937e-01, 6.7007e-03,
        2.7611e-01, 1.0513e-01, 2.1775e+00, 4.5672e-02, 2.8081e-04, 1.6020e-02,
        1.2593e+00, 2.3711e+00, 1.0382e+00, 2.9188e+01, 1.0701e+01, 1.1831e+00,
        1.0727e-01, 3.3602e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.3555e-01, 8.2803e-04, 1.5787e-01, 4.6120e-04, 3.0319e-05, 2.0761e-04,
        3.0857e-05, 2.0650e-02, 9.8102e-04, 1.5058e-04, 3.4934e-05, 7.7063e-05,
        1.0767e-05, 4.3978e-02, 1.6822e-03, 9.4633e-03, 4.7588e-03, 1.0396e-03,
        3.0901e-04, 1.8645e-04, 6.5380e-06, 1.7309e-05, 1.3877e-05, 2.5919e-05,
        1.5258e-02, 1.0095e-03, 3.2778e-01, 4.5057e-03, 2.7323e-04, 4.5320e-04,
        6.4581e-04, 4.3343e-03, 1.3997e-01, 3.5452e-05, 1.3236e-03, 1.7080e-02,
        1.2211e-02, 2.3227e-02, 1.0531e-04, 3.8905e-01, 3.4826e+00, 6.3832e+01,
        6.3011e+01, 8.5722e+01, 1.1025e+02, 8.9517e+01, 6.2730e-03, 6.4773e-03,
        7.3614e-02, 1.6387e-02, 6.4185e-03, 4.0939e-02, 7.7054e-02, 5.2235e-02,
        4.2219e-02, 2.1243e-01, 9.2163e+00, 1.1147e-01, 5.8629e-01, 9.0839e-04,
        8.6578e+01, 6.6745e-04, 1.0735e+00, 1.0431e-01, 2.2333e-01, 1.9492e-02,
        2.5075e-01, 1.2328e-01, 2.9987e+00, 4.7935e-02, 4.9631e-05, 1.4985e-02,
        1.9797e+00, 2.5103e+00, 9.5952e-01, 2.9704e+01, 1.6097e+01, 9.3163e-01,
        5.4006e-03, 3.8415e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.2797e+01, 5.3464e-01, 6.2804e+00, 2.3815e-01, 3.3975e-02, 8.8323e-02,
        7.0386e-02, 1.6542e+01, 2.3679e-01, 1.1398e-01, 9.2459e-02, 7.0952e-02,
        4.8551e-02, 1.4741e+01, 4.6048e-01, 2.3713e+00, 8.9808e-01, 1.4454e-01,
        7.6610e-02, 4.0367e-02, 9.0096e-02, 3.2783e-02, 9.8848e-03, 3.1051e-02,
        4.5923e+00, 1.3182e+00, 2.7443e+01, 6.8718e+00, 7.2936e-01, 3.0607e-01,
        4.3920e-01, 8.9728e+00, 9.3563e+00, 1.8150e-01, 9.1365e-01, 1.6372e+00,
        1.4649e+01, 1.2352e+01, 2.0730e-02, 3.9303e+01, 2.7270e+01, 1.2223e+02,
        7.2032e+01, 1.1063e+02, 1.1261e+02, 1.0037e+02, 9.2075e+00, 6.2986e+00,
        3.7414e+01, 9.1603e+00, 1.9990e+01, 4.6524e+01, 5.8386e+01, 2.4208e+01,
        5.7738e+01, 6.9564e+01, 9.7971e+01, 1.5588e+01, 9.1166e+00, 5.1762e-01,
        9.8551e+01, 2.9583e-01, 1.0324e+01, 1.0337e+01, 1.1253e+01, 4.5168e+00,
        8.9080e+00, 1.3908e+01, 7.0895e+00, 3.0325e+00, 7.2999e-02, 8.5469e+00,
        6.8506e+00, 4.1110e+01, 1.0050e+00, 2.9705e+01, 1.6097e+01, 9.3166e-01,
        1.1482e-02, 9.7263e-01], device='cuda:0')
Outer loop valEpocw Maximum [32/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 475.5
model_train val_loss valEpocw [32/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 571.6
model_train val_loss valEpocw [32/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1475.5
Sum_Val Meta Model:  tensor([5.3116e+00, 7.0413e-03, 9.9978e-01, 5.2671e-03, 1.3345e-03, 6.4174e-02,
        8.1920e-03, 6.0363e-02, 1.4902e-02, 6.6221e-02, 5.6264e-03, 1.5092e-02,
        1.7019e-04, 1.1505e-01, 1.2017e-01, 1.8395e-02, 1.9795e-02, 2.1119e-02,
        4.0619e-03, 8.6796e-03, 2.0292e-04, 7.4790e-04, 1.4965e-03, 3.0308e-03,
        1.5149e-01, 4.1862e-03, 6.9527e-01, 3.0491e-03, 1.9951e-02, 2.3698e-03,
        2.6719e-03, 4.3377e-04, 1.1806e-01, 3.0310e-02, 5.2158e-02, 2.0678e-01,
        8.1936e-04, 7.4310e-03, 8.2276e-02, 3.5788e-01, 1.8338e+00, 1.5259e+01,
        8.0976e+00, 8.0239e+00, 9.7389e+00, 1.5904e+01, 1.0869e-03, 1.5736e-03,
        3.8326e-03, 2.1429e-03, 4.7762e-04, 7.9085e-04, 8.3323e-04, 3.4211e-02,
        1.7388e-03, 1.1874e-02, 3.0554e+00, 6.7357e-02, 1.3061e+00, 8.0955e-03,
        3.3382e+01, 4.7837e-03, 7.1700e-01, 2.1402e-01, 2.7648e-01, 1.5986e-02,
        3.4659e-01, 5.2266e-01, 5.3020e-01, 6.6810e-02, 1.6701e-04, 1.0134e-02,
        1.3020e+00, 1.0102e+00, 4.3181e+02, 1.2609e+01, 1.3928e+00, 7.1979e+00,
        4.0776e-02, 7.9583e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.5146e+00, 1.0045e-02, 7.8432e-01, 7.7059e-03, 1.9549e-03, 5.3173e-02,
        1.0336e-02, 5.4623e-02, 1.2249e-02, 5.6143e-02, 4.5848e-03, 2.8397e-02,
        8.3531e-04, 1.1471e-01, 1.4243e-01, 7.9416e-03, 5.1310e-03, 8.8272e-03,
        5.4494e-04, 1.0810e-03, 1.1444e-04, 7.7937e-05, 5.2920e-04, 1.5407e-03,
        1.4892e-01, 4.8869e-03, 5.6972e-01, 3.4884e-03, 2.4491e-02, 4.2841e-04,
        8.8158e-04, 3.2086e-04, 2.1991e-03, 1.6593e-04, 4.3851e-04, 1.7253e-03,
        1.1926e-03, 9.9958e-04, 5.0102e-04, 4.6959e-02, 1.4153e-01, 7.6959e-01,
        8.8268e-02, 2.5822e-01, 9.0982e-02, 5.9050e-02, 3.7807e-04, 5.7898e-04,
        2.4209e-04, 4.4082e-04, 3.0615e-05, 1.3291e-04, 3.4048e-04, 1.1892e-03,
        5.3299e-04, 3.9882e-03, 1.1429e+00, 1.1593e-02, 1.0050e+00, 1.4072e-03,
        1.4234e+01, 2.9571e-03, 3.0326e-01, 9.1059e-03, 1.0808e-02, 8.8897e-03,
        4.2519e-02, 1.2745e-01, 2.0213e-02, 6.0503e-03, 2.4991e-05, 1.3609e-03,
        9.6754e-02, 5.2898e-01, 1.3442e+02, 2.0035e+00, 1.3898e-02, 6.5511e+00,
        1.1359e-03, 5.2839e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.5660e+01, 2.7356e+00, 2.0599e+01, 1.6995e+00, 7.8146e-01, 1.0079e+01,
        7.4882e+00, 1.6867e+01, 1.4452e+00, 1.6923e+01, 4.0019e+00, 1.0070e+01,
        1.1093e+00, 1.7648e+01, 1.8873e+01, 1.0400e+00, 5.4431e-01, 6.6083e-01,
        6.6397e-02, 1.1745e-01, 3.6177e-01, 5.2019e-02, 1.7945e-01, 7.3697e-01,
        2.4236e+01, 2.2320e+00, 2.5610e+01, 1.7554e+00, 1.9772e+01, 1.2543e-01,
        2.6539e-01, 2.4266e-01, 8.1385e-02, 2.0763e-01, 1.1480e-01, 8.3552e-02,
        5.6618e-01, 2.3804e-01, 3.9752e-02, 2.3762e+00, 7.9860e-01, 1.4733e+00,
        1.0858e-01, 3.6880e-01, 9.4698e-02, 6.9584e-02, 2.2179e-01, 2.3376e-01,
        5.5078e-02, 1.1380e-01, 3.3383e-02, 6.7217e-02, 1.1402e-01, 2.2105e-01,
        2.8751e-01, 6.7027e-01, 8.4081e+00, 7.7951e-01, 1.1557e+01, 3.2312e-01,
        1.6960e+01, 5.2197e-01, 2.1573e+00, 4.2555e-01, 2.7335e-01, 8.9494e-01,
        8.1407e-01, 6.2273e+00, 4.6723e-02, 2.1269e-01, 1.2895e-02, 3.0057e-01,
        2.9292e-01, 5.6548e+00, 1.4520e+02, 2.0038e+00, 1.3898e-02, 6.5520e+00,
        2.7162e-03, 8.0178e-02], device='cuda:0')
Outer loop valEpocw Maximum [32/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 563.5
model_train val_loss valEpocw [32/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 165.6
model_train val_loss valEpocw [32/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 462.3
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.84858859 97.40987561 93.08609788 98.1323327  98.55752245 97.64379089
 98.28705791 95.19986355 98.04217785 96.99808726 98.66838854 98.96565588
 99.4420146  95.32169442 97.48053752 97.45129811 96.41208075 97.72054434
 98.91205029 98.45883944 98.73052229 99.33967666 99.56019054 99.0241347
 95.23884943 96.878693   94.44329382 97.12357306 98.07872711 98.28096636
 98.37233952 98.53437458 97.00296049 98.58310693 98.68300825 98.85235316
 97.71567111 98.39305077 98.76341663 93.47473837 98.24563541 96.21227812
 98.92301507 98.3565015  99.07774028 98.30411423 98.23832556 98.69153641
 98.12745946 98.76950817 98.77316309 98.67204347 99.04728256 98.44543804
 98.75367016 97.75465699 93.44915388 97.0041788  96.78610153 97.64013596
 97.9093822  98.74148707 97.7010514  97.44886149 98.88037426 97.64135427
 98.68544487 96.00029239 99.35551467 98.35893812 99.81603538 97.51708678
 99.17520498 96.12577819 99.12890925 99.3177471  99.76852134 99.56140885
 99.85258464 99.18373314]
Accuracy th:0.7 is [86.3732167  97.32824892 93.11533729 97.95689624 98.37721275 97.50246708
 98.11283976 95.00615246 97.87283293 96.90062256 98.60138156 98.94007139
 99.42739489 95.42646898 97.42327701 97.30510106 96.34629208 97.66206552
 98.8572264  98.39914231 98.58554355 99.27754291 99.51511312 98.92423338
 95.24737759 96.78975646 94.34582912 97.00052387 98.03974123 98.21152276
 98.16766365 98.5928534  96.61432    98.51000841 98.62696605 98.87306441
 97.4781009  98.25538188 98.47224084 93.10802744 98.17010027 96.8238691
 98.75488846 98.39670569 98.97296573 98.11283976 98.18228335 98.64767729
 98.03852292 98.7463603  98.6756984  98.61112803 99.03266286 98.36137474
 98.73905045 97.74003728 93.10559082 96.81412263 96.63015801 97.49272061
 97.92034697 98.65255053 97.77780485 97.3111926  98.79631096 97.52074171
 98.67813501 95.96983468 99.23855704 98.21517769 99.81603538 97.20398143
 99.10454307 95.91135586 99.00342345 99.32993019 99.72953546 99.57602856
 99.84892972 99.19104299]
Avg Prec: is [96.43817373 35.46620206 72.43361742 67.85758433 77.98681734 64.44627867
 74.61338005 46.92148378 57.9281034  52.28400921 35.3386093  55.33584386
 25.55588994 28.18056583 32.33557002 57.93066217 29.87948135 42.16975167
 49.90655839 40.78803099 60.60051429 52.20971683 89.7543268  83.09061441
 25.94412775 34.36204835 38.40465752 41.44724162 26.71362965 39.47625212
 72.55939169 36.38334481 58.2551779  62.51194817 72.03983803 79.39393354
 60.17418446 74.45436211 87.16492955 47.22165844 52.443892   87.70658755
 90.35324545 86.92361746 92.23003266 93.66830534 41.2116785  33.52897193
 43.93441427 46.98297569 64.62740896 40.90442779 26.56556477 73.58790995
 29.56903797 47.18418776 75.24147992 59.57588559 48.84114092 60.09729463
 96.45167022 83.77436141 76.13845143 51.81087364 62.48641785 44.55683889
 62.57472546 26.97965394 82.11160517 68.86849838 13.08775026 72.45235648
 86.56706831 53.28822266 94.0385963  94.94773465 90.62029658 94.01910938
 50.64882521 31.66941557]
Accuracy th:0.5 is [45.63053569 97.2137279  70.65459729 97.02489005 97.26733349 75.13675516
 75.46204359 75.21594522 76.31973295 96.45959479 76.58166933 98.52097319
 99.41399349 80.07090557 76.27952876 96.56680596 96.29512311 76.05170502
 98.65376884 98.30776915 80.17202519 77.11894348 98.38695922 83.56501505
 83.71852195 96.65086926 94.0778012  75.68986733 98.01293844 76.50369757
 97.30875598 98.57457877 96.36213009 98.02024829 88.47967252 76.15647958
 76.02612054 91.63874709 97.11504489 73.27883432 78.7660969  92.05906361
 75.43889572 74.99543134 96.9627563  93.87434364 98.02877645 98.57336046
 97.85943154 90.37048769 89.38487592 98.55508583 98.99976852 75.51321256
 98.70615611 75.96886003 70.92018859 93.48204822 96.24273583 96.9067141
 89.79300934 97.17717864 94.97691305 76.02490223 98.42838172 76.60481719
 98.20786784 75.20863537 76.98127459 97.55972759 77.52707691 95.99054592
 77.0239154  95.45083515 75.45107881 83.74897967 90.47526224 76.57070455
 77.55509801 99.14718388]
Accuracy th:0.7 is [45.60616952 97.2137279  70.65459729 97.02489005 97.26733349 75.13675516
 75.46204359 75.59971248 76.31973295 96.4754328  76.58166933 98.52097319
 99.41399349 80.59356002 76.27952876 96.56680596 96.29512311 76.05170502
 98.65376884 98.30776915 80.54482767 77.11894348 98.38695922 84.70535203
 84.66880277 96.65086926 94.0778012  75.68986733 98.01293844 76.50369757
 97.30875598 98.57457877 96.36213009 98.02024829 88.68069346 76.15647958
 76.02612054 91.890937   97.11504489 73.27883432 79.38377944 92.05906361
 75.43889572 74.99543134 96.9627563  93.87434364 98.02877645 98.57336046
 97.98126241 90.94065618 89.64193906 98.55508583 98.99976852 75.51321256
 98.70615611 75.96886003 70.92018859 93.95962525 96.24273583 96.9067141
 89.79300934 97.17717864 95.05122988 76.02733885 98.42838172 76.60481719
 98.20786784 75.20863537 76.98127459 97.55972759 77.58921066 95.99054592
 77.24564759 95.45083515 75.45107881 83.9487823  90.85415626 76.57070455
 77.55509801 99.14718388]
Avg Prec: is [55.85429954  2.96667985 11.02223265  3.36760548  2.218176    3.71407689
  3.33703257  5.57120999  2.44165136  3.75668086  1.53408516  1.60810198
  0.61309925  4.96671326  2.65640876  3.14868451  3.67326957  2.62550247
  1.42827996  1.68222699  1.89907659  0.84545775  1.95329389  2.40849419
  5.09489556  3.69788397  6.52489214  3.24839341  2.13474316  1.98834424
  2.60752461  1.35015066  3.78951837  1.60096763  2.36588876  2.40938236
  3.09852441  2.61319426  2.83688413  7.46891777  2.33231805  8.27239807
  3.38110257  4.15291519  3.24037711  6.34021157  2.08651645  1.52149088
  2.1754528   1.66355291  1.81927574  1.61321883  1.10571575  3.15376983
  1.40896148  2.68873985 11.1309112   3.5832858   3.92026582  2.78967469
 10.85978491  2.13933096  3.6950883   3.00227935  1.53306875  2.33085481
  1.75984636  4.16872154  1.29184226  2.38807735  0.21876318  3.3085058
  1.91231975  4.53545449  3.86218033  3.07257841  0.82762205  1.81597311
  0.1397094   0.71405727]
mAP score regular 58.98, mAP score EMA 3.75
starting validation
Accuracy th:0.5 is [87.94130104 97.44126367 92.66512196 98.27839649 98.94361811 97.61566634
 98.50511996 95.36088896 98.14385729 97.10740713 98.69447144 99.01337918
 99.37713332 94.95976281 97.45122954 97.3939258  96.39983058 97.78259461
 98.95109251 98.38552956 98.87385704 99.36218452 99.59638239 99.25006852
 95.39327802 96.87570073 94.35184493 97.30672447 97.89969355 98.23355009
 98.68699704 98.60477863 97.03764606 98.77668984 98.89877171 99.08064878
 98.01928395 98.25348182 98.91122904 93.28300571 97.97942048 91.04317712
 97.33911354 95.8442335  96.72621272 94.65331241 98.4054613  98.75924957
 98.08406209 98.76921544 98.88382291 98.6521165  98.87634851 98.44532476
 98.76174104 97.61317488 89.87717069 97.12733886 96.48952338 97.67297008
 92.46331315 98.86389117 97.08498393 97.51102474 98.8090789  97.49607594
 98.65709943 95.8517079  98.7866557  98.27092209 99.81563146 97.62563221
 98.35314049 95.9414007  97.24692927 97.29925007 99.24259412 98.59730423
 99.82559733 99.04576824]
Accuracy th:0.7 is [87.74945811 97.3864514  93.1933129  98.20365249 98.82651917 97.60320901
 98.35563196 95.35839749 98.06662182 96.96788499 98.64713357 99.04327678
 99.37464185 95.24129855 97.41385754 97.37648554 96.28771458 97.73525675
 98.96853278 98.4054613  98.8090789  99.31982958 99.59389092 99.13795251
 95.46054762 96.73617859 94.5561452  97.2145402  97.86979595 98.18870369
 98.52754316 98.6894885  96.67887485 98.69945437 98.86139971 99.12798665
 97.80003488 98.15133169 98.7343349  93.10112863 97.98440342 92.16433715
 97.44126367 96.22293644 96.84829459 94.83518948 98.36808929 98.79911304
 98.01928395 98.77668984 98.77419837 98.6296933  98.90126317 98.4577821
 98.76174104 97.80501781 90.69686324 97.042629   96.46959165 97.53843087
 92.83703316 98.72436904 97.266861   97.44873807 98.8090789  97.61317488
 98.67952263 95.8292847  98.82153624 98.06163889 99.81563146 97.44375514
 98.41044423 95.89406283 97.27682687 97.42631487 99.25754292 98.6670653
 99.82559733 99.14293545]
Avg Prec: is [96.41143229 34.44926853 70.23692325 72.6608467  77.73353692 64.72126419
 80.05091106 47.84299276 62.30577446 55.51910437 38.82719244 57.51909966
 24.41264136 29.8951521  33.71439125 63.02471083 32.50757077 44.10891666
 50.7190633  37.41712219 68.12615122 57.19354304 91.61019016 87.16954005
 24.5285835  38.22544725 33.11280849 46.3841836  27.96579894 38.38874153
 77.41855663 37.57102116 55.37314402 65.86398406 74.12967348 82.75416563
 60.16655471 76.18969285 89.57176427 44.46943089 37.30053923 49.17884707
 50.16996999 37.1551725  28.38267589 49.81403964 40.42727502 28.84796585
 42.00698046 42.91064154 70.0471965  37.70041301 26.64260103 76.63606157
 29.52214494 42.82754577 54.63270051 56.74102716 39.75944747 63.74423044
 64.72023825 86.82953547 67.18616965 53.62327872 62.01665902 38.72606432
 62.43885874 26.85524031 41.54498025 64.92136094  9.36072743 74.04816876
 52.85913404 45.27049099 60.2864572  48.45129431 12.80070114 50.46255099
  1.67464919 21.05079853]
Accuracy th:0.5 is [45.33223709 97.22450607 68.62246805 96.96290206 97.90716795 73.84458231
 73.95171537 73.34379749 75.43912101 96.41976231 75.55871141 98.5325261
 99.34972718 77.50703839 75.57116875 96.31262924 96.21047911 74.92837033
 98.78167277 98.34068316 78.22956374 76.19403543 98.31327703 85.13590951
 78.68550216 96.52938685 94.3393876  75.04546927 97.81747515 75.52632235
 97.52597354 98.67204823 96.39983058 98.18870369 89.67536189 75.16755114
 75.1999402  92.83205023 97.0276802  72.49919027 76.58021277 92.37362035
 74.28557192 74.02895084 97.03764606 94.02795426 98.18621222 98.77668984
 97.95201435 90.82392805 88.65884346 98.55993223 98.87385704 74.25567432
 98.6969629  75.00311433 69.44465207 94.41164013 96.16314124 96.78102499
 90.13379176 97.04761193 95.24379002 75.0952986  98.32075143 75.93990582
 98.13139996 74.29055485 76.2712709  97.53593941 76.61509331 96.07843137
 76.11679996 95.44559882 74.19338765 85.08608018 92.39106062 75.65338715
 76.68983731 99.15040985]
Accuracy th:0.7 is [45.60380696 97.22450607 68.62246805 96.96290206 97.90716795 73.84458231
 73.95171537 73.58048683 75.43912101 96.41976231 75.55871141 98.5325261
 99.34972718 77.93806214 75.57615168 96.31262924 96.21047911 74.92837033
 98.78167277 98.34068316 78.52604828 76.19403543 98.31327703 85.9157386
 79.46283977 96.52938685 94.3393876  75.04546927 97.81747515 75.52632235
 97.52597354 98.67204823 96.39983058 98.18870369 89.85225602 75.16755114
 75.1999402  92.99399557 97.0276802  72.49919027 77.10840372 92.37362035
 74.28557192 74.02895084 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 91.27239206 88.97276827 98.55993223 98.87385704 74.25567432
 98.6969629  75.0056058  69.44465207 94.78785161 96.16314124 96.78102499
 90.13379176 97.04761193 95.30856815 75.11273887 98.32075143 75.93990582
 98.13139996 74.29055485 76.2712709  97.53593941 76.61758477 96.07843137
 76.22891596 95.44559882 74.19338765 85.34020978 92.75481476 75.65338715
 76.68983731 99.15040985]
Avg Prec: is [54.52394263  3.76871083 14.9794626   4.57473445  1.53114232  4.29478445
 10.56871392  8.77616089  7.13716006  5.17097586  2.26451796  5.31282057
  1.55501749  5.90075328  3.01112882  4.25667754 26.02211921  6.21004233
  1.55363213  2.72379345  3.67078641  1.43195354  1.35323909  5.51752035
  5.77429264 12.8876594   8.26823948  4.47547271  3.89463143  6.94165118
  2.32379886  0.88220659  3.07127579  1.10850798  1.71061592  2.41624968
  2.02766639  2.19795269  2.24314492  6.23906325  1.73213955  6.02273561
  2.20595906  2.73587832  2.36595379  4.86483592  1.79290608  1.14491384
  1.37202192  1.16339215  1.21523457  0.99216846  0.73903131  2.35582045
  0.87135734  1.84877236 10.11889138  2.9821804   3.84116893  2.7792515
  7.88276106  2.0151286   3.18107453  2.61835909  1.36662714  1.85404572
  1.57709042  3.43700333  1.06673165  2.19705098  0.19246202  3.14403919
  1.57180015  3.92455127  3.52354217  2.30803162  0.59274446  1.56393873
  0.1288456   0.59335292]
mAP score regular 51.25, mAP score EMA 4.38
Train_data_mAP: current_mAP = 58.98, highest_mAP = 58.98
Val_data_mAP: current_mAP = 51.25, highest_mAP = 51.91
tensor([1.8490e-02, 1.0959e-03, 2.0801e-02, 1.3148e-03, 6.1129e-04, 1.5680e-03,
        2.7872e-04, 8.6243e-04, 3.1156e-03, 8.9214e-04, 2.3794e-04, 7.0809e-04,
        1.4222e-04, 2.2778e-03, 2.6223e-03, 2.8458e-03, 3.7309e-03, 4.7393e-03,
        2.7158e-03, 3.2956e-03, 4.4793e-05, 3.3111e-04, 8.0749e-04, 5.4888e-04,
        2.5458e-03, 5.0532e-04, 1.0979e-02, 4.2504e-04, 2.3948e-04, 9.1909e-04,
        9.2283e-04, 2.7260e-04, 1.0965e-02, 1.2391e-04, 9.7876e-04, 8.5060e-03,
        4.6835e-04, 1.2364e-03, 3.9511e-03, 8.7841e-03, 1.5291e-01, 5.4074e-01,
        8.9063e-01, 7.8989e-01, 9.8532e-01, 9.1096e-01, 4.0427e-04, 6.4867e-04,
        1.1633e-03, 1.1001e-03, 1.9451e-04, 5.6059e-04, 8.2190e-04, 1.4673e-03,
        4.3775e-04, 2.0730e-03, 8.7541e-02, 5.4876e-03, 6.6662e-02, 1.1700e-03,
        8.8932e-01, 1.5938e-03, 1.0398e-01, 8.2780e-03, 1.9166e-02, 3.2353e-03,
        2.7715e-02, 8.6748e-03, 4.9955e-01, 1.3987e-02, 4.5050e-04, 1.2316e-03,
        3.4393e-01, 5.7894e-02, 9.6961e-01, 9.9998e-01, 1.0000e+00, 9.9999e-01,
        5.5411e-01, 3.9111e-02], device='cuda:0')
Sum Train Loss:  tensor([4.4044e-01, 1.2506e-02, 3.7267e-01, 1.2763e-02, 3.6973e-03, 1.1187e-02,
        2.8731e-03, 1.4917e-02, 1.5945e-02, 1.0942e-02, 1.5527e-03, 3.2780e-03,
        1.2653e-04, 3.5044e-02, 2.2254e-02, 5.4775e-02, 5.9122e-02, 7.9069e-02,
        2.8745e-02, 3.1992e-02, 1.0079e-04, 3.0596e-04, 2.7937e-03, 2.7932e-03,
        4.7359e-02, 8.4700e-03, 2.9598e-01, 4.3613e-03, 1.1234e-03, 5.6900e-03,
        6.0788e-03, 1.3402e-03, 5.9253e-02, 9.6705e-04, 3.8359e-03, 6.1251e-02,
        3.9433e-03, 5.9443e-03, 2.9964e-03, 2.6025e-01, 3.9240e-01, 5.9229e+00,
        2.9247e+00, 1.7551e+00, 2.3160e+00, 1.7999e+00, 6.6517e-04, 1.2120e-03,
        6.6839e-03, 2.7229e-03, 4.5246e-04, 1.5738e-03, 8.4641e-04, 3.2332e-03,
        1.7744e-03, 1.9354e-02, 1.6596e+00, 2.7495e-02, 1.2685e+00, 8.4182e-03,
        3.8290e+00, 5.4822e-03, 6.4383e-01, 2.9258e-02, 8.8716e-02, 1.8081e-02,
        1.3372e-01, 1.0411e-01, 2.2565e+00, 1.2013e-01, 1.7592e-04, 1.2361e-02,
        2.3512e+00, 7.2346e-01, 4.2180e+00, 5.1933e+00, 1.8689e-01, 5.1195e-01,
        1.0415e+00, 1.9417e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [33/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 41.8
Sum Train Loss:  tensor([4.9161e-01, 1.9318e-02, 4.4672e-01, 1.3465e-02, 5.9396e-03, 2.4203e-02,
        1.0704e-03, 1.5624e-02, 2.6663e-02, 1.5093e-02, 2.5287e-03, 1.0837e-02,
        6.3533e-04, 4.5144e-02, 3.3403e-02, 2.8091e-02, 2.9020e-02, 1.6700e-02,
        4.3959e-03, 7.3403e-03, 2.3738e-04, 2.0472e-04, 5.2621e-04, 2.2711e-03,
        8.8100e-02, 6.6162e-03, 1.9098e-01, 3.0671e-03, 1.4057e-03, 8.2353e-03,
        1.3030e-02, 2.5294e-04, 9.0333e-02, 5.4069e-04, 4.3179e-03, 2.2087e-02,
        2.5075e-03, 4.1741e-03, 7.9931e-03, 1.3325e-01, 6.9292e-01, 4.4485e+00,
        5.7359e+00, 3.7442e+00, 7.3456e+00, 8.1062e+00, 3.5838e-03, 5.1168e-03,
        1.6890e-02, 3.8475e-03, 1.3733e-04, 8.4925e-03, 5.2183e-03, 8.9341e-03,
        1.9771e-03, 1.4723e-02, 2.1294e+00, 4.4824e-02, 9.4045e-01, 9.9503e-03,
        4.8184e+00, 4.6173e-03, 1.0554e+00, 1.0839e-01, 1.3273e-01, 9.8996e-03,
        9.0279e-02, 1.0762e-01, 1.0007e+00, 7.4504e-02, 1.2758e-04, 9.3397e-03,
        2.3837e+00, 8.1298e-01, 1.1470e+01, 1.5722e+00, 3.2180e-01, 2.7808e-01,
        5.5925e-02, 2.0634e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [33/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 59.6
Sum Train Loss:  tensor([5.7975e-01, 6.4992e-03, 5.6854e-01, 1.7857e-02, 1.7841e-03, 2.4034e-02,
        2.2081e-03, 1.3459e-02, 3.5250e-02, 8.8860e-03, 1.5599e-03, 1.1415e-03,
        2.7192e-04, 3.8544e-02, 3.2682e-02, 1.9261e-02, 4.8325e-02, 2.8639e-02,
        4.3018e-03, 4.7254e-02, 2.8076e-04, 6.5559e-04, 2.6581e-03, 4.0381e-04,
        5.1868e-02, 8.1424e-03, 2.7246e-01, 5.6322e-03, 1.6106e-03, 6.3244e-03,
        7.8396e-03, 1.9999e-03, 5.8762e-02, 9.4956e-04, 1.1370e-03, 1.0523e-02,
        6.2167e-03, 3.5304e-03, 1.3783e-02, 1.5732e-01, 6.4641e-01, 7.3728e+00,
        2.1145e+00, 2.4514e+00, 7.3635e+00, 5.9590e+00, 2.4826e-03, 5.8053e-03,
        5.1462e-03, 8.7117e-03, 1.7456e-03, 3.5335e-03, 1.6494e-03, 5.5818e-03,
        3.0837e-03, 8.1657e-03, 1.4736e+00, 6.3889e-02, 1.0948e+00, 6.0183e-03,
        4.2495e+00, 5.7943e-03, 4.6991e-01, 6.0772e-02, 6.2735e-02, 2.7971e-02,
        6.6720e-02, 1.4202e-01, 1.9536e+00, 9.8278e-02, 6.4199e-05, 1.5937e-02,
        1.4978e+00, 4.5378e-01, 1.9525e+00, 1.8206e+00, 7.6399e-02, 7.0318e-01,
        1.0108e-01, 1.4347e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [33/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 44.6
Sum Train Loss:  tensor([6.5326e-01, 3.1365e-03, 4.5132e-01, 9.1890e-03, 2.6752e-03, 1.4919e-02,
        1.5050e-03, 7.9186e-03, 1.4070e-02, 1.1162e-02, 6.1115e-04, 8.0403e-04,
        2.0309e-04, 2.9830e-02, 3.9225e-02, 2.3264e-02, 6.6769e-02, 8.8000e-02,
        8.1157e-03, 7.8032e-02, 8.2756e-05, 7.2317e-04, 2.7430e-03, 1.2876e-03,
        4.9528e-02, 3.3055e-03, 1.6076e-01, 5.7799e-03, 2.7231e-03, 4.4881e-03,
        1.7962e-02, 2.6779e-03, 1.3303e-01, 9.0565e-04, 2.4014e-03, 1.0954e-01,
        3.1737e-03, 3.7609e-03, 4.0228e-03, 2.8451e-01, 1.1796e+00, 7.2749e+00,
        6.9196e+00, 2.5107e+00, 7.9803e-01, 6.3567e+00, 1.8718e-03, 3.4293e-03,
        8.6224e-03, 3.2359e-03, 1.1872e-04, 2.5775e-03, 2.1558e-03, 5.2752e-03,
        1.9967e-03, 2.1238e-02, 1.5967e+00, 6.7466e-02, 7.8182e-01, 1.0052e-02,
        4.7590e+00, 4.8746e-03, 1.0472e+00, 3.8854e-02, 6.2707e-02, 3.8148e-02,
        1.3529e-01, 1.1009e-01, 6.3991e-01, 4.3785e-02, 7.3349e-05, 1.7403e-02,
        6.8503e-01, 1.0528e+00, 4.6624e+00, 6.3324e-01, 2.7291e-01, 1.8789e+00,
        1.5763e-01, 4.3162e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [33/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 46.1
Sum Train Loss:  tensor([6.1019e-01, 2.8856e-03, 3.6749e-01, 9.0067e-03, 2.3481e-03, 8.1119e-03,
        6.0831e-04, 1.6766e-02, 2.6128e-02, 5.0374e-03, 2.0502e-03, 7.2429e-03,
        1.2174e-04, 3.9118e-02, 2.5373e-02, 1.6770e-02, 4.6845e-02, 2.1722e-02,
        4.5239e-03, 4.0127e-02, 3.8195e-04, 1.3593e-04, 9.5216e-04, 3.0554e-03,
        5.4077e-02, 5.3681e-03, 1.5782e-01, 3.0668e-03, 1.5967e-03, 1.1898e-02,
        5.7398e-03, 1.3811e-03, 3.8367e-02, 5.1160e-04, 3.2532e-03, 5.8290e-02,
        3.0150e-03, 5.3490e-03, 1.0150e-02, 1.7088e-01, 2.2803e+00, 6.0941e+00,
        3.6183e+00, 9.2032e+00, 3.5265e+00, 5.1913e+00, 1.7646e-03, 6.1233e-03,
        5.0121e-03, 5.5049e-03, 5.1130e-04, 4.3010e-03, 1.0785e-02, 1.0440e-02,
        2.6417e-03, 7.8964e-03, 1.6773e+00, 4.3820e-02, 9.4036e-01, 1.1339e-02,
        5.7581e+00, 7.4855e-03, 1.6995e+00, 1.2416e-01, 6.2763e-02, 2.0076e-02,
        1.2131e-01, 1.6666e-01, 9.5727e-01, 6.6719e-02, 1.3510e-04, 6.9178e-03,
        1.6266e+00, 6.0627e-01, 1.0875e+00, 2.2359e+00, 3.2368e-01, 5.3746e-01,
        7.8248e-02, 1.2290e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [33/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 50.0
Sum Train Loss:  tensor([5.9593e-01, 1.0814e-02, 5.2188e-01, 4.8588e-03, 5.6343e-03, 1.3514e-02,
        1.7973e-03, 1.1432e-02, 5.8788e-02, 4.2648e-03, 3.6837e-04, 7.7500e-04,
        5.3572e-04, 4.6408e-02, 2.9710e-02, 2.1948e-02, 3.7198e-02, 1.3135e-02,
        1.4009e-02, 3.9357e-03, 9.1337e-05, 6.8955e-04, 2.6610e-04, 2.6884e-03,
        4.9081e-02, 7.2420e-03, 2.3905e-01, 7.4213e-03, 3.8116e-03, 3.6587e-03,
        1.0904e-02, 2.0278e-03, 5.5139e-02, 3.9289e-04, 3.1311e-03, 1.9188e-02,
        3.0109e-03, 4.3243e-03, 1.0591e-02, 1.7736e-01, 6.1403e-01, 6.1731e+00,
        3.5268e+00, 4.1062e+00, 2.2279e+00, 5.1420e+00, 1.5991e-03, 2.9893e-03,
        3.6050e-03, 5.7980e-03, 1.1996e-03, 9.7515e-03, 4.7266e-03, 8.1699e-03,
        4.7172e-03, 3.2307e-02, 1.5325e+00, 1.0749e-01, 9.9635e-01, 4.7064e-03,
        9.2697e+00, 1.1636e-02, 2.6581e+00, 8.8390e-02, 1.2794e-01, 3.4633e-02,
        1.9462e-01, 1.6496e-01, 4.4398e+00, 2.0607e-01, 1.2101e-04, 1.3037e-02,
        2.3807e+00, 1.1413e+00, 5.7021e+00, 4.6996e-01, 1.7912e+00, 5.8356e-01,
        3.8241e-01, 3.0941e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [33/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 56.2
Sum Train Loss:  tensor([5.9114e-01, 5.6581e-03, 2.7536e-01, 1.2193e-02, 1.2455e-03, 1.2618e-02,
        1.9423e-03, 1.6317e-02, 5.3273e-02, 4.9581e-03, 3.7084e-04, 7.6432e-04,
        1.2587e-04, 3.6940e-02, 4.1942e-02, 2.3458e-02, 8.5433e-02, 3.6034e-02,
        2.3978e-02, 3.4092e-02, 4.5312e-04, 7.4018e-04, 2.8385e-03, 1.5139e-03,
        4.7218e-02, 7.3188e-03, 2.2497e-01, 3.8719e-03, 1.7759e-03, 7.8348e-03,
        6.0819e-03, 3.8725e-04, 7.3730e-02, 7.1351e-04, 1.7843e-03, 4.5307e-02,
        2.5798e-03, 8.2764e-03, 1.5842e-02, 1.8174e-01, 1.2674e+00, 5.6393e+00,
        7.2779e+00, 4.4760e+00, 1.0659e+00, 5.7569e+00, 2.1014e-03, 8.8631e-04,
        6.3584e-03, 4.2673e-03, 1.2290e-03, 7.6400e-04, 1.6244e-03, 2.0796e-02,
        1.9190e-03, 1.0485e-02, 1.5563e+00, 4.8380e-02, 5.2280e-01, 7.1154e-03,
        4.4722e+00, 8.1994e-03, 5.4213e-01, 8.0495e-02, 3.2146e-02, 1.4035e-02,
        6.0809e-02, 1.8965e-01, 3.7402e-01, 1.3248e-01, 1.2591e-04, 1.0164e-02,
        6.5157e-01, 8.6647e-01, 1.5702e+01, 6.7463e-01, 1.0227e+00, 3.8383e+00,
        5.6759e-02, 2.3460e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [33/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 58.5
Sum_Val Meta Model:  tensor([9.7753e-01, 7.5584e-02, 1.8009e+00, 6.3005e-02, 9.1819e-04, 2.0969e-02,
        9.3881e-04, 1.4703e-02, 2.2291e-02, 7.5732e-03, 1.5999e-03, 9.0503e-03,
        1.0423e-03, 3.2640e-02, 2.3262e-02, 2.5136e-02, 1.8708e+00, 1.0600e-02,
        4.0041e-03, 3.4366e-03, 3.4728e-05, 9.1117e-05, 3.3425e-04, 2.3345e-04,
        7.9040e-02, 9.6695e-03, 3.2362e-01, 1.2245e-03, 2.6986e-03, 1.1338e-02,
        1.0840e-03, 2.3824e-04, 7.7675e-02, 8.5665e-05, 8.4496e-04, 6.3834e-03,
        1.3026e-03, 7.7097e-03, 4.8979e-03, 4.4618e-01, 1.5753e+00, 1.7829e+01,
        7.8788e+00, 2.0205e+01, 2.4983e+01, 2.1841e+01, 8.4150e-04, 3.8937e-03,
        8.1487e-04, 5.3215e-03, 8.5262e-05, 2.9298e-04, 6.9430e-03, 1.9837e-03,
        5.1475e-04, 3.5555e-03, 2.5280e+00, 4.2706e-02, 9.2931e-01, 4.5736e-03,
        1.4671e+01, 2.1912e-02, 6.2121e-01, 3.7180e-02, 9.2296e-03, 5.4783e-03,
        1.2143e-02, 6.2919e-02, 4.7706e+00, 2.7392e-01, 5.8404e-05, 2.3779e-02,
        5.7983e+00, 5.8961e-01, 2.3924e+01, 5.7366e+00, 2.2617e-01, 1.1810e+01,
        5.1220e-02, 2.0403e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([9.4166e-01, 4.7674e-02, 1.1134e+00, 3.2813e-02, 3.6950e-04, 2.0016e-02,
        8.2186e-04, 1.4744e-02, 1.9968e-02, 8.3837e-03, 1.6288e-03, 9.7092e-03,
        1.2456e-03, 3.3737e-02, 2.2620e-02, 8.9970e-02, 1.2226e+00, 2.2552e-02,
        2.4014e-03, 5.6057e-03, 2.2000e-05, 5.7201e-05, 1.1274e-04, 7.8453e-04,
        6.7868e-02, 8.2994e-03, 2.9042e-01, 1.7635e-03, 3.6702e-03, 1.2860e-02,
        4.1994e-04, 2.4048e-04, 7.7311e-02, 1.0810e-04, 6.6137e-04, 5.5806e-03,
        3.2579e-03, 5.4124e-03, 6.5621e-03, 3.9924e-01, 1.7849e+00, 2.6393e+01,
        7.8291e+00, 2.0324e+01, 2.7965e+01, 2.5505e+01, 5.8021e-04, 3.9241e-03,
        3.5035e-04, 3.4736e-03, 1.9850e-05, 1.1164e-04, 6.2871e-03, 4.0957e-04,
        2.4267e-04, 1.9560e-03, 1.8243e+00, 3.8052e-02, 9.0501e-01, 5.1301e-03,
        1.9140e+01, 1.3876e-02, 4.4717e-01, 5.8212e-02, 5.5208e-03, 5.6627e-03,
        6.9321e-03, 7.9010e-02, 6.0730e+00, 2.7840e-01, 7.2574e-05, 2.2409e-02,
        5.5941e+00, 5.8441e-01, 3.1949e+01, 1.4158e+01, 1.4270e-02, 1.4395e+01,
        6.8865e-02, 3.7715e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.0929e+01, 4.3504e+01, 5.3524e+01, 2.4956e+01, 6.0445e-01, 1.2766e+01,
        2.9487e+00, 1.7096e+01, 6.4091e+00, 9.3973e+00, 6.8451e+00, 1.3712e+01,
        8.7584e+00, 1.4812e+01, 8.6262e+00, 3.1615e+01, 3.2770e+02, 4.7586e+00,
        8.8426e-01, 1.7010e+00, 4.9115e-01, 1.7276e-01, 1.3961e-01, 1.4293e+00,
        2.6659e+01, 1.6424e+01, 2.6452e+01, 4.1490e+00, 1.5326e+01, 1.3992e+01,
        4.5506e-01, 8.8217e-01, 7.0509e+00, 8.7238e-01, 6.7572e-01, 6.5608e-01,
        6.9561e+00, 4.3776e+00, 1.6609e+00, 4.5450e+01, 1.1673e+01, 4.8809e+01,
        8.7905e+00, 2.5730e+01, 2.8381e+01, 2.7997e+01, 1.4352e+00, 6.0494e+00,
        3.0117e-01, 3.1577e+00, 1.0205e-01, 1.9914e-01, 7.6495e+00, 2.7914e-01,
        5.5435e-01, 9.4360e-01, 2.0839e+01, 6.9342e+00, 1.3576e+01, 4.3848e+00,
        2.1522e+01, 8.7066e+00, 4.3004e+00, 7.0322e+00, 2.8805e-01, 1.7503e+00,
        2.5012e-01, 9.1080e+00, 1.2157e+01, 1.9904e+01, 1.6110e-01, 1.8194e+01,
        1.6265e+01, 1.0095e+01, 3.2951e+01, 1.4158e+01, 1.4270e-02, 1.4396e+01,
        1.2428e-01, 9.6430e-01], device='cuda:0')
Outer loop valEpocw Maximum [33/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 172.4
model_train val_loss valEpocw [33/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 210.0
model_train val_loss valEpocw [33/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1225.9
Sum_Val Meta Model:  tensor([2.0429e+00, 3.3580e-02, 2.0897e+00, 2.7289e-02, 9.2534e-04, 1.2225e-01,
        3.9461e-02, 1.0970e-01, 1.4298e-01, 5.8100e-02, 9.7607e-03, 4.0780e-01,
        3.1739e-03, 2.3084e-02, 1.0962e-01, 1.1700e-01, 7.6130e-02, 1.4195e-01,
        4.9150e-02, 2.2400e-01, 7.7909e-06, 3.0509e-05, 4.2443e-04, 7.8916e-03,
        1.0245e-01, 2.4067e-02, 5.1517e-01, 1.5383e-02, 6.5738e-03, 8.4719e-05,
        2.0468e-04, 3.7668e-05, 1.8605e-03, 4.5338e-05, 9.4195e-05, 6.7601e-04,
        5.1957e-03, 2.8118e-04, 2.4456e-04, 1.7234e-02, 2.1777e-02, 1.9775e+00,
        7.7145e-02, 1.9447e-01, 2.9676e-01, 4.3099e+00, 1.0907e-03, 1.0143e-03,
        1.6749e-04, 1.3374e-02, 2.7624e-05, 1.6103e-04, 7.2769e-05, 8.8931e-05,
        1.4157e-04, 1.2183e-02, 1.2890e+00, 4.0778e-02, 5.5415e-01, 9.0087e-03,
        1.6134e+00, 2.2152e-03, 2.2693e-01, 2.4852e-02, 1.3247e-02, 3.5058e-03,
        2.1656e-02, 2.0865e-01, 5.1912e-02, 7.8972e-02, 1.8698e-05, 3.3279e-03,
        2.1257e+00, 3.7330e-01, 5.3916e+00, 3.5336e-01, 1.2761e-01, 4.8775e-01,
        2.0685e-02, 5.0533e-03], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.6649e+00, 3.6607e-02, 1.1750e+00, 2.3389e-02, 3.6531e-03, 9.5826e-02,
        3.8265e-02, 7.0178e-02, 1.1149e-01, 4.5369e-02, 8.1092e-03, 1.0689e-01,
        2.9344e-03, 3.9515e-02, 7.2614e-02, 1.8913e-02, 5.4961e-02, 1.5255e-01,
        1.5135e-02, 1.1943e-01, 4.3302e-05, 1.1090e-04, 2.0788e-04, 5.4088e-03,
        8.7195e-02, 2.1893e-02, 4.8734e-01, 1.3580e-02, 6.7344e-03, 1.6921e-03,
        2.5921e-04, 1.4907e-04, 1.4223e-02, 8.8640e-04, 6.6872e-04, 1.5752e-03,
        2.4990e-03, 1.7159e-03, 2.1418e-03, 2.1650e-02, 2.0437e-02, 2.3132e+00,
        1.2764e-02, 1.0762e-01, 3.9107e-03, 7.9976e-01, 7.6306e-04, 3.3526e-04,
        4.0465e-04, 1.0743e-02, 4.6940e-05, 1.8411e-04, 1.7012e-04, 1.4897e-03,
        2.2847e-04, 4.0642e-03, 1.0159e+00, 3.9233e-02, 4.4314e-01, 1.3424e-03,
        3.0799e+00, 1.0495e-03, 1.9345e-01, 5.9808e-03, 4.3162e-03, 1.0814e-03,
        3.3614e-03, 2.1267e-01, 3.6617e-02, 4.1854e-02, 7.8455e-06, 6.4778e-04,
        2.3095e+00, 1.5491e-01, 6.9802e+00, 1.9385e-02, 4.6208e-02, 2.3035e-02,
        1.4964e-03, 6.7495e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.6640e+01, 1.9448e+01, 4.3570e+01, 9.8403e+00, 3.2165e+00, 3.3286e+01,
        5.4624e+01, 4.0829e+01, 2.1068e+01, 2.7911e+01, 1.6246e+01, 7.3625e+01,
        9.0285e+00, 1.1700e+01, 1.5549e+01, 3.6471e+00, 9.1798e+00, 1.8904e+01,
        2.7737e+00, 1.8532e+01, 3.5011e-01, 1.5647e-01, 1.3499e-01, 4.7934e+00,
        2.4725e+01, 2.0418e+01, 3.1658e+01, 1.4249e+01, 1.1951e+01, 1.0181e+00,
        1.5737e-01, 2.4638e-01, 9.0678e-01, 3.2487e+00, 3.8078e-01, 1.4237e-01,
        2.5111e+00, 7.7309e-01, 3.3495e-01, 1.9588e+00, 1.5830e-01, 4.5402e+00,
        1.5142e-02, 1.4912e-01, 4.0094e-03, 9.1117e-01, 8.8759e-01, 2.5798e-01,
        1.7558e-01, 5.0169e+00, 1.1355e-01, 1.7980e-01, 1.0999e-01, 5.7382e-01,
        2.5267e-01, 1.2270e+00, 9.4756e+00, 4.3506e+00, 6.3637e+00, 5.7728e-01,
        3.5952e+00, 3.7598e-01, 1.7004e+00, 4.6995e-01, 1.7512e-01, 1.9744e-01,
        9.7377e-02, 1.8986e+01, 8.1928e-02, 2.4960e+00, 9.2511e-03, 2.9228e-01,
        7.9467e+00, 2.1276e+00, 7.3365e+00, 1.9385e-02, 4.6208e-02, 2.3035e-02,
        3.0066e-03, 1.4989e-01], device='cuda:0')
Outer loop valEpocw Maximum [33/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 26.5
model_train val_loss valEpocw [33/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 22.4
model_train val_loss valEpocw [33/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 701.2
Sum_Val Meta Model:  tensor([1.5726e+00, 4.9819e-03, 2.5649e-01, 5.4362e-03, 1.1406e-03, 7.2091e-03,
        8.7209e-04, 1.6711e-02, 9.9346e-03, 2.9722e-03, 5.8356e-04, 2.0844e-03,
        1.1940e-04, 5.9514e-02, 1.4670e-02, 1.8580e-02, 4.3855e-02, 3.8245e-02,
        6.6769e-03, 1.1768e-02, 1.0000e-04, 1.4509e-03, 1.4961e-02, 2.1745e-03,
        2.7678e-02, 2.9542e-03, 2.9649e-01, 5.4262e-03, 5.4194e-04, 8.6252e-03,
        7.0897e-03, 3.7341e-03, 1.3684e-01, 8.2101e-05, 5.7357e-03, 4.0009e-02,
        1.7108e-02, 3.0803e-02, 2.8722e-03, 2.8033e-01, 4.4708e+00, 3.9071e+01,
        6.3292e+01, 7.4571e+01, 5.0165e+01, 7.6609e+01, 1.3894e-02, 1.7677e-02,
        8.9241e-02, 4.0215e-02, 1.1511e-02, 3.7042e-02, 8.9257e-02, 3.3640e-02,
        5.3710e-02, 3.3249e-01, 1.7659e+01, 8.1824e-02, 6.1870e-01, 5.6216e-03,
        9.9762e+01, 5.9547e-03, 1.1769e+00, 1.4595e-01, 3.1151e-01, 5.7157e-03,
        3.0147e-01, 1.0537e-01, 2.3237e+00, 4.0108e-02, 2.3824e-04, 1.4841e-02,
        1.2190e+00, 2.3420e+00, 9.0826e-01, 3.0549e+01, 1.0599e+01, 1.0751e+00,
        1.0130e-01, 3.4716e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.1639e-01, 7.0674e-04, 1.3652e-01, 5.3686e-04, 3.3198e-05, 2.4860e-04,
        2.4975e-05, 1.8363e-02, 2.4986e-03, 1.0978e-04, 1.6356e-05, 5.2213e-05,
        3.7565e-06, 4.3750e-02, 1.7086e-03, 4.9323e-03, 6.0211e-03, 8.2150e-04,
        3.4051e-04, 1.4341e-04, 2.4308e-06, 7.8026e-06, 9.0013e-06, 1.8784e-05,
        1.2686e-02, 7.1424e-04, 3.0917e-01, 3.4193e-03, 2.2347e-04, 4.0330e-04,
        4.4967e-04, 3.5183e-03, 1.2451e-01, 1.8200e-05, 1.3204e-03, 5.9788e-03,
        8.6286e-03, 1.9697e-02, 7.8324e-04, 3.6971e-01, 3.9982e+00, 5.2114e+01,
        7.1955e+01, 7.8934e+01, 1.0233e+02, 8.8657e+01, 5.9914e-03, 6.9513e-03,
        7.0899e-02, 1.8228e-02, 5.8436e-03, 3.5081e-02, 9.5577e-02, 4.3300e-02,
        3.9712e-02, 2.0889e-01, 1.2353e+01, 9.7176e-02, 5.9307e-01, 1.8997e-03,
        1.2743e+02, 5.3570e-04, 9.1932e-01, 1.1922e-01, 2.8457e-01, 1.5831e-02,
        3.1716e-01, 1.3989e-01, 3.1810e+00, 4.9803e-02, 4.0176e-05, 1.3654e-02,
        2.0937e+00, 2.1420e+00, 1.5196e+00, 2.7197e+01, 1.4822e+01, 2.1017e-01,
        2.1427e-02, 2.0515e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.3527e+01, 5.0682e-01, 5.9248e+00, 3.1463e-01, 4.1415e-02, 1.1562e-01,
        6.2110e-02, 1.6001e+01, 6.4635e-01, 9.2947e-02, 4.8136e-02, 5.0627e-02,
        1.8845e-02, 1.5837e+01, 5.1368e-01, 1.4042e+00, 1.2650e+00, 1.2165e-01,
        9.3867e-02, 3.3905e-02, 3.5807e-02, 1.6397e-02, 7.3511e-03, 2.4768e-02,
        4.2467e+00, 1.0070e+00, 2.7401e+01, 5.8287e+00, 6.6328e-01, 3.0103e-01,
        3.4896e-01, 8.0245e+00, 9.1278e+00, 1.0154e-01, 9.7478e-01, 5.6648e-01,
        1.1630e+01, 1.1305e+01, 1.5937e-01, 4.0076e+01, 2.8280e+01, 9.4875e+01,
        8.1228e+01, 1.0121e+02, 1.0416e+02, 9.8168e+01, 9.5844e+00, 6.8721e+00,
        3.7678e+01, 1.0701e+01, 1.9133e+01, 4.2373e+01, 7.5180e+01, 2.2159e+01,
        6.1379e+01, 7.3633e+01, 1.3550e+02, 1.4414e+01, 9.8058e+00, 1.1424e+00,
        1.4464e+02, 2.6993e-01, 9.1130e+00, 1.2250e+01, 1.4122e+01, 3.8429e+00,
        1.0887e+01, 1.5940e+01, 7.2629e+00, 3.6788e+00, 6.6738e-02, 8.5143e+00,
        7.3928e+00, 3.6124e+01, 1.5864e+00, 2.7198e+01, 1.4822e+01, 2.1017e-01,
        4.4433e-02, 5.2388e-01], device='cuda:0')
Outer loop valEpocw Maximum [33/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 481.2
model_train val_loss valEpocw [33/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 593.7
model_train val_loss valEpocw [33/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1534.4
Sum_Val Meta Model:  tensor([3.6367e+00, 1.3360e-02, 8.9686e-01, 9.1684e-03, 2.3895e-03, 4.6647e-02,
        6.8105e-03, 4.4074e-02, 2.3549e-02, 4.6576e-02, 3.9493e-03, 1.3107e-02,
        3.9082e-04, 9.7452e-02, 8.5208e-02, 2.9569e-02, 3.6945e-02, 4.1500e-02,
        8.7315e-03, 1.4231e-02, 3.6995e-04, 2.1436e-03, 3.1484e-03, 4.5387e-03,
        1.2865e-01, 6.3548e-03, 6.0786e-01, 5.4854e-03, 1.2634e-02, 4.0349e-03,
        4.0366e-03, 7.9144e-04, 1.9901e-01, 5.7407e-03, 2.1748e-02, 1.7697e-01,
        1.5660e-03, 1.0296e-02, 1.5092e-01, 5.0179e-01, 3.4015e+00, 1.6921e+01,
        9.2866e+00, 8.0724e+00, 1.0273e+01, 1.4955e+01, 1.9915e-03, 2.3439e-03,
        7.3450e-03, 4.1219e-03, 1.0401e-03, 1.7054e-03, 2.2162e-03, 7.7401e-02,
        2.7974e-03, 2.0065e-02, 4.2863e+00, 9.9503e-02, 1.4716e+00, 1.8868e-02,
        3.6425e+01, 1.0936e-02, 1.1688e+00, 3.7475e-01, 4.6524e-01, 3.8506e-02,
        7.4522e-01, 6.6094e-01, 1.2128e+00, 1.1014e-01, 5.3506e-04, 1.3080e-02,
        2.4563e+00, 1.2770e+00, 3.6999e+02, 1.1671e+01, 2.1959e+00, 7.2746e+00,
        1.1389e-01, 1.9203e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.2724e+00, 6.6107e-03, 5.9577e-01, 7.9546e-03, 2.7289e-03, 3.9826e-02,
        8.7036e-03, 4.1845e-02, 4.1598e-02, 4.7160e-02, 2.7944e-03, 1.8036e-02,
        3.6842e-04, 8.6941e-02, 1.1705e-01, 7.7984e-03, 7.1105e-03, 8.0593e-03,
        5.7837e-04, 7.6986e-04, 3.0807e-05, 4.1433e-05, 2.6675e-04, 8.4561e-04,
        1.2776e-01, 3.4392e-03, 5.2309e-01, 1.7893e-03, 1.6521e-02, 4.9636e-04,
        3.8808e-04, 3.1883e-04, 4.7652e-03, 1.0614e-04, 4.1910e-04, 1.3098e-03,
        1.1798e-03, 6.1218e-04, 9.1662e-04, 4.8605e-02, 8.6070e-02, 5.5508e-01,
        2.6657e-01, 1.3084e+00, 7.9794e-02, 7.1551e-01, 5.0528e-04, 2.7531e-04,
        2.5683e-04, 8.9252e-04, 4.0435e-05, 1.0496e-04, 5.4809e-05, 9.8619e-04,
        3.3084e-04, 3.4835e-03, 4.5590e-01, 1.0411e-02, 9.4147e-01, 2.6772e-03,
        1.1346e+01, 1.9070e-03, 1.4606e-01, 1.2370e-02, 1.2074e-02, 6.8011e-03,
        2.3779e-02, 1.0465e-01, 6.6124e-02, 1.4408e-02, 3.2200e-05, 1.6494e-03,
        1.1531e-01, 5.3125e-01, 1.0391e+02, 5.8049e-01, 2.3775e-02, 7.9518e+00,
        8.1736e-03, 3.6186e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.5608e+01, 2.3119e+00, 1.8434e+01, 2.3431e+00, 1.4955e+00, 9.6253e+00,
        8.4790e+00, 1.7274e+01, 6.0667e+00, 1.9021e+01, 3.3764e+00, 8.2320e+00,
        6.7662e-01, 1.7200e+01, 1.9411e+01, 1.3039e+00, 9.4268e-01, 7.3332e-01,
        8.9580e-02, 1.0423e-01, 1.3958e-01, 3.8112e-02, 1.2099e-01, 5.4930e-01,
        2.5285e+01, 2.0877e+00, 2.6853e+01, 1.2412e+00, 1.8804e+01, 1.9267e-01,
        1.5650e-01, 3.3444e-01, 2.1824e-01, 2.0813e-01, 1.5242e-01, 7.0533e-02,
        7.9010e-01, 1.9124e-01, 9.0455e-02, 2.8866e+00, 4.5913e-01, 1.0337e+00,
        3.1695e-01, 1.7981e+00, 8.2152e-02, 8.2130e-01, 3.9844e-01, 1.3870e-01,
        7.5568e-02, 2.9028e-01, 5.8084e-02, 6.8653e-02, 2.3421e-02, 2.3129e-01,
        2.4425e-01, 7.1935e-01, 3.6388e+00, 8.4167e-01, 1.1525e+01, 7.9134e-01,
        1.3280e+01, 4.5796e-01, 1.0769e+00, 6.5172e-01, 3.1448e-01, 8.1799e-01,
        4.5099e-01, 5.8096e+00, 1.4649e-01, 6.4456e-01, 2.3322e-02, 4.8190e-01,
        3.5005e-01, 6.2758e+00, 1.1117e+02, 5.8053e-01, 2.3775e-02, 7.9524e+00,
        1.8949e-02, 5.9863e-02], device='cuda:0')
Outer loop valEpocw Maximum [33/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 512.2
model_train val_loss valEpocw [33/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 132.3
model_train val_loss valEpocw [33/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 427.3
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.65853243 97.40622068 93.32488639 98.08116373 98.64402237 97.61333317
 98.32238886 95.29245501 97.99710043 97.04072806 98.61843788 98.94007139
 99.42983151 95.42281405 97.51099524 97.44764318 96.39867935 97.78145978
 98.8718461  98.44178312 98.53559289 99.32505696 99.5260779  99.00098683
 95.24494097 96.85676344 94.43354735 97.08336887 98.08360035 98.30289592
 98.40645216 98.54412105 97.04072806 98.61234634 98.70859273 98.83529684
 97.61211486 98.42838172 98.92545169 93.42722433 98.2310157  97.00174218
 98.93397985 98.46614929 99.08870506 98.35406489 98.26878328 98.66717023
 98.16522703 98.79143773 98.85357147 98.67326178 99.02169808 98.45762113
 98.75123354 97.76562176 93.02274582 96.96762954 96.69472838 97.69374155
 97.80948088 98.68300825 97.74734713 97.46957274 98.92057845 97.61942471
 98.69275472 95.99176423 99.32140203 98.33579026 99.81603538 97.48175583
 99.19713454 96.168419   99.05337411 99.29703585 99.74415516 99.57846517
 99.87573251 99.18982469]
Accuracy th:0.7 is [86.28062524 97.32337569 92.60242931 98.0360863  98.56605061 97.51586847
 98.13476931 95.03539187 98.10918483 96.83970712 98.55995906 98.86331794
 99.4152118  95.35580707 97.40743899 97.29291797 96.35238362 97.6450092
 98.84991655 98.38574091 98.34310011 99.25804998 99.4566343  98.85844471
 95.23641281 96.74589735 94.25567427 96.95910138 98.03730461 98.22857909
 98.17131858 98.61234634 96.88722116 98.43690988 98.57701539 98.63671252
 97.36357988 98.26512835 98.83651515 93.05442185 98.09212851 96.65817912
 98.75854339 98.23345232 99.00342345 98.20177629 98.22248754 98.63793082
 98.07872711 98.75488846 98.81945883 98.62940266 99.00707837 98.36624797
 98.72321244 97.65597398 91.97012707 96.66183404 96.53756655 97.58774869
 97.23931239 98.5234098  97.54510788 97.41353054 98.843825   97.50368538
 98.59772664 95.9734896  99.19713454 98.22248754 99.81603538 97.20398143
 99.16667682 95.93450372 98.96809249 99.18251483 99.74050024 99.5821201
 99.86476773 99.17398667]
Avg Prec: is [96.3941954  35.16979822 72.47543746 66.80371866 78.82899258 64.20777961
 73.83111228 47.79294545 58.28832404 52.67246614 32.73566221 54.21790644
 26.5861262  28.38526664 33.46645365 58.53825872 28.36190838 42.96153183
 49.22403473 40.37955538 62.1787574  51.7791489  89.54222233 82.66786293
 26.02485978 34.07942936 38.4439543  40.3436838  26.00639707 39.40534919
 73.9434656  37.37896514 57.49585121 63.17180837 72.29125826 79.05904559
 58.38618413 74.73073051 86.04628085 47.04330796 52.17649371 88.33100507
 89.8636604  87.30064504 91.79217355 93.49193363 41.16424684 34.80782586
 43.51804516 47.35699457 64.25198959 39.70163589 27.09218778 73.11679966
 30.16592221 46.61496691 74.81993287 59.15305165 47.52537027 60.99344855
 96.17128967 83.54286508 75.15957881 51.64551468 63.46799219 43.32842203
 63.12678289 26.89280254 82.26396903 68.53073558 12.13123091 73.2820208
 86.67284502 52.31435528 93.30786892 94.78667792 89.69513632 94.02837442
 52.98960729 31.54961512]
Accuracy th:0.5 is [46.04232405 97.2137279  70.37073135 97.02489005 97.26733349 74.9893398
 75.37067044 75.17208611 76.20643023 96.46812295 76.50735249 98.52097319
 99.41399349 79.92592683 76.10165568 96.56680596 96.29512311 75.88966996
 98.65376884 98.30776915 80.19639137 76.95934504 98.38695922 84.04015546
 83.9353809  96.65086926 94.0778012  75.6715927  98.01293844 76.39526809
 97.30875598 98.57457877 96.36213009 98.02024829 88.40048245 76.08216274
 75.80560666 91.60950768 97.11504489 73.31660189 78.7234561  92.05906361
 75.33046625 75.0307623  96.9627563  93.87434364 98.02877645 98.57336046
 97.87039632 90.68846627 89.74915023 98.55508583 98.99976852 75.49250131
 98.70615611 75.83850099 70.86780132 93.48570315 96.24273583 96.9067141
 89.79300934 97.17717864 95.16209598 75.84215592 98.42838172 76.40379625
 98.20786784 74.99055811 76.92157747 97.55972759 77.40768265 95.99054592
 76.88137328 95.45083515 75.26224096 83.98289495 90.53617768 76.4111061
 77.44910515 99.14718388]
Accuracy th:0.7 is [46.11785919 97.2137279  70.37073135 97.02489005 97.26733349 74.99177642
 75.37067044 75.5473252  76.20643023 96.47665111 76.50735249 98.52097319
 99.41399349 80.45710944 76.10287399 96.56680596 96.29512311 75.88966996
 98.65376884 98.30776915 80.56432061 76.95934504 98.38695922 85.12445024
 84.90637297 96.65086926 94.0778012  75.6715927  98.01293844 76.39526809
 97.30875598 98.57457877 96.36213009 98.02024829 88.6161231  76.08216274
 75.80560666 91.88362715 97.11504489 73.31660189 79.33992032 92.05906361
 75.33046625 75.0307623  96.9627563  93.87434364 98.02877645 98.57336046
 97.9800441  91.26838123 89.99768521 98.55508583 98.99976852 75.49250131
 98.70615611 75.84215592 70.86780132 93.98642804 96.24273583 96.9067141
 89.79300934 97.17717864 95.23763112 75.84581085 98.42838172 76.40379625
 98.20786784 74.99055811 76.92157747 97.55972759 77.4698164  95.99054592
 77.10432378 95.45083515 75.26224096 84.17051449 90.96745897 76.4111061
 77.44910515 99.14718388]
Avg Prec: is [55.60303932  3.05612131 11.36805824  3.40967667  2.2364371   3.75021221
  3.28581451  5.73444944  2.54910302  3.75891334  1.68039267  1.58052905
  0.66569673  5.20977823  2.63379744  3.1608329   3.70849926  2.66898332
  1.42123005  1.77980409  1.82320083  0.88529376  1.92474752  2.48337599
  5.11586295  3.55491266  6.57036719  3.25203272  2.12564823  1.89948076
  2.60860771  1.35745888  3.68584452  1.70459828  2.33801792  2.34776842
  3.16395982  2.5710463   2.79278658  7.37119796  2.33552987  8.37374398
  3.36949555  3.87256985  3.16560841  6.44003945  2.13859998  1.49176448
  2.14952101  1.63028904  1.78861297  1.52520799  1.10602597  3.03476474
  1.33819127  2.63535691 11.21930685  3.71276598  3.90966707  2.88002839
 10.78864623  2.10632533  3.88899779  3.18555152  1.6141322   2.45815334
  1.88475107  4.19592666  1.2343836   2.41236874  0.18310483  3.35008074
  1.90880781  4.66798769  3.83156155  3.09470396  0.81484742  1.97794216
  0.15351422  0.70517812]
mAP score regular 58.87, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [87.6896629  97.47365274 93.04133343 98.29085383 98.92617784 97.61068341
 98.5101029  95.31853402 97.95699728 97.080001   98.64713357 99.00590478
 99.37713332 95.24628149 97.46368687 97.4088746  96.41228791 97.84737275
 98.96604131 98.42539303 98.7791813  99.37215038 99.61880559 99.23262825
 95.43812442 96.76109326 94.50133293 97.33662207 97.91464235 98.23853302
 98.68450557 98.58733837 96.93798739 98.77668984 98.90624611 99.13047811
 97.96198022 98.31825996 99.11303785 93.32286917 98.01430102 92.30385928
 97.3266562  96.04105937 96.67638339 94.3991828  98.37058076 98.82153624
 98.11645115 98.69945437 98.91870344 98.6670653  98.89628024 98.47023943
 98.7866557  97.76764581 90.92109525 97.22699753 96.48703192 97.69539328
 92.74235743 98.7791813  97.29925007 97.45870394 98.83648504 97.57081994
 98.6670653  95.8890799  98.84645091 98.32324289 99.81563146 97.64556394
 98.42539303 95.82430177 97.38395994 97.60071754 99.16785011 98.65709943
 99.81812293 99.16535865]
Accuracy th:0.7 is [87.69215437 97.37648554 92.78222089 98.31078556 98.88133144 97.60320901
 98.40047836 95.35092309 98.18372076 96.92303859 98.60477863 98.96853278
 99.35969305 95.18150335 97.41385754 97.2145402  96.30764631 97.73276528
 98.98597304 98.39549543 98.6296933  99.29740638 99.55153599 99.10805491
 95.46553056 96.67887485 94.46146947 97.15723647 97.86481302 98.22358422
 98.47522236 98.6894885  96.99778259 98.6147445  98.82402771 98.96604131
 97.78010315 98.16628049 99.05324264 93.13850064 97.99436929 92.86693076
 97.35904527 96.31013778 96.80095672 94.61344894 98.39798689 98.85641677
 98.05914742 98.7642325  98.90375464 98.64713357 98.88133144 98.46027356
 98.76672397 97.77262875 90.78406458 97.07003513 96.39235618 97.59822608
 92.72242569 98.71689464 97.28430127 97.55337967 98.79911304 97.61068341
 98.5848469  95.8218103  98.83399357 98.16378902 99.81563146 97.53843087
 98.49266263 95.93143484 97.38146847 97.59822608 99.22515385 98.65709943
 99.8206144  99.16785011]
Avg Prec: is [96.37794801 35.36167094 69.90255691 73.38205834 76.90666703 64.86657426
 80.82521854 47.68317783 62.72061759 55.10176598 37.77946055 56.65923411
 23.85577013 31.12426364 33.49102125 63.6042503  32.03339066 45.59086849
 50.34170946 38.50944683 69.582897   58.79046669 92.24983795 88.00354922
 25.13348693 37.13319562 32.86789234 47.13699668 27.90909337 38.45969629
 77.31665939 36.93945157 55.57106939 65.36351551 74.38321836 82.31368018
 60.76507327 77.12904735 89.5600864  44.84896015 38.75103776 47.65755712
 50.78370288 34.99306311 30.96641106 49.04157239 39.77755811 30.29297418
 42.02027596 42.73447513 70.18140888 37.48719514 25.77470063 77.17824747
 32.46824296 42.82791994 55.76657889 58.33357134 40.05466992 62.92430197
 63.29911002 86.38202489 66.934046   54.91570262 63.18282968 38.87109333
 62.55419589 27.98276444 43.23660338 66.55813921  9.34084198 74.52909157
 56.71305432 44.53054297 60.38887681 49.6385421  15.32600201 50.07850875
  2.03788588 23.55737934]
Accuracy th:0.5 is [45.35964322 97.22450607 68.43560804 96.96290206 97.90716795 73.6427735
 73.74492364 73.17437776 75.24727807 96.41976231 75.35191968 98.5325261
 99.34972718 77.41983706 75.38430874 96.31262924 96.21047911 74.73154446
 98.78167277 98.34068316 78.08755014 75.99720956 98.31327703 85.51959539
 78.83748163 96.52938685 94.3393876  74.85362633 97.81747515 75.32949647
 97.52597354 98.67204823 96.39983058 98.18870369 89.71522535 74.98567407
 74.9981314  92.83952463 97.0276802  72.32727907 76.44816503 92.37362035
 74.09871191 73.85703964 97.03764606 94.02795426 98.18621222 98.77668984
 97.95201435 91.15778459 89.18703441 98.55993223 98.87385704 74.06383138
 98.6969629  74.82123726 69.3076214  94.41164013 96.16314124 96.78102499
 90.13379176 97.04761193 95.31105962 74.92837033 98.32075143 75.76301168
 98.13139996 74.10369485 76.06447916 97.53593941 76.40830157 96.07843137
 75.94239729 95.44559882 74.01151058 85.31529511 92.63771582 75.44659541
 76.48304557 99.15040985]
Accuracy th:0.7 is [45.6312131  97.22450607 68.43560804 96.96290206 97.90716795 73.6427735
 73.74492364 73.4110671  75.24727807 96.41976231 75.35191968 98.5325261
 99.34972718 77.86082667 75.38430874 96.31262924 96.21047911 74.73154446
 98.78167277 98.34068316 78.38652615 75.99720956 98.31327703 86.30191594
 79.61481924 96.52938685 94.3393876  74.85362633 97.81747515 75.32949647
 97.52597354 98.67204823 96.39983058 98.18870369 89.89461096 74.98567407
 74.9981314  92.9989785  97.0276802  72.32727907 76.94396691 92.37362035
 74.09871191 73.85703964 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 91.5614022  89.56075442 98.55993223 98.87385704 74.06383138
 98.6969629  74.82871166 69.3076214  94.79532601 96.16314124 96.78102499
 90.13379176 97.04761193 95.35590602 74.93584473 98.32075143 75.76301168
 98.13139996 74.10369485 76.06447916 97.53593941 76.41079303 96.07843137
 76.06447916 95.44559882 74.01151058 85.58188205 93.0214017  75.44659541
 76.48304557 99.15040985]
Avg Prec: is [54.44405542  3.81418755 14.98211683  4.5852017   1.54318327  4.29892477
 10.37885504  8.78025608  7.07758282  5.18294421  2.27143478  5.31713765
  1.73038566  5.88325223  2.98367837  4.23969318 25.70272602  6.1070106
  1.5554089   2.84712099  3.68279068  1.42389906  1.41116935  5.45520106
  5.77359581 13.25371986  8.29890941  4.4604508   3.87121263  6.99297791
  2.31353744  0.87250449  3.08462437  1.10920447  1.69827441  2.40951349
  2.06465253  2.19958568  2.24485851  6.21018514  1.73208055  6.02743748
  2.20981788  2.73532549  2.36531016  4.87166821  1.79481713  1.05283621
  1.39594388  1.18130638  1.22178192  0.98788083  0.73739049  2.3507168
  0.86348932  1.84609212 10.08961401  2.96654922  3.81026744  2.7626821
  7.87629691  2.00044023  3.1836644   2.61511203  1.367338    1.85026881
  1.57022315  3.45017108  1.07491431  2.1953655   0.19241861  3.11464776
  1.56396969  3.90940558  3.53760863  2.31221444  0.58944125  1.53275799
  0.12831329  0.59320204]
mAP score regular 51.60, mAP score EMA 4.38
Train_data_mAP: current_mAP = 58.87, highest_mAP = 58.98
Val_data_mAP: current_mAP = 51.60, highest_mAP = 51.91
tensor([1.9209e-02, 1.1127e-03, 1.9764e-02, 1.2948e-03, 6.1362e-04, 1.6084e-03,
        3.0000e-04, 8.6980e-04, 3.0794e-03, 8.8610e-04, 2.4613e-04, 7.4968e-04,
        1.5219e-04, 2.2257e-03, 2.6417e-03, 2.7227e-03, 3.5704e-03, 4.9037e-03,
        2.6883e-03, 3.2692e-03, 5.0686e-05, 3.3785e-04, 7.9246e-04, 5.4201e-04,
        2.4395e-03, 5.2966e-04, 1.0888e-02, 4.4943e-04, 2.4966e-04, 9.3839e-04,
        9.0929e-04, 2.8289e-04, 1.0772e-02, 1.2633e-04, 9.7401e-04, 9.0269e-03,
        4.7773e-04, 1.2619e-03, 3.9419e-03, 8.8824e-03, 1.4830e-01, 5.4131e-01,
        8.9063e-01, 7.9112e-01, 9.8656e-01, 9.1652e-01, 4.1532e-04, 7.0003e-04,
        1.2351e-03, 1.1533e-03, 2.1150e-04, 5.5189e-04, 8.2656e-04, 1.5202e-03,
        4.3145e-04, 2.0544e-03, 8.5423e-02, 5.5731e-03, 6.4609e-02, 1.2099e-03,
        8.8992e-01, 1.5846e-03, 1.0132e-01, 8.4235e-03, 2.0742e-02, 3.4180e-03,
        2.9616e-02, 9.3218e-03, 4.9358e-01, 1.2543e-02, 4.3531e-04, 1.2391e-03,
        3.3583e-01, 5.4541e-02, 9.6589e-01, 9.9999e-01, 1.0000e+00, 9.9999e-01,
        5.0441e-01, 3.9098e-02], device='cuda:0')
Sum Train Loss:  tensor([7.9079e-01, 8.6842e-03, 3.5425e-01, 8.9826e-03, 1.8034e-03, 9.7544e-03,
        1.0883e-03, 1.2508e-02, 1.6041e-02, 7.4998e-03, 2.3766e-04, 6.5273e-03,
        5.5224e-05, 2.4569e-02, 1.5160e-02, 2.4007e-02, 7.0162e-02, 1.1243e-01,
        4.0558e-02, 4.9800e-02, 4.2895e-04, 4.2150e-04, 1.5603e-03, 3.4444e-03,
        3.9443e-02, 6.2515e-03, 2.4008e-01, 4.8873e-03, 7.6528e-04, 8.7294e-03,
        4.1560e-03, 1.5600e-03, 1.5222e-01, 8.4217e-04, 4.1281e-03, 5.0827e-02,
        3.6473e-03, 6.7574e-03, 3.9091e-02, 2.3700e-01, 2.1831e+00, 5.8542e+00,
        2.2195e+00, 2.5572e+00, 1.9661e+00, 3.4217e+00, 4.5425e-03, 4.0021e-03,
        4.5828e-03, 7.9119e-03, 1.1345e-03, 3.2861e-03, 4.8212e-04, 8.8765e-03,
        1.9273e-03, 1.6672e-02, 1.5986e+00, 7.1630e-02, 1.0568e+00, 7.1261e-03,
        3.3174e+00, 6.5065e-03, 4.2889e-01, 6.9183e-02, 1.2977e-01, 3.7634e-02,
        1.5164e-01, 1.2915e-01, 6.0326e-01, 1.0847e-01, 7.8056e-05, 1.7572e-02,
        2.6509e+00, 5.9471e-01, 5.2570e-01, 2.3989e+00, 1.5723e-01, 3.3207e-01,
        4.2650e-02, 4.8514e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [34/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 35.1
Sum Train Loss:  tensor([6.5302e-01, 1.3026e-02, 4.5635e-01, 5.6916e-03, 3.4010e-03, 4.4028e-03,
        1.4302e-03, 1.3785e-02, 2.7454e-02, 1.7121e-02, 2.5758e-03, 8.2791e-03,
        1.5126e-03, 3.8973e-02, 5.8476e-02, 4.3721e-02, 4.0109e-02, 8.5034e-02,
        8.0562e-03, 2.4216e-02, 4.4948e-04, 1.7753e-03, 4.8975e-03, 4.4943e-03,
        3.1308e-02, 6.1415e-03, 3.1474e-01, 4.8612e-03, 2.2895e-03, 2.8304e-03,
        4.7249e-03, 1.8350e-03, 1.0379e-01, 6.4608e-04, 5.2511e-03, 6.9797e-02,
        5.2835e-03, 7.2702e-03, 2.9021e-02, 1.5637e-01, 4.6215e-01, 6.6734e+00,
        2.3363e+00, 3.3592e+00, 1.8563e+00, 1.8884e+00, 3.1844e-03, 4.9145e-03,
        1.5703e-02, 7.9887e-03, 1.4166e-03, 2.3509e-03, 4.4335e-03, 1.1787e-02,
        3.0441e-03, 2.7213e-02, 1.7113e+00, 9.4755e-02, 1.0668e+00, 1.3425e-02,
        1.0681e+01, 7.4536e-03, 1.4886e+00, 5.7949e-02, 1.1176e-01, 2.2139e-02,
        5.3399e-02, 1.7698e-01, 5.2670e+00, 9.7511e-02, 7.7773e-05, 1.0805e-02,
        1.3844e+00, 7.9580e-01, 1.2008e+00, 7.2913e-01, 3.0639e-01, 4.2326e+00,
        7.6218e-02, 3.3221e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [34/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 48.8
Sum Train Loss:  tensor([6.4496e-01, 1.2644e-02, 3.7520e-01, 1.4635e-02, 1.4368e-03, 2.1707e-02,
        1.4898e-03, 8.5348e-03, 1.8736e-02, 1.0749e-02, 1.4766e-03, 4.9769e-03,
        8.1292e-05, 3.6867e-02, 3.4028e-02, 2.3536e-02, 3.4079e-02, 2.7740e-02,
        8.1457e-03, 2.3872e-02, 2.8080e-04, 1.3773e-03, 4.5241e-03, 2.6572e-03,
        2.7930e-02, 6.2136e-03, 1.6359e-01, 7.2919e-03, 2.0267e-03, 2.1812e-03,
        1.7388e-03, 5.8473e-04, 6.0135e-02, 7.3459e-04, 2.7900e-03, 2.4839e-02,
        2.8785e-03, 2.0487e-03, 4.5626e-03, 2.6235e-01, 6.7247e-01, 6.4239e+00,
        2.4061e+00, 2.3019e+00, 2.3764e+00, 5.9526e+00, 6.9425e-03, 1.8186e-03,
        1.0863e-02, 2.4290e-03, 2.9420e-03, 7.4014e-03, 3.9965e-03, 4.2527e-03,
        1.9829e-03, 2.5421e-02, 1.8173e+00, 2.7930e-02, 7.0529e-01, 1.0823e-02,
        5.0962e+00, 4.8258e-03, 1.2267e+00, 9.6238e-02, 7.4696e-02, 1.9480e-02,
        3.3167e-01, 1.3897e-01, 1.5578e+00, 7.7066e-02, 2.0159e-03, 1.4198e-02,
        7.3701e-01, 5.4971e-01, 2.4067e+00, 1.3597e+00, 3.2041e+00, 8.5962e-01,
        2.9758e+00, 4.6214e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [34/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 45.8
Sum Train Loss:  tensor([6.8158e-01, 1.2494e-02, 3.7739e-01, 2.6405e-02, 4.7986e-03, 9.2428e-03,
        2.6610e-03, 1.7868e-02, 3.8834e-02, 1.0899e-02, 1.4650e-03, 7.6366e-03,
        6.7960e-04, 4.3532e-02, 2.1509e-02, 2.4375e-02, 4.8087e-02, 5.8108e-02,
        1.3492e-02, 2.6111e-02, 3.5628e-04, 1.6990e-03, 2.5442e-03, 6.1483e-04,
        4.9301e-02, 7.5287e-03, 2.2146e-01, 7.5453e-03, 1.0182e-03, 4.8035e-03,
        9.1365e-03, 1.0382e-03, 1.4399e-01, 1.0800e-03, 7.0975e-03, 3.1859e-02,
        7.2937e-03, 1.5641e-02, 1.6565e-02, 2.1957e-01, 1.4415e+00, 5.0989e+00,
        6.0491e-01, 1.9409e+00, 3.1762e+00, 7.8521e+00, 1.7239e-03, 1.0646e-03,
        6.5044e-03, 4.3027e-03, 3.1775e-04, 1.9938e-03, 4.2809e-03, 4.0668e-03,
        2.5209e-03, 1.4991e-02, 2.1266e+00, 5.7656e-02, 5.6225e-01, 1.5367e-02,
        4.5283e+00, 3.5693e-03, 3.6221e-01, 6.1722e-02, 2.5084e-02, 8.8742e-03,
        8.0011e-02, 1.3662e-01, 2.4587e-01, 5.4640e-02, 1.3955e-04, 9.8261e-03,
        3.5039e-01, 7.6169e-01, 2.9543e+00, 2.0615e+00, 3.3707e+00, 7.8085e-01,
        1.0323e-01, 7.5815e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [34/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 41.1
Sum Train Loss:  tensor([5.8110e-01, 5.0114e-03, 4.8438e-01, 9.9160e-03, 5.7732e-03, 1.9154e-02,
        3.1876e-03, 1.0287e-02, 3.9287e-02, 7.4191e-03, 1.0070e-03, 2.0744e-03,
        8.6485e-04, 3.2030e-02, 3.6979e-02, 4.0058e-02, 5.9437e-02, 8.3229e-02,
        1.1956e-02, 2.1816e-02, 1.9355e-04, 2.0051e-04, 4.4660e-03, 4.5973e-04,
        4.8378e-02, 4.2235e-03, 2.0922e-01, 2.0559e-03, 1.1591e-03, 1.0888e-02,
        7.6193e-03, 1.9854e-03, 1.0461e-01, 2.9299e-04, 4.7371e-03, 3.6017e-02,
        2.3875e-03, 8.1035e-03, 1.1645e-02, 1.6057e-01, 6.8089e-01, 8.7401e+00,
        2.5387e+00, 4.7638e+00, 2.8388e+00, 1.0640e+01, 3.3443e-03, 1.4266e-03,
        2.0200e-02, 4.6548e-03, 1.2322e-03, 1.8647e-03, 8.4474e-04, 1.7571e-03,
        3.5438e-03, 1.8558e-02, 1.8063e+00, 8.2770e-02, 6.1800e-01, 5.5241e-03,
        1.1530e+01, 7.7545e-03, 4.7424e-01, 8.1272e-02, 6.6040e-02, 3.8872e-02,
        5.5460e-02, 1.2931e-01, 1.4782e+00, 3.2974e-02, 9.5188e-05, 9.0097e-03,
        1.2132e+00, 5.9965e-01, 6.1549e+00, 3.1496e+00, 7.8812e-02, 2.7988e-01,
        2.7342e-01, 1.7847e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [34/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 60.7
Sum Train Loss:  tensor([5.5012e-01, 1.0759e-02, 2.8332e-01, 8.7586e-03, 1.0487e-03, 8.7713e-03,
        2.2639e-03, 7.9525e-03, 2.2725e-02, 1.1570e-02, 2.1244e-03, 1.2550e-03,
        1.4695e-04, 6.2014e-02, 6.2015e-03, 2.8259e-02, 4.4020e-02, 7.2059e-02,
        2.8584e-03, 1.8637e-02, 4.1611e-04, 1.6859e-04, 5.8366e-03, 1.2618e-03,
        5.1021e-02, 8.0797e-03, 1.2115e-01, 3.5958e-03, 1.0101e-03, 1.5666e-02,
        4.8587e-03, 8.7790e-04, 1.2046e-01, 9.0339e-04, 2.9919e-03, 2.9236e-02,
        7.3792e-03, 1.5709e-02, 1.0194e-02, 2.5909e-01, 7.8095e-01, 7.2843e+00,
        4.3220e+00, 3.5497e+00, 4.1792e+00, 7.8515e+00, 2.3005e-03, 9.3169e-04,
        1.1742e-02, 4.5163e-03, 2.1094e-04, 4.8428e-04, 3.2690e-03, 1.4365e-02,
        1.6002e-03, 2.7118e-02, 1.6590e+00, 4.2406e-02, 9.5266e-01, 7.4034e-03,
        6.7362e+00, 6.8755e-03, 9.4174e-01, 3.2948e-02, 1.0867e-02, 1.6194e-02,
        1.6989e-01, 5.5764e-02, 1.9552e+00, 2.3342e-02, 1.8655e-03, 8.0648e-03,
        2.1721e-01, 5.7769e-01, 7.5226e-01, 1.9958e+00, 2.7770e+00, 7.9185e+00,
        2.0005e-01, 6.3504e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [34/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 57.0
Sum Train Loss:  tensor([6.4448e-01, 1.5982e-02, 3.8309e-01, 1.3561e-02, 1.0574e-03, 8.4478e-03,
        1.7280e-03, 9.6002e-03, 1.8654e-02, 5.9984e-03, 2.8921e-03, 4.4544e-03,
        7.7891e-05, 4.5316e-02, 2.8110e-02, 2.8841e-02, 3.8270e-02, 3.2076e-02,
        2.4650e-03, 2.3814e-02, 3.4705e-04, 3.6331e-03, 1.7266e-04, 2.3234e-03,
        4.2227e-02, 1.2609e-02, 3.8427e-01, 6.2579e-03, 2.4372e-03, 1.0920e-02,
        1.4976e-03, 1.4473e-03, 1.9415e-01, 7.3894e-04, 1.2167e-02, 6.4391e-02,
        9.3028e-03, 2.1632e-03, 1.6205e-02, 2.7007e-01, 1.2449e+00, 3.6664e+00,
        2.8343e+00, 4.2749e+00, 6.9341e-01, 9.9457e+00, 3.9359e-03, 3.7809e-03,
        2.3212e-03, 5.4820e-03, 1.9792e-03, 8.5319e-03, 6.6702e-04, 1.3189e-02,
        2.0401e-03, 5.5196e-03, 1.6712e+00, 6.2124e-02, 3.9405e-01, 5.6203e-03,
        4.2433e+00, 6.6556e-03, 5.3741e-01, 9.2503e-02, 6.8429e-02, 5.0124e-02,
        1.9465e-01, 1.4053e-01, 5.1589e-01, 8.3786e-02, 6.1677e-05, 9.1658e-03,
        4.9128e-01, 5.3163e-01, 1.2046e+00, 3.0164e-01, 1.1664e-01, 5.8638e+00,
        2.2558e-01, 6.8387e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [34/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 41.9
Sum_Val Meta Model:  tensor([9.3795e-01, 7.4633e-02, 1.6027e+00, 4.9172e-02, 6.4039e-04, 2.2110e-02,
        1.1393e-03, 1.5203e-02, 2.0340e-02, 8.9809e-03, 1.5777e-03, 9.2180e-03,
        1.1394e-03, 3.2143e-02, 2.2183e-02, 2.4073e-02, 1.9421e+00, 1.0410e-02,
        3.4172e-03, 4.6648e-03, 3.9634e-05, 1.4610e-04, 1.7611e-04, 2.9041e-04,
        6.6749e-02, 1.0168e-02, 3.1592e-01, 1.7697e-03, 2.6097e-03, 1.1944e-02,
        1.4948e-03, 2.4677e-04, 8.3939e-02, 1.2011e-04, 1.0560e-03, 1.0152e-02,
        1.9674e-03, 7.6907e-03, 7.3518e-03, 4.5623e-01, 1.5177e+00, 1.5104e+01,
        7.3950e+00, 1.8285e+01, 2.0444e+01, 2.1381e+01, 8.0286e-04, 4.4384e-03,
        6.2397e-04, 6.0837e-03, 6.2268e-05, 3.3655e-04, 6.0739e-03, 1.6096e-03,
        4.8797e-04, 4.7752e-03, 2.5460e+00, 3.8129e-02, 8.3050e-01, 2.6063e-03,
        1.3182e+01, 2.1686e-02, 4.9321e-01, 4.2939e-02, 1.4469e-02, 8.6978e-03,
        1.8907e-02, 6.7673e-02, 5.5295e+00, 2.4500e-01, 5.2958e-05, 2.2817e-02,
        6.1878e+00, 5.8058e-01, 2.9641e+01, 1.1431e+01, 1.3111e-01, 8.8652e+00,
        5.4688e-02, 1.5778e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([9.5895e-01, 5.0367e-02, 1.1042e+00, 3.0245e-02, 1.1026e-04, 2.1421e-02,
        9.1566e-04, 1.4723e-02, 1.9749e-02, 8.6364e-03, 1.9168e-03, 1.0408e-02,
        1.2691e-03, 3.1097e-02, 2.4917e-02, 1.0616e-01, 1.1845e+00, 2.0943e-02,
        2.0729e-03, 1.0253e-02, 4.0098e-05, 1.4036e-04, 6.3308e-05, 6.1594e-04,
        6.5111e-02, 9.2636e-03, 3.0628e-01, 2.7086e-03, 3.3901e-03, 1.2951e-02,
        6.6505e-04, 1.5958e-04, 7.9474e-02, 1.2755e-04, 6.3387e-04, 5.5137e-03,
        4.7441e-03, 5.8882e-03, 6.2801e-03, 4.0083e-01, 1.7609e+00, 2.0827e+01,
        6.8314e+00, 1.9811e+01, 2.0035e+01, 1.9400e+01, 5.2163e-04, 4.3627e-03,
        1.5198e-04, 5.0717e-03, 8.9890e-06, 9.9815e-05, 6.1309e-03, 2.6721e-04,
        2.5144e-04, 1.8937e-03, 2.2716e+00, 3.4561e-02, 8.0922e-01, 3.1719e-03,
        1.8960e+01, 1.0881e-02, 4.3480e-01, 6.5140e-02, 9.9227e-03, 1.0570e-02,
        1.1167e-02, 8.4426e-02, 6.0119e+00, 2.7905e-01, 4.3435e-05, 2.4221e-02,
        5.9168e+00, 4.9413e-01, 4.0189e+01, 2.2187e+01, 4.3967e-02, 8.6373e+00,
        2.0589e-01, 3.0766e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.9921e+01, 4.5264e+01, 5.5866e+01, 2.3358e+01, 1.7969e-01, 1.3318e+01,
        3.0522e+00, 1.6927e+01, 6.4133e+00, 9.7466e+00, 7.7880e+00, 1.3883e+01,
        8.3388e+00, 1.3972e+01, 9.4322e+00, 3.8991e+01, 3.3174e+02, 4.2709e+00,
        7.7109e-01, 3.1363e+00, 7.9111e-01, 4.1547e-01, 7.9888e-02, 1.1364e+00,
        2.6690e+01, 1.7490e+01, 2.8130e+01, 6.0267e+00, 1.3579e+01, 1.3801e+01,
        7.3140e-01, 5.6410e-01, 7.3776e+00, 1.0097e+00, 6.5078e-01, 6.1082e-01,
        9.9305e+00, 4.6660e+00, 1.5932e+00, 4.5126e+01, 1.1874e+01, 3.8475e+01,
        7.6703e+00, 2.5042e+01, 2.0308e+01, 2.1167e+01, 1.2560e+00, 6.2321e+00,
        1.2305e-01, 4.3974e+00, 4.2502e-02, 1.8086e-01, 7.4174e+00, 1.7578e-01,
        5.8278e-01, 9.2175e-01, 2.6593e+01, 6.2014e+00, 1.2525e+01, 2.6216e+00,
        2.1306e+01, 6.8668e+00, 4.2916e+00, 7.7331e+00, 4.7839e-01, 3.0924e+00,
        3.7706e-01, 9.0568e+00, 1.2180e+01, 2.2247e+01, 9.9780e-02, 1.9546e+01,
        1.7619e+01, 9.0597e+00, 4.1608e+01, 2.2187e+01, 4.3967e-02, 8.6374e+00,
        4.0819e-01, 7.8689e-01], device='cuda:0')
Outer loop valEpocw Maximum [34/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 169.9
model_train val_loss valEpocw [34/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 199.9
model_train val_loss valEpocw [34/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1238.2
Sum_Val Meta Model:  tensor([1.7337e+00, 2.8041e-02, 1.6189e+00, 1.8733e-02, 4.2423e-04, 9.4167e-02,
        2.7126e-02, 8.2114e-02, 1.1878e-01, 4.8976e-02, 7.6135e-03, 2.9339e-01,
        2.4801e-03, 1.5171e-02, 9.0249e-02, 1.1983e-01, 5.2878e-02, 1.2621e-01,
        4.4783e-02, 1.9234e-01, 4.6249e-06, 2.2805e-05, 2.8118e-04, 4.6839e-03,
        7.7932e-02, 1.6558e-02, 4.1976e-01, 1.0660e-02, 4.4978e-03, 7.7401e-05,
        1.3683e-04, 2.4845e-05, 1.4101e-03, 2.6015e-05, 7.0479e-05, 6.2863e-04,
        4.3833e-03, 2.3164e-04, 2.0387e-04, 1.5964e-02, 2.0038e-02, 1.9191e+00,
        9.2708e-02, 2.6731e-01, 3.2253e-01, 4.6202e+00, 7.9321e-04, 8.1582e-04,
        1.5521e-04, 1.1190e-02, 1.9193e-05, 1.0326e-04, 5.8315e-05, 6.8880e-05,
        1.0065e-04, 9.2607e-03, 1.3061e+00, 3.7358e-02, 4.9727e-01, 8.1189e-03,
        1.5886e+00, 1.4610e-03, 2.3715e-01, 2.2271e-02, 1.1396e-02, 2.8765e-03,
        1.8751e-02, 1.8175e-01, 5.0747e-02, 6.2456e-02, 1.6021e-05, 1.9083e-03,
        5.3427e+00, 3.4432e-01, 6.0080e+00, 4.2059e-01, 1.5072e-01, 8.0617e-01,
        1.6763e-02, 4.4455e-03], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.3802e+00, 3.1175e-02, 1.0683e+00, 2.6806e-02, 1.8657e-03, 7.7622e-02,
        2.9500e-02, 5.2945e-02, 9.4544e-02, 3.1581e-02, 6.4836e-03, 7.7386e-02,
        1.8303e-03, 2.3706e-02, 5.1092e-02, 1.3826e-02, 4.0158e-02, 1.0100e-01,
        1.3004e-02, 8.3645e-02, 5.8882e-05, 1.8584e-04, 1.0693e-04, 3.6236e-03,
        7.1748e-02, 1.4895e-02, 3.7487e-01, 9.3374e-03, 4.4452e-03, 1.4632e-03,
        2.8774e-04, 7.9817e-05, 5.3171e-03, 5.4484e-04, 4.0961e-04, 1.7641e-03,
        2.5309e-03, 2.0310e-03, 1.1833e-03, 1.5746e-02, 2.7496e-02, 6.5562e-01,
        1.4970e-02, 9.0625e-02, 1.6488e-01, 4.1981e-02, 5.7741e-04, 9.0126e-04,
        1.7816e-04, 9.8507e-03, 1.4180e-05, 1.4523e-04, 1.5145e-04, 5.5557e-04,
        1.8788e-04, 2.2679e-03, 7.6155e-01, 3.5103e-02, 3.1895e-01, 5.6904e-04,
        2.6923e+00, 8.0232e-04, 1.0447e-01, 6.3077e-03, 3.6243e-03, 1.2944e-03,
        2.7907e-03, 1.8842e-01, 6.0893e-02, 6.0073e-02, 5.0173e-06, 2.8381e-04,
        2.6024e+00, 1.3971e-01, 1.3307e+01, 1.3277e-02, 1.1558e-02, 2.5252e-02,
        2.6684e-03, 3.6182e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.5025e+01, 2.1771e+01, 4.6082e+01, 1.5239e+01, 2.2831e+00, 3.3827e+01,
        5.8445e+01, 4.0741e+01, 2.3081e+01, 2.6656e+01, 1.8807e+01, 6.9567e+01,
        7.8760e+00, 9.5025e+00, 1.3861e+01, 3.3264e+00, 8.5996e+00, 1.5581e+01,
        3.1655e+00, 1.6118e+01, 7.1335e-01, 3.7046e-01, 9.5798e-02, 4.6926e+00,
        2.4206e+01, 1.9319e+01, 2.9307e+01, 1.3893e+01, 1.1369e+01, 1.1438e+00,
        2.4136e-01, 1.9127e-01, 4.3550e-01, 2.8787e+00, 3.1675e-01, 1.7681e-01,
        3.6680e+00, 1.2043e+00, 2.6060e-01, 1.7331e+00, 2.2760e-01, 1.2963e+00,
        1.7313e-02, 1.2081e-01, 1.6805e-01, 4.6663e-02, 9.2920e-01, 9.1038e-01,
        1.0409e-01, 6.1203e+00, 4.6048e-02, 1.9810e-01, 1.3585e-01, 2.9441e-01,
        3.0289e-01, 9.2508e-01, 8.2169e+00, 4.7348e+00, 5.1310e+00, 3.3002e-01,
        3.0871e+00, 3.9600e-01, 9.6378e-01, 6.1624e-01, 1.6082e-01, 3.0342e-01,
        8.9461e-02, 1.9323e+01, 1.3767e-01, 4.5157e+00, 8.5420e-03, 1.7894e-01,
        9.0698e+00, 2.2451e+00, 1.3907e+01, 1.3277e-02, 1.1558e-02, 2.5252e-02,
        5.9604e-03, 9.2236e-02], device='cuda:0')
Outer loop valEpocw Maximum [34/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 29.7
model_train val_loss valEpocw [34/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 25.1
model_train val_loss valEpocw [34/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 701.2
Sum_Val Meta Model:  tensor([1.7104e+00, 4.6849e-03, 2.4256e-01, 4.6593e-03, 8.2990e-04, 5.6975e-03,
        8.2136e-04, 1.8963e-02, 1.0138e-02, 2.6999e-03, 5.6108e-04, 1.4519e-03,
        9.3636e-05, 1.0238e-01, 1.4338e-02, 1.6833e-02, 3.6170e-02, 3.7371e-02,
        4.4675e-03, 1.1231e-02, 6.0505e-05, 1.1147e-03, 1.8156e-02, 1.5281e-03,
        2.8262e-02, 3.0535e-03, 3.4693e-01, 7.1085e-03, 3.6414e-04, 8.2149e-03,
        5.8014e-03, 3.9916e-03, 1.3113e-01, 6.0221e-05, 4.2879e-03, 5.6923e-02,
        3.4536e-02, 2.8805e-02, 2.0939e-03, 3.1669e-01, 4.5893e+00, 3.5850e+01,
        6.0689e+01, 6.0280e+01, 5.1458e+01, 6.3546e+01, 1.4103e-02, 1.8136e-02,
        9.1984e-02, 3.5693e-02, 1.1732e-02, 3.9253e-02, 1.0958e-01, 3.5157e-02,
        6.5443e-02, 3.4664e-01, 1.8932e+01, 1.0276e-01, 5.4554e-01, 4.2114e-03,
        6.8535e+01, 3.9600e-03, 1.3739e+00, 1.7204e-01, 4.0237e-01, 4.4764e-03,
        3.5156e-01, 1.1942e-01, 2.4080e+00, 2.8353e-02, 1.8660e-04, 1.5635e-02,
        1.2220e+00, 2.4129e+00, 8.9006e-01, 2.5700e+01, 1.2158e+01, 6.9253e-01,
        5.8636e-02, 2.2465e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([6.5647e-01, 8.6791e-04, 1.7215e-01, 1.3613e-03, 1.3327e-05, 2.9172e-04,
        2.9082e-05, 2.3186e-02, 1.4592e-03, 1.0958e-04, 4.3485e-05, 5.7303e-05,
        7.1465e-06, 4.7591e-02, 2.7783e-03, 1.0109e-02, 7.7076e-03, 1.2452e-03,
        4.2788e-04, 3.5543e-04, 6.2754e-06, 2.1247e-05, 9.3555e-06, 3.0620e-05,
        1.5714e-02, 1.7082e-03, 3.1101e-01, 5.2348e-03, 2.3409e-04, 5.3701e-04,
        1.0873e-03, 4.5098e-03, 1.3826e-01, 3.4197e-05, 2.5547e-03, 1.5392e-02,
        9.9287e-03, 2.1060e-02, 6.5181e-04, 3.8206e-01, 3.8142e+00, 4.8874e+01,
        7.3089e+01, 6.6204e+01, 9.8760e+01, 8.2195e+01, 7.1618e-03, 9.8403e-03,
        8.3624e-02, 1.9146e-02, 7.4606e-03, 4.1392e-02, 1.0246e-01, 5.3440e-02,
        4.1068e-02, 1.8772e-01, 1.2342e+01, 1.1965e-01, 5.5753e-01, 1.6320e-03,
        1.2826e+02, 4.3344e-04, 1.0209e+00, 1.5079e-01, 3.2635e-01, 3.3592e-02,
        3.3502e-01, 1.6668e-01, 2.3910e+00, 4.6994e-02, 4.6058e-05, 1.4789e-02,
        2.3206e+00, 2.3615e+00, 1.7840e+00, 2.6338e+01, 2.2245e+01, 1.4046e+00,
        4.5261e-02, 2.0359e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.7308e+01, 5.4923e-01, 7.0780e+00, 7.1831e-01, 1.4569e-02, 1.1803e-01,
        6.0662e-02, 1.7478e+01, 3.4548e-01, 8.1229e-02, 1.0990e-01, 4.6515e-02,
        2.9326e-02, 1.6348e+01, 7.6342e-01, 2.6939e+00, 1.5462e+00, 1.6335e-01,
        1.0964e-01, 7.6017e-02, 7.1492e-02, 3.9549e-02, 6.7549e-03, 3.7267e-02,
        4.6233e+00, 2.0582e+00, 2.5386e+01, 7.5551e+00, 6.0364e-01, 3.3682e-01,
        7.7627e-01, 9.0169e+00, 9.6509e+00, 1.5626e-01, 1.7157e+00, 1.1699e+00,
        1.1945e+01, 1.0690e+01, 1.3025e-01, 3.8674e+01, 2.7162e+01, 8.9410e+01,
        8.2870e+01, 8.5067e+01, 1.0056e+02, 9.0684e+01, 9.9478e+00, 8.2797e+00,
        3.8942e+01, 9.9852e+00, 2.0040e+01, 4.5825e+01, 7.3699e+01, 2.5304e+01,
        5.7964e+01, 6.3614e+01, 1.3682e+02, 1.5815e+01, 9.2932e+00, 8.7767e-01,
        1.4480e+02, 1.9953e-01, 9.7781e+00, 1.4568e+01, 1.4350e+01, 7.4426e+00,
        1.0482e+01, 1.6542e+01, 5.5831e+00, 3.3696e+00, 6.9347e-02, 8.6140e+00,
        8.1485e+00, 3.9434e+01, 1.8700e+00, 2.6338e+01, 2.2245e+01, 1.4047e+00,
        1.0909e-01, 5.0206e-01], device='cuda:0')
Outer loop valEpocw Maximum [34/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 416.6
model_train val_loss valEpocw [34/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 577.6
model_train val_loss valEpocw [34/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1528.3
Sum_Val Meta Model:  tensor([4.2496e+00, 9.7247e-03, 9.5239e-01, 8.1237e-03, 2.0772e-03, 5.4886e-02,
        8.8048e-03, 4.9320e-02, 1.7570e-02, 6.3928e-02, 4.3272e-03, 1.4139e-02,
        2.5206e-04, 9.2773e-02, 9.6406e-02, 2.9057e-02, 2.8332e-02, 3.3242e-02,
        6.3539e-03, 9.8071e-03, 2.3664e-04, 1.5925e-03, 2.8500e-03, 3.3662e-03,
        1.4826e-01, 6.3214e-03, 6.4443e-01, 5.0520e-03, 1.7068e-02, 2.9014e-03,
        2.5394e-03, 6.0499e-04, 1.3425e-01, 1.0716e-02, 2.3183e-02, 1.7793e-01,
        1.6015e-03, 9.2584e-03, 8.1561e-02, 4.3755e-01, 2.9942e+00, 1.9788e+01,
        1.0425e+01, 9.6185e+00, 1.2298e+01, 1.6453e+01, 1.6303e-03, 2.5971e-03,
        7.3819e-03, 3.8771e-03, 9.8949e-04, 1.7699e-03, 1.9481e-03, 9.4283e-02,
        2.5913e-03, 2.0123e-02, 3.8965e+00, 7.3479e-02, 1.3668e+00, 1.5279e-02,
        4.6632e+01, 8.0151e-03, 9.0169e-01, 3.8566e-01, 4.6385e-01, 2.0984e-02,
        6.5502e-01, 8.5624e-01, 9.6544e-01, 9.9463e-02, 4.0637e-04, 1.1408e-02,
        2.0078e+00, 1.2622e+00, 3.8587e+02, 1.5902e+01, 2.3250e+00, 8.4356e+00,
        7.6907e-02, 1.1263e-01], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.2328e+00, 5.7319e-03, 5.9062e-01, 1.0910e-02, 9.6577e-04, 4.9025e-02,
        1.0495e-02, 4.5265e-02, 2.2846e-02, 4.8503e-02, 3.5333e-03, 2.1838e-02,
        7.6712e-04, 8.5992e-02, 1.2437e-01, 4.3628e-03, 3.9682e-03, 7.3074e-03,
        5.1338e-04, 1.2495e-03, 6.7101e-05, 8.6688e-05, 1.0240e-04, 1.7891e-03,
        1.3709e-01, 6.9245e-03, 5.2608e-01, 3.3135e-03, 2.1855e-02, 4.1914e-04,
        4.8299e-04, 1.7197e-04, 2.5760e-03, 1.0417e-04, 3.7288e-04, 2.2848e-03,
        1.6330e-03, 1.1221e-03, 1.1432e-03, 5.2281e-02, 1.2279e-01, 1.7674e+00,
        9.9731e-01, 2.4699e+00, 7.9182e-02, 1.0177e+00, 4.1481e-04, 6.7712e-04,
        1.6606e-04, 5.8085e-04, 1.7722e-05, 8.8952e-05, 8.4314e-05, 8.6580e-04,
        3.8522e-04, 4.6215e-03, 3.3913e-01, 6.5456e-03, 9.6491e-01, 1.2195e-03,
        1.2466e+01, 1.1075e-03, 9.6748e-02, 1.1809e-02, 8.4713e-03, 7.8216e-03,
        1.2037e-02, 1.1972e-01, 1.7084e-02, 1.0589e-02, 2.8078e-05, 1.4328e-03,
        3.4543e-01, 3.9824e-01, 5.2877e+01, 3.5756e-01, 8.5676e-03, 9.3636e+00,
        6.9074e-03, 1.6030e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.2182e+01, 1.7932e+00, 1.7065e+01, 2.9018e+00, 4.6150e-01, 1.0253e+01,
        8.7951e+00, 1.5895e+01, 3.0763e+00, 1.6892e+01, 3.6152e+00, 8.0698e+00,
        1.1639e+00, 1.5535e+01, 1.8810e+01, 6.7133e-01, 4.8388e-01, 5.9117e-01,
        7.1644e-02, 1.5409e-01, 2.3948e-01, 6.9511e-02, 3.9872e-02, 1.0187e+00,
        2.3689e+01, 3.5983e+00, 2.5422e+01, 1.9474e+00, 2.1106e+01, 1.3763e-01,
        1.7281e-01, 1.4908e-01, 1.0984e-01, 1.6487e-01, 1.2162e-01, 1.0471e-01,
        9.3712e-01, 3.0090e-01, 1.0870e-01, 2.8828e+00, 6.4503e-01, 3.3107e+00,
        1.1856e+00, 3.4009e+00, 8.1568e-02, 1.1587e+00, 2.7631e-01, 2.8646e-01,
        4.1288e-02, 1.6181e-01, 2.0864e-02, 5.1002e-02, 3.1276e-02, 1.8768e-01,
        2.5007e-01, 9.0749e-01, 2.6638e+00, 4.6721e-01, 1.2259e+01, 3.1604e-01,
        1.4474e+01, 2.4313e-01, 7.0682e-01, 5.9044e-01, 2.0567e-01, 8.6857e-01,
        2.1814e-01, 5.6507e+00, 3.8802e-02, 4.4185e-01, 1.7624e-02, 3.8431e-01,
        1.0230e+00, 4.5362e+00, 5.6777e+01, 3.5760e-01, 8.5676e-03, 9.3644e+00,
        1.8235e-02, 2.6021e-02], device='cuda:0')
Outer loop valEpocw Maximum [34/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 551.5
model_train val_loss valEpocw [34/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 86.9
model_train val_loss valEpocw [34/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 364.5
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.71579294 97.42693193 93.35778073 98.02999476 98.60747311 97.63404442
 98.31995224 95.20473678 98.08969189 96.9907774  98.64280406 98.92301507
 99.43714136 95.40697604 97.53536141 97.46835443 96.45350325 97.81800904
 98.9132686  98.42472679 98.67935332 99.372571   99.51633143 99.10697969
 95.23275789 96.90549579 94.41892764 97.04316468 98.06654402 98.32482548
 98.43690988 98.60747311 97.06631254 98.6062548  98.72808567 98.8292053
 97.7424739  98.44421973 99.03631778 93.42722433 98.2310157  96.93717182
 99.00951499 98.5087901  99.12890925 98.39183246 98.26878328 98.67813501
 98.11527637 98.77559971 98.83773346 98.68788148 99.03388117 98.4393465
 98.7463603  97.73150912 93.14701332 97.02610836 96.74833396 97.64622751
 97.97760749 98.68544487 97.77780485 97.39891083 98.88037426 97.59749516
 98.7183392  95.9734896  99.33967666 98.36015643 99.81603538 97.506122
 99.20931763 96.14161621 99.07408535 99.16180358 99.7636481  99.55531731
 99.88060574 99.19347961]
Accuracy th:0.7 is [86.84835711 97.34043201 92.84852767 98.05436094 98.47467745 97.45617134
 98.11405806 94.98300459 97.9800441  96.77148183 98.60138156 98.85478978
 99.42495827 95.32900428 97.50977693 97.32215738 96.35116531 97.68399508
 98.84626162 98.40888878 98.50148025 99.33602173 99.48587371 99.02657131
 95.23641281 96.82143249 94.36532206 97.0321999  98.02999476 98.24563541
 98.29680438 98.579452   96.66548897 98.47589576 98.60990972 98.69762795
 97.56338251 98.36137474 98.90717706 93.09462604 98.17741012 96.96884785
 99.01926146 98.61356465 99.07164874 98.3699029  98.20664953 98.66960685
 98.0360863  98.76585324 98.71346597 98.62818435 99.01560654 98.45031128
 98.72077582 97.70348802 92.21257051 96.77635506 96.57533412 97.45738965
 97.67790353 98.52462811 97.49028399 97.44642487 98.81824052 97.58043883
 98.61600127 95.97714453 99.21662748 98.15791718 99.81603538 97.09677026
 99.22515564 95.90160939 99.10697969 98.98027558 99.75999318 99.49562018
 99.86720435 99.16545851]
Avg Prec: is [96.42185192 35.81775616 72.66577709 67.22208857 78.69805033 64.73944902
 74.43472311 47.36311378 58.52464425 53.30838268 33.14586483 54.69512693
 26.6302362  28.82240331 34.71969094 59.10973214 29.59952257 45.10039875
 50.64068319 40.55802523 62.22790266 54.1163967  89.1131852  84.00936433
 26.32272852 34.87031521 38.33946963 40.29815863 25.28995916 40.14311743
 73.80909387 38.41948367 57.89407288 63.44909062 72.89383173 79.29514688
 60.07562329 74.7904969  87.68525107 47.1121049  52.30005552 88.9357462
 92.09466919 89.31263664 93.11074554 94.12059752 42.35794765 35.03314579
 43.57339851 47.93001265 64.92705648 40.88077443 27.33230642 74.23075591
 30.04949917 45.2710269  75.14534013 59.84361212 48.1273687  61.4487933
 96.52928023 83.60958884 76.3599693  52.34500599 61.73672771 44.22169561
 62.60120234 26.50647925 82.57880767 69.12842396 12.07250302 73.941276
 87.90694724 52.97576039 94.22911654 95.22220892 91.25217902 93.06245232
 55.77978902 31.08063722]
Accuracy th:0.5 is [45.69510605 97.2137279  70.43408341 97.02489005 97.26733349 75.03563553
 75.29757191 75.24640294 76.18693729 96.46690464 76.51466235 98.52097319
 99.41399349 80.06481403 76.12845847 96.56680596 96.29512311 75.84824746
 98.65376884 98.30776915 80.18542659 76.96665489 98.38695922 84.44341565
 84.492148   96.65086926 94.0778012  75.57169138 98.01293844 76.31242309
 97.30875598 98.57457877 96.36213009 98.02024829 88.57713722 76.04561348
 75.94693047 91.7508315  97.11504489 73.2337569  78.80264617 92.05906361
 75.28417051 74.89431172 96.9627563  93.87434364 98.02877645 98.57336046
 97.88136109 90.91629001 90.01230492 98.55508583 98.99976852 75.43645911
 98.70615611 75.77758556 70.97013925 93.50519609 96.24273583 96.9067141
 89.79300934 97.17717864 95.1511312  75.88966996 98.42838172 76.37455684
 98.20786784 75.13919177 76.8850282  97.55972759 77.39184464 95.99054592
 76.89477467 95.45083515 75.22081846 84.05843009 90.96745897 76.4111061
 77.43692206 99.14718388]
Accuracy th:0.7 is [45.74749333 97.2137279  70.43408341 97.02489005 97.26733349 75.03929046
 75.29757191 75.61189557 76.18693729 96.4754328  76.51466235 98.52097319
 99.41399349 80.57772201 76.12845847 96.56680596 96.29512311 75.84824746
 98.65376884 98.30776915 80.59599664 76.96665489 98.38695922 85.56791462
 85.51552734 96.65086926 94.0778012  75.57169138 98.01293844 76.31242309
 97.30875598 98.57457877 96.36213009 98.02024829 88.80617926 76.04561348
 75.94693047 92.01154957 97.11504489 73.2337569  79.4483498  92.05906361
 75.28417051 74.89431172 96.9627563  93.87434364 98.02877645 98.57336046
 97.98491734 91.45721909 90.29617086 98.55508583 98.99976852 75.43645911
 98.70615611 75.78245879 70.97013925 93.99130128 96.24273583 96.9067141
 89.79300934 97.17717864 95.24006774 75.89088827 98.42838172 76.37455684
 98.20786784 75.13919177 76.8850282  97.55972759 77.46128824 95.99054592
 77.15183782 95.45083515 75.22081846 84.27285243 91.43407122 76.4111061
 77.43692206 99.14718388]
Avg Prec: is [55.89829523  3.18574251 11.11498597  3.34164712  2.2996323   3.70189804
  3.24407716  5.64891522  2.43866623  3.72326684  1.63959519  1.60714229
  0.58757367  5.14359677  2.64859541  3.08285399  3.67440922  2.63618445
  1.32479574  1.76482727  1.9582035   0.83128508  1.78334087  2.55510311
  5.15160279  3.47012729  6.4195501   3.34107042  2.17652661  1.97859862
  2.64039308  1.29099135  3.7822835   1.76019117  2.4128815   2.50940828
  2.95606801  2.49940255  2.89083014  7.5001841   2.34377841  8.26565345
  3.33839358  4.10998893  3.21467773  6.4358094   2.21451751  1.55519989
  2.04566754  1.63936662  1.89080136  1.64724577  1.06683923  3.07676034
  1.35045141  2.69944134 11.19891427  3.53637393  3.91887095  2.75853855
 10.72154237  2.19858225  3.78536339  2.92047968  1.56442469  2.5047076
  1.75424189  4.16323004  1.24667845  2.36115971  0.21314668  3.45726615
  1.8991175   4.49950922  3.89166796  3.05846762  0.86075571  1.89498167
  0.12993192  0.80506687]
mAP score regular 59.39, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [87.6373421  97.44624661 93.00146997 98.07907915 98.93365224 97.68044448
 98.50511996 95.39576949 98.14884022 97.080001   98.72187757 98.99095598
 99.39457359 95.26372175 97.43877221 97.3715026  96.41976231 97.86979595
 98.99593891 98.36061489 98.91870344 99.33726985 99.63126292 99.28744052
 95.36587189 96.90808979 94.29204973 97.2369634  97.91215088 98.19866956
 98.7268605  98.66955677 97.03764606 98.7717069  98.92368637 99.12051225
 97.97942048 98.31825996 99.08314024 93.30293744 98.00931809 91.74327927
 97.3341306  95.89406283 96.81590552 94.39419987 98.39798689 98.70692877
 98.09402795 98.6894885  98.91621197 98.66955677 98.91372051 98.48269676
 98.76174104 97.59573461 91.16027605 97.2070658  96.48952338 97.67795301
 92.8494905  98.80409597 97.3490794  97.2593866  98.77668984 97.29925007
 98.60976157 95.80187857 98.84395944 98.22358422 99.81563146 97.58826021
 98.33071729 95.84672497 97.21204873 97.59324314 99.12549518 98.6820141
 99.81064853 99.18529038]
Accuracy th:0.7 is [87.63485064 97.40638314 93.00396143 98.33071729 98.88880584 97.55088821
 98.35812343 95.24379002 98.13887436 96.85327752 98.67703117 98.98348158
 99.37215038 95.13167402 97.46866981 97.341605   96.33505245 97.78757755
 98.95607544 98.41044423 98.78914717 99.35221865 99.58890799 99.24010265
 95.47798789 96.80593966 94.5112988  97.2519122  97.85235568 98.19617809
 98.6072701  98.6820141  96.74365299 98.64962503 98.86638264 99.05822558
 97.92460822 98.26095622 99.00839624 93.1559409  98.04419862 92.51314249
 97.43877221 96.25034258 96.90310686 94.66826121 98.39549543 98.82153624
 98.01180955 98.76174104 98.77419837 98.66457383 98.90624611 98.50511996
 98.7567581  97.82993248 90.86877445 97.1323218  96.42972818 97.53843087
 92.9242345  98.67703117 97.266861   97.49856741 98.79662157 97.59324314
 98.55744077 95.79440417 98.83399357 98.05914742 99.81563146 97.30921594
 98.44781623 95.83925057 97.36901114 97.58826021 99.17781598 98.66955677
 99.81812293 99.16286718]
Avg Prec: is [96.36785659 34.33436083 69.99486969 73.74068565 77.66475866 65.11502003
 80.71153603 47.96413736 62.78975505 55.10944399 39.37164329 56.04460909
 24.44506478 30.94630322 35.04789072 63.2907722  32.57482451 47.32461377
 51.08380869 37.63918093 70.06159952 58.65316496 92.24744279 87.74407425
 25.69044373 38.10565858 33.28431885 46.89911723 29.05524249 37.76306431
 77.59534111 39.28488279 56.0957545  65.16655119 74.24008146 82.09468247
 60.72249609 77.15293938 89.29286717 44.99867465 38.6681812  45.53464415
 52.30281523 38.28388127 29.0310073  51.96748275 40.53908072 29.68071004
 42.95082977 43.39301777 69.66523975 37.78114954 26.38748019 77.85333092
 32.09993302 43.89962314 56.69943674 58.0977353  39.87242996 63.72733844
 63.66486049 86.84477052 67.25890452 53.961814   61.46745703 39.10603362
 60.52115328 26.31537204 43.87717674 66.03698739  9.44690438 74.30805574
 55.70920633 44.52389429 65.11429639 49.41549252 16.35129896 50.18504672
  2.27256113 23.93405319]
Accuracy th:0.5 is [45.37210056 97.22450607 68.25373097 96.96290206 97.90716795 73.45093056
 73.5680295  73.02239829 75.09031567 96.41976231 75.1550938  98.5325261
 99.34972718 77.33761866 75.19744874 96.31262924 96.21047911 74.54468446
 98.78167277 98.34068316 78.00533174 75.80038369 98.31327703 85.92819593
 79.0492563  96.52938685 94.3393876  74.66178339 97.81747515 75.13765354
 97.52597354 98.67204823 96.39983058 98.18870369 89.72519122 74.79881406
 74.8112714  92.85447343 97.0276802  72.15536787 76.33604903 92.37362035
 73.90686897 73.67017963 97.03764606 94.02795426 98.18621222 98.77668984
 97.95201435 91.32471286 89.81737549 98.55993223 98.87385704 73.87697137
 98.6969629  74.62441139 69.15065899 94.40914867 96.16314124 96.78102499
 90.13379176 97.04761193 95.40324389 74.75396766 98.32075143 75.58611755
 98.13139996 73.91185191 75.87263622 97.53593941 76.2114757  96.07843137
 75.76301168 95.44559882 73.82963351 85.44485138 92.87440516 75.25475247
 76.2862197  99.15040985]
Accuracy th:0.7 is [45.65861923 97.22450607 68.25373097 96.96290206 97.90716795 73.45342203
 73.5680295  73.26157909 75.09031567 96.41976231 75.1550938  98.5325261
 99.34972718 77.78109973 75.2074146  96.31262924 96.21047911 74.54468446
 98.78167277 98.34068316 78.28686748 75.80038369 98.31327703 86.76034582
 79.80417072 96.52938685 94.3393876  74.66178339 97.81747515 75.13765354
 97.52597354 98.67204823 96.39983058 98.18870369 89.89959389 74.79881406
 74.8112714  93.0214017  97.0276802  72.15536787 76.80444478 92.37362035
 73.90686897 73.67017963 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 91.72085607 90.24092483 98.55993223 98.87385704 73.87697137
 98.6969629  74.63437726 69.15065899 94.79532601 96.16314124 96.78102499
 90.13379176 97.04761193 95.45058176 74.76393353 98.32075143 75.58611755
 98.13139996 73.91185191 75.87263622 97.53593941 76.21396716 96.07843137
 75.89505942 95.44559882 73.82963351 85.69150659 93.2605825  75.25475247
 76.2862197  99.15040985]
Avg Prec: is [54.42079449  3.75833002 14.98748288  4.57010217  1.5422403   4.29150275
 10.11459375  8.76051217  6.87714967  5.19002983  2.28106978  5.35597414
  1.55468987  5.88802704  2.99176175  4.22990785 25.32522489  5.97183983
  1.56001111  2.99866211  3.68906201  1.4162089   1.52591357  5.39981243
  5.77797307 13.70293786  8.311167    4.49349402  3.89759353  6.99591486
  2.30065438  0.8661605   3.14124642  1.09260658  1.7064084   2.41406975
  2.04193533  2.20612583  2.26028848  6.22243635  1.73637047  6.04994572
  2.23155949  2.74341462  2.37874052  4.86000821  1.7693742   1.04161614
  1.40222723  1.18025892  1.19749783  0.98062366  0.73976478  2.34200566
  0.8667712   1.84327928 10.14774179  2.99709426  3.80491012  2.76350721
  7.90839334  2.02786122  3.21325982  2.61837938  1.36466815  1.86548884
  1.57901232  3.48051406  1.06779491  2.1927805   0.19311802  3.14059454
  1.56541639  3.93372192  3.51871332  2.30183533  0.59086325  1.491786
  0.12765028  0.58056457]
mAP score regular 51.83, mAP score EMA 4.37
Train_data_mAP: current_mAP = 59.39, highest_mAP = 59.39
Val_data_mAP: current_mAP = 51.83, highest_mAP = 51.91
tensor([1.8789e-02, 1.1092e-03, 2.0028e-02, 1.2758e-03, 6.2158e-04, 1.6599e-03,
        2.9836e-04, 8.9379e-04, 3.0856e-03, 9.0527e-04, 2.5384e-04, 8.2711e-04,
        1.5902e-04, 2.1936e-03, 2.6439e-03, 2.7356e-03, 3.6812e-03, 5.1189e-03,
        2.7393e-03, 3.3161e-03, 5.3818e-05, 3.3673e-04, 8.4180e-04, 5.4833e-04,
        2.6234e-03, 5.4598e-04, 1.0934e-02, 4.5605e-04, 2.5225e-04, 9.7487e-04,
        9.2472e-04, 2.9724e-04, 1.0632e-02, 1.3181e-04, 9.7257e-04, 1.0421e-02,
        4.8224e-04, 1.3116e-03, 3.8190e-03, 9.0272e-03, 1.5787e-01, 5.4021e-01,
        9.0208e-01, 8.0365e-01, 9.8818e-01, 9.2724e-01, 4.3798e-04, 7.5663e-04,
        1.3138e-03, 1.2323e-03, 2.3138e-04, 5.8181e-04, 8.7874e-04, 1.4401e-03,
        4.4608e-04, 2.0096e-03, 8.5572e-02, 5.8226e-03, 6.2238e-02, 1.2379e-03,
        9.0075e-01, 1.5245e-03, 1.0230e-01, 8.1372e-03, 2.1062e-02, 3.3972e-03,
        3.0261e-02, 1.0220e-02, 5.0543e-01, 1.2796e-02, 4.4860e-04, 1.2022e-03,
        3.6856e-01, 5.6527e-02, 9.6643e-01, 9.9999e-01, 1.0000e+00, 9.9999e-01,
        4.7350e-01, 3.8898e-02], device='cuda:0')
Sum Train Loss:  tensor([5.8369e-01, 1.6891e-02, 2.8015e-01, 9.6871e-03, 4.4531e-03, 2.3613e-02,
        2.4981e-03, 1.5338e-02, 2.7320e-02, 7.6130e-03, 1.6002e-03, 8.0033e-04,
        7.3453e-05, 6.0709e-02, 4.8949e-02, 3.2578e-02, 4.9017e-02, 5.2527e-02,
        5.3318e-03, 1.9344e-02, 3.2897e-04, 3.3993e-04, 3.5911e-03, 1.4579e-03,
        5.7906e-02, 1.2608e-02, 2.6805e-01, 3.7062e-03, 2.7352e-03, 2.7083e-03,
        9.0797e-03, 1.4774e-03, 1.1168e-01, 8.8132e-04, 7.9455e-03, 3.4594e-02,
        5.5556e-03, 9.5511e-03, 1.4930e-02, 1.1118e-01, 6.7848e-01, 8.6039e+00,
        3.1977e+00, 5.0153e+00, 7.4600e+00, 8.1679e+00, 3.2144e-03, 8.1167e-03,
        4.7053e-03, 9.4573e-03, 2.2763e-04, 1.2538e-03, 7.7557e-04, 3.7933e-03,
        1.1800e-03, 1.3886e-02, 2.4270e+00, 6.2350e-02, 6.8732e-01, 1.9830e-02,
        4.5483e+00, 1.9486e-02, 4.3263e-01, 1.0487e-01, 3.2275e-02, 5.5580e-02,
        4.0616e-02, 1.0218e-01, 5.3266e+00, 6.0094e-02, 2.1699e-03, 1.4512e-02,
        2.2120e-01, 8.6847e-01, 3.5791e+00, 4.6926e+00, 1.4006e+00, 7.0809e-01,
        1.2834e-01, 9.1943e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [35/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 60.7
Sum Train Loss:  tensor([5.1865e-01, 1.9551e-02, 3.7963e-01, 1.1691e-02, 3.7642e-03, 1.3938e-02,
        1.1325e-03, 1.5677e-02, 1.9073e-02, 5.3708e-03, 1.7108e-03, 9.1900e-04,
        1.3130e-04, 1.8095e-02, 2.2392e-02, 3.7015e-02, 5.7332e-02, 3.8006e-02,
        2.0869e-03, 5.0976e-02, 1.3705e-04, 1.9115e-03, 4.0826e-04, 4.8618e-04,
        3.7185e-02, 5.7446e-03, 1.9712e-01, 8.7430e-03, 2.9038e-03, 6.7697e-03,
        3.1828e-03, 2.6780e-03, 6.5484e-02, 3.2374e-04, 3.3645e-03, 3.6496e-02,
        2.9479e-03, 7.4593e-03, 9.7045e-03, 1.5078e-01, 5.0943e-01, 2.3592e+00,
        3.2535e+00, 1.0064e+01, 1.2617e+00, 2.8359e+00, 3.7198e-03, 1.1842e-02,
        4.8040e-03, 2.6711e-03, 9.0729e-04, 2.6624e-03, 5.3687e-03, 5.8774e-03,
        2.9301e-03, 1.5882e-02, 1.3364e+00, 7.4568e-02, 5.7737e-01, 8.9134e-03,
        3.4927e+00, 4.7809e-03, 6.5804e-01, 5.4821e-02, 7.6185e-02, 2.8844e-02,
        4.7310e-02, 2.0645e-01, 5.9509e+00, 9.6944e-02, 1.4689e-04, 9.7855e-03,
        1.6024e+00, 6.5581e-01, 2.4787e+00, 1.4690e+00, 1.9768e+00, 4.7086e-01,
        2.3406e+00, 3.0486e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [35/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 45.7
Sum Train Loss:  tensor([7.2515e-01, 5.6421e-03, 2.8261e-01, 9.7244e-03, 6.4653e-03, 1.6363e-02,
        1.5158e-03, 8.3941e-03, 1.0844e-02, 5.1392e-03, 3.0492e-04, 7.7554e-04,
        5.5322e-05, 4.0324e-02, 2.0134e-02, 3.0432e-02, 1.1547e-01, 4.5514e-02,
        6.6699e-03, 1.6981e-02, 4.7101e-04, 1.2001e-03, 1.4780e-03, 1.1836e-03,
        4.3665e-02, 8.2630e-03, 2.6220e-01, 2.8120e-03, 1.9878e-03, 9.6551e-03,
        1.8939e-03, 6.5780e-04, 9.5785e-02, 9.2738e-04, 9.5808e-04, 1.7603e-02,
        3.3116e-03, 1.0007e-02, 4.9122e-03, 2.4460e-01, 3.5933e-01, 6.0757e+00,
        1.0706e+00, 2.2851e+00, 7.2674e+00, 5.5464e+00, 7.0791e-03, 7.3725e-03,
        1.4416e-02, 9.4125e-03, 2.3672e-03, 6.4534e-03, 4.7311e-03, 1.7126e-02,
        7.3167e-03, 2.8126e-02, 1.7459e+00, 5.4883e-02, 4.6328e-01, 2.4862e-02,
        1.0095e+01, 4.2088e-03, 1.7806e+00, 1.7042e-01, 1.9048e-01, 3.5334e-02,
        3.5863e-01, 1.2474e-01, 8.5253e-01, 1.3015e-01, 2.0899e-04, 1.2424e-02,
        6.0713e-01, 8.1091e-01, 2.2409e+00, 8.1372e+00, 6.5093e-01, 1.2977e+01,
        3.4564e-02, 1.9071e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [35/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 66.5
Sum Train Loss:  tensor([6.1247e-01, 8.4587e-03, 2.8441e-01, 4.4052e-03, 4.7272e-03, 1.2829e-02,
        1.8845e-03, 1.2606e-02, 9.1713e-03, 1.0639e-02, 1.2879e-03, 3.7048e-03,
        1.0879e-04, 5.0177e-02, 1.3726e-02, 2.8828e-02, 8.2492e-02, 2.6682e-02,
        2.2544e-02, 5.0563e-03, 1.1552e-04, 7.1839e-04, 2.8014e-04, 8.2813e-04,
        4.0603e-02, 7.3577e-03, 2.2441e-01, 5.8560e-03, 2.3799e-03, 9.5063e-03,
        4.6695e-03, 1.9038e-03, 1.3856e-01, 9.6345e-04, 7.6670e-03, 8.8473e-02,
        3.2011e-03, 1.9217e-02, 1.4177e-02, 2.2634e-01, 1.1888e+00, 7.2476e+00,
        2.7751e+00, 4.3956e+00, 3.3941e+00, 3.1644e+00, 5.1371e-03, 2.3609e-03,
        8.8728e-03, 9.2897e-03, 1.4176e-03, 2.1688e-03, 4.8626e-03, 5.4851e-03,
        2.9375e-03, 8.0339e-03, 2.0670e+00, 5.8673e-02, 7.5847e-01, 1.3211e-02,
        4.4014e+00, 8.1785e-03, 6.4063e-01, 6.6297e-02, 9.2655e-02, 2.6581e-02,
        2.8468e-01, 9.2157e-02, 1.3242e+00, 5.0961e-02, 3.1359e-04, 1.0170e-02,
        9.0661e-01, 5.9581e-01, 9.9193e-01, 1.3351e+00, 8.9757e-01, 3.7691e+00,
        6.3560e-02, 5.1041e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [35/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 42.7
Sum Train Loss:  tensor([6.1402e-01, 1.4003e-02, 3.9135e-01, 9.5081e-03, 2.6892e-03, 1.0932e-02,
        7.8164e-04, 1.6288e-02, 2.6312e-02, 7.4034e-03, 1.1659e-03, 1.0827e-03,
        8.2876e-04, 3.9801e-02, 3.8609e-02, 2.8021e-02, 5.9825e-02, 2.9444e-02,
        6.9812e-03, 3.5330e-02, 3.8023e-04, 3.7796e-04, 1.9291e-03, 2.6763e-03,
        4.5886e-02, 1.1272e-02, 2.9843e-01, 2.3515e-03, 1.3610e-03, 3.0437e-03,
        6.3107e-03, 1.4821e-03, 1.0291e-01, 8.4646e-04, 6.5629e-03, 2.8650e-02,
        3.7281e-03, 3.2518e-03, 2.2260e-03, 2.2564e-01, 1.7475e+00, 5.7604e+00,
        3.3898e+00, 2.6402e+00, 7.3028e+00, 9.9379e+00, 3.0072e-03, 7.6037e-03,
        9.1955e-03, 6.6099e-03, 1.4694e-03, 3.0209e-03, 5.3479e-03, 1.8003e-02,
        1.5413e-03, 4.4391e-03, 2.5081e+00, 7.1096e-02, 1.1191e+00, 7.8260e-03,
        7.1272e+00, 6.7378e-03, 2.1063e+00, 4.5264e-02, 2.6628e-01, 5.1727e-02,
        2.4697e-01, 1.7294e-01, 7.7856e-01, 1.0815e-01, 9.9145e-05, 1.6038e-02,
        1.2355e+00, 1.1749e+00, 3.1947e+00, 2.3497e+00, 5.4921e-01, 1.8666e-01,
        2.2419e-01, 4.0698e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [35/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 56.5
Sum Train Loss:  tensor([6.1182e-01, 4.8294e-03, 6.4345e-01, 6.2763e-03, 2.6411e-03, 1.6018e-02,
        1.2346e-03, 2.3816e-02, 6.1783e-02, 8.0006e-03, 1.2998e-03, 3.7567e-03,
        1.3291e-04, 2.2685e-02, 9.2878e-03, 3.9173e-02, 7.9669e-02, 1.6665e-02,
        1.2430e-02, 1.1386e-02, 1.1102e-04, 2.1244e-04, 4.0976e-03, 1.4570e-03,
        4.7546e-02, 5.9695e-03, 2.5338e-01, 8.1003e-03, 1.9668e-03, 2.2705e-03,
        8.4972e-03, 1.6653e-03, 1.3405e-01, 8.3203e-04, 1.3600e-03, 6.7455e-02,
        4.1727e-03, 1.1310e-02, 8.9081e-03, 1.9318e-01, 1.4171e+00, 4.3635e+00,
        4.2450e+00, 5.9699e+00, 7.5325e-01, 2.3637e+00, 4.4834e-03, 1.5584e-03,
        8.9578e-03, 1.8998e-02, 2.4461e-04, 4.1934e-03, 3.7459e-03, 8.6443e-03,
        2.8387e-03, 2.8967e-02, 1.8088e+00, 8.2600e-02, 5.7189e-01, 7.4920e-03,
        8.0150e+00, 1.1768e-02, 9.7556e-01, 7.2231e-02, 7.5181e-02, 3.7126e-02,
        1.1341e-01, 2.3587e-01, 5.9878e-01, 6.1193e-02, 9.0368e-05, 1.6076e-02,
        1.4115e+00, 1.4113e+00, 8.1840e+00, 2.7941e-01, 8.6594e-01, 4.7985e-01,
        8.1038e-02, 2.2187e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [35/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 47.1
Sum Train Loss:  tensor([8.1940e-01, 1.1039e-02, 5.3779e-01, 3.5811e-03, 1.1955e-03, 1.1973e-02,
        4.6216e-03, 1.1639e-02, 2.1955e-02, 1.3797e-02, 3.1300e-03, 1.1893e-03,
        1.0909e-04, 4.4966e-02, 1.9790e-02, 4.5051e-02, 3.7729e-02, 6.9523e-02,
        3.2893e-03, 2.6014e-02, 1.4500e-04, 6.4015e-04, 3.1861e-04, 3.7259e-03,
        3.7746e-02, 8.7741e-03, 2.5426e-01, 5.7393e-03, 6.6134e-04, 2.4120e-03,
        3.8149e-03, 3.6024e-03, 9.0973e-02, 9.5306e-04, 2.5188e-03, 2.6124e-02,
        3.9190e-03, 8.4962e-03, 7.2586e-03, 2.1615e-01, 1.2417e+00, 7.6412e+00,
        1.0564e+00, 3.5300e+00, 3.5014e+00, 7.8610e+00, 1.7596e-03, 1.5988e-03,
        1.4136e-03, 4.2952e-03, 4.1460e-04, 7.3712e-04, 1.5305e-03, 6.7297e-03,
        2.4964e-03, 2.1003e-02, 1.3863e+00, 8.1810e-02, 5.7746e-01, 9.5245e-03,
        7.8147e+00, 8.5741e-03, 1.1697e+00, 3.0957e-02, 2.4529e-02, 3.2802e-02,
        3.5873e-02, 1.1965e-01, 4.1996e-01, 6.0177e-02, 6.1405e-05, 1.0921e-02,
        7.7530e-01, 6.3085e-01, 2.8445e+00, 3.6179e+00, 1.2703e-01, 1.1591e+00,
        2.1086e+00, 2.1010e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [35/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 50.3
Sum_Val Meta Model:  tensor([1.0129e+00, 7.5351e-02, 1.6581e+00, 5.7029e-02, 6.0566e-04, 2.1650e-02,
        1.3927e-03, 1.5819e-02, 2.0342e-02, 8.1389e-03, 1.6508e-03, 1.0328e-02,
        1.2246e-03, 3.3737e-02, 1.9986e-02, 2.4773e-02, 1.8757e+00, 7.6984e-03,
        4.2384e-03, 4.7819e-03, 7.1132e-05, 1.1038e-04, 1.7798e-04, 1.8161e-04,
        8.2851e-02, 1.0662e-02, 3.1678e-01, 1.7508e-03, 2.5443e-03, 1.1406e-02,
        1.1054e-03, 1.2989e-04, 8.2309e-02, 1.1681e-04, 1.0515e-03, 1.1202e-02,
        2.6546e-03, 8.1934e-03, 8.3336e-03, 4.3075e-01, 1.7296e+00, 1.5722e+01,
        9.4314e+00, 2.1132e+01, 2.3275e+01, 1.7324e+01, 7.1801e-04, 4.8078e-03,
        6.9961e-04, 6.6214e-03, 1.1937e-04, 4.1724e-04, 6.3144e-03, 1.8725e-03,
        6.8439e-04, 2.9714e-03, 2.7071e+00, 4.4065e-02, 8.1231e-01, 4.0226e-03,
        1.6999e+01, 1.7278e-02, 6.0679e-01, 3.9563e-02, 2.7506e-02, 7.4667e-03,
        3.9634e-02, 7.6511e-02, 7.0091e+00, 2.3410e-01, 5.9939e-05, 2.1261e-02,
        8.1029e+00, 5.4920e-01, 2.6418e+01, 9.9825e+00, 9.9485e-02, 1.2832e+01,
        1.6763e-02, 1.4345e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([9.5625e-01, 4.7851e-02, 1.0421e+00, 3.2425e-02, 1.8267e-04, 1.9685e-02,
        9.6365e-04, 1.5102e-02, 1.7966e-02, 7.7575e-03, 1.9633e-03, 1.3200e-02,
        1.4562e-03, 3.8470e-02, 2.2952e-02, 9.7309e-02, 1.2324e+00, 1.3081e-02,
        3.7969e-03, 7.6181e-03, 4.9636e-05, 6.8051e-05, 1.0714e-04, 5.9181e-04,
        7.1464e-02, 8.2082e-03, 2.9799e-01, 2.0438e-03, 3.6150e-03, 1.2271e-02,
        4.0599e-04, 7.3423e-05, 6.8969e-02, 9.6241e-05, 9.3830e-04, 1.1213e-02,
        4.4248e-03, 5.7283e-03, 9.7808e-03, 3.8004e-01, 1.8611e+00, 2.2263e+01,
        9.5365e+00, 2.5604e+01, 1.8755e+01, 2.5525e+01, 4.8840e-04, 4.8126e-03,
        3.1533e-04, 5.1475e-03, 3.2294e-05, 2.1716e-04, 5.9851e-03, 5.7641e-04,
        4.0537e-04, 1.5911e-03, 2.4300e+00, 4.0356e-02, 6.7093e-01, 6.0327e-03,
        2.0450e+01, 1.0384e-02, 5.3562e-01, 6.1722e-02, 2.8099e-02, 9.3661e-03,
        4.0397e-02, 1.0189e-01, 9.4035e+00, 2.5953e-01, 7.9343e-05, 2.2669e-02,
        6.7512e+00, 6.0636e-01, 3.4836e+01, 1.9620e+01, 3.5085e-02, 1.3697e+01,
        2.9167e-02, 2.1606e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.0893e+01, 4.3139e+01, 5.2030e+01, 2.5415e+01, 2.9388e-01, 1.1859e+01,
        3.2298e+00, 1.6897e+01, 5.8225e+00, 8.5692e+00, 7.7342e+00, 1.5959e+01,
        9.1573e+00, 1.7537e+01, 8.6814e+00, 3.5571e+01, 3.3479e+02, 2.5555e+00,
        1.3861e+00, 2.2973e+00, 9.2231e-01, 2.0210e-01, 1.2727e-01, 1.0793e+00,
        2.7241e+01, 1.5034e+01, 2.7253e+01, 4.4815e+00, 1.4331e+01, 1.2588e+01,
        4.3904e-01, 2.4701e-01, 6.4869e+00, 7.3014e-01, 9.6476e-01, 1.0759e+00,
        9.1755e+00, 4.3675e+00, 2.5611e+00, 4.2100e+01, 1.1789e+01, 4.1212e+01,
        1.0572e+01, 3.1860e+01, 1.8979e+01, 2.7528e+01, 1.1151e+00, 6.3605e+00,
        2.4001e-01, 4.1771e+00, 1.3957e-01, 3.7325e-01, 6.8110e+00, 4.0026e-01,
        9.0874e-01, 7.9175e-01, 2.8397e+01, 6.9309e+00, 1.0780e+01, 4.8735e+00,
        2.2703e+01, 6.8113e+00, 5.2360e+00, 7.5852e+00, 1.3341e+00, 2.7570e+00,
        1.3349e+00, 9.9699e+00, 1.8605e+01, 2.0282e+01, 1.7687e-01, 1.8857e+01,
        1.8318e+01, 1.0727e+01, 3.6046e+01, 1.9621e+01, 3.5085e-02, 1.3697e+01,
        6.1599e-02, 5.5547e-01], device='cuda:0')
Outer loop valEpocw Maximum [35/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 181.1
model_train val_loss valEpocw [35/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 217.7
model_train val_loss valEpocw [35/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1254.2
Sum_Val Meta Model:  tensor([1.7216e+00, 1.9951e-02, 1.4094e+00, 1.2793e-02, 4.2490e-04, 7.8708e-02,
        2.1383e-02, 6.8499e-02, 1.0560e-01, 4.1520e-02, 5.7345e-03, 2.2031e-01,
        1.7790e-03, 1.2954e-02, 7.1542e-02, 7.3907e-02, 4.2391e-02, 1.2987e-01,
        2.6590e-02, 2.5465e-01, 3.9899e-06, 2.1702e-05, 2.6173e-04, 3.4665e-03,
        1.1216e-01, 1.3474e-02, 4.0316e-01, 8.0712e-03, 4.2725e-03, 7.1813e-05,
        9.8149e-05, 1.9462e-05, 1.5389e-03, 2.4152e-05, 6.1690e-05, 6.9614e-04,
        2.3939e-03, 2.4120e-04, 1.9685e-04, 1.5326e-02, 1.5886e-02, 2.0115e+00,
        7.4889e-02, 1.8977e-01, 2.2315e-01, 4.8740e+00, 5.1807e-04, 4.9899e-04,
        1.1272e-04, 8.7327e-03, 1.1647e-05, 7.1982e-05, 4.6164e-05, 5.5218e-05,
        8.0032e-05, 7.7127e-03, 9.7743e-01, 3.3421e-02, 4.8058e-01, 5.6897e-03,
        1.5554e+00, 1.3839e-03, 1.7338e-01, 1.5391e-02, 8.3912e-03, 2.6222e-03,
        1.3880e-02, 1.6034e-01, 4.5103e-02, 6.0817e-02, 1.5162e-05, 1.4477e-03,
        1.7808e+00, 3.1903e-01, 5.5635e+00, 2.7521e-01, 1.0545e-01, 5.8594e-01,
        1.6054e-02, 3.8368e-03], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.1916e+00, 2.4037e-02, 1.1127e+00, 1.5475e-02, 2.0763e-03, 6.2884e-02,
        1.6516e-02, 3.9777e-02, 8.9550e-02, 2.4382e-02, 4.1416e-03, 6.6827e-02,
        1.3254e-03, 2.5495e-02, 4.8531e-02, 1.1228e-02, 3.4618e-02, 9.0222e-02,
        1.1679e-02, 8.7746e-02, 4.4091e-05, 4.9677e-05, 6.8339e-05, 2.6677e-03,
        6.9843e-02, 1.0706e-02, 3.4913e-01, 6.7260e-03, 3.4773e-03, 1.1342e-03,
        2.1537e-04, 3.6783e-05, 5.2250e-03, 2.2645e-04, 5.2544e-04, 2.1169e-03,
        1.7955e-03, 1.5361e-03, 7.3938e-04, 2.2571e-02, 2.4019e-02, 1.6913e+00,
        5.5851e-03, 9.0327e-02, 5.2343e-02, 1.9464e+00, 2.5333e-04, 1.9406e-04,
        1.6753e-04, 7.0676e-03, 2.2863e-05, 1.2846e-04, 1.5912e-04, 7.6510e-04,
        2.2673e-04, 1.8155e-03, 8.1950e-01, 2.8920e-02, 2.3150e-01, 6.1893e-04,
        1.0312e+01, 9.1359e-04, 3.6137e-01, 5.1802e-03, 1.0956e-02, 1.4014e-03,
        1.1266e-02, 1.6080e-01, 4.0047e-02, 4.9356e-02, 3.9664e-06, 4.4850e-04,
        2.0004e+00, 1.5226e-01, 8.0222e+00, 4.2797e-03, 1.0753e-01, 1.9834e-02,
        8.4259e-04, 3.9520e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.8691e+01, 2.1384e+01, 5.4302e+01, 1.1641e+01, 3.3542e+00, 3.4348e+01,
        4.3867e+01, 3.9272e+01, 2.6305e+01, 2.7246e+01, 1.5529e+01, 7.1205e+01,
        7.3901e+00, 1.1917e+01, 1.5657e+01, 3.3062e+00, 8.7594e+00, 1.6235e+01,
        3.3161e+00, 1.9454e+01, 7.2203e-01, 1.3202e-01, 7.4347e-02, 4.3075e+00,
        2.5519e+01, 1.7340e+01, 3.1706e+01, 1.3242e+01, 1.2031e+01, 1.0877e+00,
        2.1505e-01, 1.1067e-01, 5.0866e-01, 1.5311e+00, 5.2494e-01, 2.2281e-01,
        3.5130e+00, 1.1104e+00, 1.9868e-01, 2.8310e+00, 1.9896e-01, 3.3017e+00,
        6.2816e-03, 1.1412e-01, 5.3082e-02, 2.1258e+00, 5.0773e-01, 2.2966e-01,
        1.1948e-01, 5.1559e+00, 9.1163e-02, 2.1161e-01, 1.7093e-01, 5.5097e-01,
        4.5815e-01, 8.9331e-01, 9.9162e+00, 4.4796e+00, 4.0023e+00, 4.4228e-01,
        1.1632e+01, 5.8214e-01, 3.6496e+00, 6.3143e-01, 5.7550e-01, 3.9228e-01,
        4.1413e-01, 1.9144e+01, 8.4540e-02, 4.1290e+00, 8.4448e-03, 3.5851e-01,
        6.3046e+00, 2.5583e+00, 8.3553e+00, 4.2797e-03, 1.0753e-01, 1.9834e-02,
        1.7453e-03, 1.1201e-01], device='cuda:0')
Outer loop valEpocw Maximum [35/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 24.5
model_train val_loss valEpocw [35/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 29.6
model_train val_loss valEpocw [35/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 712.2
Sum_Val Meta Model:  tensor([1.5462e+00, 4.4945e-03, 2.5717e-01, 4.2891e-03, 7.9440e-04, 5.9296e-03,
        8.2174e-04, 1.9225e-02, 9.5478e-03, 2.9140e-03, 4.1704e-04, 1.4818e-03,
        8.0513e-05, 5.7588e-02, 1.4010e-02, 1.4981e-02, 3.0586e-02, 3.5726e-02,
        5.5365e-03, 9.9012e-03, 6.4284e-05, 6.8820e-04, 1.9965e-02, 1.3035e-03,
        2.7237e-02, 3.2210e-03, 3.0899e-01, 5.3543e-03, 2.1718e-04, 8.7425e-03,
        5.7774e-03, 4.1290e-03, 1.1608e-01, 5.9958e-05, 4.2315e-03, 5.0720e-02,
        1.8003e-02, 2.8492e-02, 1.4245e-03, 2.9169e-01, 4.5938e+00, 4.3455e+01,
        6.6055e+01, 7.5421e+01, 5.3256e+01, 6.1919e+01, 1.6838e-02, 2.0633e-02,
        8.0451e-02, 4.3623e-02, 1.1833e-02, 3.6541e-02, 1.0303e-01, 2.9844e-02,
        6.7365e-02, 3.4800e-01, 1.7299e+01, 8.8384e-02, 5.0755e-01, 4.1068e-03,
        1.1550e+02, 3.4978e-03, 1.2462e+00, 1.2833e-01, 3.1812e-01, 4.1791e-03,
        2.7063e-01, 1.1201e-01, 2.3490e+00, 2.2730e-02, 1.6601e-04, 1.3782e-02,
        1.4797e+00, 2.3585e+00, 9.9285e-01, 1.8283e+01, 1.1209e+01, 7.5312e-01,
        6.0247e-02, 2.1945e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([4.8863e-01, 9.0718e-04, 1.2195e-01, 7.6023e-04, 1.5034e-05, 2.6184e-04,
        8.6392e-05, 1.9675e-02, 9.9551e-04, 9.0112e-05, 2.3214e-05, 2.6917e-05,
        9.8854e-06, 3.9150e-02, 1.4678e-03, 5.1766e-03, 5.9391e-03, 7.7283e-04,
        6.5717e-04, 2.3158e-04, 5.8565e-06, 1.1396e-05, 7.4087e-06, 3.3855e-05,
        1.2560e-02, 1.7854e-03, 2.9999e-01, 3.7062e-03, 2.7679e-04, 3.0679e-04,
        6.9499e-04, 4.2210e-03, 1.3400e-01, 1.7324e-05, 2.0098e-03, 1.8965e-02,
        9.1132e-03, 2.0822e-02, 2.4730e-04, 4.0925e-01, 4.0465e+00, 4.9692e+01,
        1.0290e+02, 8.0394e+01, 9.7455e+01, 8.3778e+01, 6.1286e-03, 8.2499e-03,
        7.5473e-02, 1.7161e-02, 6.5740e-03, 3.7639e-02, 9.2548e-02, 3.8350e-02,
        3.6769e-02, 1.9890e-01, 8.7083e+00, 1.0473e-01, 5.5230e-01, 1.9466e-03,
        1.0933e+02, 6.4803e-04, 1.0930e+00, 1.1429e-01, 2.7578e-01, 1.8831e-02,
        3.0123e-01, 1.4789e-01, 3.7485e+00, 5.8175e-02, 3.2882e-05, 1.3384e-02,
        2.5982e+00, 2.2054e+00, 3.6168e+00, 2.4743e+01, 2.0372e+01, 5.7712e+00,
        5.2202e-03, 9.0152e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.3508e+01, 6.6436e-01, 5.4964e+00, 4.7552e-01, 1.9311e-02, 1.1857e-01,
        2.0706e-01, 1.6730e+01, 2.6618e-01, 7.8293e-02, 6.5881e-02, 2.3272e-02,
        4.5062e-02, 1.4751e+01, 4.5263e-01, 1.5965e+00, 1.3246e+00, 1.0662e-01,
        1.8859e-01, 5.4942e-02, 7.8592e-02, 2.4469e-02, 5.6224e-03, 4.6135e-02,
        3.8496e+00, 2.3007e+00, 2.7150e+01, 6.3049e+00, 8.4343e-01, 2.1147e-01,
        5.3027e-01, 9.1485e+00, 1.0359e+01, 8.7151e-02, 1.5452e+00, 1.4176e+00,
        1.2914e+01, 1.1688e+01, 5.4955e-02, 4.3679e+01, 2.8701e+01, 9.2277e+01,
        1.1486e+02, 9.9878e+01, 9.8962e+01, 9.1704e+01, 9.1419e+00, 7.1081e+00,
        3.8882e+01, 9.2216e+00, 1.8657e+01, 4.6133e+01, 7.2885e+01, 2.1888e+01,
        5.8509e+01, 7.5346e+01, 1.0129e+02, 1.4645e+01, 9.8218e+00, 1.1611e+00,
        1.2286e+02, 3.4040e-01, 1.1268e+01, 1.2793e+01, 1.3642e+01, 4.5195e+00,
        1.0688e+01, 1.5648e+01, 8.2331e+00, 4.3803e+00, 5.5556e-02, 8.7734e+00,
        8.3481e+00, 3.7514e+01, 3.7906e+00, 2.4744e+01, 2.0372e+01, 5.7713e+00,
        1.1788e-02, 2.3676e-01], device='cuda:0')
Outer loop valEpocw Maximum [35/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 481.4
model_train val_loss valEpocw [35/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 604.2
model_train val_loss valEpocw [35/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1523.5
Sum_Val Meta Model:  tensor([4.1005e+00, 6.3724e-03, 8.8506e-01, 5.6511e-03, 1.0538e-03, 1.1565e-01,
        7.0618e-03, 4.6526e-02, 1.2234e-02, 4.9104e-02, 3.9862e-03, 1.4469e-02,
        1.3497e-04, 8.0795e-02, 9.8349e-02, 1.6449e-02, 1.5435e-02, 2.2316e-02,
        4.1105e-03, 8.5809e-03, 1.7973e-04, 6.0885e-04, 2.0085e-03, 1.9205e-03,
        1.4293e-01, 5.2454e-03, 6.1090e-01, 2.2646e-03, 1.5826e-02, 2.0513e-03,
        2.8136e-03, 4.4334e-04, 9.7013e-02, 1.9257e-02, 2.3728e-02, 1.5472e-01,
        6.6826e-04, 6.5712e-03, 4.4644e-02, 3.2965e-01, 1.6112e+00, 1.5933e+01,
        9.4008e+00, 8.9269e+00, 9.9704e+00, 1.8407e+01, 1.2104e-03, 1.6012e-03,
        4.1513e-03, 2.4995e-03, 6.0994e-04, 7.8042e-04, 8.9399e-04, 5.9142e-02,
        1.9741e-03, 1.0334e-02, 3.4591e+00, 6.0659e-02, 1.1391e+00, 8.3012e-03,
        4.2126e+01, 4.4358e-03, 6.7253e-01, 2.4435e-01, 2.9642e-01, 1.7227e-02,
        3.5622e-01, 5.8702e-01, 6.1098e-01, 6.2128e-02, 1.9857e-04, 7.5843e-03,
        1.3435e+00, 1.0370e+00, 4.1951e+02, 1.3839e+01, 1.1145e+00, 8.3403e+00,
        3.6747e-02, 7.8152e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.2490e+00, 6.9136e-03, 5.9848e-01, 5.8956e-03, 1.3604e-03, 5.2043e-02,
        1.0478e-02, 4.4442e-02, 3.2834e-02, 4.5065e-02, 2.8470e-03, 1.9768e-02,
        6.5546e-04, 8.2724e-02, 1.0236e-01, 4.3992e-03, 4.9526e-03, 6.1158e-03,
        1.0611e-03, 2.1336e-03, 1.1195e-04, 8.4077e-05, 9.3649e-05, 1.5382e-03,
        1.4446e-01, 7.2466e-03, 4.9454e-01, 2.2711e-03, 1.9106e-02, 3.9397e-04,
        3.8748e-04, 1.2555e-04, 3.1828e-03, 7.6062e-05, 4.8457e-04, 3.0291e-03,
        1.3757e-03, 8.3777e-04, 2.2606e-03, 4.2930e-02, 8.7709e-02, 1.4489e-01,
        2.7763e-02, 4.2680e-02, 2.9283e-01, 6.0217e-01, 4.3814e-04, 3.3741e-04,
        1.4159e-04, 5.9356e-04, 4.3724e-05, 9.4349e-05, 7.6606e-05, 9.3936e-04,
        4.6503e-04, 1.6985e-03, 5.0596e-01, 1.2350e-02, 1.0152e+00, 3.4152e-03,
        1.4080e+01, 2.7919e-03, 2.3255e-01, 1.2097e-02, 3.0150e-02, 4.8977e-03,
        4.0611e-02, 1.1641e-01, 7.0898e-02, 2.2343e-02, 2.1409e-05, 1.4914e-03,
        1.1252e-01, 5.5724e-01, 7.6530e+01, 1.1130e+00, 1.2108e-01, 1.0351e+01,
        5.0411e-03, 2.0735e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.4835e+01, 2.3625e+00, 1.8437e+01, 1.7465e+00, 6.9947e-01, 1.1818e+01,
        9.2238e+00, 1.6519e+01, 4.7452e+00, 1.7692e+01, 3.0520e+00, 7.5942e+00,
        1.0459e+00, 1.5903e+01, 1.6874e+01, 7.4080e-01, 6.6107e-01, 5.0210e-01,
        1.6330e-01, 2.7884e-01, 4.3164e-01, 7.0735e-02, 3.7968e-02, 9.3424e-01,
        2.5902e+01, 3.8973e+00, 2.5284e+01, 1.4802e+00, 2.0133e+01, 1.3844e-01,
        1.4467e-01, 1.2129e-01, 1.4390e-01, 1.1716e-01, 1.6231e-01, 1.3542e-01,
        8.8830e-01, 2.3894e-01, 2.3059e-01, 2.4801e+00, 4.8866e-01, 2.6920e-01,
        3.2689e-02, 5.6848e-02, 3.0100e-01, 6.8334e-01, 3.0758e-01, 1.4680e-01,
        3.6105e-02, 1.7499e-01, 5.3743e-02, 6.0422e-02, 3.0858e-02, 2.2118e-01,
        3.3773e-01, 3.6322e-01, 4.1214e+00, 9.0497e-01, 1.3605e+01, 9.4050e-01,
        1.6312e+01, 6.6532e-01, 1.7745e+00, 6.6999e-01, 8.1627e-01, 5.7489e-01,
        8.2940e-01, 5.8073e+00, 1.5585e-01, 9.7251e-01, 1.4583e-02, 4.2673e-01,
        3.2167e-01, 6.5051e+00, 8.2242e+01, 1.1131e+00, 1.2108e-01, 1.0351e+01,
        1.2662e-02, 3.4631e-02], device='cuda:0')
Outer loop valEpocw Maximum [35/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 566.2
model_train val_loss valEpocw [35/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 109.1
model_train val_loss valEpocw [35/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 400.7
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.76330698 97.4500798  93.17990765 98.09822005 98.60381818 97.65110074
 98.32969871 95.27783531 98.10796652 96.99321402 98.63183928 98.94007139
 99.43470474 95.46545486 97.54510788 97.41231223 96.43522862 97.7424739
 98.88402919 98.42838172 98.64524068 99.36160622 99.51876805 99.05459241
 95.27052546 96.89453101 94.46156845 97.08946041 98.08847358 98.32604379
 98.34431842 98.59772664 96.99930556 98.57457877 98.66717023 98.88402919
 97.69252324 98.46249437 98.92788831 93.46255528 98.25660019 97.11504489
 98.86819118 98.71224766 99.15083881 98.39061415 98.26147342 98.68909979
 98.12136792 98.76341663 98.84626162 98.65133222 99.03022624 98.46249437
 98.7463603  97.75343868 93.53321719 97.02976328 96.70691147 97.7290725
 97.85090338 98.79021942 97.57191067 97.43911502 98.77072648 97.6450092
 98.68422656 95.96617975 99.36160622 98.38452261 99.81603538 97.44886149
 99.19957116 96.17572885 99.15327542 99.28728938 99.81238045 99.56262716
 99.86964096 99.17155005]
Accuracy th:0.7 is [85.37785846 97.35627003 93.01178105 97.97638918 98.42472679 97.45129811
 98.29314945 95.04879327 98.00197366 96.81168602 98.57457877 98.90595875
 99.42373996 95.38260986 97.4366784  97.26977011 96.36456671 97.61576979
 98.90352213 98.37964937 98.47102253 99.27632461 99.45054276 98.98758543
 95.23153958 96.83483388 94.28369537 96.98590417 98.03364969 98.23223401
 98.11283976 98.579452   96.66914389 98.36868459 98.56361399 98.74514199
 97.46104458 98.33822687 98.75488846 93.26884419 98.12502284 96.51441868
 98.59772664 98.53193796 99.16302189 98.3565015  98.24076217 98.65376884
 98.02999476 98.7183392  98.77316309 98.60869141 99.01438823 98.38817753
 98.74026876 97.65719229 93.33097794 96.81412263 96.55949611 97.58774869
 98.02390322 98.66838854 97.77414993 97.42571362 98.83895177 97.57800222
 98.6903181  95.97227129 99.25561336 98.3418818  99.81603538 97.10042519
 99.18251483 95.904046   99.09845153 99.11185293 99.80385229 99.59430319
 99.86476773 99.15693035]
Avg Prec: is [96.45834682 35.55942842 71.91578131 67.18815152 78.83913789 64.91430243
 74.27261396 47.42750894 57.33254469 52.69334479 35.19257666 54.47769336
 25.19389509 29.43490916 33.5309688  58.35910097 29.54620317 44.30278904
 50.45748305 38.91143335 62.84427901 52.11351577 89.74059977 82.77089967
 26.67484979 34.74545527 38.34837907 41.76908682 27.80697877 40.24328682
 73.13911567 39.04610923 57.53193962 63.9438513  72.23311655 80.01591488
 58.45516262 75.32031793 86.36082985 47.56889662 55.37062548 90.67299943
 92.02798548 90.8113408  93.47417767 94.14647516 41.4800065  35.42068532
 43.1034966  48.97766161 64.57181039 41.95576933 25.86253429 74.37715841
 29.21977173 46.5498463  76.01915413 59.81257173 47.37837133 61.35170656
 96.57708763 84.51146493 75.67107005 51.68214683 61.0981415  45.207658
 62.8963872  26.06699827 83.44969431 68.81644822 11.84883307 72.59925415
 87.79160214 53.56700049 94.42090596 95.56194194 92.46451698 94.10866478
 52.74296616 32.09915825]
Accuracy th:0.5 is [45.46850063 97.2137279  70.19529489 97.02489005 97.26733349 74.83339628
 75.15137486 75.20741706 76.13333171 96.45593986 76.32216956 98.52097319
 99.41399349 79.93323668 76.05535995 96.56680596 96.29512311 75.6996138
 98.65376884 98.30776915 80.22197585 76.81558461 98.38695922 84.88809834
 84.98068981 96.65086926 94.0778012  75.51564918 98.01293844 76.25638089
 97.30875598 98.57457877 96.36213009 98.02024829 88.55033443 75.83362776
 75.75931092 91.68869775 97.11504489 73.14603867 78.78558984 92.05906361
 75.15502979 74.76029775 96.9627563  93.87434364 98.02877645 98.57336046
 97.87648786 91.06370536 90.49962842 98.55508583 98.99976852 75.32193809
 98.70615611 75.65088145 70.62413957 93.52347072 96.24273583 96.9067141
 89.79300934 97.17717864 95.29854656 75.73250813 98.42838172 76.24541611
 98.20786784 74.95157223 76.78269027 97.55972759 77.27123208 95.99054592
 76.79609167 95.45083515 75.08924112 84.12787369 91.22452212 76.28927523
 77.31509119 99.14718388]
Accuracy th:0.7 is [45.57449349 97.2137279  70.19529489 97.02489005 97.26733349 74.84192444
 75.15137486 75.55950829 76.13333171 96.4754328  76.32216956 98.52097319
 99.41399349 80.44614466 76.06145149 96.56680596 96.29512311 75.6996138
 98.65376884 98.30776915 80.6045248  76.81558461 98.38695922 85.95046357
 86.0381818  96.65086926 94.0778012  75.51564918 98.01293844 76.25638089
 97.30875598 98.57457877 96.36213009 98.02024829 88.75257368 75.83362776
 75.75931092 91.97134538 97.11504489 73.14603867 79.42642024 92.05906361
 75.15502979 74.76029775 96.9627563  93.87434364 98.02877645 98.57336046
 97.98491734 91.66189496 90.81517038 98.55508583 98.99976852 75.32193809
 98.70615611 75.66306453 70.62413957 94.01323083 96.24273583 96.9067141
 89.79300934 97.17717864 95.3874831  75.7422546  98.42838172 76.24541611
 98.20786784 74.95157223 76.78269027 97.55972759 77.35895031 95.99054592
 77.05924635 95.45083515 75.08924112 84.31914816 91.68382451 76.28927523
 77.31509119 99.14718388]
Avg Prec: is [55.98866694  3.08863774 11.48173665  3.38299167  2.21810342  3.78813779
  3.31218077  5.64285842  2.47724992  3.96687208  1.65755769  1.77621286
  0.63871615  5.09094267  2.56241241  3.11712675  3.73341263  2.68386913
  1.33840554  1.71046413  1.87546768  0.89430534  1.92773953  2.34943155
  5.04332012  3.59054233  6.62720607  3.12984098  2.06038823  1.90196545
  2.57687908  1.34824178  3.72529225  1.60657372  2.44789523  2.54760257
  3.04432299  2.58596187  2.81930619  7.57732311  2.25970194  8.19819619
  3.4207385   4.07770123  3.13558476  6.40202302  2.11539943  1.50619204
  2.14287935  1.56424579  1.87160185  1.62704499  1.0524378   2.98931449
  1.4027198   2.80188971 11.45906897  3.66221284  3.90765447  2.76805708
 10.75902724  2.18045956  3.83747452  2.98494618  1.53583117  2.56842489
  1.75456474  4.11563337  1.27588135  2.44009415  0.17466019  3.47598221
  1.95090428  4.58758006  3.95853974  2.99758001  0.77747558  1.78045551
  0.13062301  0.77348125]
mAP score regular 59.41, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [87.91389491 97.44624661 92.84450756 98.26344769 98.92866931 97.65802128
 98.46276503 95.41320976 98.10648529 97.0725266  98.68450557 99.02583651
 99.38709919 95.25126442 97.46368687 97.34409647 96.47457458 97.78757755
 98.95607544 98.41791863 98.86638264 99.37215038 99.62627999 99.28993198
 95.45307322 96.88317513 94.45399507 97.32416474 97.94453995 98.23355009
 98.70692877 98.70692877 97.03266313 98.6894885  98.89877171 99.13296958
 97.98938635 98.33819169 99.08064878 93.28300571 98.00931809 92.81460996
 97.39890874 96.21297058 96.63153699 94.3767596  98.4353589  98.81904477
 98.06163889 98.7717069  98.92866931 98.6670653  98.90624611 98.51259436
 98.74430077 97.81000075 90.78655605 97.2145402  96.45713431 97.69788474
 92.01734061 98.82153624 96.99279966 97.46866981 98.70194584 97.51351621
 98.6147445  95.80935297 98.78416424 98.27590503 99.81563146 97.57580288
 98.44781623 95.98873857 97.2444378  97.50853327 99.16535865 98.62471037
 99.82310586 99.19276478]
Accuracy th:0.7 is [87.15399756 97.42631487 93.01641877 98.24102449 98.82901064 97.54341381
 98.5175773  95.38082069 98.14136582 96.87819219 98.6222189  98.98846451
 99.36965892 95.19396068 97.41385754 97.25689513 96.32259511 97.68542741
 98.98597304 98.41791863 98.71440317 99.34225278 99.56399332 99.21518798
 95.46054762 96.80095672 94.5262476  97.192117   97.86730448 98.23355009
 98.5175773  98.6820141  96.69631512 98.56491517 98.7941301  99.07317438
 97.82993248 98.21361836 98.91621197 93.23566784 97.97692902 93.06126517
 97.41136607 96.43471111 96.77604206 94.58105987 98.37556369 98.83399357
 98.00184369 98.7791813  98.86638264 98.6371677  98.90375464 98.46774796
 98.78167277 97.77761168 91.18020779 97.11488153 96.35000125 97.68542741
 92.44836435 98.73931784 97.29426714 97.52597354 98.7866557  97.59324314
 98.68450557 95.8218103  98.80409597 98.24600742 99.81563146 97.36153674
 98.49017116 95.86416523 97.39641727 97.52099061 99.22017091 98.66955677
 99.82310586 99.17532451]
Avg Prec: is [96.42122998 35.02215356 70.23178026 73.00681177 77.54769687 64.99159288
 80.99433574 48.47925795 61.12652136 54.92585044 40.64763243 57.00931709
 24.34362986 30.80042138 33.67849683 63.89552377 32.26281997 45.76008647
 51.047817   38.06611221 69.61052814 59.18273434 92.0156458  88.45107358
 25.43930554 38.90043531 33.16540528 46.99824509 28.8208838  38.7334525
 77.65631034 38.14257527 55.30964793 65.70876181 74.24224294 81.09385531
 58.80895182 76.99775849 88.9138012  45.38360065 37.14282315 47.40050965
 47.31871649 37.2491451  28.92348416 47.55927332 40.0341374  28.30077066
 41.25042632 42.77402284 69.83118973 38.18147802 25.58640225 77.23228595
 31.2938528  43.25267875 58.00931938 57.15076226 38.45997393 64.15283969
 63.74383506 86.53936775 67.0252195  54.44537586 61.95675088 38.8837405
 62.23760947 26.90036446 42.6955882  65.64297081 13.10517563 73.53086182
 56.56166574 45.93340397 63.92927987 49.11810948 16.25044485 53.35185349
  3.01188415 24.62318473]
Accuracy th:0.5 is [45.41694696 97.22450607 68.10424297 96.96290206 97.90716795 73.29147669
 73.41355856 72.92024815 74.93584473 96.41976231 75.00062287 98.5325261
 99.34972718 77.29028079 75.065401   96.31262924 96.21047911 74.39021352
 98.78167277 98.34068316 77.9181304  75.64092982 98.31327703 86.32184767
 79.39307871 96.52938685 94.3393876  74.52724419 97.81747515 74.9831826
 97.52597354 98.67204823 96.39983058 98.18870369 89.75508882 74.64434312
 74.65680046 92.86693076 97.0276802  72.02082866 76.2264245  92.37362035
 73.75239804 73.52567456 97.03764606 94.02795426 98.18621222 98.77668984
 97.95201435 91.67102673 90.58225577 98.55993223 98.87385704 73.72250044
 98.6969629  74.46495752 69.03106859 94.40914867 96.16314124 96.78102499
 90.13379176 97.04761193 95.49044523 74.62690286 98.32075143 75.43662954
 98.13139996 73.76734684 75.71318235 97.53593941 76.05700476 96.07843137
 75.62598102 95.44559882 73.67516257 85.60679672 93.1036201  75.10028154
 76.12676583 99.15040985]
Accuracy th:0.7 is [45.6685851  97.22450607 68.10424297 96.96290206 97.90716795 73.29396816
 73.41355856 73.15195456 74.93584473 96.41976231 75.00062287 98.5325261
 99.34972718 77.71881307 75.07536687 96.31262924 96.21047911 74.39021352
 98.78167277 98.34068316 78.20464908 75.64092982 98.31327703 87.16894636
 80.21277126 96.52938685 94.3393876  74.52724419 97.81747515 74.9831826
 97.52597354 98.67204823 96.39983058 98.18870369 89.92700002 74.64434312
 74.65680046 93.04880783 97.0276802  72.02082866 76.71226051 92.37362035
 73.75239804 73.52567456 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 92.07713581 90.94351845 98.55993223 98.87385704 73.72250044
 98.6969629  74.47492339 69.03106859 94.79034307 96.16314124 96.78102499
 90.13379176 97.04761193 95.55024043 74.63686872 98.32075143 75.43662954
 98.13139996 73.76734684 75.71318235 97.53593941 76.06198769 96.07843137
 75.76301168 95.44559882 73.67516257 85.83102873 93.50972918 75.10028154
 76.12676583 99.15040985]
Avg Prec: is [54.3994609   3.76882832 14.98345793  4.57114921  1.54962915  4.30447499
  9.96478436  8.75625693  6.84853455  5.20534809  2.27076447  5.33829428
  1.55442551  5.88003848  2.95453075  4.22408004 24.71643534  5.90108089
  1.56379763  3.21677391  3.70636777  1.42539891  1.55969835  5.30419908
  5.78091257 14.00502966  8.3686598   4.44035094  3.8867717   6.9534166
  2.29225781  0.87437831  3.08661114  1.11594854  1.69947858  2.41713544
  2.05794148  2.17901034  2.24691938  6.21907925  1.73785164  6.0625492
  2.21769704  2.73913226  2.3804919   4.85153596  1.74389576  1.0288754
  1.41628663  1.19829085  1.24703695  0.98322873  0.74027326  2.35715858
  0.86611031  1.86003504 10.16256867  3.00330057  3.82643847  2.74605053
  7.9159843   2.03143849  3.26335776  2.61680237  1.37218085  1.85134113
  1.57020345  3.4449912   1.07745712  2.19143392  0.19024085  3.10915985
  1.57533636  3.90205239  3.52352391  2.3131181   0.59253685  1.48993959
  0.12855142  0.58856253]
mAP score regular 51.68, mAP score EMA 4.37
Train_data_mAP: current_mAP = 59.41, highest_mAP = 59.41
Val_data_mAP: current_mAP = 51.68, highest_mAP = 51.91
tensor([1.5494e-02, 8.3102e-04, 1.7082e-02, 9.3784e-04, 4.6534e-04, 1.2843e-03,
        2.2746e-04, 7.0084e-04, 2.4855e-03, 6.5262e-04, 1.9174e-04, 6.6001e-04,
        1.1941e-04, 1.7801e-03, 2.0655e-03, 2.1476e-03, 2.8821e-03, 4.3461e-03,
        2.1099e-03, 2.6976e-03, 3.7774e-05, 2.5996e-04, 6.6490e-04, 4.2022e-04,
        2.2176e-03, 4.2968e-04, 9.4226e-03, 3.2922e-04, 1.8137e-04, 7.6740e-04,
        7.4329e-04, 2.1046e-04, 8.7406e-03, 1.0518e-04, 8.0414e-04, 9.9459e-03,
        3.4174e-04, 1.0285e-03, 3.0630e-03, 7.5026e-03, 1.6065e-01, 5.5507e-01,
        9.1859e-01, 8.3693e-01, 9.9070e-01, 9.3435e-01, 3.3755e-04, 6.1341e-04,
        1.0637e-03, 9.7120e-04, 1.7951e-04, 4.3473e-04, 6.6742e-04, 1.1175e-03,
        3.1869e-04, 1.6009e-03, 7.4412e-02, 4.9354e-03, 5.6194e-02, 9.7444e-04,
        9.0880e-01, 1.1713e-03, 9.3635e-02, 6.6194e-03, 1.7722e-02, 2.7278e-03,
        2.5226e-02, 8.4079e-03, 5.2962e-01, 1.0984e-02, 3.3134e-04, 9.3957e-04,
        3.7853e-01, 5.1880e-02, 9.7132e-01, 9.9999e-01, 1.0000e+00, 9.9999e-01,
        5.1109e-01, 3.4272e-02], device='cuda:0')
Sum Train Loss:  tensor([4.9858e-01, 2.9957e-03, 4.1674e-01, 8.7083e-03, 4.1425e-03, 5.8862e-03,
        2.2581e-03, 1.5316e-02, 1.6762e-02, 2.5412e-03, 3.4948e-04, 5.4004e-04,
        1.0677e-03, 2.6916e-02, 2.7665e-02, 2.2119e-02, 4.6211e-02, 3.5351e-02,
        4.2496e-03, 9.0230e-03, 6.1287e-04, 2.0864e-04, 4.3838e-04, 1.0275e-03,
        4.2038e-02, 3.6695e-03, 1.8606e-01, 2.6949e-03, 1.0624e-03, 3.4263e-03,
        6.3924e-03, 3.4673e-03, 1.0393e-01, 4.3300e-04, 3.5228e-03, 1.4427e-02,
        2.0647e-03, 7.2877e-03, 4.1460e-03, 1.6936e-01, 9.6891e-01, 3.6571e+00,
        8.3504e-01, 3.5939e+00, 2.8634e+00, 1.1443e+01, 9.0889e-04, 1.1728e-03,
        6.6632e-03, 4.8447e-03, 8.9137e-04, 3.0568e-03, 6.9398e-03, 4.7374e-03,
        1.9641e-03, 1.9515e-02, 1.8875e+00, 5.4759e-02, 8.7839e-01, 1.9173e-02,
        1.6356e+01, 2.0286e-03, 8.1637e-01, 3.9228e-02, 1.8996e-01, 1.7682e-02,
        1.4156e-01, 1.8540e-01, 6.6002e-01, 2.9816e-02, 1.7898e-05, 1.4472e-03,
        2.4245e-01, 8.3484e-01, 1.0102e+00, 3.7211e+00, 8.0633e-02, 6.1883e-01,
        1.5119e-02, 9.6166e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [36/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 53.0
Sum Train Loss:  tensor([5.8817e-01, 1.7137e-02, 5.0308e-01, 4.0943e-03, 2.1067e-03, 1.2564e-02,
        7.4465e-04, 2.1069e-02, 1.9734e-02, 1.3001e-02, 7.8073e-04, 6.9374e-03,
        9.3145e-04, 3.8680e-02, 3.3450e-02, 2.6712e-02, 5.7190e-02, 3.6443e-02,
        2.5879e-02, 1.7658e-02, 2.3818e-04, 2.7438e-04, 4.3575e-03, 1.5515e-03,
        2.2762e-02, 2.7104e-03, 1.4431e-01, 2.2118e-03, 7.0192e-04, 5.6167e-03,
        2.5877e-03, 4.7021e-04, 7.5144e-02, 4.8743e-04, 9.6318e-03, 7.9574e-02,
        7.3440e-03, 5.2154e-03, 2.7193e-03, 1.3087e-01, 1.1367e+00, 3.1095e+00,
        1.5638e+00, 1.3978e+00, 9.0338e-01, 7.0819e+00, 4.4798e-03, 1.4165e-03,
        3.4501e-03, 2.3558e-03, 3.4971e-04, 2.3891e-03, 8.7983e-04, 6.0516e-03,
        1.3351e-03, 9.8082e-03, 1.2725e+00, 3.1079e-02, 8.1172e-01, 4.5647e-03,
        5.4331e+00, 2.5174e-03, 8.9551e-01, 4.4475e-02, 1.1868e-01, 2.1131e-02,
        6.9061e-02, 9.1464e-02, 3.6270e-01, 6.2013e-02, 3.0920e-05, 5.4354e-03,
        1.9814e-01, 7.4140e-01, 3.1129e+00, 1.2424e+00, 4.1229e-01, 4.6189e-01,
        3.7577e-02, 2.9960e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [36/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 32.6
Sum Train Loss:  tensor([5.9762e-01, 4.0822e-03, 3.1521e-01, 4.2862e-03, 3.1414e-03, 7.5711e-03,
        2.0059e-03, 8.4196e-03, 2.4152e-02, 4.6051e-03, 1.0577e-03, 1.8301e-03,
        8.9952e-04, 3.3395e-02, 2.9776e-02, 1.9998e-02, 3.7805e-02, 2.6977e-02,
        5.0855e-03, 2.7742e-02, 3.0458e-04, 1.7210e-03, 1.7913e-03, 1.0021e-03,
        3.7916e-02, 5.9923e-03, 1.6759e-01, 5.6180e-03, 5.9405e-04, 1.0147e-02,
        1.8056e-03, 1.4061e-03, 4.9732e-02, 5.9939e-04, 7.3026e-03, 5.6525e-02,
        2.3618e-03, 7.1230e-03, 3.0788e-02, 1.8357e-01, 5.9310e-01, 6.9513e+00,
        7.1867e+00, 7.5930e+00, 6.0794e+00, 8.6427e+00, 6.0122e-03, 1.2548e-03,
        1.1727e-02, 8.3658e-03, 1.2210e-03, 4.7299e-03, 6.0196e-03, 6.4612e-03,
        2.9185e-03, 6.9448e-03, 1.2756e+00, 3.0849e-02, 5.1196e-01, 5.2114e-03,
        4.3518e+00, 8.8991e-03, 3.6610e-01, 3.8833e-02, 2.8522e-02, 1.5144e-02,
        7.9666e-02, 9.3326e-02, 1.6628e+00, 3.3485e-02, 9.5271e-05, 1.7899e-02,
        1.0739e+00, 1.0797e+00, 6.5535e+00, 1.2348e+00, 8.7098e-02, 5.3768e-01,
        6.2279e-02, 1.5663e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [36/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 58.0
Sum Train Loss:  tensor([6.1587e-01, 4.2360e-03, 4.1182e-01, 6.4259e-03, 1.7040e-03, 1.4377e-02,
        1.3764e-03, 8.0661e-03, 1.4944e-02, 9.1434e-03, 1.3369e-03, 4.0731e-03,
        4.4743e-04, 3.4180e-02, 3.4281e-02, 1.7502e-02, 5.9316e-02, 4.4031e-02,
        1.9555e-02, 1.6633e-02, 1.1174e-04, 1.5642e-04, 3.2181e-04, 7.4463e-04,
        5.4615e-02, 7.3267e-03, 1.6348e-01, 2.4533e-03, 3.1172e-03, 3.2135e-03,
        5.0115e-03, 1.5738e-03, 6.8352e-02, 1.1525e-04, 1.0098e-02, 2.8630e-02,
        2.6255e-03, 3.8536e-03, 8.9334e-03, 2.3318e-01, 1.3692e+00, 4.9110e+00,
        1.8375e+00, 5.6092e+00, 3.1215e+00, 5.8749e+00, 5.2293e-03, 3.0757e-03,
        6.0336e-03, 5.5708e-03, 3.3760e-04, 1.7184e-03, 7.3444e-03, 7.1473e-03,
        2.6410e-03, 1.8530e-02, 2.4050e+00, 6.7969e-02, 8.3622e-01, 6.0103e-03,
        9.5855e+00, 4.6622e-03, 1.1398e+00, 3.0615e-02, 2.8487e-02, 2.3799e-02,
        1.2304e-01, 4.4312e-02, 3.0779e+00, 9.3349e-02, 1.4381e-04, 8.0269e-03,
        8.9398e-01, 6.5881e-01, 1.3118e+00, 2.0998e+00, 1.2203e-01, 1.0921e+00,
        1.0892e-01, 5.5072e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [36/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 48.5
Sum Train Loss:  tensor([4.5418e-01, 9.4727e-03, 3.6740e-01, 8.2450e-03, 2.2230e-03, 1.5165e-02,
        1.8976e-03, 1.0656e-02, 3.0720e-02, 3.5456e-03, 3.8158e-04, 3.1315e-03,
        3.3749e-04, 3.1535e-02, 2.3709e-02, 2.4902e-02, 3.1175e-02, 6.5909e-02,
        2.4476e-02, 2.8107e-02, 3.1298e-04, 7.0884e-04, 3.5382e-04, 2.6342e-03,
        2.3957e-02, 4.7517e-03, 1.9718e-01, 1.2561e-03, 7.9783e-04, 2.4499e-03,
        4.9841e-03, 8.4780e-04, 5.6703e-02, 3.6355e-04, 5.4351e-03, 2.8897e-02,
        4.5463e-03, 5.1401e-03, 1.7501e-02, 1.7472e-01, 3.6183e-01, 4.7917e+00,
        2.8992e+00, 1.9459e+00, 5.4738e+00, 2.5003e+00, 3.5117e-03, 8.9422e-04,
        3.3149e-03, 6.5050e-03, 1.5127e-03, 5.1792e-03, 6.1125e-03, 7.7893e-03,
        3.6104e-03, 6.6641e-03, 1.2694e+00, 4.3857e-02, 8.2506e-01, 3.8670e-03,
        7.3348e+00, 4.5788e-03, 7.0670e-01, 3.7767e-02, 1.8645e-02, 2.1690e-02,
        8.6838e-02, 5.7761e-02, 4.6705e-01, 6.4816e-02, 2.1712e-03, 4.5676e-03,
        4.4246e-01, 3.2357e-01, 2.7850e+00, 8.6001e-01, 5.1323e-01, 1.4125e-01,
        4.7952e-02, 1.3175e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [36/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 35.9
Sum Train Loss:  tensor([5.9823e-01, 2.8426e-03, 4.6194e-01, 5.8413e-03, 4.9856e-04, 9.4381e-03,
        1.5890e-03, 7.4516e-03, 1.8117e-02, 7.7831e-03, 1.0319e-03, 4.9684e-03,
        3.2702e-04, 1.7954e-02, 3.4410e-02, 1.9731e-02, 3.4416e-02, 2.7317e-02,
        1.5460e-02, 2.4761e-02, 5.4417e-05, 1.0161e-04, 2.6714e-04, 6.2859e-04,
        5.5839e-02, 4.5021e-03, 3.0487e-01, 4.3662e-03, 2.9325e-03, 8.9702e-03,
        3.1875e-03, 1.3282e-03, 3.0251e-02, 1.0935e-03, 2.8809e-03, 3.9616e-02,
        3.6388e-03, 7.2775e-03, 2.1775e-02, 1.6186e-01, 1.4211e+00, 7.1647e+00,
        9.4135e-01, 4.3662e+00, 1.2879e+00, 7.2358e+00, 4.0024e-03, 3.2595e-03,
        6.4276e-03, 1.4151e-03, 9.0225e-04, 3.3994e-03, 2.0151e-03, 1.5739e-03,
        1.8025e-03, 6.7069e-03, 1.6732e+00, 6.6673e-02, 6.5809e-01, 8.9487e-03,
        1.3754e+01, 8.0787e-03, 6.9532e-01, 4.6810e-02, 7.8737e-02, 3.3422e-02,
        1.9064e-01, 1.6461e-01, 4.6962e-01, 1.0053e-01, 3.7447e-03, 9.5915e-03,
        1.5915e+00, 4.5055e-01, 1.0918e+01, 7.3278e-01, 7.5613e-01, 1.3560e+00,
        1.5889e-01, 2.1868e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [36/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 58.5
Sum Train Loss:  tensor([6.3938e-01, 1.2045e-02, 3.5184e-01, 8.5120e-03, 2.9358e-03, 1.9776e-02,
        1.1634e-03, 1.1632e-02, 2.6786e-02, 1.2451e-02, 6.1600e-04, 3.2704e-03,
        1.0694e-04, 4.3840e-02, 1.6838e-02, 4.2310e-02, 4.6131e-02, 4.1146e-02,
        1.8723e-02, 1.0033e-02, 8.4185e-05, 1.1304e-04, 3.5818e-04, 7.4050e-04,
        3.1276e-02, 6.4568e-03, 2.4883e-01, 4.8453e-03, 2.1204e-03, 1.0101e-03,
        1.6561e-03, 1.1506e-03, 5.8326e-02, 1.1606e-03, 1.3588e-03, 9.5675e-03,
        2.5282e-03, 3.2687e-03, 3.9180e-03, 1.6108e-01, 1.8682e+00, 7.5695e+00,
        1.2418e+00, 3.1583e+00, 1.2277e+00, 2.3629e+00, 3.1072e-03, 8.6688e-04,
        1.4830e-03, 7.0364e-04, 2.3261e-03, 1.6278e-03, 2.5062e-03, 6.0568e-03,
        9.6970e-04, 2.8931e-03, 1.1673e+00, 7.2511e-02, 9.7383e-01, 2.5552e-02,
        8.9889e+00, 1.2206e-03, 6.8288e-01, 7.5979e-02, 3.8499e-02, 4.3114e-02,
        8.2756e-02, 1.2277e-01, 1.3852e+00, 1.3172e-01, 1.1704e-03, 1.3007e-02,
        3.2330e+00, 8.1832e-01, 4.1783e+00, 7.3357e+00, 1.8086e+00, 5.0657e+00,
        1.3028e-01, 2.2322e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [36/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 55.9
Sum_Val Meta Model:  tensor([8.8094e-01, 5.1152e-02, 1.4164e+00, 3.8436e-02, 5.4798e-04, 1.6140e-02,
        9.4259e-04, 1.3615e-02, 1.5053e-02, 6.4261e-03, 1.3685e-03, 8.6612e-03,
        9.3227e-04, 2.4571e-02, 1.7017e-02, 1.9320e-02, 1.4857e+00, 9.9129e-03,
        1.7885e-03, 4.0985e-03, 4.3115e-05, 1.1575e-04, 2.0832e-04, 1.4182e-04,
        6.7606e-02, 7.4401e-03, 2.6273e-01, 1.6961e-03, 1.8580e-03, 8.2975e-03,
        1.1826e-03, 1.4552e-04, 6.8045e-02, 1.6630e-04, 6.5199e-04, 6.4900e-03,
        1.1441e-03, 6.0573e-03, 9.3499e-03, 3.6332e-01, 1.6314e+00, 1.6307e+01,
        1.0721e+01, 2.8413e+01, 2.3245e+01, 2.5005e+01, 4.9551e-04, 3.5767e-03,
        3.4062e-04, 4.3520e-03, 9.7225e-05, 1.7398e-04, 4.7177e-03, 1.3473e-03,
        2.8852e-04, 3.0200e-03, 1.8858e+00, 3.6513e-02, 8.1160e-01, 3.1852e-03,
        1.8134e+01, 1.2965e-02, 4.5736e-01, 2.8651e-02, 8.7184e-03, 4.3717e-03,
        1.7329e-02, 5.7845e-02, 6.9940e+00, 1.9999e-01, 5.6732e-05, 1.7883e-02,
        7.0809e+00, 5.1093e-01, 3.0592e+01, 1.2375e+01, 9.1010e-02, 1.2886e+01,
        2.4658e-02, 1.1186e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([7.9629e-01, 3.4402e-02, 9.5583e-01, 2.3798e-02, 1.4332e-04, 1.6117e-02,
        6.0822e-04, 1.1920e-02, 1.5790e-02, 6.0678e-03, 1.6559e-03, 9.3824e-03,
        1.0278e-03, 2.4447e-02, 1.7692e-02, 7.3214e-02, 1.0363e+00, 1.5355e-02,
        1.1037e-03, 6.5739e-03, 2.8033e-05, 8.2234e-05, 7.2098e-05, 2.4896e-04,
        6.1073e-02, 6.4579e-03, 2.6235e-01, 2.3204e-03, 2.8815e-03, 9.5854e-03,
        3.7735e-04, 6.9416e-05, 5.9584e-02, 2.0759e-04, 3.6252e-04, 3.3738e-03,
        2.8035e-03, 4.4853e-03, 9.3188e-03, 3.1622e-01, 2.0241e+00, 2.7042e+01,
        1.1251e+01, 3.0231e+01, 2.7835e+01, 3.4386e+01, 2.9076e-04, 4.3214e-03,
        8.4534e-05, 3.1329e-03, 1.8020e-05, 4.6397e-05, 5.3291e-03, 1.9033e-04,
        1.4438e-04, 1.7452e-03, 1.9947e+00, 3.0126e-02, 7.1364e-01, 4.9714e-03,
        2.7258e+01, 7.5128e-03, 3.4141e-01, 3.9654e-02, 5.3433e-03, 7.3921e-03,
        1.4814e-02, 7.1550e-02, 7.1905e+00, 2.2549e-01, 5.5904e-05, 1.8410e-02,
        5.9422e+00, 6.3178e-01, 4.3074e+01, 1.1814e+01, 1.9979e-01, 1.1556e+01,
        7.4659e-02, 1.6135e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.1394e+01, 4.1397e+01, 5.5954e+01, 2.5375e+01, 3.0799e-01, 1.2549e+01,
        2.6739e+00, 1.7008e+01, 6.3528e+00, 9.2976e+00, 8.6358e+00, 1.4216e+01,
        8.6071e+00, 1.3733e+01, 8.5655e+00, 3.4091e+01, 3.5956e+02, 3.5330e+00,
        5.2311e-01, 2.4370e+00, 7.4212e-01, 3.1633e-01, 1.0843e-01, 5.9246e-01,
        2.7540e+01, 1.5030e+01, 2.7842e+01, 7.0480e+00, 1.5887e+01, 1.2491e+01,
        5.0768e-01, 3.2983e-01, 6.8169e+00, 1.9736e+00, 4.5082e-01, 3.3921e-01,
        8.2038e+00, 4.3609e+00, 3.0424e+00, 4.2148e+01, 1.2599e+01, 4.8717e+01,
        1.2248e+01, 3.6121e+01, 2.8097e+01, 3.6802e+01, 8.6138e-01, 7.0450e+00,
        7.9471e-02, 3.2258e+00, 1.0039e-01, 1.0673e-01, 7.9846e+00, 1.7031e-01,
        4.5304e-01, 1.0902e+00, 2.6806e+01, 6.1040e+00, 1.2700e+01, 5.1018e+00,
        2.9994e+01, 6.4140e+00, 3.6461e+00, 5.9906e+00, 3.0151e-01, 2.7099e+00,
        5.8724e-01, 8.5099e+00, 1.3577e+01, 2.0530e+01, 1.6872e-01, 1.9594e+01,
        1.5698e+01, 1.2178e+01, 4.4346e+01, 1.1814e+01, 1.9979e-01, 1.1556e+01,
        1.4608e-01, 4.7078e-01], device='cuda:0')
Outer loop valEpocw Maximum [36/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 202.4
model_train val_loss valEpocw [36/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 247.8
model_train val_loss valEpocw [36/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1306.8
Sum_Val Meta Model:  tensor([1.4490e+00, 1.8459e-02, 1.4036e+00, 1.1923e-02, 4.7477e-04, 6.4731e-02,
        2.1260e-02, 5.9655e-02, 1.1292e-01, 3.3710e-02, 6.0480e-03, 2.2569e-01,
        1.7289e-03, 1.2373e-02, 6.5399e-02, 8.5686e-02, 3.7465e-02, 1.0336e-01,
        2.5041e-02, 2.2386e-01, 5.2935e-06, 2.9722e-05, 2.5373e-04, 2.3339e-03,
        1.0884e-01, 1.2330e-02, 3.7692e-01, 6.7061e-03, 3.1804e-03, 7.9589e-05,
        1.4896e-04, 2.3932e-05, 1.4294e-03, 2.5316e-05, 7.9027e-05, 1.0485e-03,
        2.0948e-03, 1.9816e-04, 2.5393e-04, 1.3848e-02, 2.3654e-02, 1.8338e+00,
        1.0840e-01, 2.9313e-01, 3.5993e-01, 4.0222e+00, 5.5021e-04, 6.2138e-04,
        1.3618e-04, 8.3360e-03, 1.6204e-05, 9.4853e-05, 5.4573e-05, 6.0586e-05,
        8.5953e-05, 5.9092e-03, 1.0598e+00, 3.2245e-02, 4.8255e-01, 7.8894e-03,
        1.9345e+00, 1.4690e-03, 2.0881e-01, 2.1066e-02, 1.0831e-02, 2.9140e-03,
        2.0448e-02, 1.5085e-01, 5.6651e-02, 6.1180e-02, 1.6322e-05, 1.7826e-03,
        1.9213e+00, 3.4568e-01, 6.1210e+00, 4.9048e-01, 1.5637e-01, 5.8366e-01,
        2.3273e-02, 4.4259e-03], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.1545e+00, 2.2641e-02, 9.2685e-01, 1.3676e-02, 1.3949e-03, 5.5061e-02,
        1.7575e-02, 3.8912e-02, 8.7830e-02, 2.3450e-02, 4.1346e-03, 5.8616e-02,
        1.3555e-03, 1.6960e-02, 4.3756e-02, 9.1408e-03, 3.1473e-02, 8.7541e-02,
        6.2594e-03, 7.4861e-02, 3.3944e-05, 6.8835e-05, 2.6670e-04, 2.0934e-03,
        6.8809e-02, 1.0709e-02, 3.2526e-01, 6.2549e-03, 3.0500e-03, 7.1005e-04,
        3.7492e-04, 5.2512e-05, 3.4200e-03, 5.5524e-04, 2.4860e-04, 1.2574e-03,
        1.1769e-03, 1.3667e-03, 1.0154e-03, 1.4702e-02, 2.1979e-02, 1.2648e+00,
        7.1259e-03, 4.6096e-02, 1.9844e-03, 6.8629e-02, 2.1773e-04, 2.4432e-04,
        7.6218e-05, 6.6461e-03, 2.4310e-05, 5.2530e-05, 1.5106e-04, 3.6142e-04,
        1.3005e-04, 1.2097e-03, 5.9930e-01, 2.6959e-02, 3.1665e-01, 8.8193e-04,
        7.6844e+00, 8.8769e-04, 1.5179e-01, 4.1789e-03, 1.1331e-03, 8.3654e-04,
        3.2241e-03, 1.5574e-01, 1.6313e-01, 4.7524e-02, 4.8907e-06, 1.5969e-04,
        1.8143e+00, 1.7039e-01, 1.1306e+01, 9.2668e-02, 3.1906e-02, 1.2678e-02,
        1.1671e-03, 3.4539e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.8484e+01, 2.2447e+01, 4.5903e+01, 1.1332e+01, 2.3922e+00, 3.3627e+01,
        5.0662e+01, 4.0855e+01, 2.7580e+01, 2.8178e+01, 1.5754e+01, 6.3543e+01,
        7.9986e+00, 8.3175e+00, 1.5555e+01, 2.8257e+00, 8.9002e+00, 1.6110e+01,
        2.0142e+00, 1.7836e+01, 6.0227e-01, 1.9609e-01, 3.0625e-01, 3.6869e+00,
        2.4352e+01, 1.8280e+01, 2.9575e+01, 1.3766e+01, 1.1782e+01, 7.0755e-01,
        3.8000e-01, 1.7770e-01, 3.5442e-01, 3.7772e+00, 2.5299e-01, 1.1768e-01,
        2.6345e+00, 1.0614e+00, 2.8311e-01, 2.0115e+00, 1.6780e-01, 2.4709e+00,
        7.8837e-03, 5.6328e-02, 2.0078e-03, 7.4422e-02, 4.6260e-01, 2.9280e-01,
        5.5965e-02, 5.0670e+00, 9.9214e-02, 9.0786e-02, 1.7584e-01, 2.6864e-01,
        2.9313e-01, 6.3533e-01, 7.7451e+00, 4.4272e+00, 5.6040e+00, 6.6559e-01,
        8.5882e+00, 6.0626e-01, 1.5645e+00, 5.4425e-01, 6.1869e-02, 2.4841e-01,
        1.2249e-01, 1.9065e+01, 3.2743e-01, 4.0151e+00, 1.1033e-02, 1.3549e-01,
        5.5875e+00, 2.9528e+00, 1.1721e+01, 9.2669e-02, 3.1906e-02, 1.2679e-02,
        2.2945e-03, 1.0195e-01], device='cuda:0')
Outer loop valEpocw Maximum [36/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 24.8
model_train val_loss valEpocw [36/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 27.1
model_train val_loss valEpocw [36/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 689.1
Sum_Val Meta Model:  tensor([1.1144e+00, 2.0180e-03, 1.8427e-01, 2.1363e-03, 4.0189e-04, 3.2915e-03,
        3.2771e-04, 9.1360e-03, 5.8443e-03, 1.6132e-03, 2.1411e-04, 9.6998e-04,
        5.2768e-05, 3.2617e-02, 7.8403e-03, 1.0178e-02, 1.6978e-02, 2.9656e-02,
        2.8971e-03, 6.7039e-03, 3.1278e-05, 2.3776e-04, 1.1572e-02, 9.0898e-04,
        1.9774e-02, 2.1258e-03, 2.1799e-01, 2.6286e-03, 1.2357e-04, 4.7066e-03,
        4.6120e-03, 1.8492e-03, 7.8241e-02, 3.5229e-05, 3.3096e-03, 5.3531e-02,
        7.4735e-03, 1.5270e-02, 1.0037e-03, 1.9018e-01, 4.6113e+00, 3.6596e+01,
        6.4340e+01, 6.9407e+01, 5.1135e+01, 8.0064e+01, 7.5394e-03, 1.2635e-02,
        4.3246e-02, 2.5303e-02, 6.3115e-03, 2.0089e-02, 5.7243e-02, 1.6618e-02,
        2.9645e-02, 2.0402e-01, 1.2220e+01, 6.0262e-02, 4.5825e-01, 2.7859e-03,
        8.5324e+01, 2.5544e-03, 9.2119e-01, 8.9309e-02, 2.3442e-01, 2.6958e-03,
        1.9942e-01, 7.4435e-02, 2.6071e+00, 1.8501e-02, 7.9907e-05, 8.0439e-03,
        1.3502e+00, 1.8915e+00, 1.1172e+00, 2.9911e+01, 1.1717e+01, 7.2272e-01,
        6.9173e-02, 1.7219e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([3.5962e-01, 7.6496e-04, 1.0065e-01, 2.6016e-04, 1.3112e-05, 8.8584e-05,
        2.1688e-05, 1.0957e-02, 3.4187e-04, 9.3361e-05, 2.0439e-05, 3.8623e-05,
        7.1432e-06, 2.6748e-02, 9.9627e-04, 3.0080e-03, 2.7761e-03, 4.9519e-04,
        1.2797e-04, 1.1061e-04, 2.4562e-06, 6.8065e-06, 8.8616e-06, 1.6389e-05,
        8.7704e-03, 5.6285e-04, 2.0346e-01, 2.1671e-03, 1.5332e-04, 1.4243e-04,
        4.3507e-04, 1.8807e-03, 8.7535e-02, 1.8445e-05, 5.6254e-04, 7.5474e-03,
        3.9586e-03, 1.1191e-02, 3.3356e-04, 2.4897e-01, 3.9793e+00, 6.3739e+01,
        8.1417e+01, 8.3051e+01, 1.4886e+02, 9.0997e+01, 2.8850e-03, 4.1622e-03,
        4.2319e-02, 1.0979e-02, 3.5875e-03, 2.5172e-02, 4.6860e-02, 1.6326e-02,
        1.8583e-02, 1.0527e-01, 6.8845e+00, 7.2468e-02, 5.2195e-01, 1.2913e-03,
        1.3390e+02, 3.6577e-04, 7.8183e-01, 7.2073e-02, 2.2779e-01, 1.1101e-02,
        2.0676e-01, 9.9879e-02, 4.0168e+00, 2.1690e-02, 2.4939e-05, 7.6077e-03,
        1.6919e+00, 1.7900e+00, 1.5429e+00, 3.4110e+01, 1.7670e+01, 1.7284e+00,
        2.0487e-02, 7.8144e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.5174e+01, 1.0499e+00, 6.1815e+00, 3.0294e-01, 3.2381e-02, 7.4587e-02,
        1.0830e-01, 1.7061e+01, 1.4571e-01, 1.5713e-01, 1.1555e-01, 5.9259e-02,
        6.9353e-02, 1.6718e+01, 5.2388e-01, 1.4607e+00, 1.0396e+00, 1.0573e-01,
        6.3656e-02, 4.2436e-02, 8.0861e-02, 2.9320e-02, 1.2729e-02, 4.2259e-02,
        3.9452e+00, 1.3962e+00, 2.6000e+01, 7.4543e+00, 1.0472e+00, 1.7925e-01,
        5.6743e-01, 8.8581e+00, 1.0230e+01, 1.9285e-01, 7.3082e-01, 6.4452e-01,
        1.2158e+01, 1.1292e+01, 1.2337e-01, 4.1817e+01, 2.8716e+01, 1.1592e+02,
        8.7633e+01, 9.6718e+01, 1.5014e+02, 9.7231e+01, 8.5515e+00, 6.3013e+00,
        3.8114e+01, 1.0466e+01, 2.0059e+01, 5.6739e+01, 6.9244e+01, 1.6515e+01,
        6.0644e+01, 6.8683e+01, 1.0567e+02, 1.6192e+01, 1.0888e+01, 1.3848e+00,
        1.4662e+02, 3.5117e-01, 9.3371e+00, 1.2659e+01, 1.5556e+01, 4.5253e+00,
        9.8104e+00, 1.5634e+01, 8.2954e+00, 2.3478e+00, 8.3304e-02, 9.1260e+00,
        5.4894e+00, 3.9591e+01, 1.5917e+00, 3.4110e+01, 1.7670e+01, 1.7284e+00,
        4.2927e-02, 2.7310e-01], device='cuda:0')
Outer loop valEpocw Maximum [36/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 457.6
model_train val_loss valEpocw [36/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 678.8
model_train val_loss valEpocw [36/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1597.9
Sum_Val Meta Model:  tensor([4.7316e+00, 4.1073e-03, 8.5924e-01, 2.8137e-03, 8.1850e-04, 4.8734e-02,
        7.8432e-03, 5.0009e-02, 9.2305e-03, 5.1548e-02, 4.0102e-03, 1.4460e-02,
        9.3903e-05, 7.7593e-02, 9.5577e-02, 1.2798e-02, 1.0175e-02, 1.9903e-02,
        2.9116e-03, 5.6348e-03, 1.0439e-04, 2.2698e-04, 7.9932e-04, 1.1520e-03,
        1.4160e-01, 3.8661e-03, 6.8303e-01, 1.3684e-03, 1.6525e-02, 1.1676e-03,
        2.7120e-03, 3.5326e-04, 7.7563e-02, 1.4259e-02, 2.5365e-02, 1.0715e-01,
        4.6041e-04, 3.9032e-03, 3.2132e-02, 2.3544e-01, 1.8148e+00, 1.5960e+01,
        1.1643e+01, 1.2005e+01, 1.3102e+01, 2.2618e+01, 8.0258e-04, 1.0474e-03,
        3.3751e-03, 1.2628e-03, 3.5106e-04, 5.7365e-04, 6.2232e-04, 4.8903e-02,
        1.3433e-03, 7.5355e-03, 2.6541e+00, 3.4019e-02, 1.0585e+00, 4.6501e-03,
        5.0842e+01, 2.7353e-03, 4.5606e-01, 1.4141e-01, 2.8985e-01, 8.0534e-03,
        3.9589e-01, 9.2951e-01, 5.0668e-01, 5.1598e-02, 1.1039e-04, 5.5755e-03,
        1.1742e+00, 9.0678e-01, 4.1055e+02, 1.2603e+01, 9.4006e-01, 6.6378e+00,
        1.7720e-02, 2.8350e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.2789e+00, 8.5266e-03, 5.4647e-01, 4.6463e-03, 9.0774e-04, 4.1828e-02,
        8.4911e-03, 4.2449e-02, 1.4507e-02, 4.5360e-02, 3.0594e-03, 2.0394e-02,
        7.9327e-04, 8.0907e-02, 1.0063e-01, 5.5202e-03, 3.6648e-03, 6.9169e-03,
        2.1440e-04, 1.0440e-03, 6.5636e-05, 7.2145e-05, 2.2198e-04, 1.1971e-03,
        1.3106e-01, 4.4308e-03, 4.5739e-01, 2.8644e-03, 1.7225e-02, 3.2650e-04,
        3.2472e-04, 1.7482e-04, 2.9370e-03, 1.6324e-04, 5.4071e-04, 2.9762e-03,
        1.2280e-03, 5.3893e-04, 1.9341e-03, 3.4201e-02, 8.9737e-02, 3.6261e-01,
        5.2927e-02, 6.5685e-02, 2.7729e-03, 6.9333e-02, 3.2061e-04, 3.3200e-04,
        7.5075e-05, 7.8560e-04, 4.1231e-05, 3.6834e-05, 9.5438e-05, 5.2250e-04,
        2.0403e-04, 1.8301e-03, 5.9008e-01, 1.7236e-02, 8.5692e-01, 3.4294e-03,
        2.4308e+01, 2.1672e-03, 8.9017e-02, 1.2214e-02, 5.9789e-03, 5.0477e-03,
        1.8585e-02, 1.1692e-01, 4.4247e-02, 5.2489e-03, 2.2183e-05, 6.8936e-04,
        2.5052e-01, 4.6538e-01, 8.4311e+01, 5.3823e+00, 7.8057e-03, 9.9755e+00,
        3.7634e-03, 1.3104e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.5954e+01, 3.1350e+00, 1.8209e+01, 1.4442e+00, 4.9362e-01, 1.0134e+01,
        7.7054e+00, 1.6512e+01, 2.1853e+00, 1.8914e+01, 3.3007e+00, 7.8614e+00,
        1.3050e+00, 1.6187e+01, 1.7620e+01, 9.7932e-01, 5.2758e-01, 5.7742e-01,
        3.3977e-02, 1.4039e-01, 2.6036e-01, 6.0355e-02, 9.3115e-02, 7.5162e-01,
        2.5418e+01, 2.4032e+00, 2.4434e+01, 1.9331e+00, 1.9507e+01, 1.1827e-01,
        1.2123e-01, 1.7428e-01, 1.3634e-01, 2.5234e-01, 1.7508e-01, 1.2020e-01,
        8.2873e-01, 1.6524e-01, 1.9401e-01, 2.1087e+00, 4.6845e-01, 6.7595e-01,
        6.1038e-02, 8.3542e-02, 2.8401e-03, 7.7852e-02, 2.3055e-01, 1.4329e-01,
        1.9341e-02, 2.3413e-01, 5.0695e-02, 2.4432e-02, 3.9767e-02, 1.2912e-01,
        1.6265e-01, 4.1596e-01, 4.9739e+00, 1.3153e+00, 1.2289e+01, 9.5662e-01,
        2.7867e+01, 5.3667e-01, 7.1027e-01, 7.6097e-01, 1.6760e-01, 6.1853e-01,
        3.8968e-01, 5.8170e+00, 9.2566e-02, 2.3490e-01, 1.5305e-02, 2.0276e-01,
        7.1734e-01, 5.4905e+00, 9.0104e+01, 5.3827e+00, 7.8057e-03, 9.9761e+00,
        9.3879e-03, 2.4243e-02], device='cuda:0')
Outer loop valEpocw Maximum [36/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 574.8
model_train val_loss valEpocw [36/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 130.0
model_train val_loss valEpocw [36/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 414.0
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.88391954 97.4500798  93.37971029 98.11405806 98.61843788 97.63526273
 98.37599444 95.23763112 98.05923417 96.9907774  98.64524068 98.95590941
 99.44932445 95.43743375 97.51830509 97.45251642 96.44984832 97.74978375
 98.90108551 98.42472679 98.57214215 99.32505696 99.53704268 99.04728256
 95.25712406 96.86894653 94.42989242 97.04194637 98.09700174 98.28462129
 98.45762113 98.60381818 96.99321402 98.59163509 98.6756984  98.78412787
 97.66937537 98.44421973 98.95834602 93.52590734 98.29558607 97.38916436
 99.22150071 98.72199413 99.22759226 98.52462811 98.24198048 98.65864207
 98.06654402 98.77925464 98.80727574 98.62818435 99.03388117 98.46005775
 98.74270538 97.76318515 93.49788623 97.03341821 96.76295367 97.75953022
 98.19690306 98.75367016 97.70836125 97.42693193 98.88890243 97.63282611
 98.72930398 96.00882056 99.44932445 98.34797334 99.81603538 97.36236157
 99.20809932 96.20862319 99.11550785 98.96687419 99.71126083 99.60404966
 99.88426067 99.19591623]
Accuracy th:0.7 is [85.54476675 97.40378407 92.70720386 97.97273425 98.44421973 97.36236157
 98.16522703 94.91599761 97.93618499 96.8652916  98.58067031 98.8986489
 99.43226813 95.33509582 97.41231223 97.30631937 96.34872869 97.62795288
 98.81336728 98.36746628 98.41863525 99.32749357 99.53338775 98.90595875
 95.2351945  96.74346073 94.28613199 96.97006615 98.04095954 98.19568475
 98.23954387 98.58798017 96.6277214  98.54777598 98.52584642 98.59650833
 97.41840377 98.31507901 98.90595875 93.19696397 98.19081152 97.15159416
 99.13500079 98.51853657 99.06433888 98.35284658 98.20786784 98.64036744
 98.01415675 98.75610677 98.77559971 98.58798017 99.00829668 98.25050864
 98.72199413 97.69617817 92.79735871 96.91524226 96.63015801 97.60602332
 97.96055116 98.61843788 97.37454466 97.40500238 98.80483912 97.54510788
 98.65864207 95.96617975 99.36891607 98.2029946  99.81603538 96.92255211
 99.26535983 96.05146136 99.23002887 99.22881057 99.74659178 99.60283135
 99.87085927 99.16180358]
Avg Prec: is [96.46549373 36.67073673 72.661439   66.75779043 79.11901182 64.7608956
 75.24502868 47.43363443 57.61806966 53.66462506 33.72410524 54.90384148
 26.74628238 29.56080108 33.83613603 59.1483555  31.15864317 43.75360672
 51.47848071 39.84618886 61.24879952 52.18992354 89.95871948 83.57056736
 25.79201875 35.0512619  38.60085855 39.97134724 27.18600526 40.09165743
 73.7480689  37.12694625 58.36316619 63.04032063 72.47492684 79.85968106
 59.08314455 74.99095628 86.80885974 47.58304353 55.48789168 90.86609497
 94.23403036 90.93189664 94.89843842 94.52958938 40.78793617 34.50627407
 42.37979966 47.41993917 63.00231457 40.89494093 28.06638406 74.11768665
 27.59842425 46.39187876 75.14876461 59.86741602 48.91924133 62.58623668
 97.08788677 83.81946639 75.77556551 51.34290902 63.74796131 45.25296792
 62.79557248 27.17377144 86.29194821 68.09538119 11.73848801 72.37146378
 88.61975636 53.57409761 95.00143043 94.79242892 91.05677724 94.43106433
 58.52346745 31.90535501]
Accuracy th:0.5 is [45.88638053 97.2137279  70.04787953 97.02489005 97.26733349 74.55927681
 74.82121319 74.98568487 75.73250813 96.46446803 76.03586701 98.52097319
 99.41399349 79.73708897 75.65453637 96.56680596 96.29512311 75.35483242
 98.65376884 98.30776915 79.84917338 76.48785955 98.38695922 85.25480927
 85.31937964 96.65086926 94.0778012  75.17330442 98.01293844 75.87748687
 97.30875598 98.57457877 96.36213009 98.02024829 88.59663016 75.5327055
 75.51686748 91.78494414 97.11504489 72.74765171 78.46030141 92.05906361
 74.79562871 74.49105152 96.9627563  93.87434364 98.02877645 98.57336046
 97.89232587 91.04786735 90.68481134 98.55508583 98.99976852 74.94304407
 98.70615611 75.35970566 70.49865377 93.48935807 96.24273583 96.9067141
 89.79300934 97.17717864 95.40332111 75.37188874 98.42838172 75.81047989
 98.20786784 74.60191762 76.42816242 97.55972759 76.8850282  95.99054592
 76.47445816 95.45083515 74.72984004 84.27163412 91.56077533 75.97373326
 76.95812673 99.14718388]
Accuracy th:0.7 is [45.96191567 97.2137279  70.04787953 97.02489005 97.26733349 74.56415005
 74.82121319 75.33899441 75.73250813 96.4754328  76.03586701 98.52097319
 99.41399349 80.19395475 75.65940961 96.56680596 96.29512311 75.35483242
 98.65376884 98.30776915 80.19395475 76.48785955 98.38695922 86.27453369
 86.43656876 96.65086926 94.0778012  75.17330442 98.01293844 75.87748687
 97.30875598 98.57457877 96.36213009 98.02024829 88.82201728 75.5327055
 75.51686748 92.07490162 97.11504489 72.74765171 79.08773041 92.05906361
 74.79562871 74.49105152 96.9627563  93.87434364 98.02877645 98.57336046
 97.98735396 91.60828937 91.01131809 98.55508583 98.99976852 74.94304407
 98.70615611 75.37067044 70.49865377 93.9108929  96.24273583 96.9067141
 89.79300934 97.17717864 95.46423655 75.38041691 98.42838172 75.81047989
 98.20786784 74.60191762 76.42816242 97.55972759 76.9678732  95.99054592
 76.75710579 95.45083515 74.72984004 84.47143675 92.07733824 75.97373326
 76.95812673 99.14718388]
Avg Prec: is [55.59021208  3.00161038 10.99257831  3.45155301  2.16992901  3.56461266
  3.32300858  5.56352022  2.52182004  3.80463059  1.61442788  1.67006822
  0.63448374  5.18229884  2.64869158  3.09501039  3.67481756  2.82097644
  1.33178742  1.72106724  1.94005511  0.86292136  1.89711245  2.46901528
  5.04619427  3.6670615   6.38605194  3.31291909  2.06875947  1.86764243
  2.62553904  1.28844312  3.66335053  1.63219347  2.38621572  2.3411592
  3.04888955  2.58757786  2.84885812  7.37470483  2.36501495  8.39654295
  3.44871915  4.13466341  3.30677933  6.50079139  2.06576648  1.51906854
  2.21405705  1.62795138  1.91738689  1.59922982  1.0800474   3.02749292
  1.39128437  2.61647272 11.20440945  3.71976672  3.99070486  2.81685623
 10.82029455  2.11973654  3.80543063  3.08432207  1.66333694  2.54732946
  1.79090089  4.14375237  1.24230165  2.41206209  0.1938006   3.36567097
  1.90216733  4.5862688   3.85384599  3.11546278  0.86146652  1.84707422
  0.15988564  0.73564719]
mAP score regular 59.59, mAP score EMA 3.76
starting validation
Accuracy th:0.5 is [87.87153998 97.33662207 92.9915041  98.31825996 98.94112664 97.62314074
 98.52754316 95.41819269 98.13638289 97.10740713 98.68450557 99.00341331
 99.37215038 95.28863642 97.45870394 97.36901114 96.36744151 97.71034208
 98.97849864 98.40296983 98.82153624 99.32481252 99.60883972 99.25754292
 95.42815856 96.88317513 94.44901213 97.21952313 97.96696315 98.26095622
 98.71689464 98.6820141  97.01273139 98.75924957 98.87385704 99.07068291
 97.99686075 98.34317463 99.08064878 93.28051424 97.98689489 92.77723796
 97.341605   96.48204898 96.78102499 94.53123054 98.40047836 98.79662157
 98.05914742 98.68699704 98.93116077 98.64215063 98.91621197 98.46774796
 98.76672397 97.69788474 90.84385978 97.03266313 96.39733911 97.69041034
 92.90679423 98.86887411 97.31419887 97.49109301 98.8016045  97.53593941
 98.6670653  95.88409697 98.78914717 98.24849889 99.81812293 97.50105887
 98.27590503 95.73710043 97.01273139 96.35249271 99.15539278 98.66208237
 99.8206144  99.16785011]
Accuracy th:0.7 is [87.12908289 97.42631487 92.84450756 98.26843063 98.84395944 97.51351621
 98.38303809 95.19645215 98.03921569 96.95293619 98.6371677  98.96853278
 99.37215038 95.15160575 97.45122954 97.25689513 96.27276578 97.69290181
 98.95109251 98.38802103 98.69447144 99.36716745 99.63126292 99.13795251
 95.45058176 96.72372125 94.54368787 97.19709993 97.87228742 98.17624636
 98.5997957  98.6820141  96.69133219 98.73931784 98.75426664 98.85143384
 97.84238981 98.20863542 99.05324264 93.21324464 98.01430102 93.16839824
 97.51600767 96.57921618 96.87819219 94.68321001 98.38303809 98.83399357
 97.97692902 98.73931784 98.96354984 98.61225303 98.89378877 98.27341356
 98.73682637 97.82245808 91.02822832 97.22450607 96.46211725 97.64307248
 92.9541321  98.79662157 97.14976206 97.53593941 98.76174104 97.56832847
 98.6521165  95.8218103  98.81904477 98.11395969 99.81563146 97.10491566
 98.41044423 95.9115031  97.2369634  96.72621272 99.21020505 98.67703117
 99.82310586 99.17034158]
Avg Prec: is [96.3660105  34.73946876 70.27902531 72.78920886 77.47017776 64.66401804
 80.87821181 48.13421347 62.08482046 55.70012757 39.19865282 55.87747288
 24.16677417 30.9602127  33.89679585 63.6874782  32.31408849 43.52925675
 52.11108394 37.18884537 69.01584209 58.71931559 92.34664977 88.71734058
 24.98889731 38.87195448 33.16987272 45.71296327 29.11498871 38.18194295
 77.12370756 37.19312336 56.45797496 65.71234803 73.99110934 82.31343663
 59.92162864 77.34386515 89.1171684  44.76262427 38.38200644 48.6328423
 52.09623312 38.09300369 26.56713355 49.37115813 40.17479066 27.92438384
 41.8648034  40.99103792 69.53245039 37.60337623 26.32412038 76.62459542
 29.81897154 43.51865244 56.49168782 57.68704399 38.77523485 63.66605502
 64.55491272 87.07180022 66.52323161 54.92728405 62.97269035 39.22745364
 62.906244   27.19609299 42.46659153 65.51028622 10.20675519 73.72182003
 55.88081057 44.890653   62.96832137 45.95430845 14.56579993 52.25033005
  1.79427153 23.9277982 ]
Accuracy th:0.5 is [45.42940429 97.22450607 67.9672123  96.96290206 97.90716795 73.14946309
 73.27154496 72.81311508 74.79881406 96.41976231 74.85362633 98.5325261
 99.34972718 77.25290879 74.9383362  96.31262924 96.21047911 74.24819992
 98.78167277 98.34068316 77.8433864  75.49891621 98.31327703 86.69556768
 79.62229364 96.52938685 94.3393876  74.38523059 97.81747515 74.85113486
 97.52597354 98.67204823 96.39983058 98.18870369 89.76007175 74.49734659
 74.51478686 92.86942223 97.0276802  71.89376386 76.14171463 92.37362035
 73.61038443 73.38366096 97.03764606 94.02795426 98.18621222 98.77668984
 97.95201435 91.88280141 91.35211899 98.55993223 98.87385704 73.5904527
 98.6969629  74.31796098 68.90898672 94.40914867 96.16314124 96.78102499
 90.13379176 97.04761193 95.59259536 74.48987219 98.32075143 75.29959887
 98.13139996 73.64526497 75.57116875 97.53593941 75.91000822 96.07843137
 75.50639061 95.44559882 73.5381319  85.70645539 93.33781797 74.95826793
 75.97976929 99.15040985]
Accuracy th:0.7 is [45.6835339  97.22450607 67.9672123  96.96290206 97.90716795 73.15195456
 73.27154496 73.04980442 74.79881406 96.41976231 74.85362633 98.5325261
 99.34972718 77.6640008  74.953285   96.31262924 96.21047911 74.24819992
 98.78167277 98.34068316 78.12243067 75.49891621 98.31327703 87.5476493
 80.51922167 96.52938685 94.3393876  74.38523059 97.81747515 74.85113486
 97.52597354 98.67204823 96.39983058 98.18870369 89.94942323 74.49734659
 74.51478686 93.0662481  97.0276802  71.89376386 76.63751651 92.37362035
 73.61038443 73.38366096 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 92.28641901 91.75822807 98.55993223 98.87385704 73.5904527
 98.6969629  74.32792685 68.90898672 94.80529188 96.16314124 96.78102499
 90.13379176 97.04761193 95.63744176 74.49983805 98.32075143 75.29959887
 98.13139996 73.64526497 75.57116875 97.53593941 75.91499116 96.07843137
 75.64342128 95.44559882 73.5381319  85.95809353 93.76385878 74.95826793
 75.97976929 99.15040985]
Avg Prec: is [54.18323099  3.77642058 14.93416771  4.60650597  1.68352029  4.30431353
  9.75933574  8.74136215  6.85595185  5.17186546  2.27403479  5.35901841
  1.55207569  5.87334888  2.95192141  4.11807753 24.6962028   5.80059588
  1.55656007  3.42090328  3.69683043  1.42411882  1.6210963   5.2473991
  5.74952228 13.96924796  8.33461791  4.55908007  3.9078904   6.9091709
  2.2893548   0.8701932   3.01749502  1.12165175  1.71004501  2.39459067
  2.04418356  2.19568378  2.2129474   6.3864222   1.76334819  6.05452049
  2.22192506  2.7440971   2.34751831  4.8858998   1.78179459  1.06798701
  1.44434871  1.20258688  1.23925764  1.01004528  0.76159114  2.32222201
  0.92013399  1.89527529 10.24678701  3.0041717   3.90055636  2.72765271
  7.9987212   2.04126197  3.28220301  2.57391778  1.34222666  1.86528033
  1.58484807  3.56199634  1.07302053  2.17472188  0.19087867  3.12242519
  1.53266946  3.96422976  3.19540782  2.28165099  0.55780691  1.42908436
  0.12534892  0.59973948]
mAP score regular 51.53, mAP score EMA 4.37
Train_data_mAP: current_mAP = 59.59, highest_mAP = 59.59
Val_data_mAP: current_mAP = 51.53, highest_mAP = 51.91
tensor([1.3555e-02, 6.5231e-04, 1.4800e-02, 7.6670e-04, 3.7221e-04, 1.0251e-03,
        1.8037e-04, 5.7067e-04, 2.0937e-03, 5.2572e-04, 1.5925e-04, 5.6615e-04,
        9.4782e-05, 1.5201e-03, 1.7308e-03, 1.7961e-03, 2.3627e-03, 3.7998e-03,
        1.7877e-03, 2.3354e-03, 2.8813e-05, 2.1595e-04, 5.4896e-04, 3.4426e-04,
        1.9864e-03, 3.5895e-04, 8.5997e-03, 2.6364e-04, 1.3743e-04, 6.3964e-04,
        6.4048e-04, 1.6775e-04, 7.7623e-03, 8.6522e-05, 7.2423e-04, 1.0942e-02,
        2.7122e-04, 8.3461e-04, 2.7280e-03, 6.3064e-03, 1.8181e-01, 5.6273e-01,
        9.3689e-01, 8.7190e-01, 9.9323e-01, 9.4539e-01, 2.7595e-04, 5.4088e-04,
        9.1343e-04, 8.3054e-04, 1.4968e-04, 3.6059e-04, 5.5262e-04, 9.2315e-04,
        2.3898e-04, 1.3457e-03, 6.7822e-02, 4.2774e-03, 5.0722e-02, 8.3804e-04,
        9.1825e-01, 9.7223e-04, 8.5872e-02, 5.1363e-03, 1.5488e-02, 2.3083e-03,
        2.2725e-02, 6.7455e-03, 5.7111e-01, 9.7189e-03, 2.7698e-04, 7.7987e-04,
        3.8537e-01, 4.8818e-02, 9.7628e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        5.3298e-01, 2.9100e-02], device='cuda:0')
Sum Train Loss:  tensor([5.3216e-01, 8.9999e-03, 3.8683e-01, 7.9755e-03, 2.2307e-03, 7.0765e-03,
        1.3524e-03, 1.2571e-02, 2.1450e-02, 5.7372e-03, 3.2322e-04, 1.9771e-03,
        9.0719e-05, 2.4695e-02, 1.7199e-02, 1.4050e-02, 2.5839e-02, 2.5692e-02,
        7.7129e-03, 9.2885e-03, 5.7129e-05, 6.7123e-04, 3.5939e-03, 1.0799e-03,
        3.9744e-02, 5.5439e-03, 2.3605e-01, 3.1305e-03, 8.4380e-04, 1.0599e-02,
        2.8618e-03, 7.1604e-04, 6.9973e-02, 2.6592e-04, 1.1455e-03, 1.5632e-02,
        1.8186e-03, 4.9255e-03, 1.6731e-02, 1.2864e-01, 2.7671e+00, 2.5778e+00,
        3.6398e-01, 4.4734e+00, 6.7993e-01, 2.0473e+00, 1.0992e-03, 7.9168e-04,
        6.2983e-03, 8.1231e-03, 2.0155e-04, 4.7566e-04, 4.4100e-03, 1.2220e-03,
        3.8792e-04, 1.3268e-02, 6.5029e-01, 4.2898e-02, 8.1833e-01, 1.1828e-02,
        2.6123e+00, 5.6335e-03, 6.1243e-01, 4.3242e-02, 4.4879e-02, 1.5654e-02,
        5.5671e-02, 8.3949e-02, 5.7712e-01, 4.5384e-02, 5.7072e-05, 5.5582e-03,
        8.1661e-01, 4.8074e-01, 4.4591e+00, 1.4703e+00, 8.5709e-02, 3.7911e+00,
        6.6519e-02, 1.2454e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [37/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 31.5
Sum Train Loss:  tensor([4.0526e-01, 7.8308e-03, 3.9114e-01, 7.6185e-03, 3.4610e-03, 7.0963e-03,
        4.1055e-04, 9.1789e-03, 9.0329e-03, 6.8082e-03, 1.6995e-03, 4.5362e-03,
        1.0083e-04, 1.5167e-02, 2.4274e-02, 1.0580e-02, 3.9038e-02, 2.9667e-02,
        3.6132e-03, 2.3480e-02, 1.3491e-04, 5.1701e-04, 6.5650e-04, 5.4555e-04,
        2.7302e-02, 4.3249e-03, 2.0955e-01, 3.7014e-03, 2.0134e-03, 7.1870e-03,
        6.6236e-03, 1.3236e-03, 1.0169e-01, 5.6993e-04, 6.9424e-03, 7.5666e-02,
        4.2374e-03, 7.8150e-03, 2.0634e-02, 7.1991e-02, 6.1427e-01, 3.1352e+00,
        1.3669e+00, 2.7671e+00, 7.7846e-01, 4.5503e+00, 1.6794e-03, 1.1996e-03,
        4.0895e-03, 3.5819e-03, 2.5081e-04, 2.4009e-03, 2.7368e-03, 3.1965e-03,
        2.1222e-03, 1.7799e-02, 1.6909e+00, 1.3753e-02, 2.1825e-01, 3.6003e-03,
        2.9679e+00, 2.6912e-03, 2.6571e-01, 1.9956e-02, 1.4766e-02, 5.8063e-03,
        1.6298e-02, 1.4289e-01, 2.8164e-01, 3.3405e-02, 1.6088e-05, 4.8988e-03,
        8.0050e-02, 7.5564e-01, 5.6126e-01, 1.6694e+00, 1.0046e-01, 1.2738e-01,
        2.3984e-02, 6.2090e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [37/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 23.9
Sum Train Loss:  tensor([3.7904e-01, 6.7716e-03, 3.5102e-01, 7.3449e-03, 1.7173e-03, 1.0223e-02,
        5.4644e-04, 1.4936e-02, 2.5687e-02, 8.7725e-03, 2.9087e-04, 2.0630e-03,
        4.9983e-04, 3.4606e-02, 8.3436e-03, 1.2140e-02, 5.4440e-02, 3.1730e-02,
        6.6603e-03, 1.1428e-02, 1.4126e-04, 2.3524e-04, 2.5437e-04, 3.3652e-03,
        2.2570e-02, 6.5821e-03, 1.6626e-01, 2.9220e-03, 1.6524e-03, 2.5836e-03,
        5.0947e-03, 2.1186e-03, 1.3525e-01, 1.3279e-04, 6.6339e-03, 5.1531e-02,
        1.3445e-03, 2.6633e-03, 8.8668e-03, 1.1521e-01, 1.4111e+00, 2.8398e+00,
        2.3546e+00, 5.5356e+00, 2.3898e+00, 4.3542e+00, 3.8020e-03, 5.3184e-03,
        3.2987e-03, 2.2888e-03, 3.4494e-04, 5.1918e-04, 4.9450e-04, 2.2639e-03,
        9.6350e-04, 1.2090e-02, 1.9211e+00, 3.8406e-02, 7.5387e-01, 4.4343e-03,
        4.1742e+00, 3.7475e-03, 3.4722e-01, 5.0766e-02, 3.2378e-02, 1.4796e-02,
        4.9349e-02, 1.1955e-01, 2.9786e-01, 2.7928e-02, 5.2056e-05, 1.0175e-02,
        3.3267e-01, 4.8985e-01, 7.2667e-01, 2.8133e+00, 6.1558e-02, 2.6554e-01,
        9.6384e-02, 8.8787e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [37/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 33.1
Sum Train Loss:  tensor([4.9025e-01, 5.7507e-03, 3.6582e-01, 9.3117e-03, 3.9910e-03, 1.9257e-02,
        4.5941e-04, 8.3728e-03, 6.4325e-03, 7.7993e-03, 2.5013e-03, 1.8916e-03,
        9.4580e-04, 2.6297e-02, 2.6193e-02, 1.4981e-02, 3.5602e-02, 3.9185e-02,
        1.0942e-02, 8.5105e-03, 6.1366e-05, 2.2452e-03, 6.5024e-05, 1.5166e-03,
        3.9413e-02, 3.2649e-03, 1.8366e-01, 3.2807e-03, 2.2040e-03, 1.8523e-03,
        3.5757e-03, 7.5048e-04, 5.8977e-02, 2.7920e-04, 4.1436e-03, 3.8476e-02,
        1.8600e-03, 5.9510e-03, 7.6208e-03, 1.3004e-01, 9.8391e-01, 1.3548e+01,
        1.6466e+00, 5.8308e+00, 1.2642e+00, 4.2785e+00, 1.8938e-03, 5.4868e-03,
        7.8016e-03, 9.6480e-03, 1.1976e-03, 3.9311e-03, 2.5530e-03, 2.1503e-03,
        1.6945e-03, 1.1632e-02, 1.7484e+00, 4.7099e-02, 5.3558e-01, 1.4036e-02,
        8.7056e+00, 8.7507e-03, 9.9578e-01, 7.6137e-02, 1.1461e-01, 1.0078e-02,
        2.0056e-01, 7.2311e-02, 1.7784e+00, 3.8524e-02, 1.5286e-03, 4.7504e-03,
        4.3117e-01, 5.6858e-01, 1.0102e+00, 1.6872e+00, 2.3649e-01, 4.4636e-01,
        5.4369e-01, 3.5669e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [37/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 48.5
Sum Train Loss:  tensor([4.2678e-01, 2.1955e-03, 2.8367e-01, 9.3720e-03, 1.2832e-03, 8.9071e-03,
        6.1159e-04, 7.8323e-03, 4.0806e-03, 6.8652e-03, 1.6109e-03, 2.9581e-03,
        6.3011e-04, 4.7529e-02, 3.4813e-02, 1.4684e-02, 3.9497e-02, 5.0977e-02,
        1.3652e-03, 8.2242e-03, 2.0829e-04, 4.3547e-04, 2.0592e-03, 6.0102e-04,
        3.6229e-02, 2.5966e-03, 1.5497e-01, 4.5167e-03, 7.5942e-04, 2.1340e-03,
        1.9384e-03, 6.1062e-04, 7.6024e-02, 4.6199e-04, 1.6220e-03, 9.7083e-02,
        3.3670e-03, 1.2865e-02, 9.4985e-03, 1.5344e-01, 4.7991e-01, 7.5231e+00,
        8.7843e+00, 5.0170e+00, 2.5715e+00, 3.0097e+00, 3.3694e-03, 4.3512e-03,
        4.1160e-03, 3.2068e-03, 3.3598e-04, 9.9741e-04, 4.9593e-04, 5.7922e-03,
        2.1201e-03, 1.3144e-02, 2.0189e+00, 3.6663e-02, 3.8856e-01, 8.4421e-03,
        3.6827e+00, 5.0539e-03, 4.9835e-01, 7.4831e-02, 1.2032e-02, 2.9495e-02,
        9.3847e-02, 1.5823e-01, 7.6932e-01, 9.3125e-02, 4.0776e-05, 6.5250e-03,
        1.3936e+00, 5.9021e-01, 2.7224e+00, 5.8481e-01, 9.4055e-02, 3.3834e-01,
        6.4821e-02, 1.1228e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [37/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 42.6
Sum Train Loss:  tensor([5.1736e-01, 5.5275e-03, 3.1355e-01, 2.4552e-03, 3.9976e-03, 9.0051e-03,
        1.3454e-03, 5.9029e-03, 1.9493e-02, 7.0041e-03, 9.1134e-04, 3.8236e-03,
        7.7923e-05, 2.7155e-02, 2.4858e-02, 2.8437e-02, 3.3933e-02, 2.5198e-02,
        1.3286e-02, 2.7825e-02, 7.7212e-05, 1.4233e-03, 6.5942e-04, 1.3720e-03,
        3.3254e-02, 3.9079e-03, 2.0514e-01, 4.4398e-03, 2.6768e-03, 5.9274e-03,
        5.1983e-03, 9.7127e-04, 7.1836e-02, 3.7641e-04, 1.1863e-03, 1.2518e-02,
        2.0920e-03, 6.9938e-03, 1.8306e-02, 2.1519e-01, 1.6115e+00, 1.0544e+01,
        2.3056e+00, 1.3584e+01, 3.1991e+00, 7.7909e+00, 3.4826e-03, 1.3515e-03,
        3.1220e-03, 1.5250e-03, 4.4329e-04, 2.5409e-03, 1.8182e-03, 8.2740e-03,
        1.9392e-03, 1.5440e-02, 1.7537e+00, 9.0790e-02, 6.2384e-01, 1.3772e-02,
        1.0376e+01, 7.5472e-03, 1.3351e+00, 3.3101e-02, 4.9664e-02, 4.9541e-02,
        2.7912e-02, 1.2783e-01, 2.7739e+00, 3.2578e-02, 1.7952e-03, 6.9179e-03,
        4.7732e-01, 5.3724e-01, 1.7274e+00, 1.0022e+01, 1.9997e-01, 1.6094e-01,
        5.4660e-02, 5.3880e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [37/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 71.3
Sum Train Loss:  tensor([4.6449e-01, 8.3379e-03, 4.1734e-01, 4.0677e-03, 6.4479e-04, 8.5901e-03,
        1.2127e-03, 9.4364e-03, 1.2288e-02, 3.4358e-03, 1.0984e-03, 6.2920e-04,
        9.3754e-05, 3.6328e-02, 3.3438e-02, 2.5347e-02, 4.5573e-02, 8.8778e-03,
        6.0115e-03, 7.4456e-03, 1.0199e-04, 1.1906e-03, 1.6111e-04, 2.1778e-03,
        5.2376e-02, 7.7308e-03, 2.1977e-01, 4.1896e-03, 1.3456e-03, 1.3892e-03,
        1.0470e-02, 4.6430e-04, 8.0708e-02, 4.1631e-04, 2.3818e-03, 4.1506e-02,
        1.1728e-03, 1.7969e-03, 8.3115e-03, 9.1569e-02, 9.6675e-01, 6.4864e+00,
        3.4612e+00, 3.1945e+00, 4.2760e+00, 7.6165e+00, 2.3858e-03, 7.7452e-04,
        4.9691e-03, 3.6013e-03, 7.3392e-04, 4.5832e-03, 3.7878e-03, 2.8108e-03,
        4.3721e-04, 1.2209e-02, 1.5622e+00, 2.4186e-02, 6.8080e-01, 8.3354e-03,
        5.2599e+00, 5.7422e-03, 2.0622e-01, 4.2563e-02, 2.1981e-02, 2.6245e-02,
        3.3181e-02, 7.1897e-02, 2.7449e+00, 6.9386e-02, 4.8170e-05, 3.3431e-03,
        1.2989e+00, 6.1645e-01, 3.2939e+00, 6.2371e+00, 4.6573e-01, 4.2081e-01,
        6.5765e-01, 4.0604e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [37/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 51.5
Sum_Val Meta Model:  tensor([7.4768e-01, 4.3767e-02, 1.2833e+00, 3.0016e-02, 4.8907e-04, 1.2912e-02,
        8.4060e-04, 1.0466e-02, 1.1876e-02, 4.9560e-03, 1.0284e-03, 7.0783e-03,
        7.1172e-04, 2.1742e-02, 1.4280e-02, 1.6060e-02, 1.1342e+00, 7.8004e-03,
        1.5806e-03, 2.2783e-03, 4.6880e-05, 4.8469e-05, 1.0623e-04, 2.5300e-04,
        5.4479e-02, 6.2709e-03, 2.3879e-01, 1.0711e-03, 1.5090e-03, 7.1300e-03,
        5.6782e-04, 1.1741e-04, 6.1027e-02, 8.1548e-05, 9.3205e-04, 1.5502e-02,
        1.3589e-03, 3.8969e-03, 6.9363e-03, 2.9864e-01, 1.9094e+00, 1.7913e+01,
        9.4936e+00, 3.2790e+01, 2.3663e+01, 1.6606e+01, 5.1131e-04, 2.9036e-03,
        7.5804e-04, 3.8413e-03, 6.3812e-05, 2.2705e-04, 3.9595e-03, 1.2944e-03,
        2.4967e-04, 2.2712e-03, 1.8360e+00, 3.2213e-02, 6.9141e-01, 1.9942e-03,
        1.6204e+01, 1.3385e-02, 4.7980e-01, 2.1070e-02, 1.0346e-02, 3.6113e-03,
        1.3328e-02, 4.1032e-02, 8.8814e+00, 1.7272e-01, 4.6998e-05, 1.5604e-02,
        8.3462e+00, 4.7548e-01, 3.3361e+01, 1.0408e+01, 2.2513e-01, 1.1828e+01,
        1.4911e-02, 9.5594e-03], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([6.6805e-01, 2.7916e-02, 8.3822e-01, 1.8061e-02, 1.3631e-04, 1.2869e-02,
        5.4648e-04, 1.1828e-02, 1.2589e-02, 4.8332e-03, 1.3324e-03, 7.9805e-03,
        8.6184e-04, 2.3277e-02, 1.5379e-02, 7.0967e-02, 6.9626e-01, 1.1257e-02,
        1.4066e-03, 4.6842e-03, 3.2544e-05, 3.2558e-05, 5.4374e-05, 1.0970e-03,
        4.9442e-02, 5.9188e-03, 2.5114e-01, 1.2010e-03, 1.7106e-03, 8.8395e-03,
        1.5149e-04, 8.9983e-05, 5.3076e-02, 5.9445e-05, 7.0288e-04, 1.2550e-02,
        2.8957e-03, 2.9491e-03, 6.3990e-03, 2.6778e-01, 2.1091e+00, 2.3292e+01,
        7.2545e+00, 3.2095e+01, 2.2415e+01, 2.0076e+01, 3.9534e-04, 3.0295e-03,
        3.8538e-04, 2.9469e-03, 1.7623e-05, 9.1235e-05, 3.9691e-03, 2.8598e-04,
        2.4408e-04, 1.7439e-03, 1.8639e+00, 2.9108e-02, 5.8123e-01, 2.8787e-03,
        2.3745e+01, 6.8265e-03, 3.9570e-01, 3.2801e-02, 5.9502e-03, 4.1221e-03,
        1.2695e-02, 4.3416e-02, 1.1227e+01, 1.5691e-01, 7.7306e-05, 1.7821e-02,
        5.5634e+00, 5.1045e-01, 4.1001e+01, 1.1924e+01, 1.0314e-02, 1.3663e+01,
        4.3208e-02, 1.6020e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.9285e+01, 4.2796e+01, 5.6635e+01, 2.3557e+01, 3.6623e-01, 1.2554e+01,
        3.0298e+00, 2.0726e+01, 6.0127e+00, 9.1935e+00, 8.3670e+00, 1.4096e+01,
        9.0928e+00, 1.5313e+01, 8.8853e+00, 3.9512e+01, 2.9469e+02, 2.9627e+00,
        7.8682e-01, 2.0057e+00, 1.1295e+00, 1.5077e-01, 9.9050e-02, 3.1864e+00,
        2.4890e+01, 1.6489e+01, 2.9203e+01, 4.5553e+00, 1.2447e+01, 1.3820e+01,
        2.3653e-01, 5.3641e-01, 6.8376e+00, 6.8705e-01, 9.7053e-01, 1.1469e+00,
        1.0677e+01, 3.5335e+00, 2.3457e+00, 4.2461e+01, 1.1601e+01, 4.1392e+01,
        7.7432e+00, 3.6811e+01, 2.2568e+01, 2.1236e+01, 1.4327e+00, 5.6012e+00,
        4.2190e-01, 3.5482e+00, 1.1774e-01, 2.5301e-01, 7.1823e+00, 3.0979e-01,
        1.0213e+00, 1.2959e+00, 2.7482e+01, 6.8049e+00, 1.1459e+01, 3.4350e+00,
        2.5859e+01, 7.0215e+00, 4.6079e+00, 6.3861e+00, 3.8417e-01, 1.7858e+00,
        5.5862e-01, 6.4363e+00, 1.9658e+01, 1.6145e+01, 2.7910e-01, 2.2851e+01,
        1.4436e+01, 1.0456e+01, 4.1997e+01, 1.1924e+01, 1.0314e-02, 1.3663e+01,
        8.1068e-02, 5.5052e-01], device='cuda:0')
Outer loop valEpocw Maximum [37/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 199.5
model_train val_loss valEpocw [37/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 221.2
model_train val_loss valEpocw [37/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1212.1
Sum_Val Meta Model:  tensor([1.1529e+00, 1.3238e-02, 1.1166e+00, 9.3240e-03, 2.0457e-04, 4.8457e-02,
        1.7414e-02, 3.8778e-02, 7.8479e-02, 2.3131e-02, 4.3463e-03, 1.5822e-01,
        1.1561e-03, 9.4337e-03, 4.1911e-02, 7.6590e-02, 3.3260e-02, 8.7961e-02,
        1.9012e-02, 1.8736e-01, 3.4783e-06, 2.8805e-05, 1.5447e-04, 1.9293e-03,
        7.9391e-02, 9.2756e-03, 3.0804e-01, 5.2436e-03, 2.1267e-03, 5.5642e-05,
        8.9918e-05, 1.5675e-05, 1.4593e-03, 1.8741e-05, 6.5168e-05, 1.0547e-03,
        1.7127e-03, 1.1970e-04, 2.0647e-04, 1.0345e-02, 2.9161e-02, 2.0922e+00,
        1.0796e-01, 2.1726e-01, 2.9163e-01, 3.7258e+00, 3.5191e-04, 4.5807e-04,
        1.1572e-04, 6.4256e-03, 1.3713e-05, 9.5723e-05, 4.2807e-05, 4.2339e-05,
        5.9561e-05, 5.8151e-03, 9.2702e-01, 3.0061e-02, 4.1120e-01, 7.3110e-03,
        1.7936e+00, 9.3398e-04, 2.0988e-01, 1.3346e-02, 8.8713e-03, 3.0399e-03,
        1.6966e-02, 1.2823e-01, 6.7753e-02, 5.3941e-02, 1.4940e-05, 1.3114e-03,
        2.1725e+00, 3.2287e-01, 5.3121e+00, 3.9159e-01, 1.3260e-01, 5.3261e-01,
        2.2613e-02, 4.1117e-03], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([9.3320e-01, 1.5501e-02, 8.2383e-01, 1.0610e-02, 1.2732e-03, 3.9200e-02,
        1.2530e-02, 2.8058e-02, 6.3606e-02, 1.6015e-02, 3.0682e-03, 4.7947e-02,
        9.9215e-04, 1.6648e-02, 3.4276e-02, 7.3147e-03, 2.5397e-02, 8.8296e-02,
        5.7432e-03, 7.0753e-02, 3.3412e-05, 2.7034e-05, 8.4286e-05, 2.0287e-03,
        5.4036e-02, 9.2088e-03, 2.8992e-01, 4.7689e-03, 2.0918e-03, 8.8572e-04,
        7.5775e-05, 4.2228e-05, 6.0316e-03, 1.8923e-04, 3.8618e-04, 3.8544e-03,
        1.2444e-03, 7.4141e-04, 6.7362e-04, 1.4558e-02, 4.6964e-02, 2.2517e+00,
        6.2307e-02, 1.5296e-01, 4.5550e-02, 1.0172e+00, 2.5238e-04, 4.1658e-04,
        3.3333e-04, 5.9254e-03, 2.0893e-05, 1.5148e-04, 1.7871e-04, 9.4808e-04,
        1.5553e-04, 2.1370e-03, 7.0304e-01, 2.3543e-02, 3.4169e-01, 4.4756e-04,
        8.3809e+00, 5.2079e-04, 3.7513e-01, 1.9180e-03, 3.0022e-03, 7.8125e-04,
        3.4342e-03, 1.2616e-01, 1.3414e-01, 3.5122e-02, 8.4799e-06, 4.0718e-04,
        2.8894e+00, 1.7167e-01, 9.3789e+00, 1.8277e-02, 3.4250e-02, 3.4304e-01,
        8.0933e-04, 3.1998e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.8077e+01, 2.0891e+01, 4.8478e+01, 1.1952e+01, 3.1536e+00, 3.3424e+01,
        5.0589e+01, 3.9517e+01, 2.5392e+01, 2.6347e+01, 1.5836e+01, 6.7122e+01,
        8.3584e+00, 9.9665e+00, 1.6111e+01, 2.9908e+00, 9.2039e+00, 1.9746e+01,
        2.3630e+00, 2.0505e+01, 8.9455e-01, 1.0246e-01, 1.2654e-01, 4.7658e+00,
        2.3442e+01, 2.0744e+01, 3.0226e+01, 1.4312e+01, 1.1928e+01, 1.1657e+00,
        9.4599e-02, 1.9984e-01, 7.3970e-01, 1.7462e+00, 4.6633e-01, 3.5384e-01,
        3.8446e+00, 7.6044e-01, 2.3151e-01, 2.4855e+00, 3.4116e-01, 4.1602e+00,
        6.6854e-02, 1.7750e-01, 4.5899e-02, 1.0856e+00, 7.3320e-01, 6.3415e-01,
        3.0559e-01, 5.9109e+00, 1.1385e-01, 3.3615e-01, 2.6436e-01, 9.1186e-01,
        5.2152e-01, 1.3990e+00, 9.9741e+00, 4.5989e+00, 6.5934e+00, 4.1355e-01,
        9.1822e+00, 4.8102e-01, 4.0888e+00, 3.3510e-01, 1.8652e-01, 2.8367e-01,
        1.4046e-01, 1.9304e+01, 2.3905e-01, 3.6243e+00, 2.5074e-02, 4.5864e-01,
        8.5188e+00, 3.1420e+00, 9.6515e+00, 1.8277e-02, 3.4250e-02, 3.4304e-01,
        1.3977e-03, 1.0855e-01], device='cuda:0')
Outer loop valEpocw Maximum [37/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 22.6
model_train val_loss valEpocw [37/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 29.2
model_train val_loss valEpocw [37/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 717.4
Sum_Val Meta Model:  tensor([9.1370e-01, 1.6315e-03, 1.5295e-01, 1.7337e-03, 2.4295e-04, 2.5577e-03,
        2.2991e-04, 5.3470e-03, 3.6697e-03, 9.5848e-04, 1.5437e-04, 6.2032e-04,
        3.1161e-05, 2.2069e-02, 6.4839e-03, 8.1604e-03, 1.4934e-02, 1.5800e-02,
        2.8002e-03, 6.0507e-03, 1.7504e-05, 1.3136e-04, 7.6370e-03, 5.9856e-04,
        1.5598e-02, 1.1791e-03, 1.4753e-01, 1.4641e-03, 6.8826e-05, 3.8822e-03,
        2.6474e-03, 9.7734e-04, 5.8815e-02, 3.2066e-05, 2.4117e-03, 4.2336e-02,
        3.9037e-03, 1.0010e-02, 1.0640e-03, 1.2485e-01, 4.6690e+00, 3.8117e+01,
        6.7673e+01, 8.1655e+01, 5.0934e+01, 7.9439e+01, 4.8658e-03, 7.6231e-03,
        2.8349e-02, 1.7061e-02, 4.0856e-03, 1.1798e-02, 3.0860e-02, 1.0125e-02,
        1.6197e-02, 1.2477e-01, 9.9121e+00, 4.0870e-02, 3.7085e-01, 2.4486e-03,
        9.9470e+01, 1.3800e-03, 8.4977e-01, 5.3196e-02, 1.7287e-01, 2.4568e-03,
        1.6492e-01, 5.4114e-02, 2.6754e+00, 1.5926e-02, 7.8298e-05, 4.4170e-03,
        1.5132e+00, 1.5031e+00, 1.2073e+00, 3.0139e+01, 1.0457e+01, 1.0534e+00,
        1.1086e-01, 2.0941e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([2.3580e-01, 2.4744e-04, 6.4646e-02, 1.9207e-04, 5.6903e-06, 5.1288e-05,
        1.1613e-05, 5.6153e-03, 4.1595e-04, 2.8177e-05, 1.3731e-05, 1.8455e-05,
        4.8621e-06, 1.7437e-02, 4.2993e-04, 2.1714e-03, 2.5393e-03, 2.2419e-04,
        1.0665e-04, 6.0437e-05, 1.5819e-06, 1.6185e-06, 4.2567e-06, 1.8508e-05,
        6.9348e-03, 1.6016e-04, 1.4733e-01, 9.0108e-04, 3.2979e-05, 1.3512e-04,
        6.2517e-04, 8.4608e-04, 6.0226e-02, 4.6634e-06, 9.9959e-04, 1.8073e-02,
        2.2983e-03, 7.6802e-03, 1.3238e-04, 1.8477e-01, 4.5503e+00, 6.3925e+01,
        7.2635e+01, 9.1556e+01, 1.5621e+02, 1.0257e+02, 1.8908e-03, 2.6562e-03,
        2.5197e-02, 8.6342e-03, 1.8319e-03, 1.3195e-02, 2.9756e-02, 2.0845e-02,
        8.6157e-03, 6.5459e-02, 5.9630e+00, 4.8437e-02, 4.0218e-01, 4.1955e-04,
        1.1290e+02, 1.4170e-04, 7.4190e-01, 4.0488e-02, 1.7202e-01, 6.6650e-03,
        1.9586e-01, 6.0675e-02, 3.9060e+00, 3.6202e-02, 2.2502e-05, 4.5594e-03,
        2.6516e+00, 1.6222e+00, 4.6230e-01, 2.9994e+01, 2.3531e+01, 3.0324e-01,
        2.1588e-02, 8.3518e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.3411e+01, 5.8001e-01, 5.2167e+00, 3.7703e-01, 2.6057e-02, 7.4258e-02,
        1.0963e-01, 1.5662e+01, 2.8134e-01, 8.2838e-02, 1.4030e-01, 4.7252e-02,
        9.2969e-02, 1.6144e+01, 3.5743e-01, 1.6692e+00, 1.4988e+00, 7.4920e-02,
        8.3395e-02, 3.4348e-02, 1.1233e-01, 1.2442e-02, 1.0228e-02, 8.1391e-02,
        4.3971e+00, 6.9095e-01, 2.5824e+01, 5.6926e+00, 4.5236e-01, 2.8012e-01,
        1.2653e+00, 7.4355e+00, 1.0045e+01, 9.0458e-02, 1.9374e+00, 1.7368e+00,
        1.2813e+01, 1.2734e+01, 7.4190e-02, 4.5685e+01, 3.1351e+01, 1.1007e+02,
        7.5993e+01, 1.0108e+02, 1.5692e+02, 1.0744e+02, 9.7890e+00, 6.5300e+00,
        3.6008e+01, 1.3305e+01, 1.8186e+01, 4.7895e+01, 6.9666e+01, 3.4805e+01,
        5.4355e+01, 6.4706e+01, 1.1276e+02, 1.5521e+01, 9.6953e+00, 7.0137e-01,
        1.2083e+02, 2.3369e-01, 1.0034e+01, 1.1321e+01, 1.5053e+01, 4.0999e+00,
        1.1043e+01, 1.3918e+01, 7.0697e+00, 5.5603e+00, 1.2788e-01, 9.2286e+00,
        8.3038e+00, 4.1655e+01, 4.7219e-01, 2.9994e+01, 2.3531e+01, 3.0324e-01,
        3.8985e-02, 3.7876e-01], device='cuda:0')
Outer loop valEpocw Maximum [37/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 484.1
model_train val_loss valEpocw [37/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 675.5
model_train val_loss valEpocw [37/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1587.3
Sum_Val Meta Model:  tensor([2.8520e+00, 6.0493e-03, 7.6278e-01, 3.9666e-03, 9.2937e-04, 4.1587e-02,
        7.4754e-03, 4.4276e-02, 8.7011e-03, 4.5391e-02, 5.7545e-03, 1.3474e-02,
        1.6648e-04, 8.9168e-02, 8.8528e-02, 1.2924e-02, 1.6146e-02, 2.1680e-02,
        3.4893e-03, 8.6089e-03, 1.3277e-04, 3.9071e-04, 1.7697e-03, 1.6689e-03,
        1.2741e-01, 4.4913e-03, 5.8056e-01, 2.5666e-03, 1.3603e-02, 2.1253e-03,
        2.1143e-03, 4.5273e-04, 7.9898e-02, 7.1870e-03, 1.5038e-02, 2.0490e-01,
        6.4316e-04, 3.5916e-03, 4.8698e-02, 2.8783e-01, 2.5318e+00, 1.7408e+01,
        1.1799e+01, 1.1114e+01, 1.1485e+01, 1.7198e+01, 1.1622e-03, 1.3899e-03,
        5.0789e-03, 2.3063e-03, 5.7488e-04, 8.1823e-04, 1.0515e-03, 3.6512e-02,
        1.5276e-03, 8.3399e-03, 3.3458e+00, 4.7236e-02, 1.9468e+00, 6.8122e-03,
        4.6888e+01, 3.9751e-03, 6.0499e-01, 2.0483e-01, 2.9848e-01, 1.2118e-02,
        3.9596e-01, 6.0738e-01, 8.6617e-01, 6.2573e-02, 2.0749e-04, 6.0774e-03,
        1.6049e+00, 1.0638e+00, 4.7254e+02, 3.2297e+01, 1.6193e+00, 7.5522e+00,
        4.4921e-02, 5.6637e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.2645e+00, 4.6117e-03, 5.2259e-01, 7.2107e-03, 1.1824e-03, 3.7829e-02,
        8.5670e-03, 3.7362e-02, 1.5331e-02, 4.6353e-02, 3.1099e-03, 1.8069e-02,
        9.7144e-04, 8.1510e-02, 9.8165e-02, 5.1029e-03, 6.8117e-03, 4.5291e-03,
        2.8334e-04, 6.9004e-04, 6.5024e-05, 4.6721e-05, 7.0139e-05, 1.0056e-03,
        1.2510e-01, 2.4628e-03, 4.2321e-01, 1.7677e-03, 1.7891e-02, 3.6535e-04,
        1.5028e-04, 1.5689e-04, 2.8662e-03, 5.3846e-05, 3.0955e-04, 2.5468e-03,
        1.5690e-03, 4.3205e-04, 1.1277e-03, 3.4633e-02, 2.5044e-01, 1.3171e+00,
        2.1728e-01, 5.0910e-01, 6.2781e-03, 2.5571e+00, 4.2957e-04, 6.9627e-04,
        3.9152e-04, 1.2602e-03, 5.7047e-05, 1.0645e-04, 9.4614e-05, 1.1629e-03,
        5.0795e-04, 2.9285e-03, 6.3669e-01, 1.4190e-02, 9.2867e-01, 1.8806e-03,
        1.7834e+01, 1.3408e-03, 9.4392e-02, 5.4117e-03, 5.4884e-03, 7.6455e-03,
        1.1477e-02, 1.1160e-01, 5.1319e-02, 1.4871e-02, 4.9337e-05, 2.1039e-03,
        1.1248e-01, 5.1786e-01, 5.7232e+01, 3.5100e+00, 1.1809e-03, 1.2550e+01,
        1.7755e-03, 1.5769e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.8685e+01, 1.8656e+00, 1.8129e+01, 2.4728e+00, 7.2202e-01, 1.0217e+01,
        8.4936e+00, 1.5789e+01, 2.5183e+00, 2.0920e+01, 3.5728e+00, 7.2077e+00,
        1.7531e+00, 1.6688e+01, 1.8733e+01, 9.9244e-01, 1.0839e+00, 4.0105e-01,
        4.7738e-02, 9.8196e-02, 2.7900e-01, 4.1500e-02, 3.1848e-02, 6.6228e-01,
        2.4445e+01, 1.4261e+00, 2.3814e+01, 1.2418e+00, 2.2663e+01, 1.4453e-01,
        5.6596e-02, 1.6372e-01, 1.4377e-01, 9.6437e-02, 1.0865e-01, 8.9277e-02,
        1.1460e+00, 1.4657e-01, 1.2093e-01, 2.2578e+00, 1.2063e+00, 2.3587e+00,
        2.4619e-01, 6.3096e-01, 6.4022e-03, 2.8471e+00, 3.3398e-01, 3.1546e-01,
        1.0390e-01, 3.9693e-01, 7.4051e-02, 7.3368e-02, 3.9933e-02, 3.1196e-01,
        4.5620e-01, 7.1482e-01, 5.4559e+00, 1.1512e+00, 1.3510e+01, 5.3286e-01,
        2.0228e+01, 3.5940e-01, 7.7327e-01, 3.6580e-01, 1.5740e-01, 9.7972e-01,
        2.4255e-01, 6.2337e+00, 9.7390e-02, 6.9668e-01, 3.5152e-02, 6.7724e-01,
        3.0941e-01, 6.0091e+00, 6.0974e+01, 3.5102e+00, 1.1809e-03, 1.2551e+01,
        3.8374e-03, 2.9582e-02], device='cuda:0')
Outer loop valEpocw Maximum [37/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 649.1
model_train val_loss valEpocw [37/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 101.3
model_train val_loss valEpocw [37/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 394.5
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.30278627 97.4500798  93.311485   98.07507218 98.60869141 97.64135427
 98.37355783 95.07194113 98.0494877  97.0321999  98.69153641 98.92910661
 99.45054276 95.43499714 97.52196002 97.4220587  96.46324972 97.73150912
 98.89255735 98.42960003 98.64645899 99.30921894 99.554099   99.05459241
 95.25834237 96.84336204 94.44329382 97.10042519 98.05801586 98.35284658
 98.12380453 98.62331112 97.07971394 98.49660701 98.68057163 98.87915596
 97.64257258 98.35040996 98.96809249 93.44428065 98.34553673 97.44276995
 99.0521558  98.7463603  99.18738807 98.38574091 98.29314945 98.67204347
 98.19690306 98.75854339 98.81093067 98.65864207 99.03388117 98.39061415
 98.76219832 97.78755132 93.46864682 97.08458718 96.76539029 97.6596289
 98.16888196 98.76707155 97.73638235 97.52196002 98.89255735 97.65231905
 98.70006457 95.986891   99.39328225 98.34431842 99.81603538 97.48297414
 99.25683167 96.17451054 99.22881057 99.3871907  99.75755656 99.64181723
 99.87938743 99.20322608]
Accuracy th:0.7 is [84.40199315 97.3806362  92.83512628 98.05679755 98.5087901  97.40622068
 98.20786784 95.19011708 97.92278359 96.78488323 98.61965619 98.88646581
 99.43957798 95.33753244 97.43058686 97.32093907 96.42426384 97.60845994
 98.83042361 98.36502967 98.47589576 99.23855704 99.48831033 99.00464176
 95.23763112 96.74224242 94.34461081 96.95910138 98.0214666  98.24076217
 97.80460764 98.58919847 96.7946297  98.32726209 98.55386752 98.74270538
 97.59871347 98.17253688 98.85478978 93.17137949 98.23954387 97.10408012
 99.18007822 98.80483912 99.08261352 98.45762113 98.23588894 98.69519134
 98.11040314 98.75732508 98.65986038 98.61234634 99.0107333  98.49782532
 98.74026876 97.73394574 92.77299253 96.83605219 96.63137632 97.45129811
 98.11283976 98.65864207 97.53779803 97.37941789 98.84138838 97.56947406
 98.63183928 95.9588699  99.27876122 98.31995224 99.81603538 97.38794605
 99.17885991 96.09775709 99.24221196 99.42495827 99.73928193 99.62963414
 99.86964096 99.17033175]
Avg Prec: is [96.38701662 36.19243144 72.53143594 67.1803952  78.34713857 64.66953185
 75.37722877 47.09248088 56.97094197 53.42216745 35.01239088 53.62510898
 25.35393176 28.61142062 33.33161932 57.96270401 30.06324187 43.67578686
 49.29460059 40.62118487 60.66400478 52.59845103 90.58814482 83.45793255
 25.89020262 35.18275414 38.07466725 41.56137704 26.41437148 40.66614581
 73.92986475 38.23312098 57.57113339 62.33046332 72.26053902 79.3128978
 58.46754405 73.43656006 86.83086129 47.14057971 56.05919951 91.56178736
 93.74651219 91.89305512 93.7759726  94.81453023 41.87250416 36.03433656
 44.35024837 47.97715273 63.64485287 40.21920229 28.81354722 74.34071291
 29.29439211 47.63749771 75.11408491 59.70647501 47.60260655 61.20882192
 97.07296159 83.77628293 75.88671003 53.56838852 62.67272911 45.06300741
 63.05072816 26.54353945 85.87625539 69.09661404 13.93762747 73.9009374
 88.89110464 53.53500321 95.31155207 96.14018831 91.06788989 94.94151385
 57.43459478 31.59730051]
Accuracy th:0.5 is [45.52454283 97.2137279  70.06493586 97.02489005 97.26733349 74.59338946
 74.92111451 75.03441722 75.81778974 96.46324972 76.08216274 98.52097319
 99.41399349 80.05750417 75.73128982 96.56680596 96.29512311 75.47179006
 98.65376884 98.30776915 80.00633521 76.60238058 98.38695922 85.44242882
 85.67147086 96.65086926 94.0778012  75.26589588 98.01293844 76.03343039
 97.30875598 98.57457877 96.36213009 98.02024829 88.64292589 75.71788843
 75.48031822 91.72524701 97.11504489 72.81831362 78.59918861 92.05906361
 74.84436106 74.53247402 96.9627563  93.87434364 98.02877645 98.57336046
 97.89232587 91.33538821 91.33051498 98.55508583 98.99976852 75.01857921
 98.70615611 75.45229712 70.41337216 93.52834395 96.24273583 96.9067141
 89.79300934 97.17717864 95.50687735 75.42914926 98.42838172 75.97373326
 98.20786784 74.59704438 76.54755668 97.55972759 77.00685908 95.99054592
 76.57314117 95.45083515 74.83461459 84.30940169 91.68260621 75.98835297
 77.06290128 99.14718388]
Accuracy th:0.7 is [45.6183526  97.2137279  70.06493586 97.02489005 97.26733349 74.60679085
 74.92111451 75.35848735 75.81778974 96.4754328  76.08216274 98.52097319
 99.41399349 80.54482767 75.73616306 96.56680596 96.29512311 75.47179006
 98.65376884 98.30776915 80.396194   76.60238058 98.38695922 86.4840828
 86.76794873 96.65086926 94.0778012  75.26589588 98.01293844 76.03343039
 97.30875598 98.57457877 96.36213009 98.02024829 88.86953132 75.71788843
 75.48031822 92.03347912 97.11504489 72.81831362 79.24611055 92.05906361
 74.84436106 74.53247402 96.9627563  93.87434364 98.02877645 98.57336046
 97.98613565 91.88606377 91.67164143 98.55508583 98.99976852 75.01857921
 98.70615611 75.47422668 70.41337216 94.03759701 96.24273583 96.9067141
 89.79300934 97.17717864 95.56048294 75.4352408  98.42838172 75.97373326
 98.20786784 74.59704438 76.54755668 97.55972759 77.10554209 95.99054592
 76.89111975 95.45083515 74.83461459 84.5201691  92.23693668 75.98835297
 77.06290128 99.14718388]
Avg Prec: is [55.95301268  3.21932143 11.32999542  3.39286887  2.22473657  3.83893386
  3.40468813  5.57752827  2.4952195   4.00303262  1.56276982  1.61542738
  0.61857805  5.09631375  2.73632623  3.09507611  3.77873956  2.6761127
  1.48393704  1.82817209  2.00263506  0.82806495  1.97258056  2.47325322
  5.26998523  3.67777104  6.61841761  3.36508885  2.03716943  1.90199154
  2.65167301  1.33949362  3.6275905   1.60576135  2.31826522  2.33592913
  3.01442274  2.50893232  2.72717015  7.45167279  2.38874271  8.08919887
  3.43903176  4.0641313   3.36600808  6.44884539  2.16501355  1.5087486
  2.07772239  1.56141372  1.8810855   1.68908698  1.0320517   3.03992918
  1.34722697  2.69498398 11.33078829  3.77339927  3.93623818  2.76693276
 11.04019821  2.11383785  3.86522542  3.1107901   1.61833864  2.52329563
  1.79547111  4.21950245  1.2191572   2.35523556  0.18689345  3.29511242
  1.92599338  4.49279616  3.83945929  3.16336189  0.81058802  1.90409276
  0.13044311  0.72477451]
mAP score regular 59.62, mAP score EMA 3.78
starting validation
Accuracy th:0.5 is [87.78433864 97.44624661 93.17338117 98.21112689 98.92119491 97.68542741
 98.5549493  94.97969455 98.16378902 97.07750953 98.7193861  99.01088771
 99.33726985 95.29112789 97.46617834 97.3640282  96.42474525 97.76515435
 98.99095598 98.4129357  98.83399357 99.33477838 99.63375439 99.28494905
 95.40324389 96.80593966 94.36430226 97.2967586  97.86481302 98.25099036
 98.39549543 98.61972743 97.03266313 98.6595909  98.90873757 99.14791838
 97.86481302 98.26593916 99.08563171 93.32037771 97.90467648 92.63771582
 97.13481326 95.83177617 96.74614446 94.07778359 98.4054613  98.7418093
 98.09901089 98.66208237 98.87385704 98.67703117 98.91122904 98.33071729
 98.77668984 97.67546154 90.83887685 97.18962553 96.40730498 97.69041034
 92.77474649 98.8090789  97.31419887 97.55088821 98.76672397 97.52846501
 98.63218477 95.81931883 98.76174104 98.27839649 99.81563146 97.56583701
 98.45529063 95.83925057 97.29426714 97.15225353 99.20273065 98.6745397
 99.8206144  99.18030745]
Accuracy th:0.7 is [86.36420261 97.42631487 93.03136757 98.35064903 98.86139971 97.51849914
 98.4278845  95.44559882 98.13139996 96.82836286 98.6595909  98.95109251
 99.38211625 95.13665695 97.4014002  97.3191818  96.39733911 97.67546154
 98.96853278 98.38303809 98.7567581  99.27000025 99.57395919 99.30488078
 95.45805616 96.68136632 94.5486708  97.117373   97.82993248 98.21860129
 98.11146822 98.69197997 96.82337992 98.5101029  98.80658744 99.09559758
 97.90218502 98.12143409 99.00590478 93.24065077 98.00184369 92.88437103
 97.3640282  96.19303884 96.83832872 94.384234   98.40296983 98.8165533
 98.07658769 98.7567581  98.75177517 98.6446421  98.89378877 98.44532476
 98.7866557  97.83491541 91.05065152 97.1846426  96.44218551 97.51351621
 92.8868625  98.78167277 97.20208287 97.44126367 98.7941301  97.58576874
 98.61225303 95.80437003 98.79662157 98.24102449 99.81563146 97.61815781
 98.48269676 95.9563495  97.42133194 97.3715026  99.22764531 98.69197997
 99.82310586 99.17532451]
Avg Prec: is [96.34278593 34.67416359 70.71984307 73.29338398 76.93677519 65.22302827
 81.01704799 48.6558133  62.18587807 55.53671562 39.75064344 56.56769769
 23.819491   31.26861741 34.11079311 63.28961092 32.63841574 45.25300006
 51.71115855 37.73238171 67.94326666 58.12529701 92.2207871  89.03679338
 24.99821134 37.28111593 33.40552029 46.39002006 27.84987974 38.84408689
 77.18934468 35.81955209 55.19387513 64.08534343 73.7863444  82.41508667
 58.77904848 76.2899542  89.16192921 44.7665155  37.01523471 47.43222759
 49.75852569 37.15773251 25.81979215 47.42127585 40.82594861 27.76438425
 41.79077269 42.53448861 68.93342093 37.92747349 27.24594525 76.84173889
 32.62355032 42.87349967 56.34662716 57.85664554 38.79844201 63.86486814
 63.9042115  86.60280633 66.38996544 54.56928537 62.01480437 39.28631403
 62.15697916 26.69908943 36.4810046  65.6178812  13.17203563 74.06561161
 56.57661564 45.37150344 64.09097971 49.93946516 10.7275279  52.11811667
  1.63701686 23.69972413]
Accuracy th:0.5 is [45.46179336 97.22450607 67.78284376 96.96290206 97.90716795 72.94516282
 73.06226175 72.66861001 74.58454792 96.41976231 74.64434312 98.5325261
 99.34972718 77.17816479 74.76144206 96.31262924 96.21047911 74.02895084
 98.78167277 98.34068316 77.71881307 75.27966714 98.31327703 87.03689862
 79.91628672 96.52938685 94.3393876  74.18093031 97.81747515 74.63686872
 97.52597354 98.67204823 96.39983058 98.18870369 89.81239256 74.29802925
 74.30052072 92.88187956 97.0276802  71.70441239 75.98973516 92.37362035
 73.39611829 73.18434362 97.03764606 94.02795426 98.18621222 98.77668984
 97.95201435 92.02232354 91.91768194 98.55993223 98.87385704 73.38615243
 98.6969629  74.10369485 68.72960112 94.40416573 96.16314124 96.78102499
 90.13379176 97.04761193 95.61751003 74.30301218 98.32075143 75.1102474
 98.13139996 73.4559135  75.36188554 97.53593941 75.69075915 96.07843137
 75.31703914 95.44559882 73.32884869 85.85843486 93.55208411 74.73901886
 75.76550315 99.15040985]
Accuracy th:0.7 is [45.68851683 97.22450607 67.78284376 96.96290206 97.90716795 72.94765428
 73.06226175 72.89035055 74.58454792 96.41976231 74.64434312 98.5325261
 99.34972718 77.574308   74.76642499 96.31262924 96.21047911 74.02895084
 98.78167277 98.34068316 77.9629768  75.27966714 98.31327703 87.85908264
 80.86553554 96.52938685 94.3393876  74.18093031 97.81747515 74.63686872
 97.52597354 98.67204823 96.39983058 98.18870369 89.99925256 74.29802925
 74.30052072 93.0811969  97.0276802  71.70441239 76.4955029  92.37362035
 73.39611829 73.18434362 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 92.40850088 92.26897875 98.55993223 98.87385704 73.38615243
 98.6969629  74.11615218 68.72960112 94.80778334 96.16314124 96.78102499
 90.13379176 97.04761193 95.66983083 74.31546952 98.32075143 75.1102474
 98.13139996 73.4559135  75.36188554 97.53593941 75.69574208 96.07843137
 75.46652714 95.44559882 73.32884869 86.11256447 94.01300546 74.73901886
 75.76550315 99.15040985]
Avg Prec: is [54.35736639  3.74605146 14.93369974  4.5768023   1.57108202  4.28549325
  9.45722229  8.72103023  6.8093343   5.13712048  2.28511746  5.3193696
  1.55356182  5.82375249  2.97591507  4.06332214 24.22170167  5.72893034
  1.58562255  4.01533561  3.70292989  1.42752691  1.75922003  5.20387208
  5.7343591  13.94729564  8.29210978  4.65436729  3.85845275  6.85716798
  2.29645831  0.8657849   3.15091587  1.09097255  1.70497801  2.40958322
  2.02588844  2.18373279  2.26435186  6.23014377  1.74147173  6.03846219
  2.2378905   2.73975221  2.37873276  4.87234558  1.76932548  1.04256703
  1.39871416  1.18499934  1.20635399  0.99072994  0.74271501  2.34301886
  0.86222232  1.85236066 10.20121764  3.02417528  3.80858679  2.71051843
  7.92959693  2.02871857  3.23081915  2.59586138  1.37142855  1.85907673
  1.57082298  3.47890371  1.06630211  2.17849989  0.19028507  3.12509649
  1.55583154  3.89768526  3.51858669  2.30624929  0.59485318  1.47319117
  0.12724643  0.57652771]
mAP score regular 51.38, mAP score EMA 4.36
Train_data_mAP: current_mAP = 59.62, highest_mAP = 59.62
Val_data_mAP: current_mAP = 51.38, highest_mAP = 51.91
tensor([1.2180e-02, 5.6856e-04, 1.4069e-02, 6.5926e-04, 3.0848e-04, 8.7864e-04,
        1.5431e-04, 5.0340e-04, 1.8684e-03, 4.6363e-04, 1.4250e-04, 5.2514e-04,
        8.0273e-05, 1.4167e-03, 1.5285e-03, 1.5852e-03, 2.0533e-03, 3.4192e-03,
        1.6048e-03, 2.1089e-03, 2.4756e-05, 1.9376e-04, 4.8271e-04, 3.1924e-04,
        1.9122e-03, 3.2209e-04, 7.7167e-03, 2.3590e-04, 1.1425e-04, 5.5893e-04,
        6.3442e-04, 1.5529e-04, 6.6991e-03, 7.0071e-05, 6.1809e-04, 1.1666e-02,
        2.3345e-04, 7.2067e-04, 2.3586e-03, 5.6430e-03, 1.7763e-01, 5.6777e-01,
        9.4671e-01, 8.9120e-01, 9.9466e-01, 9.5129e-01, 2.3766e-04, 4.8547e-04,
        8.3864e-04, 7.4692e-04, 1.3166e-04, 3.3018e-04, 5.3092e-04, 8.0455e-04,
        1.9763e-04, 1.1936e-03, 6.2436e-02, 3.8356e-03, 4.8589e-02, 7.9561e-04,
        9.2768e-01, 8.4162e-04, 8.1487e-02, 4.5896e-03, 1.4822e-02, 2.0648e-03,
        2.2497e-02, 6.2988e-03, 6.1055e-01, 8.8600e-03, 2.5185e-04, 6.6420e-04,
        3.8939e-01, 4.6950e-02, 9.7907e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        6.0910e-01, 2.7676e-02], device='cuda:0')
Sum Train Loss:  tensor([5.0405e-01, 8.7272e-03, 2.4014e-01, 3.3785e-03, 4.5374e-04, 6.1647e-03,
        1.4900e-03, 6.2485e-03, 4.5022e-03, 2.0099e-03, 2.3943e-03, 1.2071e-03,
        1.0701e-04, 3.2833e-02, 2.9236e-02, 1.0942e-02, 5.4322e-02, 4.1570e-02,
        6.4165e-03, 3.4383e-02, 1.3742e-04, 6.7885e-04, 3.9518e-03, 2.5520e-03,
        3.0972e-02, 4.6820e-03, 1.6728e-01, 2.9087e-03, 8.9407e-04, 3.2322e-03,
        7.5877e-03, 3.8672e-04, 7.3895e-02, 2.6888e-04, 1.5063e-03, 6.3912e-02,
        1.1216e-03, 6.4645e-03, 1.8560e-02, 1.7063e-01, 1.1668e+00, 3.1214e+00,
        2.0713e+00, 2.6877e+00, 1.3530e+00, 4.1496e+00, 1.4819e-03, 3.1347e-03,
        5.0654e-03, 1.9810e-03, 1.8544e-03, 1.8708e-03, 2.5143e-03, 4.5236e-03,
        1.2886e-03, 8.2599e-03, 1.5955e+00, 2.1824e-02, 5.0232e-01, 8.0779e-03,
        5.2624e+00, 2.0127e-03, 4.7949e-01, 4.4661e-02, 6.9019e-02, 2.2045e-02,
        1.1960e-01, 1.2737e-01, 5.6400e-01, 1.4608e-02, 4.7561e-05, 2.8106e-03,
        2.5202e-01, 7.4721e-01, 4.0618e+00, 6.1490e-01, 7.0834e-02, 6.2905e-01,
        5.7323e-02, 1.7952e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [38/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 31.6
Sum Train Loss:  tensor([4.6290e-01, 1.1402e-02, 3.3044e-01, 6.5647e-03, 6.0170e-04, 6.1016e-03,
        7.8934e-04, 6.5018e-03, 1.6586e-02, 9.4781e-03, 1.4208e-03, 4.4604e-03,
        9.8477e-04, 2.0202e-02, 1.2646e-02, 6.8461e-03, 4.4714e-02, 4.2692e-02,
        4.3322e-03, 5.4513e-03, 1.2822e-04, 4.4350e-04, 3.3140e-04, 9.6217e-04,
        3.8202e-02, 4.6108e-03, 1.2906e-01, 2.4406e-03, 2.3668e-04, 1.9541e-03,
        1.1306e-03, 4.9733e-04, 3.2315e-02, 3.4772e-04, 2.6178e-03, 4.0691e-02,
        1.7573e-03, 3.9141e-03, 6.4080e-03, 1.0391e-01, 1.4393e+00, 5.0426e+00,
        1.1920e+01, 6.5849e+00, 2.8972e+00, 3.7506e+00, 9.1776e-04, 2.3360e-03,
        1.0950e-03, 4.9955e-03, 2.2762e-04, 1.2249e-03, 2.8639e-03, 1.1466e-03,
        8.3066e-04, 1.3472e-02, 1.9978e+00, 4.6696e-02, 6.6227e-01, 4.5304e-03,
        7.6054e+00, 2.0114e-03, 8.0027e-01, 5.5880e-02, 7.9001e-02, 2.2647e-02,
        1.2351e-01, 1.4913e-01, 4.5251e+00, 5.9663e-02, 5.7352e-05, 6.3323e-03,
        2.2945e-01, 7.1437e-01, 2.8763e+00, 3.0748e+00, 6.7544e-02, 6.1741e-01,
        8.3418e-02, 2.7107e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [38/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 56.9
Sum Train Loss:  tensor([4.3894e-01, 6.9106e-03, 3.2391e-01, 9.0329e-03, 6.5402e-04, 5.7393e-03,
        3.1568e-04, 4.8137e-03, 8.0940e-03, 4.4908e-03, 8.4844e-04, 2.9080e-03,
        5.7194e-04, 2.5768e-02, 1.9748e-02, 1.5456e-02, 3.4259e-02, 3.8356e-02,
        3.4375e-03, 3.6876e-03, 6.8731e-05, 4.3041e-04, 2.1120e-04, 7.1495e-04,
        2.4134e-02, 4.7819e-03, 1.2641e-01, 1.0291e-03, 1.1580e-03, 3.1019e-03,
        4.7430e-03, 1.3181e-04, 8.2941e-02, 1.3547e-04, 7.9357e-03, 5.3309e-02,
        1.2530e-03, 5.3670e-03, 6.6413e-03, 1.3438e-01, 1.3196e+00, 6.9202e+00,
        1.7790e+00, 2.4349e+00, 4.1709e+00, 1.1748e+01, 4.1201e-04, 1.9408e-03,
        6.2212e-03, 4.5909e-03, 2.2132e-04, 5.8821e-04, 4.6994e-03, 2.5045e-03,
        1.8144e-03, 1.2108e-02, 1.8025e+00, 4.7217e-02, 6.5413e-01, 4.8981e-03,
        4.1553e+00, 6.8335e-03, 9.0786e-01, 6.7093e-02, 1.3770e-01, 4.0086e-02,
        1.1318e-01, 1.6259e-01, 3.7146e-01, 1.8482e-02, 3.5346e-05, 5.1017e-03,
        4.9916e-01, 7.0112e-01, 1.8858e+00, 5.0103e+00, 3.3887e-01, 4.1021e+00,
        2.9319e-02, 1.1582e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [38/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 51.0
Sum Train Loss:  tensor([4.6632e-01, 9.4317e-03, 2.8293e-01, 5.7463e-03, 7.6047e-04, 9.1003e-03,
        3.0126e-04, 7.9664e-03, 8.0754e-03, 6.6038e-03, 8.6422e-04, 5.2215e-03,
        3.5361e-05, 1.8603e-02, 1.1733e-02, 1.7683e-02, 1.9807e-02, 1.3964e-02,
        1.5456e-02, 6.1249e-03, 1.7386e-04, 1.2230e-04, 1.4676e-03, 1.5131e-03,
        3.8161e-02, 1.4199e-03, 1.1634e-01, 3.3512e-03, 1.7060e-03, 6.6501e-03,
        2.7231e-03, 3.5529e-04, 9.4485e-02, 1.7782e-04, 1.3980e-03, 2.9555e-02,
        1.1738e-03, 3.2008e-03, 2.6227e-02, 1.4784e-01, 2.3267e+00, 8.2422e+00,
        5.0250e+00, 5.7663e+00, 7.6547e+00, 8.4171e+00, 3.7459e-03, 4.1891e-03,
        1.7054e-03, 1.0056e-02, 3.2716e-04, 9.7127e-04, 6.0777e-04, 1.3304e-03,
        1.3557e-03, 1.0947e-02, 1.4766e+00, 4.9710e-02, 5.0555e-01, 6.3411e-03,
        4.2849e+00, 5.3175e-03, 1.2193e+00, 4.0659e-02, 4.7445e-02, 3.6373e-02,
        2.1161e-01, 1.1744e-01, 1.5774e+00, 1.1099e-01, 3.6800e-05, 9.7865e-03,
        9.5027e-01, 1.0065e+00, 2.7805e+00, 3.4565e+00, 8.9641e-02, 1.3534e-01,
        1.0947e+00, 5.5773e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [38/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 58.1
Sum Train Loss:  tensor([4.2269e-01, 5.6191e-03, 3.5774e-01, 4.6498e-03, 1.8535e-03, 1.2221e-02,
        2.8582e-03, 6.1489e-03, 3.6337e-02, 1.8215e-03, 3.4333e-04, 3.3754e-03,
        2.0609e-04, 5.5020e-02, 9.4160e-03, 1.3646e-02, 3.4082e-02, 1.4684e-02,
        1.3507e-02, 5.0571e-03, 2.3909e-04, 7.6869e-04, 9.3133e-04, 2.9864e-03,
        3.8567e-02, 3.0263e-03, 1.2309e-01, 5.3838e-03, 1.6363e-03, 5.3311e-03,
        1.2924e-03, 2.8627e-04, 4.5440e-02, 5.4302e-04, 5.6879e-03, 1.2532e-02,
        2.4597e-03, 8.4619e-03, 1.8190e-02, 1.6334e-01, 1.2355e+00, 2.2682e+00,
        2.4786e+00, 4.0274e+00, 5.3572e+00, 2.9436e+00, 2.3533e-03, 2.5524e-03,
        5.1433e-03, 5.7045e-03, 1.5687e-03, 2.5408e-04, 6.6340e-03, 3.3122e-03,
        1.0654e-03, 2.8571e-03, 1.0158e+00, 6.6343e-02, 4.7763e-01, 4.7111e-03,
        5.2384e+00, 4.3971e-03, 1.6251e-01, 4.1053e-02, 3.9735e-02, 9.3229e-03,
        1.4270e-01, 1.0024e-01, 9.2016e-01, 3.4012e-02, 7.4725e-05, 5.5602e-03,
        1.8481e+00, 5.4524e-01, 9.0960e-01, 6.6712e-01, 3.5279e-01, 4.3173e+00,
        8.0156e-02, 1.5395e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [38/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 36.9
Sum Train Loss:  tensor([5.2634e-01, 7.7686e-03, 3.9518e-01, 3.6986e-03, 9.5490e-04, 1.0071e-02,
        2.3746e-03, 1.0081e-02, 2.0666e-02, 4.2388e-03, 8.9225e-04, 4.4857e-03,
        1.3117e-04, 3.2373e-02, 3.3854e-02, 9.2324e-03, 2.7220e-02, 4.2972e-02,
        4.8242e-03, 4.2301e-03, 6.6920e-05, 8.4190e-04, 3.7479e-04, 3.3365e-03,
        1.7835e-02, 5.3163e-03, 2.0991e-01, 2.3976e-03, 1.6594e-03, 3.3741e-03,
        1.0484e-03, 4.5445e-04, 4.9262e-02, 2.5859e-04, 3.0727e-03, 8.3294e-02,
        3.5990e-03, 1.8277e-03, 6.0779e-03, 1.1601e-01, 4.0894e-01, 7.2907e+00,
        1.7476e+00, 8.1533e+00, 5.2056e-01, 4.4888e+00, 1.8616e-03, 2.7913e-03,
        8.8480e-03, 4.9137e-03, 3.7980e-04, 3.0850e-03, 3.8813e-03, 5.0106e-03,
        7.1503e-04, 1.0882e-02, 8.3088e-01, 1.6982e-02, 6.6745e-01, 6.8057e-03,
        1.2374e+01, 4.9522e-03, 3.0438e-01, 3.1175e-02, 8.7813e-02, 8.4278e-03,
        2.6311e-01, 6.9143e-02, 2.4111e+00, 6.6364e-02, 1.9967e-05, 9.0261e-03,
        1.0819e+00, 4.5182e-01, 7.2964e+00, 1.5245e+00, 6.6305e+00, 5.1669e-01,
        2.0461e+00, 1.5347e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [38/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 61.2
Sum Train Loss:  tensor([5.8563e-01, 3.3996e-03, 5.1110e-01, 3.3933e-03, 2.3356e-03, 1.3425e-02,
        5.4989e-04, 9.7052e-03, 3.5865e-03, 3.7858e-03, 1.7372e-03, 5.9876e-03,
        2.4474e-04, 1.8817e-02, 1.6983e-02, 1.9464e-02, 1.9376e-02, 1.6532e-02,
        2.6240e-03, 7.0406e-03, 1.3422e-04, 2.0298e-04, 2.8017e-04, 2.5079e-04,
        1.8005e-02, 2.0990e-03, 1.3773e-01, 4.3683e-03, 1.5776e-03, 5.5718e-03,
        5.0242e-03, 4.7815e-04, 5.1331e-02, 4.7451e-04, 4.2121e-03, 7.8512e-02,
        1.5563e-03, 5.2451e-03, 5.8439e-03, 9.5691e-02, 5.7163e-01, 3.2143e+00,
        3.9377e+00, 7.7856e+00, 2.8782e+00, 5.1937e+00, 2.5118e-03, 2.4726e-03,
        5.3450e-03, 1.9731e-03, 7.0027e-04, 3.9658e-03, 3.7384e-03, 6.4469e-03,
        1.6122e-03, 6.0388e-03, 1.2874e+00, 1.8634e-02, 3.4703e-01, 3.4922e-03,
        2.6607e+00, 1.2765e-03, 4.6713e-01, 4.2400e-02, 1.1484e-01, 1.8038e-02,
        1.4599e-01, 1.6997e-01, 1.9759e+00, 1.0606e-01, 4.9348e-04, 3.1852e-03,
        6.3404e-01, 4.0816e-01, 2.4914e+00, 1.9916e+00, 5.2190e+00, 4.9926e-01,
        5.3337e-02, 2.6892e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [38/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 44.0
Sum_Val Meta Model:  tensor([6.1944e-01, 3.3713e-02, 1.1024e+00, 2.6689e-02, 3.3514e-04, 1.0734e-02,
        7.5929e-04, 9.1109e-03, 1.1225e-02, 4.2650e-03, 9.4902e-04, 6.2822e-03,
        5.5084e-04, 2.0576e-02, 1.1811e-02, 1.2163e-02, 9.7716e-01, 1.1722e-02,
        1.1322e-03, 2.6219e-03, 4.9378e-05, 1.1181e-04, 1.4971e-04, 1.3945e-04,
        4.9561e-02, 6.1623e-03, 2.2286e-01, 7.0912e-04, 1.1878e-03, 6.4257e-03,
        6.7077e-04, 1.2122e-04, 4.9746e-02, 8.6559e-05, 6.2218e-04, 1.6554e-02,
        7.8295e-04, 4.4104e-03, 3.3857e-03, 2.7883e-01, 1.7573e+00, 2.2168e+01,
        1.2322e+01, 3.0006e+01, 2.1582e+01, 1.8880e+01, 3.6435e-04, 2.9023e-03,
        7.2355e-04, 3.7470e-03, 6.8265e-05, 1.8793e-04, 3.7167e-03, 1.4452e-03,
        2.1597e-04, 1.5365e-03, 1.6320e+00, 2.7360e-02, 6.3504e-01, 2.6346e-03,
        1.7024e+01, 9.3035e-03, 4.6668e-01, 2.1933e-02, 6.5638e-03, 2.2062e-03,
        9.8297e-03, 4.0213e-02, 6.5430e+00, 1.5357e-01, 5.0909e-05, 1.2594e-02,
        7.2424e+00, 5.0408e-01, 3.4471e+01, 8.5232e+00, 3.3561e-01, 1.2156e+01,
        1.2678e-02, 1.6852e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.9944e-01, 2.2597e-02, 7.4292e-01, 1.6664e-02, 7.3151e-05, 1.0232e-02,
        6.1361e-04, 9.4267e-03, 1.2744e-02, 4.1406e-03, 1.2726e-03, 6.8987e-03,
        6.9806e-04, 1.9208e-02, 1.4149e-02, 4.4281e-02, 6.3565e-01, 2.0397e-02,
        1.0223e-03, 6.6059e-03, 4.5679e-05, 1.0590e-04, 8.6009e-05, 2.9429e-04,
        4.7340e-02, 5.6920e-03, 2.1934e-01, 9.8398e-04, 1.7364e-03, 7.8200e-03,
        1.9337e-04, 1.2166e-04, 4.4418e-02, 6.1849e-05, 6.0555e-04, 1.5912e-02,
        2.2514e-03, 2.8567e-03, 4.1292e-03, 2.7086e-01, 1.7401e+00, 2.6053e+01,
        1.1533e+01, 2.8978e+01, 2.7067e+01, 1.9916e+01, 2.1183e-04, 3.0645e-03,
        1.7755e-04, 2.9423e-03, 8.8212e-06, 4.7761e-05, 4.0769e-03, 3.2007e-04,
        1.1025e-04, 4.6603e-04, 1.6242e+00, 2.2287e-02, 5.8111e-01, 3.0938e-03,
        2.9575e+01, 4.9917e-03, 3.1042e-01, 2.9157e-02, 2.2767e-03, 1.6038e-03,
        4.2696e-03, 4.2902e-02, 9.2328e+00, 1.6807e-01, 5.6897e-05, 1.3464e-02,
        4.6711e+00, 5.4854e-01, 3.9381e+01, 1.7754e+01, 7.6329e-02, 1.3004e+01,
        2.2632e-02, 1.6319e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.9216e+01, 3.9743e+01, 5.2805e+01, 2.5277e+01, 2.3713e-01, 1.1646e+01,
        3.9764e+00, 1.8726e+01, 6.8207e+00, 8.9309e+00, 8.9302e+00, 1.3137e+01,
        8.6960e+00, 1.3558e+01, 9.2569e+00, 2.7934e+01, 3.0958e+02, 5.9654e+00,
        6.3700e-01, 3.1324e+00, 1.8452e+00, 5.4654e-01, 1.7818e-01, 9.2187e-01,
        2.4757e+01, 1.7672e+01, 2.8424e+01, 4.1712e+00, 1.5199e+01, 1.3991e+01,
        3.0479e-01, 7.8345e-01, 6.6304e+00, 8.8266e-01, 9.7972e-01, 1.3640e+00,
        9.6441e+00, 3.9640e+00, 1.7507e+00, 4.7999e+01, 9.7959e+00, 4.5886e+01,
        1.2182e+01, 3.2516e+01, 2.7213e+01, 2.0936e+01, 8.9129e-01, 6.3125e+00,
        2.1172e-01, 3.9392e+00, 6.7001e-02, 1.4465e-01, 7.6789e+00, 3.9782e-01,
        5.5787e-01, 3.9045e-01, 2.6014e+01, 5.8105e+00, 1.1960e+01, 3.8886e+00,
        3.1881e+01, 5.9310e+00, 3.8095e+00, 6.3529e+00, 1.5361e-01, 7.7673e-01,
        1.8979e-01, 6.8112e+00, 1.5122e+01, 1.8969e+01, 2.2592e-01, 2.0271e+01,
        1.1996e+01, 1.1684e+01, 4.0223e+01, 1.7755e+01, 7.6329e-02, 1.3004e+01,
        3.7156e-02, 5.8963e-01], device='cuda:0')
Outer loop valEpocw Maximum [38/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 200.1
model_train val_loss valEpocw [38/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 235.2
model_train val_loss valEpocw [38/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1222.9
Sum_Val Meta Model:  tensor([9.2725e-01, 1.0310e-02, 9.6930e-01, 6.3865e-03, 2.3462e-04, 3.5935e-02,
        1.1865e-02, 3.2185e-02, 6.5221e-02, 1.5959e-02, 3.0825e-03, 1.4797e-01,
        8.4069e-04, 9.1341e-03, 4.0484e-02, 5.1710e-02, 2.3040e-02, 6.6903e-02,
        1.3870e-02, 8.1106e-02, 2.2048e-06, 1.8194e-05, 1.3817e-04, 1.3689e-03,
        5.1828e-02, 1.2301e-02, 2.7624e-01, 3.9136e-03, 1.4544e-03, 4.6827e-05,
        8.6487e-05, 1.2525e-05, 9.6888e-04, 1.2574e-05, 5.0194e-05, 1.0206e-03,
        1.2997e-03, 1.1266e-04, 1.3936e-04, 1.1217e-02, 2.6980e-02, 2.1011e+00,
        9.5871e-02, 2.3664e-01, 2.9683e-01, 4.0259e+00, 2.2902e-04, 3.0709e-04,
        7.5099e-05, 5.1142e-03, 9.3250e-06, 6.6179e-05, 3.3209e-05, 3.3174e-05,
        2.9803e-05, 4.2790e-03, 8.4781e-01, 2.1806e-02, 3.8335e-01, 4.2614e-03,
        1.5819e+00, 7.0871e-04, 1.6170e-01, 9.1910e-03, 6.6251e-03, 1.7580e-03,
        1.3196e-02, 1.0245e-01, 8.0722e-02, 4.1630e-02, 1.1640e-05, 8.5910e-04,
        1.8692e+00, 2.9609e-01, 6.0963e+00, 3.2694e-01, 1.2146e-01, 6.1882e-01,
        2.9979e-02, 3.6791e-03], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([8.0192e-01, 1.1681e-02, 6.9897e-01, 6.7592e-03, 9.5806e-04, 2.9835e-02,
        9.7896e-03, 2.0063e-02, 4.8094e-02, 1.3446e-02, 2.4831e-03, 2.9667e-02,
        6.1461e-04, 1.1130e-02, 2.8730e-02, 6.7722e-03, 1.8882e-02, 7.0306e-02,
        4.9119e-03, 4.7558e-02, 3.1258e-05, 7.1901e-05, 1.3348e-04, 1.1630e-03,
        4.3482e-02, 6.5625e-03, 2.3227e-01, 3.7267e-03, 1.3568e-03, 3.7587e-04,
        1.4566e-04, 5.3962e-05, 3.9537e-03, 9.6741e-05, 2.9947e-04, 2.9436e-03,
        8.2621e-04, 7.3115e-04, 4.3503e-04, 6.0344e-03, 4.2065e-02, 2.5741e+00,
        3.3758e-02, 1.8330e-02, 2.8016e-03, 2.5073e+00, 1.0079e-04, 1.4633e-04,
        1.5005e-04, 4.6785e-03, 1.1085e-05, 4.7354e-05, 2.3462e-04, 4.4182e-04,
        7.9823e-05, 1.5705e-03, 6.0307e-01, 1.9457e-02, 2.9502e-01, 3.8649e-04,
        2.1010e+00, 7.8930e-04, 7.4085e-02, 1.2956e-03, 6.2520e-04, 1.7872e-04,
        9.2962e-04, 1.0806e-01, 6.9431e-02, 3.4137e-02, 3.8641e-06, 1.6027e-04,
        2.7904e+00, 1.2676e-01, 9.0300e+00, 8.7344e-03, 6.1614e-02, 5.6851e-02,
        2.0456e-04, 3.3048e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.7131e+01, 2.1331e+01, 4.7961e+01, 1.0188e+01, 3.2582e+00, 3.4200e+01,
        5.4774e+01, 3.8259e+01, 2.4726e+01, 2.9660e+01, 1.7359e+01, 5.2565e+01,
        7.3384e+00, 8.1993e+00, 1.6963e+01, 3.4025e+00, 9.1275e+00, 1.9732e+01,
        2.6433e+00, 1.7992e+01, 1.1978e+00, 3.6051e-01, 2.6766e-01, 3.5237e+00,
        2.3800e+01, 1.9099e+01, 2.9588e+01, 1.4727e+01, 1.1045e+01, 6.6255e-01,
        2.2686e-01, 3.3859e-01, 6.2265e-01, 1.3338e+00, 4.7106e-01, 2.9806e-01,
        3.4446e+00, 9.9085e-01, 1.9402e-01, 1.2461e+00, 3.1877e-01, 4.7014e+00,
        3.5756e-02, 2.0685e-02, 2.8183e-03, 2.6447e+00, 4.0031e-01, 2.8265e-01,
        1.7384e-01, 6.0033e+00, 8.0897e-02, 1.3945e-01, 4.3654e-01, 5.6505e-01,
        3.8129e-01, 1.3082e+00, 9.8360e+00, 4.6570e+00, 6.1800e+00, 4.5113e-01,
        2.2694e+00, 9.5716e-01, 8.7813e-01, 2.8333e-01, 4.4510e-02, 8.1589e-02,
        4.2188e-02, 1.9775e+01, 1.2093e-01, 4.2216e+00, 1.5412e-02, 2.4229e-01,
        8.4278e+00, 2.5115e+00, 9.2373e+00, 8.7344e-03, 6.1614e-02, 5.6851e-02,
        3.2294e-04, 1.3233e-01], device='cuda:0')
Outer loop valEpocw Maximum [38/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 22.3
model_train val_loss valEpocw [38/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 22.7
model_train val_loss valEpocw [38/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 688.2
Sum_Val Meta Model:  tensor([8.4744e-01, 1.2794e-03, 1.3865e-01, 1.1409e-03, 1.8421e-04, 1.7314e-03,
        1.1686e-04, 6.4010e-03, 3.7796e-03, 9.1759e-04, 1.1317e-04, 4.6372e-04,
        1.6936e-05, 2.4192e-02, 4.9968e-03, 6.4189e-03, 9.6907e-03, 1.7228e-02,
        1.7013e-03, 3.8806e-03, 1.6183e-05, 1.1187e-04, 6.0554e-03, 5.0073e-04,
        1.4392e-02, 1.1992e-03, 1.7908e-01, 1.8136e-03, 5.3311e-05, 2.4157e-03,
        2.3698e-03, 1.2475e-03, 5.2544e-02, 1.6834e-05, 2.0808e-03, 4.4839e-02,
        4.8670e-03, 9.6446e-03, 5.1937e-04, 1.3586e-01, 4.5847e+00, 5.1580e+01,
        7.2854e+01, 7.3691e+01, 5.0200e+01, 7.4496e+01, 4.3656e-03, 8.2917e-03,
        3.1489e-02, 1.5532e-02, 4.3146e-03, 1.3811e-02, 4.9988e-02, 1.1432e-02,
        1.7666e-02, 1.5958e-01, 1.1449e+01, 3.9710e-02, 3.5252e-01, 1.5468e-03,
        1.1178e+02, 1.0064e-03, 9.2817e-01, 5.7899e-02, 1.9646e-01, 1.5044e-03,
        1.8640e-01, 6.0903e-02, 3.1763e+00, 9.2964e-03, 4.1351e-05, 4.5577e-03,
        1.3873e+00, 1.8921e+00, 1.1211e+00, 3.1462e+01, 1.2164e+01, 5.8494e-01,
        5.7390e-02, 1.0232e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([2.4466e-01, 4.0541e-04, 7.4967e-02, 3.2429e-04, 5.6381e-06, 8.5093e-05,
        1.1488e-05, 6.7166e-03, 4.8964e-04, 8.7818e-05, 2.8332e-05, 5.6066e-05,
        8.5261e-06, 2.0433e-02, 5.1284e-04, 1.4286e-03, 3.4175e-03, 7.3501e-04,
        9.8293e-05, 1.1123e-04, 3.8412e-06, 8.7512e-06, 7.4987e-06, 1.2489e-05,
        9.3178e-03, 4.2301e-04, 1.6050e-01, 1.2333e-03, 1.0116e-04, 1.2018e-04,
        1.8374e-04, 1.2098e-03, 6.5971e-02, 8.0072e-06, 6.1617e-04, 1.2479e-02,
        2.4307e-03, 8.0985e-03, 8.6056e-05, 1.6348e-01, 4.5079e+00, 5.6510e+01,
        8.3646e+01, 9.3183e+01, 1.2266e+02, 8.3383e+01, 1.9785e-03, 2.8864e-03,
        3.1398e-02, 7.3648e-03, 2.3864e-03, 1.6950e-02, 2.9108e-02, 1.8380e-02,
        9.9990e-03, 8.8326e-02, 7.8753e+00, 4.6265e-02, 3.3883e-01, 9.2790e-04,
        1.8797e+02, 3.6095e-04, 7.1593e-01, 3.8217e-02, 1.7356e-01, 4.3241e-03,
        1.9430e-01, 6.9109e-02, 3.9478e+00, 3.8163e-02, 2.1796e-05, 4.8629e-03,
        2.4054e+00, 1.7603e+00, 2.4925e-01, 2.8953e+01, 2.1405e+01, 2.6494e+00,
        1.0555e-02, 1.1040e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.2598e+01, 8.7909e-01, 5.7600e+00, 5.7581e-01, 2.3296e-02, 1.1421e-01,
        8.9139e-02, 1.6177e+01, 3.0362e-01, 2.3238e-01, 2.4452e-01, 1.2064e-01,
        1.3692e-01, 1.6735e+01, 3.9274e-01, 9.9531e-01, 1.9388e+00, 2.1321e-01,
        7.0236e-02, 5.8740e-02, 2.1443e-01, 5.5894e-02, 1.6236e-02, 4.6883e-02,
        5.6345e+00, 1.4880e+00, 2.5642e+01, 6.4279e+00, 1.1769e+00, 2.3556e-01,
        3.2289e-01, 8.6854e+00, 1.0518e+01, 1.3987e-01, 1.0888e+00, 1.1052e+00,
        1.1692e+01, 1.2255e+01, 4.4302e-02, 3.7836e+01, 3.1630e+01, 9.7795e+01,
        8.7546e+01, 1.0242e+02, 1.2325e+02, 8.7315e+01, 9.3020e+00, 6.0714e+00,
        3.7890e+01, 1.0220e+01, 1.9741e+01, 5.5463e+01, 5.8553e+01, 2.7708e+01,
        5.8868e+01, 8.0669e+01, 1.4492e+02, 1.3523e+01, 8.0758e+00, 1.3158e+00,
        2.0105e+02, 5.3024e-01, 9.5864e+00, 1.0068e+01, 1.4263e+01, 2.3770e+00,
        1.0209e+01, 1.4462e+01, 7.1510e+00, 5.5040e+00, 1.0852e-01, 8.8711e+00,
        7.6012e+00, 4.2183e+01, 2.5464e-01, 2.8954e+01, 2.1405e+01, 2.6494e+00,
        1.7613e-02, 4.8998e-01], device='cuda:0')
Outer loop valEpocw Maximum [38/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 506.2
model_train val_loss valEpocw [38/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 703.7
model_train val_loss valEpocw [38/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1642.3
Sum_Val Meta Model:  tensor([3.0201e+00, 5.0834e-03, 7.0174e-01, 3.5948e-03, 1.0006e-03, 3.4879e-02,
        7.0085e-03, 3.6283e-02, 9.4893e-03, 6.1997e-02, 3.3061e-03, 1.2758e-02,
        1.2002e-04, 7.1170e-02, 7.6921e-02, 1.4544e-02, 1.3180e-02, 2.0720e-02,
        3.2579e-03, 7.3224e-03, 1.8807e-04, 4.5896e-04, 1.2947e-03, 1.4148e-03,
        1.1610e-01, 5.2729e-03, 5.5497e-01, 1.8293e-03, 1.2768e-02, 1.3960e-03,
        3.2907e-03, 4.3899e-04, 8.3621e-02, 1.5233e-02, 2.9541e-02, 1.4898e-01,
        6.5969e-04, 4.8526e-03, 4.2134e-02, 2.4394e-01, 2.2646e+00, 2.0299e+01,
        1.1991e+01, 1.1857e+01, 1.1656e+01, 1.8194e+01, 1.1720e-03, 1.2673e-03,
        4.7659e-03, 2.0194e-03, 5.5758e-04, 8.3732e-04, 1.1626e-03, 4.8622e-02,
        1.5449e-03, 8.6680e-03, 3.1355e+00, 3.9347e-02, 1.1047e+00, 6.2585e-03,
        5.1692e+01, 3.2082e-03, 6.0523e-01, 1.4416e-01, 2.1457e-01, 1.1020e-02,
        3.0910e-01, 5.4764e-01, 6.8127e-01, 5.6569e-02, 1.6049e-04, 4.6900e-03,
        1.4422e+00, 9.8675e-01, 4.4942e+02, 1.6854e+01, 1.4126e+00, 9.3012e+00,
        3.1320e-02, 4.2291e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.1203e+00, 4.8664e-03, 4.8728e-01, 4.0874e-03, 1.0376e-03, 3.4550e-02,
        7.6165e-03, 3.2419e-02, 1.6367e-02, 3.7550e-02, 3.0719e-03, 2.2642e-02,
        6.3759e-04, 7.5389e-02, 1.0319e-01, 1.3704e-03, 3.4367e-03, 8.9306e-03,
        2.0511e-04, 6.4413e-04, 8.6778e-05, 1.2360e-04, 1.8814e-04, 7.3240e-04,
        1.0749e-01, 3.4466e-03, 4.3529e-01, 1.9630e-03, 1.3793e-02, 2.1685e-04,
        2.2747e-04, 1.8973e-04, 2.2740e-03, 7.4551e-05, 3.5174e-04, 3.3934e-03,
        8.7328e-04, 2.7566e-04, 3.5247e-04, 3.0294e-02, 2.7564e-01, 1.8401e+00,
        6.7987e-02, 4.0716e-02, 3.4919e-03, 2.0498e+00, 2.1956e-04, 2.9780e-04,
        2.7074e-04, 6.1859e-04, 3.4420e-05, 7.0649e-05, 1.7168e-04, 1.5467e-03,
        3.4196e-04, 1.2492e-03, 5.0608e-01, 4.2688e-03, 8.8061e-01, 1.6442e-03,
        1.4231e+01, 2.0635e-03, 3.2560e-02, 6.0427e-03, 3.7012e-03, 2.4823e-03,
        5.8483e-03, 1.1601e-01, 5.0206e-02, 1.7731e-02, 4.3800e-05, 1.3523e-03,
        1.4744e-01, 4.9446e-01, 6.6479e+01, 4.0666e+00, 3.8954e-03, 9.1655e+00,
        7.8401e-04, 1.7443e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.5984e+01, 2.2256e+00, 1.7815e+01, 1.5196e+00, 7.1133e-01, 1.0347e+01,
        8.0116e+00, 1.4797e+01, 2.9223e+00, 1.8791e+01, 3.8166e+00, 9.5390e+00,
        1.2736e+00, 1.6691e+01, 2.1269e+01, 2.8488e-01, 6.0096e-01, 8.2567e-01,
        3.7789e-02, 9.9462e-02, 4.0780e-01, 1.1713e-01, 9.3611e-02, 5.1982e-01,
        2.3930e+01, 2.0428e+00, 2.5555e+01, 1.4768e+00, 1.9501e+01, 9.3707e-02,
        9.5495e-02, 2.2071e-01, 1.2217e-01, 1.3970e-01, 1.2553e-01, 1.3099e-01,
        7.1777e-01, 9.9250e-02, 4.1847e-02, 2.1715e+00, 1.3809e+00, 3.1874e+00,
        7.5321e-02, 4.9063e-02, 3.5525e-03, 2.2567e+00, 1.9477e-01, 1.4336e-01,
        7.5167e-02, 2.1252e-01, 4.7343e-02, 5.4134e-02, 7.7404e-02, 4.5316e-01,
        3.5330e-01, 3.2640e-01, 4.6265e+00, 3.6187e-01, 1.3059e+01, 4.9032e-01,
        1.5881e+01, 6.0114e-01, 2.6543e-01, 4.5579e-01, 1.1287e-01, 3.2656e-01,
        1.2937e-01, 6.7511e+00, 9.5204e-02, 8.8418e-01, 3.4454e-02, 4.7137e-01,
        4.0649e-01, 5.9335e+00, 7.0310e+01, 4.0667e+00, 3.8954e-03, 9.1658e+00,
        1.5668e-03, 3.4833e-02], device='cuda:0')
Outer loop valEpocw Maximum [38/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 619.7
model_train val_loss valEpocw [38/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 103.1
model_train val_loss valEpocw [38/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 388.5
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.59518037 97.45495303 93.2566611  98.04339616 98.55874076 97.56094589
 98.3284804  95.24737759 97.96055116 97.060221   98.68788148 98.95712772
 99.4566343  95.35702538 97.48906568 97.39038267 96.44253847 97.72419927
 98.8986489  98.47711407 98.63549421 99.32383865 99.57602856 99.06433888
 95.22422972 96.87381976 94.47862477 97.03829144 98.08360035 98.27974805
 98.25050864 98.59041678 96.99686895 98.50757179 98.66473362 98.8572264
 97.67424861 98.39670569 98.79387434 93.39920323 98.21517769 97.37210804
 99.17033175 98.75245185 99.18373314 98.47589576 98.25172695 98.68422656
 98.15669887 98.7743814  98.83773346 98.6342759  99.04606425 98.45883944
 98.74514199 97.73760066 93.13361192 96.99808726 96.70325654 97.69617817
 97.80582595 98.73295891 97.48175583 97.48662906 98.80971236 97.58165714
 98.61478296 95.97592622 99.40302872 98.37599444 99.81603538 97.44642487
 99.22393733 96.0709543  99.23733873 99.43226813 99.8050706  99.62963414
 99.86598604 99.19469792]
Accuracy th:0.7 is [85.99432268 97.41353054 92.63776026 97.97273425 98.32604379 97.30022782
 98.09578343 95.03173694 97.9800441  96.9201155  98.63793082 98.92423338
 99.4286132  95.32534935 97.38794605 97.1857068  96.39502443 97.71810772
 98.81336728 98.42107187 98.50635348 99.31165556 99.55653562 98.9961136
 95.22910296 96.77513676 94.28613199 96.9907774  98.034868   98.19812137
 97.96055116 98.60990972 96.7946297  98.34431842 98.56605061 98.76098001
 97.47931921 98.19690306 98.54290274 93.01665428 98.27974805 97.05291115
 99.01804315 98.47955069 99.02169808 98.10309329 98.16279041 98.64158575
 98.07141726 98.73905045 98.73783214 98.58310693 99.02657131 98.43447326
 98.73417722 97.59384023 92.16383816 96.64599603 96.53756655 97.59262192
 97.19058004 98.64767729 97.13331953 97.36236157 98.70493781 97.43058686
 98.50026194 95.96008821 99.43470474 98.31995224 99.81603538 97.18936173
 99.25926828 95.81754608 99.19835285 99.37500761 99.78070443 99.59308488
 99.85989449 99.16911344]
Avg Prec: is [96.3623864  36.76996379 71.46100939 66.26572117 77.93434586 63.37414964
 74.36368031 46.90759681 55.30584718 52.7719234  35.92715624 54.30128091
 27.17508502 28.66232642 33.63119992 58.56491961 29.38058541 41.78784415
 51.68891154 40.79494132 57.90723416 50.39943218 90.66067261 83.39205641
 26.03882638 34.29139013 38.5513852  40.41850173 26.63028735 39.74335584
 73.59148079 38.89322733 58.00160846 60.84823341 71.57495441 78.96446319
 57.96746963 74.02877038 86.49651388 47.20941686 54.67597709 91.26549071
 93.69688682 91.71083203 94.46056796 94.90650614 40.67506974 36.25430658
 43.70771989 47.98096865 65.54004867 39.35290795 28.61463488 74.37509482
 28.8882165  46.74535497 74.75438597 59.43401371 47.53747137 60.96495883
 96.79529666 83.89100077 75.34884162 52.71654986 62.13236524 44.66085719
 62.64702681 26.61114123 85.31445336 68.56922166 12.73318248 72.49417537
 88.17934106 52.76093569 95.23080798 96.333978   92.66136015 95.00760849
 66.57632514 32.6044677 ]
Accuracy th:0.5 is [45.74140179 97.2137279  69.80909102 97.02489005 97.26733349 74.24738977
 74.62141056 74.80415687 75.50346609 96.45350325 75.78733203 98.52097319
 99.41399349 79.84795507 75.47300837 96.56680596 96.29512311 75.13310023
 98.65376884 98.30776915 79.87719448 76.21983163 98.38695922 85.8578721
 86.19290701 96.65086926 94.0778012  74.97350178 98.01293844 75.71910674
 97.30875598 98.57457877 96.36213009 98.02024829 88.51013024 75.34752257
 75.2122903  91.73133856 97.11504489 72.68429966 78.32141421 92.05906361
 74.68110769 74.28637565 96.9627563  93.87434364 98.02877645 98.57336046
 97.87526955 91.527881   91.89702854 98.55508583 98.99976852 74.71887526
 98.70615611 75.17939596 70.30128775 93.53930873 96.24273583 96.9067141
 89.79300934 97.17717864 95.58972235 75.30000853 98.42838172 75.73007151
 98.20786784 74.43135439 76.20399362 97.55972759 76.67669741 95.99054592
 76.30876817 95.45083515 74.55440358 84.40321146 91.92748626 75.70570534
 76.74370439 99.14718388]
Accuracy th:0.7 is [45.76576796 97.2137279  69.80909102 97.02489005 97.26733349 74.27541087
 74.62141056 75.12091714 75.50346609 96.4754328  75.78733203 98.52097319
 99.41399349 80.32309548 75.47544499 96.56680596 96.29512311 75.13310023
 98.65376884 98.30776915 80.22075754 76.21983163 98.38695922 86.78987829
 87.22725113 96.65086926 94.0778012  74.97350178 98.01293844 75.71910674
 97.30875598 98.57457877 96.36213009 98.02024829 88.74039059 75.34752257
 75.2122903  91.97621861 97.11504489 72.68429966 79.0024488  92.05906361
 74.68110769 74.28637565 96.9627563  93.87434364 98.02877645 98.57336046
 97.9800441  92.13825368 92.26373948 98.55508583 98.99976852 74.71887526
 98.70615611 75.22203677 70.30128775 94.00348436 96.24273583 96.9067141
 89.79300934 97.17717864 95.63236315 75.31219162 98.42838172 75.73007151
 98.20786784 74.43135439 76.20399362 97.55972759 76.79121843 95.99054592
 76.65476785 95.45083515 74.55440358 84.59204932 92.60486593 75.70570534
 76.74370439 99.14718388]
Avg Prec: is [55.76385466  3.09496726 11.38462462  3.35485299  2.286663    3.88694696
  3.33815972  5.63807209  2.43588869  3.88447213  1.56471773  1.59033854
  0.5874932   5.04904446  2.67190845  3.17771279  3.66642259  2.78697584
  1.29998583  1.79841204  2.00057148  0.8508684   1.72243718  2.45475803
  5.07462785  3.68986386  6.61542996  3.17182207  2.04838342  1.89446698
  2.59931267  1.31726258  3.63302327  1.67274448  2.37380994  2.41885049
  3.16979566  2.4961665   2.72057228  7.2240817   2.28936835  8.4123749
  3.37618006  4.15933274  3.28373208  6.4534306   2.05852861  1.53881253
  2.17746108  1.57939183  1.78917906  1.65778449  1.00250376  3.08742462
  1.36707698  2.69754143 11.31735698  3.64802711  3.92245964  2.89142231
 10.76925561  2.14723352  3.75823079  3.01211432  1.63250315  2.4735933
  1.82254242  4.21878256  1.2557036   2.38837783  0.24445809  3.36175844
  1.9137741   4.57668399  3.88997676  3.22746515  0.81943617  1.88963737
  0.13537273  0.79437174]
mAP score regular 59.47, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [87.81921917 97.42631487 92.88935396 98.27092209 98.90126317 97.64058101
 98.52754316 95.29361935 98.04669009 96.95044473 98.67952263 99.00092184
 99.37962479 95.23382415 97.46368687 97.38146847 96.41727085 97.71781648
 98.98846451 98.40047836 98.7941301  99.34225278 99.61382266 99.29491492
 95.38331216 96.86573486 94.458978   97.3117074  97.96447168 98.24600742
 98.5624237  98.59481277 96.98781673 98.7044373  98.87883997 99.15539278
 97.97443755 98.20116102 98.94112664 93.25061664 97.79505195 92.75481476
 97.29177567 96.40481351 96.85576899 94.66078681 98.41791863 98.83150211
 98.16628049 98.74430077 98.94860104 98.66457383 98.91621197 98.4727309
 98.77419837 97.81000075 91.13037845 97.21952313 96.50945512 97.64805541
 92.82457583 98.85890824 97.22450607 97.52597354 98.7866557  97.61068341
 98.57488103 95.8442335  98.67204823 98.27092209 99.81812293 97.62314074
 98.29085383 95.9862471  97.47365274 97.51600767 99.22266238 98.70692877
 99.82559733 99.17283305]
Accuracy th:0.7 is [87.45297357 97.43378927 92.94416623 98.26843063 98.81157037 97.46368687
 98.38303809 95.35839749 98.12143409 96.99778259 98.70692877 99.03580238
 99.37215038 95.13665695 97.39143434 97.20457433 96.41228791 97.79256048
 98.94860104 98.4278845  98.79662157 99.37215038 99.64621172 99.26252585
 95.48047936 96.73867006 94.49635    97.19709993 97.85733862 98.19119516
 98.32822583 98.68699704 96.91058126 98.55744077 98.83897651 99.05324264
 97.86232155 98.07658769 98.73184344 93.02389317 98.00682662 93.1185689
 97.41385754 96.50696365 96.92553006 94.73054787 98.36808929 98.84645091
 98.07409622 98.7567581  98.86887411 98.60477863 98.91870344 98.48269676
 98.77668984 97.74771408 90.99334778 97.0575778  96.42972818 97.64307248
 92.81460996 98.79911304 97.0575778  97.48611007 98.7044373  97.52099061
 98.44781623 95.80437003 98.8016045  98.26095622 99.81563146 97.43877221
 98.39798689 95.83925057 97.52099061 97.57580288 99.25006852 98.70942024
 99.82559733 99.17283305]
Avg Prec: is [96.32441885 35.60420269 69.85340245 72.78643909 77.27122691 64.60425411
 80.87057591 48.22140072 61.33666345 53.93569928 39.77227267 57.5619613
 24.88460677 30.85017614 33.86574016 63.16406961 33.47526303 44.52867219
 52.22071041 38.85009512 66.4109796  58.66341066 92.44371196 88.92056293
 25.19292386 38.67006225 32.90103021 47.3624071  28.70341958 38.50600403
 76.4476959  37.77112341 55.6299045  64.20309814 73.87944098 81.77865438
 59.40841325 75.33574561 88.80804336 44.50023011 38.783078   48.36456661
 49.71630044 36.62922731 27.9776723  48.43304165 39.99842413 29.34563283
 42.77492772 42.97786924 70.8138952  39.18013499 28.55674064 77.37253643
 31.76192644 43.33106412 57.20024735 57.47174959 40.15987476 63.99538702
 64.19670325 86.9568807  67.47626638 54.03562837 62.643574   39.4387727
 60.55648696 27.03270813 45.52560216 65.70300066 13.12894096 73.96417325
 55.63664265 46.02502668 64.90935189 51.5158801  12.59224143 53.03198341
  1.74915147 23.0856385 ]
Accuracy th:0.5 is [45.42691282 97.22450607 67.65827042 96.96290206 97.90716795 72.81062361
 72.94267135 72.58888308 74.45499165 96.41976231 74.50482099 98.5325261
 99.34972718 77.14577572 74.64683459 96.31262924 96.21047911 73.89441164
 98.78167277 98.34068316 77.6415776  75.15011087 98.31327703 87.35580636
 80.27007499 96.52938685 94.3393876  74.04639111 97.81747515 74.50232952
 97.52597354 98.67204823 96.39983058 98.18870369 89.82484989 74.15850711
 74.16598151 92.88935396 97.0276802  71.58980492 75.91499116 92.37362035
 73.25659616 73.04482149 97.03764606 94.02795426 98.18621222 98.77668984
 97.95201435 92.27645315 92.73488303 98.55993223 98.87385704 73.25161322
 98.6969629  73.96915564 68.62994245 94.3991828  96.16314124 96.78102499
 90.13379176 97.04761193 95.66983083 74.20584498 98.32075143 74.9757082
 98.13139996 73.33134016 75.23232927 97.53593941 75.55621995 96.07843137
 75.19495727 95.44559882 73.21922416 86.00293993 93.79624785 74.60946259
 75.62598102 99.15040985]
Accuracy th:0.7 is [45.66360216 97.22450607 67.65827042 96.96290206 97.90716795 72.81311508
 72.94267135 72.79816628 74.45499165 96.41976231 74.50482099 98.5325261
 99.34972718 77.50703839 74.66925779 96.31262924 96.21047911 73.89441164
 98.78167277 98.34068316 77.90816454 75.15011087 98.31327703 88.12816105
 81.27413608 96.52938685 94.3393876  74.04639111 97.81747515 74.50232952
 97.52597354 98.67204823 96.39983058 98.18870369 90.01420136 74.15850711
 74.16598151 93.09365423 97.0276802  71.58980492 76.4058101  92.37362035
 73.25659616 73.04482149 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 92.73239156 93.04631637 98.55993223 98.87385704 73.25161322
 98.6969629  73.98161298 68.62994245 94.81774921 96.16314124 96.78102499
 90.13379176 97.04761193 95.72713456 74.22577671 98.32075143 74.9757082
 98.13139996 73.33134016 75.23232927 97.53593941 75.56120288 96.07843137
 75.36935994 95.44559882 73.21922416 86.222189   94.25218626 74.60946259
 75.62598102 99.15040985]
Avg Prec: is [54.18268806  3.75974763 14.85537105  4.61045534  1.70599991  4.30389291
  9.2519561   8.70014292  6.84687519  5.11549825  2.27862481  5.30986713
  1.55161526  5.78419129  2.94838041  3.95270133 23.2733099   5.7301613
  1.5761657   4.38950913  3.70231758  1.44043815  1.80447554  5.17746049
  5.71577385 14.01725967  8.32631582  4.81260593  3.8807925   6.71524969
  2.29202327  0.88000578  3.01618284  1.15847677  1.83656831  2.4220602
  2.05344281  2.14340887  2.09294242  6.22134197  1.76850533  6.05119135
  2.20514428  2.7226641   2.36509038  4.87076053  1.7865085   1.02855035
  1.47472609  1.19556379  1.20998086  1.0197878   0.76316136  2.31144975
  0.91938629  1.90333755 10.21557929  3.02364243  3.93205349  2.68991262
  8.00313281  2.02586923  3.24788285  2.54843186  1.36617035  1.85888516
  1.53964371  3.5345361   1.05469538  2.15225205  0.19090187  3.11907241
  1.53492485  3.91734164  3.1913489   2.36624948  0.56954302  1.42352622
  0.1243682   0.5974274 ]
mAP score regular 51.74, mAP score EMA 4.35
Train_data_mAP: current_mAP = 59.47, highest_mAP = 59.62
Val_data_mAP: current_mAP = 51.74, highest_mAP = 51.91
tensor([1.0172e-02, 4.2265e-04, 1.2168e-02, 5.0878e-04, 2.3075e-04, 6.7353e-04,
        1.1953e-04, 3.9582e-04, 1.5037e-03, 3.5147e-04, 1.0877e-04, 4.2300e-04,
        5.9097e-05, 1.1404e-03, 1.2160e-03, 1.3049e-03, 1.6273e-03, 2.9054e-03,
        1.2690e-03, 1.6802e-03, 1.7595e-05, 1.4978e-04, 3.7252e-04, 2.5134e-04,
        1.4820e-03, 2.6714e-04, 6.7238e-03, 1.8271e-04, 8.2751e-05, 4.3138e-04,
        4.8298e-04, 1.1351e-04, 5.6265e-03, 5.2831e-05, 5.0917e-04, 1.0274e-02,
        1.6909e-04, 5.8214e-04, 1.8773e-03, 4.4801e-03, 1.7483e-01, 6.0518e-01,
        9.6221e-01, 9.1526e-01, 9.9615e-01, 9.6094e-01, 1.7266e-04, 3.8936e-04,
        6.7552e-04, 5.7826e-04, 1.0250e-04, 2.5234e-04, 4.2146e-04, 6.1601e-04,
        1.4192e-04, 9.5667e-04, 5.5753e-02, 3.2307e-03, 4.5850e-02, 6.4379e-04,
        9.4122e-01, 6.5089e-04, 7.9777e-02, 3.7441e-03, 1.3161e-02, 1.7771e-03,
        2.0408e-02, 5.4956e-03, 6.3214e-01, 7.6523e-03, 1.8866e-04, 5.2860e-04,
        3.9625e-01, 4.3101e-02, 9.8316e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        6.6229e-01, 2.3470e-02], device='cuda:0')
Sum Train Loss:  tensor([4.3255e-01, 4.1523e-03, 3.6879e-01, 8.1319e-03, 2.7297e-03, 4.6327e-03,
        3.0221e-04, 5.5340e-03, 1.4839e-02, 1.8403e-03, 1.2319e-03, 4.3998e-03,
        6.2736e-05, 2.0389e-02, 2.3226e-02, 3.4422e-02, 1.7849e-02, 1.2319e-02,
        7.9331e-03, 1.9248e-02, 1.5564e-04, 3.6014e-04, 2.5596e-04, 1.8047e-03,
        3.0878e-02, 2.0601e-03, 6.9465e-02, 1.1852e-03, 9.3362e-04, 2.6996e-03,
        1.2143e-03, 1.7481e-04, 4.2770e-02, 1.4342e-04, 5.8095e-03, 3.6639e-02,
        1.2096e-03, 3.0028e-03, 3.5801e-03, 1.1224e-01, 1.3713e+00, 7.7087e+00,
        3.9181e+00, 4.6788e+00, 9.2871e-01, 5.6397e+00, 1.2561e-03, 5.5581e-03,
        4.0325e-03, 1.3068e-03, 1.0025e-04, 1.2697e-03, 2.6553e-03, 3.8352e-03,
        4.5927e-04, 1.2630e-02, 1.4303e+00, 3.2453e-02, 5.0276e-01, 9.9588e-03,
        2.5344e+00, 4.0574e-03, 4.6495e-01, 4.1542e-02, 1.1031e-01, 2.7328e-02,
        1.0872e-01, 8.2425e-02, 1.4368e+00, 5.6716e-02, 6.4495e-04, 3.8130e-03,
        2.6015e+00, 8.5426e-01, 1.6522e+00, 4.1534e-01, 1.9044e-01, 2.7420e-01,
        2.8873e-02, 2.6602e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [39/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 38.5
Sum Train Loss:  tensor([3.7528e-01, 1.6816e-03, 2.5400e-01, 6.6278e-03, 1.2630e-03, 2.6931e-03,
        5.7407e-04, 5.7189e-03, 9.0915e-03, 3.4826e-03, 1.7501e-04, 1.8757e-03,
        2.4381e-04, 2.1220e-02, 9.0959e-03, 2.4017e-02, 2.8941e-02, 2.5643e-02,
        2.2359e-03, 2.3384e-02, 4.5401e-05, 5.0463e-05, 5.2155e-04, 5.5407e-04,
        3.4739e-02, 3.1592e-03, 9.4668e-02, 1.2173e-03, 6.1625e-04, 5.6208e-03,
        2.6818e-03, 1.3420e-03, 1.1481e-01, 2.2365e-04, 1.1972e-03, 9.2902e-02,
        1.7465e-03, 3.5684e-03, 1.3459e-02, 1.1481e-01, 3.0620e-01, 6.5608e+00,
        2.2218e+00, 1.0172e+00, 6.4102e-01, 4.7434e+00, 2.2087e-03, 2.0959e-03,
        2.6176e-03, 3.4019e-03, 5.8118e-04, 1.4440e-04, 3.2201e-03, 2.3372e-03,
        9.7629e-04, 5.8499e-03, 1.2849e+00, 3.8744e-02, 4.8116e-01, 4.9480e-03,
        4.0871e+00, 6.9930e-03, 4.6656e-01, 1.9212e-02, 1.9796e-02, 6.5371e-03,
        1.2728e-01, 1.1732e-01, 1.0256e+00, 3.9729e-02, 6.6698e-04, 8.3716e-03,
        7.9559e-01, 5.0101e-01, 2.5155e+00, 6.6641e-01, 7.6012e-02, 5.4645e-01,
        1.5033e-02, 2.6491e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [39/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 29.9
Sum Train Loss:  tensor([3.0713e-01, 2.1021e-03, 2.6943e-01, 3.8095e-03, 1.0956e-03, 5.4003e-03,
        2.8772e-04, 3.9944e-03, 1.0869e-02, 3.0880e-03, 1.3824e-03, 2.3296e-03,
        1.7844e-04, 2.3719e-02, 1.6290e-02, 1.7270e-02, 3.9093e-02, 2.3430e-02,
        3.7426e-03, 1.2970e-02, 1.1658e-04, 3.0491e-04, 1.6322e-03, 1.9611e-04,
        1.7173e-02, 2.6620e-03, 1.1759e-01, 1.7912e-03, 4.5224e-04, 5.1236e-03,
        4.2162e-03, 4.1387e-04, 4.7712e-02, 2.7315e-04, 2.4313e-03, 3.1152e-02,
        7.6259e-04, 4.3798e-03, 1.9086e-02, 6.8844e-02, 9.9450e-01, 4.9425e+00,
        3.1669e+00, 1.2934e+00, 1.6243e+00, 8.6527e+00, 2.0212e-03, 2.0353e-03,
        5.8324e-03, 5.0685e-04, 8.2316e-04, 3.4697e-03, 6.1209e-04, 2.0716e-03,
        1.1884e-03, 7.2025e-03, 5.6133e-01, 3.1455e-02, 3.9395e-01, 2.8392e-03,
        5.2650e+00, 4.5809e-03, 5.4974e-01, 5.6712e-02, 1.6826e-02, 1.4388e-02,
        1.3365e-01, 7.2803e-02, 6.1720e-01, 2.2280e-02, 1.9066e-05, 2.6932e-03,
        1.4684e+00, 3.5061e-01, 3.3443e+00, 2.6647e+00, 6.7348e-02, 2.2231e-01,
        4.1244e-02, 1.3747e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [39/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 37.7
Sum Train Loss:  tensor([3.3966e-01, 7.7608e-03, 2.9075e-01, 1.2157e-03, 4.1560e-04, 6.6634e-03,
        3.0512e-04, 7.3053e-03, 2.0989e-02, 4.1655e-03, 7.0474e-04, 1.4626e-03,
        2.5078e-04, 2.4337e-02, 1.2135e-02, 1.5445e-02, 1.5986e-02, 9.0855e-03,
        4.8515e-03, 1.5130e-03, 3.6691e-05, 2.7456e-04, 8.5763e-04, 3.6057e-04,
        2.8815e-02, 3.1463e-03, 1.3779e-01, 3.4038e-03, 9.0347e-04, 1.1890e-03,
        1.1790e-03, 1.0946e-03, 3.2694e-02, 4.4620e-04, 3.7326e-04, 8.9062e-03,
        1.9409e-03, 2.6001e-03, 1.9845e-03, 8.5169e-02, 1.3577e+00, 2.7443e+00,
        2.0835e+00, 1.7296e+00, 1.8353e+00, 5.3336e+00, 1.2549e-03, 4.9445e-04,
        4.1937e-03, 1.2217e-03, 1.8738e-04, 1.9601e-03, 3.0215e-03, 6.2253e-03,
        2.3492e-03, 1.0666e-02, 9.8886e-01, 4.7515e-02, 6.7700e-01, 2.7164e-03,
        4.2974e+00, 4.9117e-03, 8.5408e-01, 5.3798e-02, 5.0344e-02, 1.1325e-02,
        1.0617e-01, 7.4197e-02, 2.8081e+00, 3.6018e-02, 1.7960e-03, 8.6461e-03,
        2.2510e+00, 5.1073e-01, 6.1780e-01, 1.6610e+00, 1.5881e-01, 8.2014e-01,
        1.3284e-01, 3.0510e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [39/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 32.4
Sum Train Loss:  tensor([4.2058e-01, 1.2522e-02, 3.4607e-01, 1.8200e-03, 3.6959e-04, 5.9359e-03,
        8.9363e-04, 3.4339e-03, 7.8621e-03, 3.4309e-03, 2.4729e-04, 1.0397e-03,
        1.9610e-04, 3.1135e-02, 2.1382e-02, 1.2685e-02, 3.2515e-02, 1.6645e-02,
        1.0209e-02, 2.1658e-02, 2.5123e-05, 8.0656e-05, 3.3981e-03, 1.6154e-03,
        1.8290e-02, 6.8698e-03, 1.2872e-01, 1.0928e-03, 7.1885e-04, 7.7868e-03,
        5.1597e-03, 3.3053e-04, 6.3325e-02, 3.8444e-04, 2.1393e-03, 6.8569e-02,
        1.7959e-03, 2.7084e-03, 1.1954e-02, 7.0428e-02, 9.4868e-01, 6.2754e+00,
        1.7114e+00, 1.2699e+00, 5.4454e+00, 3.9134e+00, 2.1947e-03, 7.6598e-04,
        2.5779e-03, 1.2033e-03, 4.9606e-04, 3.5382e-04, 1.3089e-03, 2.4596e-03,
        5.2307e-04, 3.3120e-03, 1.2713e+00, 2.1302e-02, 5.7331e-01, 3.4936e-03,
        5.5643e+00, 6.5624e-04, 9.9430e-01, 5.6529e-02, 2.3036e-02, 1.9053e-02,
        3.6544e-02, 1.2294e-01, 1.2957e+00, 4.5930e-02, 4.1949e-05, 1.8888e-03,
        1.1983e+00, 4.2107e-01, 7.1429e+00, 6.3278e+00, 1.5351e-01, 4.9959e+00,
        5.0596e-02, 1.7565e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [39/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 51.3
Sum Train Loss:  tensor([4.4825e-01, 3.6607e-03, 3.1163e-01, 3.5228e-03, 8.7256e-04, 5.3183e-03,
        4.5363e-04, 7.9412e-03, 9.2118e-03, 2.3998e-03, 4.0499e-04, 6.5662e-04,
        3.3830e-05, 2.2511e-02, 1.5701e-02, 1.3163e-02, 1.9657e-02, 1.3227e-02,
        8.9689e-03, 8.4119e-03, 1.7834e-04, 3.6348e-04, 2.5570e-04, 8.5650e-04,
        3.8313e-02, 6.2001e-03, 2.3104e-01, 2.1123e-03, 1.2232e-03, 3.1862e-03,
        4.3936e-03, 1.3674e-03, 4.1898e-02, 1.4226e-04, 8.3065e-04, 2.1938e-02,
        1.5809e-03, 5.5323e-03, 7.1417e-03, 7.8655e-02, 3.1869e+00, 7.6041e+00,
        4.7030e+00, 7.5166e-01, 3.3332e+00, 2.5573e+00, 1.0999e-03, 2.1124e-03,
        1.4366e-03, 2.7749e-03, 1.3365e-04, 6.8249e-04, 2.6532e-03, 2.6285e-03,
        8.0379e-04, 1.9644e-02, 9.4700e-01, 2.4371e-02, 4.0275e-01, 7.3850e-03,
        8.8207e+00, 1.7648e-03, 7.2668e-01, 1.8560e-02, 1.1149e-01, 1.3411e-02,
        1.8731e-01, 8.8600e-02, 2.0930e+00, 4.2762e-02, 1.0042e-03, 4.5047e-03,
        1.4708e+00, 3.1570e-01, 6.0419e+00, 3.7092e+00, 1.6446e-01, 4.4140e-01,
        3.6562e+00, 1.1196e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [39/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 52.9
Sum Train Loss:  tensor([3.3725e-01, 4.4318e-03, 3.8606e-01, 2.5742e-03, 9.3244e-04, 1.0027e-02,
        7.9618e-04, 4.5557e-03, 9.4463e-03, 2.5579e-03, 1.6828e-03, 1.3907e-03,
        6.0193e-05, 2.5826e-02, 1.1829e-02, 1.2538e-02, 3.2329e-02, 3.6835e-02,
        2.2375e-02, 1.0020e-02, 1.0092e-04, 9.6530e-05, 1.8634e-03, 2.2486e-03,
        4.6852e-02, 5.2970e-03, 1.2543e-01, 2.1107e-03, 4.1381e-04, 6.5271e-03,
        1.8620e-03, 5.8822e-04, 4.6776e-02, 4.3440e-04, 2.3448e-03, 8.3675e-02,
        1.2278e-03, 5.6666e-03, 3.9228e-03, 4.7956e-02, 4.3617e-01, 3.7251e+00,
        4.6069e+00, 2.6125e+00, 7.7211e-01, 1.8927e+00, 4.2298e-04, 4.0575e-04,
        1.4366e-03, 7.0288e-04, 2.5690e-04, 3.4662e-04, 1.3861e-03, 1.6137e-03,
        3.7885e-04, 1.5568e-03, 1.1157e+00, 2.7154e-02, 9.5910e-01, 2.2540e-03,
        5.7786e+00, 2.8185e-03, 1.1880e+00, 3.4490e-02, 6.5100e-02, 1.3230e-02,
        3.8635e-02, 1.3221e-01, 4.3442e-01, 2.0683e-02, 1.2012e-05, 3.0545e-03,
        1.1657e+00, 3.1221e-01, 8.5113e+00, 2.3241e+00, 5.5844e-02, 2.5646e-01,
        1.2266e-01, 3.0492e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [39/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 37.9
Sum_Val Meta Model:  tensor([4.9985e-01, 2.9371e-02, 9.8377e-01, 2.1805e-02, 3.9148e-04, 8.5460e-03,
        5.0975e-04, 7.0945e-03, 8.8771e-03, 3.2003e-03, 7.1114e-04, 5.1506e-03,
        4.2733e-04, 1.5388e-02, 9.1602e-03, 1.1894e-02, 8.2663e-01, 6.4028e-03,
        9.9371e-04, 1.9980e-03, 2.8770e-05, 7.3254e-05, 1.0521e-04, 1.0000e-04,
        4.7083e-02, 5.1336e-03, 1.9973e-01, 6.6125e-04, 8.2530e-04, 5.2773e-03,
        8.3544e-04, 1.1902e-04, 4.4475e-02, 6.7871e-05, 7.4664e-04, 1.4214e-02,
        4.1937e-04, 3.2897e-03, 5.2655e-03, 2.0394e-01, 1.5115e+00, 2.0289e+01,
        9.1328e+00, 2.5750e+01, 2.3127e+01, 1.5738e+01, 2.4908e-04, 2.3766e-03,
        5.5007e-04, 2.9066e-03, 5.0544e-05, 1.5577e-04, 2.9047e-03, 6.2070e-04,
        2.2391e-04, 1.0298e-03, 1.5747e+00, 2.3251e-02, 5.5972e-01, 2.1302e-03,
        1.4442e+01, 6.8666e-03, 3.7972e-01, 1.9410e-02, 4.3334e-03, 2.8993e-03,
        1.0348e-02, 4.0524e-02, 6.6569e+00, 1.5928e-01, 2.9527e-05, 9.7178e-03,
        7.3205e+00, 4.3903e-01, 3.3601e+01, 7.9101e+00, 2.5021e-01, 1.4175e+01,
        2.7290e-02, 1.1390e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([4.8145e-01, 2.0311e-02, 6.7843e-01, 1.2731e-02, 1.3023e-04, 8.3084e-03,
        5.0563e-04, 7.1442e-03, 8.4887e-03, 3.3715e-03, 7.3872e-04, 5.6320e-03,
        5.0697e-04, 1.6623e-02, 9.7812e-03, 3.9768e-02, 5.2782e-01, 1.0783e-02,
        9.9103e-04, 4.5931e-03, 2.4387e-05, 4.8986e-05, 3.3064e-05, 8.4233e-05,
        3.9190e-02, 4.8056e-03, 1.8916e-01, 8.0619e-04, 1.2283e-03, 6.1183e-03,
        3.1406e-04, 1.1927e-04, 3.9956e-02, 6.6550e-05, 3.8950e-04, 8.7252e-03,
        1.0732e-03, 2.2324e-03, 2.0193e-03, 1.7312e-01, 1.5883e+00, 2.6805e+01,
        6.9372e+00, 2.3795e+01, 2.9205e+01, 1.9756e+01, 1.3929e-04, 2.5250e-03,
        1.9929e-04, 2.2142e-03, 1.1674e-05, 4.5090e-05, 3.1471e-03, 1.1978e-04,
        1.3943e-04, 4.6104e-04, 1.7563e+00, 1.8415e-02, 5.1127e-01, 2.7891e-03,
        1.9301e+01, 3.6844e-03, 5.0338e-01, 2.7667e-02, 1.6126e-03, 3.8528e-03,
        7.5436e-03, 5.4896e-02, 8.0527e+00, 1.6226e-01, 4.2459e-05, 1.0140e-02,
        4.0625e+00, 4.6477e-01, 4.8666e+01, 1.7873e+01, 2.8883e-02, 1.6927e+01,
        4.1314e-02, 1.6803e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([4.7332e+01, 4.8055e+01, 5.5757e+01, 2.5022e+01, 5.6440e-01, 1.2336e+01,
        4.2301e+00, 1.8049e+01, 5.6453e+00, 9.5926e+00, 6.7915e+00, 1.3314e+01,
        8.5786e+00, 1.4577e+01, 8.0434e+00, 3.0477e+01, 3.2435e+02, 3.7114e+00,
        7.8093e-01, 2.7336e+00, 1.3860e+00, 3.2706e-01, 8.8758e-02, 3.3514e-01,
        2.6444e+01, 1.7989e+01, 2.8133e+01, 4.4124e+00, 1.4843e+01, 1.4183e+01,
        6.5025e-01, 1.0508e+00, 7.1014e+00, 1.2597e+00, 7.6497e-01, 8.4926e-01,
        6.3471e+00, 3.8348e+00, 1.0757e+00, 3.8642e+01, 9.0848e+00, 4.4293e+01,
        7.2096e+00, 2.5998e+01, 2.9318e+01, 2.0559e+01, 8.0674e-01, 6.4849e+00,
        2.9502e-01, 3.8290e+00, 1.1390e-01, 1.7869e-01, 7.4670e+00, 1.9444e-01,
        9.8244e-01, 4.8192e-01, 3.1502e+01, 5.7000e+00, 1.1151e+01, 4.3323e+00,
        2.0506e+01, 5.6605e+00, 6.3098e+00, 7.3894e+00, 1.2252e-01, 2.1681e+00,
        3.6964e-01, 9.9892e+00, 1.2739e+01, 2.1205e+01, 2.2506e-01, 1.9183e+01,
        1.0252e+01, 1.0783e+01, 4.9500e+01, 1.7873e+01, 2.8883e-02, 1.6927e+01,
        6.2380e-02, 7.1593e-01], device='cuda:0')
Outer loop valEpocw Maximum [39/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 186.2
model_train val_loss valEpocw [39/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 228.9
model_train val_loss valEpocw [39/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1231.7
Sum_Val Meta Model:  tensor([6.9217e-01, 6.4432e-03, 7.8284e-01, 4.4048e-03, 1.3523e-04, 2.4037e-02,
        8.3263e-03, 2.1869e-02, 4.6668e-02, 1.2432e-02, 2.1386e-03, 1.0795e-01,
        5.8850e-04, 6.1130e-03, 2.6362e-02, 4.3308e-02, 2.0082e-02, 5.0116e-02,
        1.4920e-02, 7.0095e-02, 2.3003e-06, 2.5395e-05, 1.1876e-04, 1.2159e-03,
        3.6528e-02, 8.3361e-03, 2.1515e-01, 2.8749e-03, 1.0047e-03, 5.5024e-05,
        8.3147e-05, 1.3635e-05, 1.0826e-03, 1.5318e-05, 4.6530e-05, 1.2045e-03,
        8.2764e-04, 1.0398e-04, 1.6813e-04, 7.6734e-03, 3.7950e-02, 2.4944e+00,
        1.6741e-01, 2.8588e-01, 3.5478e-01, 4.5244e+00, 1.7449e-04, 3.0380e-04,
        9.7570e-05, 3.4988e-03, 1.2364e-05, 7.6421e-05, 4.5602e-05, 4.8434e-05,
        4.2033e-05, 3.3058e-03, 7.5818e-01, 2.3397e-02, 3.6248e-01, 4.7502e-03,
        1.8397e+00, 7.5718e-04, 2.0238e-01, 9.3541e-03, 8.4400e-03, 2.3146e-03,
        1.5473e-02, 8.6572e-02, 1.0196e-01, 3.8797e-02, 1.6316e-05, 8.7091e-04,
        2.0430e+00, 2.7353e-01, 5.0887e+00, 5.2132e-01, 1.5007e-01, 7.7922e-01,
        4.7541e-02, 4.1001e-03], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.9354e-01, 7.7711e-03, 5.5888e-01, 5.1486e-03, 8.4756e-04, 2.1232e-02,
        7.3408e-03, 1.6005e-02, 4.0904e-02, 9.3553e-03, 1.4923e-03, 3.1385e-02,
        4.3766e-04, 9.6862e-03, 1.9265e-02, 5.8068e-03, 1.6184e-02, 5.0342e-02,
        4.9001e-03, 3.6003e-02, 2.3010e-05, 2.9962e-05, 5.8272e-05, 9.6679e-04,
        3.7552e-02, 4.8141e-03, 1.9001e-01, 2.4518e-03, 9.5062e-04, 5.3750e-04,
        2.0311e-04, 6.4804e-05, 1.9507e-03, 1.9575e-04, 1.4901e-04, 1.1954e-03,
        3.9101e-04, 5.6212e-04, 1.9913e-04, 7.5796e-03, 1.0099e-01, 2.3438e+00,
        1.7088e-02, 4.4842e-02, 2.4931e-03, 4.9289e+00, 7.3401e-05, 1.0660e-04,
        1.8378e-04, 3.7023e-03, 1.3027e-05, 6.3926e-05, 1.0049e-04, 2.3185e-04,
        8.3878e-05, 1.4791e-03, 6.4603e-01, 1.3469e-02, 2.1346e-01, 4.6883e-04,
        5.6929e+00, 5.9608e-04, 9.3404e-02, 1.4827e-03, 4.4372e-04, 5.0615e-04,
        1.2954e-03, 8.1266e-02, 3.4316e-01, 2.2247e-02, 2.6043e-06, 1.1219e-04,
        3.0194e+00, 1.0977e-01, 8.0068e+00, 4.2833e-02, 3.5968e-02, 2.7609e+00,
        1.2404e-03, 2.8379e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.5084e+01, 1.9817e+01, 4.6162e+01, 1.0712e+01, 4.1621e+00, 3.3998e+01,
        5.8634e+01, 4.0995e+01, 2.7097e+01, 2.9373e+01, 1.4658e+01, 7.2843e+01,
        7.6137e+00, 9.7100e+00, 1.5668e+01, 3.8850e+00, 1.0144e+01, 1.7613e+01,
        3.5426e+00, 1.7944e+01, 1.3940e+00, 2.0910e-01, 1.6325e-01, 3.9764e+00,
        2.6142e+01, 1.8630e+01, 2.9381e+01, 1.3843e+01, 1.1606e+01, 1.3128e+00,
        4.4285e-01, 6.0078e-01, 3.8121e-01, 4.0223e+00, 3.1181e-01, 1.3103e-01,
        2.4220e+00, 9.9123e-01, 1.1890e-01, 2.1634e+00, 7.5268e-01, 4.0097e+00,
        1.7741e-02, 4.9160e-02, 2.5031e-03, 5.1444e+00, 4.3161e-01, 2.7888e-01,
        2.8039e-01, 6.4847e+00, 1.3248e-01, 2.6234e-01, 2.4985e-01, 4.0997e-01,
        5.9600e-01, 1.5839e+00, 1.2614e+01, 4.0523e+00, 4.9054e+00, 7.1883e-01,
        6.0622e+00, 9.9608e-01, 1.1485e+00, 4.0958e-01, 3.5249e-02, 2.9346e-01,
        6.6374e-02, 1.8177e+01, 5.4985e-01, 3.2954e+00, 1.4708e-02, 2.3311e-01,
        9.0046e+00, 2.4878e+00, 8.1442e+00, 4.2833e-02, 3.5968e-02, 2.7609e+00,
        1.8075e-03, 1.3703e-01], device='cuda:0')
Outer loop valEpocw Maximum [39/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 22.5
model_train val_loss valEpocw [39/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 30.2
model_train val_loss valEpocw [39/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 724.8
Sum_Val Meta Model:  tensor([5.5059e-01, 1.0746e-03, 1.2238e-01, 1.0787e-03, 2.3562e-04, 2.0739e-03,
        1.4297e-04, 2.9275e-03, 3.1948e-03, 6.3853e-04, 1.1464e-04, 5.1806e-04,
        2.4911e-05, 1.1815e-02, 4.5698e-03, 6.0467e-03, 1.1577e-02, 1.8137e-02,
        2.7954e-03, 5.3853e-03, 1.8645e-05, 1.1456e-04, 3.8215e-03, 4.6148e-04,
        1.1799e-02, 9.2248e-04, 9.9774e-02, 8.0624e-04, 5.6783e-05, 2.0561e-03,
        2.1210e-03, 4.9350e-04, 4.0230e-02, 2.4647e-05, 1.5988e-03, 4.5800e-02,
        1.7730e-03, 4.9321e-03, 9.0951e-04, 7.1218e-02, 4.1544e+00, 4.0420e+01,
        6.5795e+01, 7.3182e+01, 5.0674e+01, 6.5584e+01, 2.2294e-03, 4.3524e-03,
        2.0232e-02, 8.6661e-03, 1.9369e-03, 5.6707e-03, 1.9875e-02, 4.6718e-03,
        5.4077e-03, 5.6457e-02, 6.5507e+00, 2.3626e-02, 3.6082e-01, 1.4575e-03,
        7.0741e+01, 1.0339e-03, 1.0994e+00, 3.2513e-02, 1.2981e-01, 1.7194e-03,
        1.1718e-01, 3.2675e-02, 3.4280e+00, 1.4519e-02, 6.7577e-05, 2.4807e-03,
        1.4770e+00, 1.1791e+00, 1.3803e+00, 1.5994e+01, 1.0970e+01, 1.3852e+00,
        1.8282e-01, 1.8570e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.3599e-01, 8.9816e-05, 5.7007e-02, 6.9125e-05, 5.7105e-06, 4.9810e-05,
        3.6588e-06, 3.0017e-03, 2.6202e-04, 4.3589e-05, 3.6376e-06, 1.0203e-05,
        1.3221e-06, 9.4321e-03, 2.4784e-04, 1.3370e-03, 1.3554e-03, 2.6164e-04,
        4.2302e-05, 4.6457e-05, 8.5741e-07, 2.2660e-06, 1.4902e-06, 2.3107e-06,
        3.6642e-03, 3.0845e-04, 1.0008e-01, 4.5792e-04, 4.3290e-05, 7.2124e-05,
        1.0208e-04, 4.1061e-04, 3.8741e-02, 3.4215e-06, 1.9115e-04, 4.4537e-03,
        1.1802e-03, 3.6751e-03, 4.7997e-05, 1.0735e-01, 4.4124e+00, 5.9396e+01,
        8.4543e+01, 1.2605e+02, 1.0687e+02, 8.1472e+01, 7.2426e-04, 1.6590e-03,
        1.4292e-02, 3.2341e-03, 9.4866e-04, 6.7416e-03, 1.4329e-02, 5.0520e-03,
        3.6094e-03, 4.5157e-02, 3.5721e+00, 2.7966e-02, 3.4320e-01, 3.9857e-04,
        9.1300e+01, 1.0387e-04, 4.4064e-01, 2.6784e-02, 1.5377e-01, 3.9963e-03,
        1.4533e-01, 4.4764e-02, 3.9018e+00, 2.3707e-02, 9.5579e-06, 1.7431e-03,
        3.6510e+00, 1.2018e+00, 8.2112e-01, 2.4960e+01, 1.9062e+01, 1.1411e+00,
        2.8453e-02, 8.3578e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.1923e+01, 4.4672e-01, 6.8576e+00, 2.8226e-01, 5.9863e-02, 1.5453e-01,
        7.6981e-02, 1.6725e+01, 3.1685e-01, 2.7455e-01, 8.0798e-02, 4.8139e-02,
        5.8904e-02, 1.6397e+01, 3.9162e-01, 1.8245e+00, 1.4710e+00, 1.3734e-01,
        6.3024e-02, 4.8726e-02, 1.6329e-01, 3.7164e-02, 7.8241e-03, 2.0111e-02,
        3.8480e+00, 2.5308e+00, 2.6915e+01, 6.2159e+00, 1.4349e+00, 3.2337e-01,
        3.9837e-01, 8.1162e+00, 1.1028e+01, 1.7179e-01, 7.2532e-01, 5.3523e-01,
        1.5303e+01, 1.1789e+01, 5.2197e-02, 4.8971e+01, 3.2556e+01, 9.4746e+01,
        8.6473e+01, 1.3347e+02, 1.0709e+02, 8.3776e+01, 8.6642e+00, 7.6483e+00,
        3.8187e+01, 9.7864e+00, 2.0110e+01, 5.1759e+01, 6.3802e+01, 1.7535e+01,
        5.6126e+01, 8.2611e+01, 9.8304e+01, 1.5013e+01, 1.0101e+01, 1.2501e+00,
        9.5382e+01, 3.5057e-01, 6.8667e+00, 1.2705e+01, 1.8133e+01, 4.3027e+00,
        1.0590e+01, 1.6278e+01, 6.2469e+00, 5.6379e+00, 1.1899e-01, 7.4038e+00,
        1.1546e+01, 3.9930e+01, 8.3082e-01, 2.4960e+01, 1.9062e+01, 1.1411e+00,
        4.2595e-02, 5.7531e-01], device='cuda:0')
Outer loop valEpocw Maximum [39/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 416.1
model_train val_loss valEpocw [39/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 614.2
model_train val_loss valEpocw [39/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1507.3
Sum_Val Meta Model:  tensor([2.8799e+00, 3.2664e-03, 6.2759e-01, 1.7239e-03, 4.4001e-04, 3.4975e-02,
        5.0447e-03, 3.4270e-02, 7.2236e-03, 3.2934e-02, 3.3985e-03, 1.0984e-02,
        6.5393e-05, 6.6914e-02, 7.6819e-02, 8.0528e-03, 6.8096e-03, 1.4779e-02,
        1.5413e-03, 3.4402e-03, 7.6303e-05, 1.4216e-04, 5.0946e-04, 9.2757e-04,
        9.7265e-02, 3.6728e-03, 4.6874e-01, 1.0140e-03, 1.1115e-02, 9.6625e-04,
        1.9709e-03, 2.8113e-04, 5.2712e-02, 2.7809e-02, 3.4382e-02, 2.1588e-01,
        2.4511e-04, 2.7367e-03, 3.9864e-02, 1.6786e-01, 2.0991e+00, 1.4545e+01,
        1.0202e+01, 1.0900e+01, 1.0893e+01, 1.8933e+01, 6.6451e-04, 9.6752e-04,
        3.2484e-03, 1.0746e-03, 2.2902e-04, 3.9691e-04, 6.4108e-04, 1.9592e-02,
        5.9930e-04, 5.5199e-03, 2.6789e+00, 2.8715e-02, 7.6098e-01, 4.7833e-03,
        4.6920e+01, 1.4878e-03, 4.3501e-01, 1.7089e-01, 2.8595e-01, 6.9414e-03,
        3.9050e-01, 4.1967e-01, 5.7928e-01, 4.2748e-02, 6.3917e-05, 3.2911e-03,
        1.2449e+00, 9.1729e-01, 4.8674e+02, 8.2999e+00, 9.9875e-01, 8.6676e+00,
        2.9562e-02, 2.9270e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([9.9722e-01, 3.4144e-03, 5.0726e-01, 5.4757e-03, 1.7653e-03, 2.7519e-02,
        6.5634e-03, 3.2470e-02, 1.1860e-02, 3.4979e-02, 2.7727e-03, 1.6762e-02,
        3.5655e-04, 6.1029e-02, 8.2719e-02, 3.4094e-03, 4.1690e-03, 6.4674e-03,
        2.4581e-04, 9.0242e-04, 8.7676e-05, 7.7074e-05, 1.3845e-04, 3.6227e-04,
        9.9414e-02, 5.6894e-03, 3.8125e-01, 1.9410e-03, 1.1402e-02, 3.7278e-04,
        4.4104e-04, 2.5358e-04, 1.6715e-03, 1.6229e-04, 3.3896e-04, 2.3746e-03,
        3.2918e-04, 4.5043e-04, 5.9296e-04, 2.5571e-02, 3.7690e-01, 7.3710e-01,
        4.7330e-02, 2.3017e-01, 6.3358e-03, 1.2041e+00, 2.7919e-04, 3.1753e-04,
        2.8511e-04, 5.4556e-04, 5.7610e-05, 9.3992e-05, 9.5694e-05, 5.6122e-04,
        4.2353e-04, 1.5150e-03, 7.2477e-01, 7.7495e-03, 7.7383e-01, 3.8329e-03,
        1.8898e+01, 1.8425e-03, 1.1273e-01, 5.5310e-03, 3.0827e-03, 4.8627e-03,
        9.7009e-03, 9.3247e-02, 4.1376e-02, 8.7035e-03, 3.2022e-05, 7.0697e-04,
        4.4677e-01, 4.2338e-01, 1.0396e+02, 9.5734e-01, 4.2747e-03, 1.2241e+01,
        4.5498e-03, 1.8937e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.6628e+01, 1.8623e+00, 2.0972e+01, 2.3896e+00, 1.4688e+00, 9.9279e+00,
        8.2278e+00, 1.7251e+01, 2.4554e+00, 2.0993e+01, 4.0439e+00, 8.1466e+00,
        8.4131e-01, 1.5968e+01, 1.9889e+01, 8.2336e-01, 8.5342e-01, 6.6156e-01,
        5.4201e-02, 1.6014e-01, 5.1250e-01, 8.5245e-02, 8.1700e-02, 3.0390e-01,
        2.4723e+01, 3.9294e+00, 2.4274e+01, 1.7115e+00, 1.9391e+01, 1.9198e-01,
        2.1925e-01, 3.5661e-01, 9.9613e-02, 3.4683e-01, 1.3475e-01, 9.3099e-02,
        3.2321e-01, 1.8359e-01, 7.8557e-02, 2.2051e+00, 1.8695e+00, 1.2630e+00,
        5.1441e-02, 2.7108e-01, 6.4240e-03, 1.3087e+00, 2.9251e-01, 1.7461e-01,
        8.7571e-02, 2.1782e-01, 9.3483e-02, 8.5628e-02, 4.8963e-02, 1.9459e-01,
        5.3095e-01, 4.4708e-01, 7.2702e+00, 7.3236e-01, 1.2199e+01, 1.2952e+00,
        2.0893e+01, 6.3272e-01, 9.6319e-01, 4.6336e-01, 9.8608e-02, 7.5209e-01,
        2.2206e-01, 6.0834e+00, 7.1970e-02, 4.8083e-01, 3.0690e-02, 2.9265e-01,
        1.2239e+00, 5.3668e+00, 1.0913e+02, 9.5736e-01, 4.2747e-03, 1.2242e+01,
        8.4205e-03, 4.2362e-02], device='cuda:0')
Outer loop valEpocw Maximum [39/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 632.2
model_train val_loss valEpocw [39/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 143.7
model_train val_loss valEpocw [39/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 441.3
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.13709628 97.41353054 93.17625273 98.08481865 98.63549421 97.65597398
 98.31142408 95.29245501 98.0494877  97.01514358 98.61600127 98.96200095
 99.4286132  95.45570839 97.47688259 97.41840377 96.43888354 97.76805838
 98.91205029 98.43081834 98.62452943 99.34089497 99.53216944 99.0241347
 95.26930715 96.94448167 94.45791352 97.10773504 98.0775088  98.29680438
 98.40645216 98.57457877 96.94448167 98.62331112 98.70859273 98.85966302
 97.48662906 98.480769   98.88037426 93.35534411 98.29436776 97.53779803
 99.21540917 98.79509265 99.25439505 98.63793082 98.24319879 98.69153641
 98.20055799 98.77194479 98.79752927 98.66717023 99.02900793 98.46127606
 98.7743814  97.73029081 93.2980836  97.06143931 96.74467904 97.74612882
 97.97517087 98.78047295 97.76562176 97.48053752 98.82189544 97.66328383
 98.66960685 96.03196842 99.4700357  98.36137474 99.81603538 97.47079105
 99.25317674 96.18060209 99.27632461 99.47247231 99.80019737 99.60161304
 99.89157052 99.18495145]
Accuracy th:0.7 is [87.16633569 97.33555878 92.71451371 97.95567793 98.5782337  97.52926987
 98.06776233 95.05122988 98.02512153 96.9627563  98.55752245 98.8998672
 99.41643011 95.36433523 97.39160098 97.2551504  96.37065825 97.62551626
 98.8572264  98.41010709 98.48320561 99.29459924 99.47612724 98.83529684
 95.2205748  96.85432682 94.32268125 96.97737601 98.04826939 98.25172695
 98.1749735  98.60990972 96.61432    98.51975488 98.57579708 98.74757861
 97.20763636 98.31020577 98.71346597 93.22133015 98.29802268 97.2685518
 99.09966984 98.57214215 99.17398667 98.54046612 98.16766365 98.66838854
 98.05801586 98.76219832 98.81458559 98.60747311 99.01804315 98.28340298
 98.73905045 97.58287545 93.20549214 96.77513676 96.64355941 97.6316078
 98.29558607 98.62940266 97.38429113 97.36967142 98.68788148 97.53536141
 98.52584642 95.98079945 99.46638077 98.24563541 99.81603538 97.10529842
 99.32871188 95.89429953 99.23855704 99.42495827 99.78923259 99.60039473
 99.88060574 99.1642402 ]
Avg Prec: is [96.29754347 35.78627931 71.75901937 66.36626839 78.73804784 64.69655344
 75.28732201 48.09732261 56.89321804 52.79089521 35.3740017  55.25684734
 26.05167691 29.63733531 33.02571749 57.95893086 29.6842867  44.40503172
 50.38157408 39.85099216 57.70401546 52.41591774 90.07791646 84.56615704
 26.48724474 35.35235263 38.63769622 42.06856691 25.54914113 39.6316343
 73.70692426 39.31001678 57.67057155 62.55263477 72.10525326 79.60197015
 58.31858944 74.99533326 86.83764129 46.68021557 57.28859031 92.03655277
 94.41619073 92.14191025 94.55057077 95.63419471 40.48538619 35.02120791
 44.31006477 47.82351764 64.21526571 41.34910152 28.57227123 75.13580953
 31.02160318 48.26882048 74.90968994 59.56463474 48.65354412 61.90028722
 97.31127088 84.34762926 77.12251636 52.39271627 62.97735606 45.03858438
 64.62002693 27.73726797 87.76767423 68.10656108 13.13851068 73.32636911
 90.50106517 53.14854767 95.8987766  96.51645378 93.10577727 94.78584574
 67.25797753 32.82223876]
Accuracy th:0.5 is [45.88881714 97.2137279  69.84076705 97.02489005 97.26733349 74.2790658
 74.62628379 74.80050194 75.52052241 96.46812295 75.79951511 98.52097319
 99.41399349 79.88937757 75.4498605  96.56680596 96.29512311 75.18183258
 98.65376884 98.30776915 79.82358889 76.23932457 98.38695922 86.21727318
 86.79962476 96.65086926 94.0778012  74.95157223 98.01293844 75.70936027
 97.30875598 98.57457877 96.36213009 98.02024829 88.77937647 75.35483242
 75.20254383 91.79590892 97.11504489 72.62825745 78.34212546 92.05906361
 74.61531901 74.19622081 96.9627563  93.87434364 98.02877645 98.57336046
 97.90450896 91.64849356 92.21988036 98.55508583 98.99976852 74.69207247
 98.70615611 75.10386082 70.32078069 93.56123829 96.24273583 96.9067141
 89.79300934 97.17717864 95.55804632 75.28782544 98.42838172 75.61067726
 98.20786784 74.38749528 76.22348656 97.55850928 76.67913403 95.99054592
 76.37821177 95.45083515 74.53978387 84.70413372 92.27104933 75.75200107
 76.71933821 99.14718388]
Accuracy th:0.7 is [45.96800721 97.2137279  69.84076705 97.02489005 97.26733349 74.29612212
 74.62628379 75.15990302 75.52052241 96.4754328  75.79951511 98.52097319
 99.41399349 80.36939121 75.45473374 96.56680596 96.29512311 75.18183258
 98.65376884 98.30776915 80.20248291 76.23932457 98.38695922 87.13709628
 87.82056749 96.65086926 94.0778012  74.95157223 98.01293844 75.70936027
 97.30875598 98.57457877 96.36213009 98.02024829 89.02303822 75.35483242
 75.20254383 92.09561287 97.11504489 72.62825745 78.98173755 92.05906361
 74.61531901 74.19622081 96.9627563  93.87434364 98.02877645 98.57336046
 97.98491734 92.22597191 92.57197159 98.55508583 98.99976852 74.69207247
 98.70615611 75.13797347 70.32078069 94.07414627 96.24273583 96.9067141
 89.79300934 97.17717864 95.60434205 75.30610007 98.42838172 75.61067726
 98.20786784 74.38749528 76.22348656 97.55972759 76.7729438  95.99054592
 76.66329601 95.45083515 74.53978387 84.89053496 92.9630487  75.75200107
 76.71933821 99.14718388]
Avg Prec: is [55.85951871  3.08176881 11.12553814  3.2721627   2.26656693  3.79260806
  3.24327621  5.52119729  2.48304963  3.8895792   1.56864774  1.6228706
  0.62690099  5.3058354   2.62702793  3.09230276  3.66703133  2.76543072
  1.33503553  1.77310182  2.00120773  0.90928628  1.81780345  2.41575641
  5.03458397  3.65514586  6.46066321  3.32409389  1.9863628   1.98137468
  2.59206273  1.30339714  3.7630416   1.64872347  2.37199978  2.38673907
  2.97307395  2.54472533  2.77946202  7.34963953  2.18429683  8.18646841
  3.40951692  4.05225454  3.24180729  6.60227101  2.08949861  1.49376092
  2.07538122  1.71089966  1.87429461  1.67948968  1.07829989  3.05985253
  1.41026331  2.72103514 11.20264073  3.71242207  4.30256013  2.80100317
 10.8173151   2.16032675  3.76484947  2.95970741  1.59198013  2.537534
  1.79978391  4.33576548  1.20099928  2.42626249  0.16361879  3.31774677
  1.93296898  4.69697243  3.92075202  3.23628274  0.9023047   1.9036966
  0.13707215  0.75505846]
mAP score regular 59.97, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [87.11662556 97.45372101 93.00146997 98.22358422 98.91122904 97.61317488
 98.4353589  95.38580362 98.08904502 96.96788499 98.6820141  98.99843038
 99.36716745 95.29112789 97.48611007 97.39641727 96.40481351 97.80003488
 98.98846451 98.37805516 98.7866557  99.38959065 99.60883972 99.21518798
 95.46553056 96.89563246 94.42658893 97.30672447 97.96696315 98.22607569
 98.6969629  98.5549493  96.91805566 98.75177517 98.87634851 99.18529038
 97.84238981 98.34317463 99.02085358 93.2381593  97.83242395 92.74983183
 97.38146847 96.44965991 96.82337992 94.66826121 98.4054613  98.8090789
 98.14385729 98.74430077 98.86887411 98.66955677 98.91870344 98.58235543
 98.8016045  97.82993248 90.3106859  97.1771682  96.40481351 97.59822608
 91.97498567 98.85143384 97.26436953 97.51849914 98.7343349  97.54590527
 98.6296933  95.84174203 98.66208237 98.26344769 99.81563146 97.66549568
 98.13389142 95.89904577 97.3341306  97.35157087 99.24259412 98.63218477
 99.8206144  99.16535865]
Accuracy th:0.7 is [87.81423624 97.36153674 92.88935396 98.27590503 98.87385704 97.64556394
 98.25348182 95.32600842 98.11395969 97.09245833 98.59730423 98.96354984
 99.35969305 95.18150335 97.35904527 97.26187807 96.36744151 97.72279941
 98.98099011 98.41791863 98.72187757 99.33228692 99.55153599 99.06320851
 95.44310736 96.81839699 94.4888756  97.13481326 97.87477888 98.22607569
 98.55744077 98.71440317 96.68385779 98.6521165  98.82901064 99.04576824
 97.60570048 98.25846476 98.89129731 93.2232105  98.03423275 93.04880783
 97.45372101 96.56177592 96.95044473 94.86508708 98.33819169 98.82901064
 98.04419862 98.7866557  98.90873757 98.62471037 98.90375464 98.4278845
 98.8016045  97.71532501 91.00082218 97.18215113 96.44965991 97.66300421
 92.49819369 98.73682637 97.1472706  97.45122954 98.67703117 97.61317488
 98.50511996 95.81184443 98.7642325  98.16129756 99.81563146 97.31669034
 98.34566609 95.87911403 97.41385754 97.46368687 99.25505145 98.6670653
 99.82310586 99.18778185]
Avg Prec: is [96.26124904 35.83087847 69.78027672 72.26264737 77.05200915 65.0044315
 81.15921877 48.13314342 61.61130489 55.58697667 38.4556732  56.88464577
 24.45892824 31.6420996  33.97946874 63.14255483 32.07861421 45.96392288
 51.51329158 38.31957665 64.65090021 58.27170343 92.12031655 87.08688112
 24.83334097 38.78291985 32.70234551 46.82635891 28.66746049 38.26409085
 76.9368816  39.73548387 55.27371795 63.44898895 74.61701064 82.40304055
 59.32037475 77.34053813 89.07461799 45.08966269 39.2107951  47.85577954
 50.55746558 36.49299205 28.91421891 51.02820925 39.17142461 28.75704765
 41.53266799 42.55692325 69.09957605 38.14234082 27.429109   78.16222887
 33.8923942  43.49507143 56.58029478 57.74414839 39.06306719 63.39555795
 64.38746385 86.98232413 66.9124624  54.11881243 61.97883662 39.06411817
 61.61059102 26.79504979 40.51714751 65.77432116 12.00156825 74.88790306
 54.08029728 45.27010077 63.17703362 49.9355121  14.18003375 53.25343455
  1.97098526 25.24115956]
Accuracy th:0.5 is [45.43937016 97.22450607 67.53120562 96.96290206 97.90716795 72.67359294
 72.80564068 72.47925854 74.32294392 96.41976231 74.37277325 98.5325261
 99.34972718 77.10092932 74.54468446 96.31262924 96.21047911 73.75738097
 98.78167277 98.34068316 77.5892568  75.0130802  98.31327703 87.60246157
 80.7633854  96.52938685 94.3393876  73.91932631 97.81747515 74.36529885
 97.52597354 98.67204823 96.39983058 98.18870369 89.87966216 74.03144231
 74.03891671 92.89931983 97.0276802  71.46772305 75.82280689 92.37362035
 73.12953136 72.92772255 97.03764606 94.02795426 98.18621222 98.77668984
 97.95201435 92.57792062 93.24065077 98.55993223 98.87385704 73.11956549
 98.6969629  73.84209084 68.53277524 94.39669133 96.16314124 96.78102499
 90.13379176 97.04761193 95.7470663  74.12611805 98.32075143 74.83867753
 98.13139996 73.20427536 75.10028154 97.53593941 75.41918928 96.07843137
 75.09280714 95.44559882 73.09714229 86.1623938  93.98559932 74.47741485
 75.48895035 99.15040985]
Accuracy th:0.7 is [45.65861923 97.22450607 67.53120562 96.96290206 97.90716795 72.67608441
 72.80564068 72.71345641 74.32294392 96.41976231 74.37277325 98.5325261
 99.34972718 77.46966639 74.55714179 96.31262924 96.21047911 73.75738097
 98.78167277 98.34068316 77.83092907 75.0130802  98.31327703 88.35737599
 81.8272417  96.52938685 94.3393876  73.91932631 97.81747515 74.36529885
 97.52597354 98.67204823 96.39983058 98.18870369 90.06403069 74.03144231
 74.03891671 93.1110945  97.0276802  71.46772305 76.3310661  92.37362035
 73.12953136 72.92772255 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 92.97904676 93.55457558 98.55993223 98.87385704 73.11956549
 98.6969629  73.85953111 68.53277524 94.81525774 96.16314124 96.78102499
 90.13379176 97.04761193 95.7694895  74.14604978 98.32075143 74.83867753
 98.13139996 73.20427536 75.10028154 97.53593941 75.42417221 96.07843137
 75.27468421 95.44559882 73.09714229 86.38164287 94.5038244  74.47741485
 75.48895035 99.15040985]
Avg Prec: is [54.1975186   3.75381929 14.86198212  4.60026903  1.72014361  4.32081644
  9.08778111  8.70357822  6.84644917  5.1105798   2.27809763  5.3217143
  1.55785129  5.79079729  2.94269351  3.93533154 21.96520061  5.62945695
  1.5818237   4.52827191  3.70122395  1.42564751  1.90557114  4.99679618
  5.70087143 14.12391359  8.35144331  4.85945602  3.942196    6.06298055
  2.28393064  0.87431928  3.07567958  1.15736938  1.71052822  2.44439845
  2.02714948  2.19365775  2.21231569  6.25028125  1.76981709  6.01997326
  2.23835281  2.74161467  2.35588161  4.9008743   1.7716837   1.04676352
  1.44555858  1.17322662  1.22574141  1.01135703  0.76256785  2.31037307
  0.91955616  1.88787182 10.237602    3.03860598  4.07166049  2.67843041
  7.99657324  2.02352664  3.27829977  2.53605291  1.32415206  1.87127356
  1.53183741  3.5377886   1.05635317  2.14907479  0.19117236  3.11688339
  1.5321765   3.924647    3.20320953  2.32699889  0.55444175  1.44485294
  0.12121658  0.59372183]
mAP score regular 51.62, mAP score EMA 4.32
Train_data_mAP: current_mAP = 59.97, highest_mAP = 59.97
Val_data_mAP: current_mAP = 51.62, highest_mAP = 51.91
tensor([8.6168e-03, 3.4404e-04, 1.0536e-02, 4.2038e-04, 1.8389e-04, 5.4612e-04,
        9.8232e-05, 3.3135e-04, 1.2523e-03, 2.8569e-04, 9.0534e-05, 3.6334e-04,
        4.9335e-05, 9.4981e-04, 1.0135e-03, 1.1002e-03, 1.3553e-03, 2.5773e-03,
        1.0232e-03, 1.4391e-03, 1.3661e-05, 1.2640e-04, 3.0341e-04, 2.0563e-04,
        1.3328e-03, 2.2497e-04, 6.0747e-03, 1.5144e-04, 6.6998e-05, 3.5823e-04,
        3.9986e-04, 9.1293e-05, 4.9885e-03, 4.3420e-05, 4.4200e-04, 1.0008e-02,
        1.4040e-04, 4.9916e-04, 1.6398e-03, 3.6226e-03, 1.5790e-01, 5.8531e-01,
        9.6859e-01, 9.2575e-01, 9.9679e-01, 9.6554e-01, 1.4255e-04, 3.3487e-04,
        5.9839e-04, 4.9060e-04, 8.5648e-05, 2.0603e-04, 3.6447e-04, 5.1303e-04,
        1.1334e-04, 8.1927e-04, 4.8042e-02, 2.7881e-03, 4.2198e-02, 5.5388e-04,
        9.4470e-01, 5.3742e-04, 7.5397e-02, 3.2140e-03, 1.1778e-02, 1.4772e-03,
        1.8567e-02, 4.8751e-03, 6.6723e-01, 6.6847e-03, 1.5158e-04, 4.3096e-04,
        3.9937e-01, 3.9546e-02, 9.8771e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        6.8470e-01, 2.0424e-02], device='cuda:0')
Sum Train Loss:  tensor([2.5652e-01, 3.1095e-03, 2.1648e-01, 5.4248e-03, 4.4791e-04, 3.3096e-03,
        3.0920e-04, 4.6927e-03, 5.0990e-03, 3.2285e-03, 2.1438e-03, 1.5472e-03,
        3.6242e-05, 2.1847e-02, 6.6398e-03, 1.0608e-02, 2.6086e-02, 1.7593e-02,
        6.1573e-03, 2.5429e-03, 1.6594e-04, 1.7839e-03, 8.4561e-04, 3.9950e-04,
        3.0390e-02, 3.6712e-03, 1.3718e-01, 2.4273e-03, 2.6167e-04, 2.2242e-03,
        1.3739e-03, 2.2263e-04, 7.3494e-02, 8.6473e-05, 3.0624e-03, 2.1670e-02,
        1.6023e-03, 6.5320e-03, 8.9904e-03, 7.7628e-02, 6.3015e-01, 3.8739e+00,
        3.5119e+00, 2.1710e+00, 5.8668e+00, 6.1956e+00, 5.5096e-04, 3.8436e-04,
        7.9894e-03, 2.5201e-03, 5.1660e-04, 5.2698e-04, 3.9538e-03, 3.3530e-03,
        3.4579e-04, 5.6961e-03, 6.7693e-01, 1.7245e-02, 9.0083e-01, 6.7828e-03,
        1.1399e+01, 8.7367e-04, 3.0809e-01, 1.9261e-02, 8.7243e-03, 1.7524e-02,
        1.7163e-02, 6.2280e-02, 1.5600e+00, 1.2100e-02, 3.1615e-05, 7.6909e-04,
        7.0653e-01, 8.7843e-01, 2.2907e+00, 1.2426e+00, 2.7105e+00, 5.4283e-01,
        1.1915e-01, 4.9617e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [40/80], Step [000/642], LR 1.0e-04, Main Model Weighted Training Loss: 46.8
Sum Train Loss:  tensor([3.1041e-01, 3.1842e-03, 2.1433e-01, 4.0507e-03, 1.2362e-03, 4.7251e-03,
        5.8733e-04, 3.6649e-03, 1.0259e-02, 5.1865e-03, 8.8791e-04, 3.1427e-03,
        7.6567e-05, 1.8120e-02, 1.2091e-02, 6.5049e-03, 1.7836e-02, 4.0699e-02,
        3.1074e-03, 1.3046e-02, 5.8505e-05, 1.0289e-04, 3.3260e-04, 8.9400e-04,
        4.3064e-02, 1.9464e-03, 1.2702e-01, 1.8439e-03, 1.6526e-04, 3.3165e-03,
        4.0309e-03, 8.3641e-04, 9.4533e-02, 2.9278e-04, 1.9448e-03, 6.2307e-02,
        2.1101e-03, 1.0018e-03, 7.6014e-03, 7.5464e-02, 9.7558e-01, 6.9108e+00,
        3.3650e+00, 2.7450e+00, 8.5825e-01, 3.1631e+00, 1.7293e-03, 4.8694e-04,
        4.2465e-03, 1.9402e-03, 1.0510e-04, 6.1160e-04, 5.2121e-04, 3.5204e-03,
        3.2672e-04, 3.5631e-03, 1.1480e+00, 1.5443e-02, 7.0784e-01, 5.6246e-03,
        9.3873e+00, 2.2329e-04, 4.8499e-01, 3.0932e-02, 1.3913e-02, 6.7199e-03,
        3.0581e-02, 5.1667e-02, 3.9937e-01, 5.6363e-02, 3.7289e-05, 1.9310e-03,
        9.0831e-01, 2.4321e-01, 4.0799e-01, 4.2459e-01, 3.0516e-01, 1.3470e+00,
        1.5149e-02, 1.3669e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [40/80], Step [100/642], LR 1.0e-04, Main Model Weighted Training Loss: 35.1
Sum Train Loss:  tensor([2.8380e-01, 2.8011e-03, 2.3057e-01, 3.2355e-03, 8.6002e-04, 4.1545e-03,
        2.0653e-03, 6.1994e-03, 1.3238e-02, 3.0185e-03, 3.0753e-04, 8.7475e-04,
        4.3517e-05, 1.1294e-02, 3.7517e-03, 5.3002e-03, 9.8481e-03, 2.1459e-02,
        8.3963e-03, 5.0448e-03, 7.3642e-05, 7.6724e-05, 2.5665e-04, 1.2134e-03,
        2.4695e-02, 3.5003e-03, 1.7850e-01, 2.7664e-03, 1.0941e-03, 1.6997e-03,
        2.0671e-03, 9.7807e-04, 2.1400e-02, 1.7457e-04, 1.2032e-03, 1.0331e-02,
        1.0240e-03, 1.2176e-03, 6.0651e-03, 6.0898e-02, 9.0521e-01, 2.2257e+00,
        3.9349e+00, 3.0623e+00, 4.6398e+00, 3.3246e+00, 1.7973e-04, 1.0526e-03,
        3.3279e-03, 1.3636e-03, 5.1486e-04, 1.4768e-03, 1.3284e-03, 3.0621e-03,
        4.3061e-04, 1.0922e-02, 7.3221e-01, 1.1161e-02, 2.7792e-01, 4.0611e-03,
        5.4477e+00, 3.5473e-03, 4.2300e-01, 2.9011e-02, 3.3512e-02, 1.0030e-02,
        8.6442e-02, 6.3781e-02, 3.0288e-01, 3.6208e-02, 1.2937e-05, 4.6235e-03,
        6.3916e-01, 4.6441e-01, 6.3104e-01, 2.3183e-01, 1.3228e-01, 1.1776e+00,
        2.7747e-02, 6.7213e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [40/80], Step [200/642], LR 1.0e-04, Main Model Weighted Training Loss: 29.9
Sum Train Loss:  tensor([2.3314e-01, 7.0024e-03, 1.9748e-01, 9.2095e-04, 6.8417e-04, 3.7114e-03,
        1.0535e-03, 5.3109e-03, 7.2788e-03, 2.4504e-03, 5.4911e-04, 2.1608e-03,
        3.2141e-05, 1.8399e-02, 1.3910e-02, 1.6511e-02, 1.6081e-02, 2.3058e-02,
        6.9533e-03, 3.1515e-03, 3.7272e-05, 2.3084e-04, 1.2231e-04, 8.0176e-04,
        2.2572e-02, 4.6652e-03, 1.0532e-01, 1.1229e-03, 5.4268e-04, 3.0139e-03,
        3.3537e-03, 3.7832e-04, 1.0694e-01, 3.1113e-04, 3.8077e-03, 1.5411e-02,
        1.1036e-03, 3.1811e-03, 1.8001e-02, 9.1137e-02, 9.4439e-01, 6.8534e+00,
        1.0776e+00, 3.7398e+00, 1.7371e+00, 4.7638e+00, 1.3675e-03, 1.6714e-03,
        4.5455e-03, 2.9531e-03, 5.0174e-04, 7.6247e-04, 2.5448e-03, 1.3316e-03,
        5.8952e-04, 7.4948e-03, 1.1751e+00, 2.9066e-02, 3.5430e-01, 3.0160e-03,
        7.0123e+00, 1.8277e-03, 3.8115e-01, 3.1196e-02, 5.1733e-02, 1.4218e-02,
        1.0815e-01, 7.9492e-02, 1.3256e+00, 1.7036e-02, 3.0815e-05, 2.6740e-03,
        1.3532e+00, 4.3846e-01, 5.2900e+00, 2.4352e+00, 2.1683e-01, 3.5244e+00,
        4.8411e-02, 9.6627e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [40/80], Step [300/642], LR 1.0e-04, Main Model Weighted Training Loss: 44.1
Sum Train Loss:  tensor([2.8650e-01, 5.1136e-03, 2.5652e-01, 1.9074e-03, 3.0153e-04, 6.3537e-03,
        1.2089e-03, 8.8758e-03, 5.1784e-03, 3.9067e-03, 3.0588e-04, 1.6733e-03,
        2.4438e-04, 2.6218e-02, 9.7516e-03, 1.0864e-02, 3.1584e-02, 2.0058e-02,
        5.8168e-03, 2.0273e-02, 1.2978e-04, 2.5587e-04, 8.0944e-04, 2.3900e-04,
        2.3785e-02, 2.0460e-03, 1.0308e-01, 6.7005e-04, 6.3746e-04, 2.1685e-03,
        5.4737e-04, 6.2573e-04, 4.0803e-02, 1.7054e-04, 6.9681e-04, 1.5877e-02,
        1.6402e-03, 2.2460e-03, 2.2358e-03, 7.2734e-02, 5.2690e-01, 2.6239e+00,
        4.2031e-01, 2.1386e+00, 2.3933e+00, 3.1799e+00, 6.9221e-04, 1.5494e-03,
        3.2626e-03, 1.6783e-03, 2.9867e-04, 1.2119e-03, 2.8329e-03, 4.6415e-03,
        1.5053e-03, 1.0932e-02, 7.8419e-01, 3.2257e-02, 3.0887e-01, 9.8234e-03,
        5.3244e+00, 2.7307e-03, 5.3827e-01, 2.7729e-02, 3.1591e-02, 1.2666e-02,
        6.6193e-02, 5.4663e-02, 3.1950e-01, 2.2182e-02, 1.5000e-05, 3.4548e-03,
        9.9435e-01, 5.1004e-01, 2.9315e+00, 1.0580e+00, 1.4765e-01, 1.7864e+00,
        2.8521e+00, 4.6938e-02], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [40/80], Step [400/642], LR 1.0e-04, Main Model Weighted Training Loss: 30.2
Sum Train Loss:  tensor([3.0160e-01, 2.4909e-03, 2.4859e-01, 2.8334e-03, 6.8497e-04, 3.8409e-03,
        5.0440e-04, 3.4031e-03, 7.0862e-03, 2.2085e-03, 1.0301e-03, 4.3030e-04,
        1.9346e-04, 1.3647e-02, 5.2497e-03, 1.1887e-02, 1.9323e-02, 1.6656e-02,
        4.4484e-03, 8.9855e-03, 4.0064e-05, 8.2805e-04, 1.5977e-03, 5.0901e-04,
        1.7410e-02, 1.6064e-03, 1.0224e-01, 1.5112e-03, 5.2614e-04, 2.4753e-03,
        9.9248e-04, 1.0239e-03, 3.6137e-02, 1.6395e-04, 4.2207e-03, 9.4473e-02,
        1.1118e-03, 1.7937e-03, 1.8311e-02, 7.9482e-02, 3.4197e-01, 6.7550e+00,
        1.7777e+00, 4.3234e+00, 1.9088e+00, 3.6405e+00, 2.5848e-04, 1.2376e-03,
        7.1523e-03, 4.6744e-03, 4.7359e-04, 2.0115e-03, 2.5582e-03, 2.6201e-03,
        7.3863e-04, 8.8332e-03, 9.7947e-01, 3.1258e-02, 3.5923e-01, 3.1040e-03,
        1.1903e+01, 9.4413e-03, 9.3607e-01, 3.8695e-02, 6.8409e-02, 7.1266e-03,
        1.6402e-01, 8.4359e-02, 4.6904e+00, 4.1605e-02, 2.5724e-05, 2.9917e-03,
        6.5702e-01, 6.1797e-01, 1.6212e+00, 1.2921e+00, 9.3130e-01, 3.0750e+00,
        1.2144e-01, 1.3048e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [40/80], Step [500/642], LR 1.0e-04, Main Model Weighted Training Loss: 47.6
Sum Train Loss:  tensor([2.1833e-01, 2.2970e-03, 2.8778e-01, 4.2364e-03, 5.1419e-04, 1.2753e-03,
        3.7142e-04, 4.4730e-03, 1.7341e-03, 2.1655e-03, 1.0701e-03, 2.0864e-03,
        4.3000e-05, 1.1361e-02, 1.5933e-02, 1.1856e-02, 1.7094e-02, 1.0050e-02,
        7.4793e-03, 6.9024e-03, 1.5701e-05, 2.5597e-04, 2.3767e-04, 2.8205e-04,
        2.0049e-02, 2.0418e-03, 1.6544e-01, 1.5103e-03, 9.8506e-04, 2.1895e-03,
        1.1645e-03, 2.9238e-04, 4.0718e-02, 1.3819e-04, 2.1249e-03, 5.8038e-02,
        1.1751e-03, 6.8872e-04, 1.1672e-02, 7.5843e-02, 1.7343e+00, 4.9837e+00,
        2.3040e+00, 4.0712e+00, 4.8652e-01, 3.2542e+00, 5.8760e-04, 4.3504e-03,
        6.5904e-03, 3.0727e-03, 6.2692e-04, 2.2002e-03, 1.0715e-03, 5.5997e-03,
        4.2641e-04, 4.4111e-03, 1.1465e+00, 4.5337e-02, 5.0519e-01, 5.2246e-03,
        5.1389e+00, 2.4220e-03, 7.0687e-01, 2.6067e-02, 4.6161e-02, 1.2621e-02,
        1.0726e-01, 4.6655e-02, 1.7888e-01, 1.3194e-02, 1.3800e-05, 5.0895e-03,
        7.6448e-01, 8.2060e-01, 5.2308e+00, 1.7292e+00, 3.8853e-01, 5.2291e-01,
        9.7680e-02, 2.2628e-01], device='cuda:0', grad_fn=<NegBackward0>)
Epoch [40/80], Step [600/642], LR 1.0e-04, Main Model Weighted Training Loss: 35.6
Sum_Val Meta Model:  tensor([4.6977e-01, 2.2466e-02, 8.4089e-01, 1.8017e-02, 1.5442e-04, 6.6017e-03,
        4.0449e-04, 5.9871e-03, 7.6729e-03, 2.7492e-03, 5.7434e-04, 4.3048e-03,
        3.7681e-04, 1.3312e-02, 7.3525e-03, 8.3908e-03, 7.0673e-01, 9.3119e-03,
        1.5021e-03, 1.1421e-03, 2.6807e-05, 4.7446e-05, 1.1684e-04, 1.6706e-04,
        3.4913e-02, 4.0921e-03, 1.7934e-01, 4.8660e-04, 7.3562e-04, 4.4398e-03,
        6.8152e-04, 5.3568e-05, 3.9387e-02, 4.7901e-05, 6.2666e-04, 1.5883e-02,
        6.1928e-04, 2.9096e-03, 4.4561e-03, 1.8361e-01, 1.6759e+00, 1.7051e+01,
        8.6581e+00, 2.5139e+01, 2.6245e+01, 1.8643e+01, 2.2674e-04, 1.9967e-03,
        6.2983e-04, 2.4283e-03, 2.8143e-05, 1.3973e-04, 2.3457e-03, 6.0467e-04,
        1.1921e-04, 1.0753e-03, 1.3639e+00, 2.2135e-02, 5.0902e-01, 2.0132e-03,
        2.1893e+01, 6.1255e-03, 3.6546e-01, 1.6843e-02, 9.0329e-03, 3.2171e-03,
        1.5076e-02, 3.7544e-02, 7.5206e+00, 1.0967e-01, 5.0918e-05, 8.2176e-03,
        9.3220e+00, 3.7419e-01, 2.4439e+01, 8.2366e+00, 1.2513e-01, 1.4054e+01,
        2.2868e-02, 1.0228e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([4.6063e-01, 1.4459e-02, 5.6641e-01, 1.0769e-02, 2.7644e-05, 6.4087e-03,
        4.2479e-04, 5.7723e-03, 7.9201e-03, 2.4382e-03, 5.9395e-04, 4.7092e-03,
        4.2565e-04, 1.4654e-02, 8.0630e-03, 2.7637e-02, 4.5878e-01, 1.1156e-02,
        8.9069e-04, 1.2969e-03, 1.8847e-05, 2.9406e-05, 3.0043e-05, 1.8522e-04,
        3.2546e-02, 3.6519e-03, 1.7374e-01, 5.7049e-04, 9.7905e-04, 6.1721e-03,
        2.2702e-04, 3.2964e-05, 3.6992e-02, 4.0588e-05, 5.5180e-04, 1.2407e-02,
        1.1605e-03, 2.1420e-03, 3.9913e-03, 1.4973e-01, 1.7624e+00, 2.6012e+01,
        9.1095e+00, 3.2518e+01, 2.9644e+01, 2.8878e+01, 1.2571e-04, 2.3133e-03,
        2.6655e-04, 2.0221e-03, 4.1963e-06, 4.8730e-05, 2.4670e-03, 1.1830e-04,
        6.5592e-05, 4.7727e-04, 1.4733e+00, 2.0608e-02, 4.5764e-01, 2.2026e-03,
        3.1458e+01, 3.8351e-03, 4.3797e-01, 2.3323e-02, 8.6899e-03, 5.0617e-03,
        1.7421e-02, 4.9300e-02, 8.6184e+00, 1.3746e-01, 6.3135e-05, 8.4199e-03,
        7.4167e+00, 4.2683e-01, 3.8886e+01, 1.9151e+01, 2.9160e-02, 1.2975e+01,
        5.7452e-02, 1.4252e-02], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([5.3457e+01, 4.2028e+01, 5.3758e+01, 2.5616e+01, 1.5033e-01, 1.1735e+01,
        4.3243e+00, 1.7420e+01, 6.3245e+00, 8.5345e+00, 6.5605e+00, 1.2961e+01,
        8.6278e+00, 1.5428e+01, 7.9555e+00, 2.5120e+01, 3.3850e+02, 4.3286e+00,
        8.7049e-01, 9.0116e-01, 1.3797e+00, 2.3264e-01, 9.9019e-02, 9.0075e-01,
        2.4419e+01, 1.6233e+01, 2.8601e+01, 3.7672e+00, 1.4613e+01, 1.7229e+01,
        5.6774e-01, 3.6108e-01, 7.4155e+00, 9.3478e-01, 1.2484e+00, 1.2397e+00,
        8.2660e+00, 4.2913e+00, 2.4341e+00, 4.1333e+01, 1.1162e+01, 4.4441e+01,
        9.4049e+00, 3.5126e+01, 2.9740e+01, 2.9909e+01, 8.8192e-01, 6.9081e+00,
        4.4545e-01, 4.1218e+00, 4.8995e-02, 2.3652e-01, 6.7687e+00, 2.3058e-01,
        5.7873e-01, 5.8256e-01, 3.0667e+01, 7.3914e+00, 1.0845e+01, 3.9766e+00,
        3.3300e+01, 7.1362e+00, 5.8089e+00, 7.2568e+00, 7.3784e-01, 3.4265e+00,
        9.3828e-01, 1.0113e+01, 1.2917e+01, 2.0564e+01, 4.1651e-01, 1.9538e+01,
        1.8571e+01, 1.0793e+01, 3.9370e+01, 1.9151e+01, 2.9160e-02, 1.2975e+01,
        8.3908e-02, 6.9782e-01], device='cuda:0')
Outer loop valEpocw Maximum [40/80], Step [000/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 188.5
model_train val_loss valEpocw [40/80], Step [000/314], LR 1.0e-04, Main Model Validation Loss: 251.6
model_train val_loss valEpocw [40/80], Step [000/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1277.5
Sum_Val Meta Model:  tensor([7.9304e-01, 6.5533e-03, 7.5102e-01, 5.2754e-03, 3.1829e-04, 2.2915e-02,
        7.0720e-03, 2.4189e-02, 4.4947e-02, 1.1853e-02, 2.1551e-03, 1.1291e-01,
        5.7575e-04, 6.7005e-03, 4.0425e-02, 4.0737e-02, 1.7942e-02, 5.1303e-02,
        1.0921e-02, 4.6847e-02, 4.4935e-06, 3.9016e-05, 1.7729e-04, 1.2609e-03,
        3.7800e-02, 6.2000e-03, 2.4404e-01, 2.9271e-03, 1.1158e-03, 8.8914e-05,
        1.3777e-04, 2.0617e-05, 1.9527e-03, 1.9050e-05, 8.1429e-05, 1.7937e-03,
        7.7318e-04, 2.0285e-04, 2.7226e-04, 9.8744e-03, 3.0415e-02, 2.2306e+00,
        1.7105e-01, 3.4321e-01, 4.3983e-01, 4.8404e+00, 2.6667e-04, 3.8779e-04,
        1.2644e-04, 3.5813e-03, 2.0691e-05, 8.9076e-05, 6.1636e-05, 6.9211e-05,
        5.1539e-05, 3.3712e-03, 7.1177e-01, 2.1403e-02, 3.4449e-01, 4.0940e-03,
        1.9791e+00, 1.2704e-03, 1.8329e-01, 1.0142e-02, 9.3903e-03, 2.2444e-03,
        1.6006e-02, 8.8906e-02, 1.2450e-01, 3.4152e-02, 2.1282e-05, 1.2254e-03,
        2.1147e+00, 2.4687e-01, 6.3529e+00, 4.9472e-01, 1.9104e-01, 8.0778e-01,
        5.3127e-02, 6.3617e-03], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([5.7362e-01, 8.1987e-03, 5.6077e-01, 5.8846e-03, 4.3779e-04, 2.0691e-02,
        6.2846e-03, 1.6432e-02, 4.8347e-02, 1.0730e-02, 1.5971e-03, 2.8102e-02,
        5.2214e-04, 1.2261e-02, 2.0968e-02, 6.3918e-03, 1.2184e-02, 4.3261e-02,
        7.2554e-03, 4.2702e-02, 2.0688e-05, 1.8899e-05, 4.8849e-05, 9.3480e-04,
        3.4172e-02, 5.3669e-03, 1.9880e-01, 2.6493e-03, 1.0042e-03, 8.5145e-04,
        2.7604e-04, 2.6645e-05, 2.8770e-03, 1.7101e-04, 4.4199e-04, 4.4880e-03,
        6.1171e-04, 4.9524e-04, 5.2330e-04, 6.5691e-03, 1.6979e-02, 3.8448e+00,
        4.0896e-03, 3.0270e-02, 4.3034e-03, 3.6948e+00, 8.8133e-05, 1.1939e-04,
        2.2509e-04, 3.7039e-03, 4.5181e-06, 4.7648e-05, 1.2692e-04, 2.1574e-04,
        4.2304e-05, 2.2841e-03, 6.5419e-01, 1.4056e-02, 2.2076e-01, 4.6239e-04,
        1.6521e+00, 1.4747e-04, 1.9800e-01, 1.7599e-03, 1.7866e-03, 9.3768e-04,
        2.9330e-03, 8.0791e-02, 1.9887e-01, 2.3476e-02, 5.1372e-06, 6.8873e-05,
        2.8301e+00, 1.2308e-01, 9.1422e+00, 4.0577e-02, 1.3592e-02, 2.2062e+00,
        7.0094e-04, 1.9380e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([6.2376e+01, 2.0779e+01, 4.8557e+01, 1.1560e+01, 2.0261e+00, 3.2315e+01,
        4.7225e+01, 4.1558e+01, 3.2949e+01, 3.1345e+01, 1.4355e+01, 6.2211e+01,
        8.4791e+00, 1.1972e+01, 1.6160e+01, 4.3209e+00, 7.9189e+00, 1.4602e+01,
        5.4684e+00, 2.1191e+01, 1.1693e+00, 1.2410e-01, 1.3236e-01, 3.7268e+00,
        2.4623e+01, 1.9587e+01, 3.0975e+01, 1.3950e+01, 1.1459e+01, 2.0087e+00,
        6.0132e-01, 2.3967e-01, 5.4204e-01, 3.2722e+00, 8.6956e-01, 4.7344e-01,
        3.5035e+00, 8.3836e-01, 2.9989e-01, 1.9308e+00, 1.4907e-01, 7.1726e+00,
        4.2554e-03, 3.3380e-02, 4.3224e-03, 3.8570e+00, 4.8400e-01, 2.9146e-01,
        3.1694e-01, 6.3647e+00, 4.2331e-02, 1.9968e-01, 3.0291e-01, 3.7210e-01,
        2.9469e-01, 2.5040e+00, 1.3676e+01, 4.1862e+00, 5.6244e+00, 6.8017e-01,
        1.7696e+00, 2.3575e-01, 2.5561e+00, 5.0085e-01, 1.5970e-01, 5.5327e-01,
        1.6306e-01, 1.7343e+01, 3.2940e-01, 3.5431e+00, 2.9226e-02, 1.3444e-01,
        8.2840e+00, 2.8840e+00, 9.2735e+00, 4.0577e-02, 1.3592e-02, 2.2062e+00,
        1.0236e-03, 1.0009e-01], device='cuda:0')
Outer loop valEpocw Maximum [40/80], Step [100/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 24.2
model_train val_loss valEpocw [40/80], Step [100/314], LR 1.0e-04, Main Model Validation Loss: 26.7
model_train val_loss valEpocw [40/80], Step [100/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 714.4
Sum_Val Meta Model:  tensor([3.9666e-01, 7.8483e-04, 1.0097e-01, 8.6106e-04, 2.4215e-04, 1.5575e-03,
        1.1321e-04, 2.1730e-03, 4.1501e-03, 4.2258e-04, 1.0750e-04, 4.7476e-04,
        2.7549e-05, 8.9720e-03, 3.8657e-03, 4.8604e-03, 9.0807e-03, 2.0140e-02,
        2.2012e-03, 5.2222e-03, 1.7764e-05, 1.4024e-04, 2.8364e-03, 4.9769e-04,
        8.6550e-03, 8.0531e-04, 8.4098e-02, 6.2502e-04, 8.0467e-05, 1.6965e-03,
        2.9833e-03, 3.2237e-04, 4.2515e-02, 1.5948e-05, 1.8099e-03, 7.0665e-02,
        1.1774e-03, 3.7873e-03, 7.1565e-04, 5.2264e-02, 3.3306e+00, 3.8151e+01,
        6.2820e+01, 6.9651e+01, 5.2070e+01, 7.1326e+01, 2.0380e-03, 2.8903e-03,
        1.4556e-02, 5.7938e-03, 1.1400e-03, 3.7503e-03, 1.3741e-02, 3.4998e-03,
        3.5767e-03, 3.9552e-02, 5.5396e+00, 1.9411e-02, 3.1373e-01, 1.1646e-03,
        7.5560e+01, 8.3529e-04, 7.1705e-01, 2.2749e-02, 8.6814e-02, 1.6046e-03,
        1.0104e-01, 2.9773e-02, 3.7152e+00, 1.3154e-02, 6.2750e-05, 1.7853e-03,
        1.5262e+00, 9.5656e-01, 1.3908e+00, 1.5043e+01, 9.7651e+00, 1.6168e+00,
        2.7639e-01, 1.7876e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([1.1367e-01, 8.0950e-05, 3.2238e-02, 7.2326e-05, 1.5004e-06, 2.4920e-05,
        4.6187e-06, 2.2293e-03, 8.7405e-05, 2.8548e-05, 3.4818e-06, 9.3717e-06,
        1.0335e-06, 6.4537e-03, 1.6672e-04, 6.8725e-04, 8.5561e-04, 3.0950e-04,
        6.4857e-05, 2.0380e-05, 6.6116e-07, 1.1318e-06, 9.3504e-07, 6.1545e-06,
        3.6100e-03, 8.9681e-05, 7.6443e-02, 3.4623e-04, 1.8242e-05, 1.2931e-04,
        5.2469e-05, 2.5879e-04, 3.1695e-02, 1.4216e-06, 2.4817e-04, 7.2569e-03,
        7.6710e-04, 2.8666e-03, 1.5840e-04, 6.9062e-02, 2.8362e+00, 7.6194e+01,
        9.3885e+01, 1.1659e+02, 1.3171e+02, 1.0876e+02, 5.7965e-04, 9.8635e-04,
        1.0508e-02, 2.3653e-03, 7.4756e-04, 4.1891e-03, 1.0309e-02, 6.4113e-03,
        2.5566e-03, 2.9384e-02, 3.1119e+00, 2.1331e-02, 2.5220e-01, 2.2164e-04,
        1.5504e+02, 6.8290e-05, 5.2952e-01, 1.8966e-02, 8.4732e-02, 3.9440e-03,
        1.0851e-01, 3.6517e-02, 4.4227e+00, 1.5654e-02, 1.4482e-05, 1.5847e-03,
        2.0138e+00, 1.0311e+00, 1.2293e+00, 3.1023e+01, 2.6200e+01, 9.6215e-01,
        2.6016e-02, 7.6720e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([2.1921e+01, 5.7353e-01, 4.8120e+00, 4.0521e-01, 2.2258e-02, 1.0854e-01,
        1.3595e-01, 1.6640e+01, 1.3822e-01, 2.4425e-01, 1.0824e-01, 5.9740e-02,
        6.5596e-02, 1.4739e+01, 3.4605e-01, 1.2195e+00, 1.2402e+00, 1.9874e-01,
        1.3920e-01, 2.6826e-02, 1.9458e-01, 2.5394e-02, 6.8294e-03, 7.5536e-02,
        5.0290e+00, 1.0119e+00, 2.4626e+01, 6.4765e+00, 8.6432e-01, 7.7958e-01,
        2.7201e-01, 7.7073e+00, 1.1002e+01, 1.0563e-01, 1.2207e+00, 9.2870e-01,
        1.3690e+01, 1.2442e+01, 2.2801e-01, 4.3174e+01, 2.6924e+01, 1.3406e+02,
        9.5709e+01, 1.2302e+02, 1.3194e+02, 1.1149e+02, 9.4250e+00, 6.1885e+00,
        3.8737e+01, 1.0135e+01, 2.2035e+01, 4.7006e+01, 6.4137e+01, 3.0983e+01,
        5.8369e+01, 7.4841e+01, 1.0623e+02, 1.4598e+01, 8.9486e+00, 9.3691e-01,
        1.6209e+02, 3.1777e-01, 9.3792e+00, 1.2132e+01, 1.3842e+01, 5.8546e+00,
        1.0206e+01, 1.6323e+01, 7.2660e+00, 4.7243e+00, 2.7749e-01, 9.0771e+00,
        6.3036e+00, 4.1130e+01, 1.2383e+00, 3.1023e+01, 2.6200e+01, 9.6215e-01,
        3.8226e-02, 6.8646e-01], device='cuda:0')
Outer loop valEpocw Maximum [40/80], Step [200/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 415.0
model_train val_loss valEpocw [40/80], Step [200/314], LR 1.0e-04, Main Model Validation Loss: 756.5
model_train val_loss valEpocw [40/80], Step [200/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 1667.8
Sum_Val Meta Model:  tensor([2.7164e+00, 5.5842e-03, 9.6134e-01, 3.5418e-03, 1.3359e-03, 2.6098e-02,
        4.1310e-03, 2.9006e-02, 9.5927e-03, 3.3282e-02, 3.7811e-03, 9.4805e-03,
        2.0311e-04, 5.8771e-02, 6.1089e-02, 1.3474e-02, 1.4787e-02, 3.1130e-02,
        4.5741e-03, 9.3705e-03, 2.8172e-04, 4.2655e-04, 1.3008e-03, 2.2236e-03,
        8.4747e-02, 4.2096e-03, 4.6470e-01, 1.8012e-03, 7.6467e-03, 1.8188e-03,
        3.0959e-03, 6.2273e-04, 1.0117e-01, 1.3426e-02, 4.1699e-02, 1.5901e-01,
        8.9712e-04, 6.9369e-03, 4.1726e-02, 1.4772e-01, 1.1805e+00, 1.4689e+01,
        9.2758e+00, 9.9967e+00, 9.3246e+00, 1.7010e+01, 1.0851e-03, 1.4702e-03,
        5.2233e-03, 2.3195e-03, 5.6983e-04, 9.7582e-04, 1.5597e-03, 2.6247e-02,
        1.5023e-03, 6.8950e-03, 2.4160e+00, 3.9338e-02, 8.0608e-01, 9.6041e-03,
        3.9383e+01, 3.4966e-03, 6.0017e-01, 1.5139e-01, 2.0371e-01, 1.0568e-02,
        2.8647e-01, 3.4042e-01, 8.2591e-01, 5.0954e-02, 2.1125e-04, 6.1352e-03,
        1.4675e+00, 8.3308e-01, 4.5102e+02, 9.8945e+00, 1.1576e+00, 7.1595e+00,
        5.8989e-02, 4.8834e-02], device='cuda:0', grad_fn=<NegBackward0>)
Sum_Val Main Model Weighted Val Loss:  tensor([9.2627e-01, 3.6098e-03, 4.0771e-01, 3.9148e-03, 6.0477e-04, 2.2885e-02,
        5.2096e-03, 2.6540e-02, 3.8328e-03, 2.7647e-02, 2.2383e-03, 1.5653e-02,
        2.2864e-04, 5.1649e-02, 6.9872e-02, 3.3027e-03, 2.9255e-03, 1.0073e-02,
        3.9509e-04, 4.6322e-04, 7.0010e-05, 4.6332e-05, 1.0567e-04, 1.7522e-03,
        7.6512e-02, 3.5491e-03, 3.4335e-01, 1.4870e-03, 1.0134e-02, 4.5221e-04,
        2.0250e-04, 7.1755e-05, 1.3104e-03, 1.0316e-04, 4.1339e-04, 4.7785e-03,
        6.8857e-04, 2.4720e-04, 5.4017e-04, 1.8587e-02, 1.6149e-01, 2.8274e+00,
        9.6508e-02, 5.2848e-02, 2.6183e-01, 1.7499e+00, 2.1607e-04, 2.7613e-04,
        2.9257e-04, 3.8092e-04, 1.8682e-05, 6.9350e-05, 8.0018e-05, 7.6728e-04,
        1.6372e-04, 1.5160e-03, 7.6063e-01, 7.8606e-03, 6.9315e-01, 1.7212e-03,
        1.8793e+01, 7.2154e-04, 2.2874e-01, 5.2125e-03, 7.7430e-03, 4.4358e-03,
        1.9470e-02, 7.8399e-02, 9.4989e-02, 8.1729e-03, 4.7597e-05, 5.6703e-04,
        2.0361e-02, 4.1433e-01, 6.3141e+01, 2.3023e+00, 7.0684e-03, 8.6597e+00,
        3.4103e-03, 1.5925e-03], device='cuda:0')
Sum_Val Main Model Unweighted Val Loss:  tensor([3.5954e+01, 2.3945e+00, 1.9582e+01, 2.0042e+00, 5.9074e-01, 9.9956e+00,
        7.6735e+00, 1.6605e+01, 9.2433e-01, 1.9420e+01, 3.8369e+00, 8.9869e+00,
        6.5114e-01, 1.5845e+01, 1.9041e+01, 9.3617e-01, 7.0241e-01, 1.1551e+00,
        1.0296e-01, 9.1298e-02, 5.0640e-01, 5.9875e-02, 7.1986e-02, 1.7354e+00,
        2.4101e+01, 2.8256e+00, 2.5406e+01, 1.5308e+00, 2.0871e+01, 2.6997e-01,
        1.1745e-01, 1.2544e-01, 8.4019e-02, 2.5124e-01, 1.7776e-01, 2.0516e-01,
        8.0123e-01, 1.1573e-01, 8.4445e-02, 1.9422e+00, 1.0414e+00, 5.1429e+00,
        1.0448e-01, 6.2375e-02, 2.6541e-01, 1.8907e+00, 2.6443e-01, 1.7380e-01,
        1.0124e-01, 1.7919e-01, 3.4582e-02, 7.7381e-02, 4.8825e-02, 3.0307e-01,
        2.3553e-01, 5.5032e-01, 8.4569e+00, 8.4386e-01, 1.2756e+01, 6.6412e-01,
        2.0818e+01, 2.8946e-01, 2.1324e+00, 5.1979e-01, 3.0703e-01, 8.0016e-01,
        5.3262e-01, 5.7319e+00, 1.7788e-01, 5.2641e-01, 5.7800e-02, 2.7235e-01,
        5.8838e-02, 5.9333e+00, 6.5575e+01, 2.3023e+00, 7.0684e-03, 8.6598e+00,
        6.7125e-03, 4.2699e-02], device='cuda:0')
Outer loop valEpocw Maximum [40/80], Step [300/314], LR 1.0e-04, Meta Learning Summed up Validation Loss: 583.4
model_train val_loss valEpocw [40/80], Step [300/314], LR 1.0e-04, Main Model Validation Loss: 102.5
model_train val_loss valEpocw [40/80], Step [300/314], LR 1.0e-04, Main Model Unweighted Validation Loss: 395.7
---------evaluating the models---------

starting validation
Accuracy th:0.5 is [87.39294112 97.43058686 93.21036537 98.07385388 98.54046612 97.63526273
 98.3418818  95.22910296 97.97517087 97.00783373 98.63549421 98.96078264
 99.42373996 95.40453942 97.44276995 97.38672774 96.41329906 97.76196684
 98.86453625 98.43325496 98.55995906 99.32505696 99.52485959 99.11550785
 95.21448325 96.84336204 94.42867411 97.07849563 98.0909102  98.2724382
 98.34431842 98.59529002 96.96762954 98.53071965 98.70859273 98.88768412
 97.69495986 98.45031128 98.94981786 93.45402712 98.27121989 97.143066
 99.22637395 98.93154323 99.26535983 98.47833238 98.30167761 98.68057163
 98.17984674 98.80362081 98.76707155 98.68422656 99.04362764 98.4393465
 98.76585324 97.78755132 93.35168919 97.05900269 96.77513676 97.73394574
 98.30411423 98.74514199 97.77293162 97.48906568 98.8158039  97.5475445
 98.69275472 95.98810931 99.49074695 98.38574091 99.80994384 97.37819958
 99.34820482 96.21471473 99.24586689 99.43470474 99.81603538 99.56506378
 99.90619023 99.18495145]
Accuracy th:0.7 is [86.59982213 97.36357988 92.64507011 98.02999476 98.30411423 97.45738965
 98.13964255 94.96472996 97.8545583  96.96519292 98.57457877 98.91448691
 99.41399349 95.35215214 97.3660165  97.18692511 96.33898222 97.71688941
 98.85966302 98.36137474 98.40523385 99.26414152 99.4846554  99.03388117
 95.24494097 96.76295367 94.31415309 96.94204505 98.04461447 98.30898746
 98.11527637 98.5782337  96.62041154 98.38574091 98.64402237 98.82433206
 97.48541075 98.31629732 98.76219832 93.17016118 98.13111439 97.59140361
 99.13987403 98.81458559 99.15571204 98.06532571 98.25050864 98.64280406
 98.10065667 98.75123354 98.60016325 98.61721958 99.01560654 98.50026194
 98.71955751 97.65231905 93.21645691 96.88356623 96.61188338 97.54145296
 98.34553673 98.60990972 97.79486117 97.38429113 98.8852475  97.63282611
 98.69153641 95.97958115 99.50780327 98.34675503 99.81603538 97.00296049
 99.22271902 96.09775709 99.23977534 99.34820482 99.81969031 99.62963414
 99.89278883 99.17033175]
Avg Prec: is [96.29083828 35.85798305 71.83081767 66.10148596 79.06607067 64.76245652
 74.909942   47.37833457 56.69329702 53.12870408 34.24756613 55.34470375
 25.49476533 28.40585143 33.57624724 58.06182668 29.43857261 43.89921526
 49.64765626 42.41472291 56.67195315 52.46845548 89.23427875 84.58197142
 26.03620762 33.47623767 37.85488819 40.78511417 26.4447012  40.45966063
 73.31260087 37.02275763 58.15852192 62.18536327 72.31212929 79.39939248
 58.96791817 75.80696316 87.14171642 46.69862798 57.09414588 92.41629745
 94.44861135 93.21578466 94.82754443 95.86926092 42.72812614 33.95157211
 43.56092188 48.58630506 64.19858809 42.19225131 28.26783944 74.38281956
 31.98508793 48.66630812 75.00032219 60.79224889 48.21991036 61.80688367
 97.52094682 84.3577247  76.51417681 52.26275061 62.710008   45.31220643
 63.09997232 27.32469448 89.50520317 69.60404592 11.38777007 72.87928052
 91.21292937 53.44103629 95.60046183 96.67566397 93.86842319 95.04447024
 69.81471184 31.48795143]
Accuracy th:0.5 is [45.66586664 97.2137279  69.4155773  97.02489005 97.26733349 73.82950987
 74.08900964 74.49714307 75.02467075 96.46934126 75.23300155 98.52097319
 99.41399349 79.73465236 74.92355113 96.56680596 96.29512311 74.65917813
 98.65376884 98.30776915 79.54946943 75.70205041 98.38695922 86.16488591
 86.95556828 96.65086926 94.0778012  74.41186145 98.01293844 75.16721288
 97.30875598 98.57457877 96.36213009 98.02024829 88.41388385 74.82730474
 74.65064997 91.66433157 97.11504489 72.12996918 78.03754828 92.05906361
 74.10728427 73.67112974 96.9627563  93.87434364 98.02877645 98.57336046
 97.9093822  92.0164228  92.93868252 98.55508583 98.99976852 74.22789683
 98.70615611 74.62262887 69.90046418 93.41382293 96.24273583 96.9067141
 89.79300934 97.17717864 95.68596874 74.76760761 98.42838172 75.1562481
 98.20786784 73.82829157 75.67402931 97.55972759 76.16622605 95.99054592
 76.00297267 95.45083515 74.05367868 84.29600029 92.31490844 75.27320574
 76.16744435 99.14718388]
Accuracy th:0.7 is [45.79500737 97.2137279  69.4155773  97.02489005 97.26733349 73.8611859
 74.08900964 74.84679768 75.02467075 96.4754328  75.23300155 98.52097319
 99.41399349 80.23050401 74.92720605 96.56680596 96.29512311 74.65917813
 98.65376884 98.30776915 79.91252543 75.70205041 98.38695922 87.13709628
 88.07884894 96.65086926 94.0778012  74.41186145 98.01293844 75.16721288
 97.30875598 98.57457877 96.36213009 98.02024829 88.70505964 74.82730474
 74.65064997 91.96281722 97.11504489 72.12996918 78.6674139  92.05906361
 74.10728427 73.67112974 96.9627563  93.87434364 98.02877645 98.57336046
 97.98735396 92.54273218 93.31513992 98.55508583 98.99976852 74.22789683
 98.70615611 74.67745276 69.90046418 93.88530841 96.24273583 96.9067141
 89.79300934 97.17717864 95.72982785 74.77613577 98.42838172 75.1562481
 98.20786784 73.82829157 75.67402931 97.55972759 76.26734567 95.99054592
 76.38917655 95.45083515 74.05367868 84.51895079 93.05807678 75.27320574
 76.16744435 99.14718388]
Avg Prec: is [55.92027062  3.08383987 11.28409833  3.34096963  2.2678763   3.82946482
  3.2898205   5.58415732  2.5273711   3.84806875  1.65103489  1.54746784
  0.64962838  5.14838082  2.62011654  3.10422378  3.66204101  2.69844967
  1.39437216  1.76325468  2.01561472  0.8792131   1.8627956   2.50332813
  5.18809233  3.57893732  6.62113122  3.31244624  2.05618099  1.83645851
  2.63857422  1.33519772  3.75006816  1.67088419  2.43772151  2.41592929
  2.9442814   2.52632249  2.7421605   7.43848074  2.35541683  8.2922794
  3.37064153  4.10610158  3.19656134  6.47876743  2.09483079  1.57094442
  2.19651892  1.61097908  1.85872426  1.61922562  1.00046229  2.96552619
  1.41711128  2.72465103 11.22405998  3.63569986  4.07140226  2.7602502
 10.90232933  2.12389142  3.77383226  2.98861358  1.5959527   2.52478923
  1.72293811  4.29004418  1.27919608  2.36543836  0.17795743  3.34359187
  1.89611007  4.65288278  3.89354896  3.07982594  0.80249921  1.8541151
  0.1902742   0.7213225 ]
mAP score regular 59.97, mAP score EMA 3.77
starting validation
Accuracy th:0.5 is [87.3533149  97.45372101 92.89931983 98.20863542 98.90375464 97.64307248
 98.53750903 95.40573536 98.13887436 97.0202058  98.6670653  99.01088771
 99.37215038 95.23880709 97.45122954 97.33911354 96.43471111 97.76515435
 98.89877171 98.4278845  98.7642325  99.36218452 99.62129706 99.29989785
 95.31355109 96.82337992 94.4440292  97.2892842  97.92460822 98.05665595
 98.66208237 98.69447144 96.97535939 98.70942024 98.88382291 99.14044398
 98.04419862 98.28088796 99.04078531 93.22570197 98.05416449 91.04816005
 97.33163914 96.12327777 96.79846526 94.88003588 98.40296983 98.7941301
 98.07907915 98.7343349  98.85641677 98.67703117 98.90624611 98.44532476
 98.81406184 97.86481302 90.06153923 97.10740713 96.48952338 97.69539328
 92.49071929 98.76672397 97.01522286 97.46368687 98.7717069  97.2220146
 98.64215063 95.82430177 98.6820141  98.30829409 99.8206144  97.54341381
 98.48518823 95.7096943  97.43628074 97.56334554 99.21020505 98.52754316
 99.81812293 99.15539278]
Accuracy th:0.7 is [87.77188131 97.41385754 93.0139273  98.25348182 98.7343349  97.58327728
 98.34068316 95.23382415 97.98191195 97.080001   98.61225303 98.99344744
 99.35720158 95.16406308 97.35157087 97.15225353 96.34252685 97.75518848
 99.00839624 98.37805516 98.67952263 99.29740638 99.57894212 99.28993198
 95.45556469 96.69382365 94.50880733 97.18215113 97.84986422 98.24102449
 98.55744077 98.6894885  96.73617859 98.62720183 98.86139971 99.11552931
 97.91464235 98.21112689 98.97849864 93.27054837 98.01679249 91.89775021
 97.40638314 96.31013778 96.90061539 94.87007001 98.40047836 98.84395944
 98.09402795 98.77668984 98.6745397  98.62720183 98.90624611 98.4802053
 98.74679224 97.73525675 90.93604405 97.1397962  96.43969405 97.64805541
 92.69252809 98.63965917 97.26187807 97.48112714 98.83399357 97.59075168
 98.65460797 95.83177617 98.76921544 98.26095622 99.81563146 97.26935247
 98.47023943 95.92894337 97.50853327 97.61815781 99.25006852 98.56740663
 99.82559733 99.16535865]
Avg Prec: is [96.30778797 35.60150949 70.23024871 72.20483053 77.65858045 64.20487202
 81.16382626 47.81673639 61.81933607 55.77886076 38.54783112 56.88638649
 22.40377364 30.37673942 33.49402796 62.75464056 33.34867837 44.99826508
 50.59122263 38.40545269 65.09254158 57.81141591 92.20198915 89.131207
 25.26327029 35.65249249 32.84518454 46.84541642 28.97216622 38.47733037
 77.9000482  37.32425665 55.39601914 64.12947725 73.83285686 81.4775887
 60.44309337 76.55128486 88.70268751 44.85673796 37.7766447  48.38370846
 50.37154738 35.71229398 28.14905171 49.91763106 40.68014708 29.47478297
 41.57192191 43.63826822 70.00101252 38.61302949 27.34956984 76.7932247
 33.21013499 44.37135533 55.9895109  56.86408718 39.5010617  64.25484864
 62.74819474 86.41756197 66.26217192 54.24848316 64.29503321 38.68027137
 62.18099644 26.53483324 41.81831611 66.30244539 12.98847388 74.3864991
 55.87247511 44.89890376 64.90134457 50.6931958  17.87980399 48.04457304
  2.12707919 22.36342229]
Accuracy th:0.5 is [45.45930189 97.22450607 67.32192242 96.96290206 97.90716795 72.4294292
 72.56645987 72.29738147 74.09871191 96.41976231 74.13359245 98.5325261
 99.34972718 76.99628771 74.33290978 96.31262924 96.21047911 73.52816603
 98.78167277 98.34068316 77.42731146 74.77888233 98.31327703 87.79928744
 81.08727608 96.52938685 94.3393876  73.6950943  97.81747515 74.14106685
 97.52597354 98.67204823 96.39983058 98.18870369 89.87966216 73.80222737
 73.79475297 92.89931983 97.0276802  71.26342278 75.63843835 92.37362035
 72.90031642 72.69850761 97.03764606 94.02795426 98.18621222 98.77668984
 97.95201435 92.90679423 93.76635025 98.55993223 98.87385704 72.88038468
 98.6969629  73.6128759  68.35338964 94.3767596  96.16314124 96.78102499
 90.13379176 97.04761193 95.75204923 73.90437751 98.32075143 74.62441139
 98.13139996 72.97506042 74.87604953 97.53593941 75.18000847 96.07843137
 74.9009642  95.44559882 72.87789322 86.19727434 94.18740813 74.23325111
 75.24976954 99.15040985]
Accuracy th:0.7 is [45.67356803 97.22450607 67.32192242 96.96290206 97.90716795 72.43192067
 72.56645987 72.53407081 74.09871191 96.41976231 74.13359245 98.5325261
 99.34972718 77.35256746 74.35782445 96.31262924 96.21047911 73.52816603
 98.78167277 98.34068316 77.6938984  74.77888233 98.31327703 88.55420186
 82.23584224 96.52938685 94.3393876  73.6950943  97.81747515 74.14106685
 97.52597354 98.67204823 96.39983058 98.18870369 90.09143683 73.80222737
 73.79475297 93.10611157 97.0276802  71.26342278 76.17908663 92.37362035
 72.90031642 72.69850761 97.03764606 94.02795426 98.18621222 98.77668984
 97.96198022 93.26556544 94.01549692 98.55993223 98.87385704 72.88038468
 98.6969629  73.6352991  68.35338964 94.78037721 96.16314124 96.78102499
 90.13379176 97.04761193 95.79440417 73.93925804 98.32075143 74.62441139
 98.13139996 72.97506042 74.87604953 97.53593941 75.1849914  96.07843137
 75.11523034 95.44559882 72.87789322 86.42648927 94.73303934 74.23325111
 75.24976954 99.15040985]
Avg Prec: is [54.24549664  3.76073715 14.84301873  4.60344926  1.73597848  4.3369497
  8.86970157  8.68567114  6.83882252  5.10643541  2.27519219  5.31663162
  1.56572089  5.78070902  2.94265345  3.89539519 20.16168386  5.47990342
  1.57814264  4.47284384  3.71081496  1.42203353  2.03396567  4.62284858
  5.71772204 14.28857387  8.39186618  5.03101034  3.99638855  5.85491091
  2.28227749  0.87655031  3.02248986  1.15819354  1.71148647  2.43703436
  2.06247521  2.18854647  2.20909635  6.28787253  1.79028848  6.06524383
  2.21184456  2.7492369   2.36199637  4.84590703  1.76885564  1.06465349
  1.43255902  1.22114605  1.29746719  1.01826983  0.75824583  2.31445332
  0.92622329  1.88783221 10.30200963  3.0416978   4.29770149  2.60952155
  8.03893801  2.02946681  3.31095656  2.52073373  1.315023    1.85626131
  1.53528945  3.56128292  1.05073387  2.16446137  0.18788091  3.10908846
  1.5342947   3.93981726  3.2004041   2.30415402  0.55556937  1.44850399
  0.1250329   0.59345304]
mAP score regular 51.55, mAP score EMA 4.30
Train_data_mAP: current_mAP = 59.97, highest_mAP = 59.97
Val_data_mAP: current_mAP = 51.55, highest_mAP = 51.91
tensor([7.4192e-03, 2.3170e-04, 8.1223e-03, 2.9660e-04, 1.2689e-04, 3.7040e-04,
        6.7971e-05, 2.3421e-04, 9.2780e-04, 1.9608e-04, 6.2288e-05, 2.5461e-04,
        3.1988e-05, 6.9269e-04, 7.4716e-04, 8.1211e-04, 9.8613e-04, 2.0189e-03,
        7.2661e-04, 1.1038e-03, 8.2827e-06, 8.8681e-05, 2.1755e-04, 1.3939e-04,
        8.9819e-04, 1.5939e-04, 4.6537e-03, 1.0469e-04, 4.3206e-05, 2.5130e-04,
        2.8380e-04, 5.7596e-05, 4.0686e-03, 3.3651e-05, 3.6488e-04, 8.9212e-03,
        9.4551e-05, 3.6087e-04, 1.1854e-03, 2.5585e-03, 1.2679e-01, 5.7557e-01,
        9.7512e-01, 9.3247e-01, 9.9744e-01, 9.7082e-01, 9.8658e-05, 2.4181e-04,
        4.4470e-04, 3.3897e-04, 6.0682e-05, 1.3622e-04, 2.4813e-04, 3.7561e-04,
        7.9195e-05, 5.6534e-04, 3.9440e-02, 2.0882e-03, 3.4504e-02, 4.0506e-04,
        9.4944e-01, 3.7853e-04, 6.4032e-02, 2.3244e-03, 8.7238e-03, 1.0535e-03,
        1.4209e-02, 3.8997e-03, 6.2120e-01, 4.8721e-03, 9.4157e-05, 3.0893e-04,
        3.6691e-01, 3.1673e-02, 9.9095e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
        6.5141e-01, 1.5492e-02], device='cuda:0')
